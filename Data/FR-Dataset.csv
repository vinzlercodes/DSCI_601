Class,Text,ID
Extract Method,Add RelMetadataProvider parameter to standard planner Programs The Programs class has a static method standard() used by the default Calcite driver. So far Calcite-Phoenix is using the same sql prepare logic with some slight customization like plugging in a set of its own rules. The static standard() though does not work properly for Phoenix since some sub-programs it returns are initiated with the default RelMetadataProvider while Phoenix needs to plug-in its own otherwise it just could not work.{code}  /** Program that converts filters and projects to {@link Calc}s. */  public static final Program CALC_PROGRAM =      hep(CALC_RULES true new DefaultRelMetadataProvider());  /** Program that expands sub-queries. */  public static final Program SUB_QUERY_PROGRAM =      hep(          ImmutableList.of((RelOptRule) SubQueryRemoveRule.FILTER              SubQueryRemoveRule.PROJECT              SubQueryRemoveRule.JOIN) true new DefaultRelMetadataProvider());{code},1
Extract Method,Allowing SqlOperator to be overridden in validation Calcite allows function to be overridden at validation step. To be more specific users can provide their SqlOperatorTable and at validation step their SqlOperatorTable will be called (method: lookupOperatorOverloads) to get a overriding function.  However so far SqlOperator (e.g. + -  * etc.) does not have this mechanism yet. Since other systems (e.g. Apache Drill) would have more flexible type-checks for SqlOperator's operands this mechanism is necessary for those systems to pass through the validation step.,2
Extract Method,Extend simplify for reducing expressions We would like to cover more cases in expression simplification such as:x>5 and x is not null => x>5x>5 and x is null => not satisfiablex>5 and x<=5 => not satisfiable,4
Rename Method,Add unary operator support for IS_NULL and IS_NOT_NULL to RexImplicationChecker Currently we support only SQL Comparison operators (SqlKind.COMPARE) for checking if one predicate implies another using RexImplicationChecker.We would like to extend it for couple of Unary operators based on user request ( CALCITE-1104): IS_NULL and IS_NOT_NULL,5
Extract Method,Send back ErrorResponse on failure to parse requests In trying to debug an issue on ASF jenkins I noticed that serialization errors did not result in an ErrorResponse sent back to the client (so the server-side stacktrace was lost).,6
Extract Method,"Add new rules for Materialised view optimisation of join queries This is to keep track of adding new rules that would enable optimisation using view of join queries. For instance when we have materialised view of  table 'X' named 'X_part' defined by query: "" select * from X where X.a > '2016-01-01' "" then we expect following query to be optimised by 'X_part':select * from X inner join Y on X.id = Y.xid inner join Z on Y.id=Z.yid where X.a > '2016-02-02' and Y.b = ""Bangalore"" Following are the changes done in Quark which we are planning to pull into Calcite:1. Add a new Rule for Filter on TableScan. Basically after predicate has been pushed through join onto table scan new rule checks if it can be optimised by Materialised View.https://github.com/qubole/quark/blob/master/optimizer/src/main/java/com/qubole/quark/planner/MaterializedViewFilterScanRule.java2. Add a new Unify rule to MaterialisedSubstitutionVisitor:https://github.com/qubole/incubator-calcite/commit/2d031d14d23810291377d92dc5ef2eaa515d35b7",7
Extract Method,Add a method to SqlOperatorBinding to determine whether operand is a literal When Drill is doing type inference a decimal {color:red} literal {color} can be treated as double in some cases. However SqlOperatorBinding has not had a method to tell the caller if an operand is literal or not. This JIRA proposes adding such a helper method. ,8
Extract Method,Add view name to the ViewExpander The ViewExpander.expandView() call already has the schema path that contains the view but not the view name itself.In some context it is useful to also know the name of the view being expanded.current call:{code}RelRoot expandView(        RelDataType rowType        String queryString        SchemaPlus rootSchema        List<String> schemaPath);{code}proposed:{code}RelRoot expandView(        RelDataType rowType        String queryString        SchemaPlus rootSchema        List<String> schemaPath        String viewName);{code},10
Extract Method,Lazy evaluate RexCall digests Currently RexCall compute digests eagerly in its constructor also it compute digest every time when toString invoked. It may cause performance issue when the RexCall tree is very large.,11
Extract Method,Use user-given names in RelOptUtil.createProject and createRename 0,12
Extract Method,Allow TableMacro to consume Maps and Collections Actually it is not possible to pass to TableMacro smth like{code}TABLE(MY_TABLE('simpleName' MAP[ 'key' 'value' 'anotherKey' 'anotherValue' ] )){code}SqlUserDefinedTableMacro.convertArguments leads to IllegalArgumentException: _All arguments of call to macro XXX  should be literal._,13
Extract Method,"Implement ""connectionSync"" RPC Per thread on dev list titled ""Avatica handling of connection state"" this ticket implements such an rpc.",14
Extract Method,Use a Factory in MaterializationService to create tables Follow up of this [conversation|https://mail-archives.apache.org/mod_mbox/incubator-calcite-dev/201503.mbox/%3CCANG6QPx7eLmUrkXRTY09yGw7GchMNRSSDnRCQEnd_3z%3DJCX%3Dgg%40mail.gmail.com%3E]There are two tasks:1. Use a Factory to create tables that represent a materialized view.2. Add an entry to the schema only if a materialized table was created in defineMaterialization.I'll open a WIP progress soon. 1. was easy to implement.2. is proving to be hard because rest of the code requires a CalciteSchema.TableEntry object which is available only when a table is added to schema. I dont know how to get an object for an existing entry.  ,15
Extract Method,"Commit functionality not exposed by the RPC server It seems that the commit/rollback functionality is not exposed by the RPC server which means that it's only usable in autocommit mode. Avatica itself doesn't have a concept of commit in the RPC and the remote JDBC connection raises an exception when calling commit() on it but Phoenix's native JDBC connection does implement commit() so the RPC needs to be extended to allow calling that remotely.The easiest way to test this ""!autocommit off"" and then ""!commit"" fails in ""sqline-thin.py"" but works in ""sqline.py"".",16
Extract Method,"Register all combinations of materialization substitutions When a query has multiple table references there could be:1) Multiple combinations of substituted Rels if one materialization is applicable for more than one sub-tree.2) Multiple combinations of substituted Rels if different materializations are applicable for different sub-trees respectively.{code}  @Test public void testSingleMaterializationMultiUsage() {    String q = ""select *\n""        + ""from (select * from \""emps\"" where \""empid\"" < 300)\n""        + ""join (select * from \""emps\"" where \""empid\"" < 200) using (\""empid\"")"";    try {      Prepare.THREAD_TRIM.set(true);      MaterializationService.setThreadLocal();      CalciteAssert.that()          .withMaterializations(JdbcTest.HR_MODEL               ""m0"" ""select * from \""emps\"" where \""empid\"" < 500"")                        .query(q)          .enableMaterializations(true)          .explainMatches("""" new Function<ResultSet Void>() {            public Void apply(ResultSet s) {              try {                final String actual = Util.toLinux(CalciteAssert.toString(s));                final String scan = ""EnumerableTableScan(table=[[hr m0]])"";                assertTrue(actual + "" should have had two occurrences of "" + scan                     StringUtils.countMatches(actual scan) == 2);                return null;              } catch (SQLException e) {                throw new RuntimeException(e);              }            }          })          .sameResultWithMaterializationsDisabled();    } finally {      Prepare.THREAD_TRIM.set(false);    }  }  @Test public void testMultiMaterializationMultiUsage() {    String q = ""select *\n""        + ""from (select * from \""emps\"" where \""empid\"" < 300)\n""        + ""join (select * from \""emps\"" where \""deptno\"" < 10) using (\""empid\"")"";    try {      Prepare.THREAD_TRIM.set(true);      MaterializationService.setThreadLocal();      CalciteAssert.that()          .withMaterializations(JdbcTest.HR_MODEL               ""m0"" ""select * from \""emps\"" where \""empid\"" < 500""              ""m1"" ""select * from \""emps\"" where \""deptno\"" < 20"")                        .query(q)          .enableMaterializations(true)          .explainContains(""EnumerableTableScan(table=[[hr m0]])"")          .explainContains(""EnumerableTableScan(table=[[hr m1]])"")          .sameResultWithMaterializationsDisabled();    } finally {      Prepare.THREAD_TRIM.set(false);    }  }{code}",17
Extract Method,"Pass URL connection properties to avatica server This ticket proposes the ability to pass connection-creation-time properties (the ""info"") from the remote avatica driver to the avatica server.Use cases: * Phoenix properties like CurrentSCN and TenantId (and similar use-cases for a custom driver at the company I work for) * user and password properties (enables authentication cfr. CALCITE-519 and CALCITE-643)See also discussion on mailing list:http://mail-archives.apache.org/mod_mbox/incubator-calcite-dev/201510.mbox/%3CCAAF1JdiZbRuMfo2CFRUoxPhBBf%3DyQ5T%2Bd%2B1ig1FzKnVDj8RGOA%40mail.gmail.com%3EThings to consider: * The remote driver might have its own properties (like ""url"") which should not be passed to the server. Remote driver is responsible for removing those. * The server might want to disallow setting some properties remotely e.g. to not allow to connect to a different database.Currently the avatica server creates a default connection (though it is marked for removal). When using authentication this is particularly annoying since you need to start up the server with an url containing user and password.I will attach a patch based on calcite 1.4 which adds the ability to pass properties as part of a new OpenConnectionRequest rpc call (a CreationConnetionRequest was proposed for different reasons in CALCITE-663). For the authentication reason mentioned the default connection is also dropped. There is one JdbcMeta instance on the server side but it manages a map of connections. Since connections were already created per client-side connection this is not different than before except that the connections are now created explicitly. Implicit connection creation has been dropped we can look into auto-recreation of the connection as part of recovery (CALCITE-903).The somewhat hacky things in the patch are:* addition of AvaticaConnection.setId(): this is because I let the server decide on the connection id. This seemed more logical to me at first though it can be easily reversed.* remote.Driver which creates an extra service instance to be able to do the OpenConnection request while all other requests happen from within RemoteMeta.The patch also passes error messages back to the client (a simple improvement towards CALCITE-645).Because the calls for metadata (such as getColumns) now also have a connection id fixing CALCITE-871 becomes easy and I have a patch ready for that as well.If the patch is acceptable I can rework it for master (now it is for 1.4).",18
Extract Method,Match materialized views when predicates contain strings and ranges This is in continuation of CALCITE-786 where queries are optimized using materialized views. Earlier we were not supporting predicates that were specifying ranges over a variable for e.g. x < 90 and x > 30. No we would be able to optimize even when such predicates are used either in query or to specify a view. Also adding support for strings for the same optimization.,19
Extract Method,"Make HttpServer to be configurable We met a case to increase ""requestHeaderSize"" but currently it's not possible.",20
Extract Method,Organize applicable materializations in reversed topological order With CALCITE-890 we now try applying each materialization with all possible substitution combinations. And this works fine if all applicable materializations are independent of each other. But with dependent materializations the order of materializations in which we perform substitution matters. For example if we have tables {{A}} and {{B}} a materialization table {{C}} that uses {{A}} and another materialization table {{D}} that uses tables {{C}} and {{B}}. Thus we should apply materialization {{C}} before applying materialization {{D}}.Right now we output the applicable materialization list as we iterate through the input materialization list which could be of random order (and it actually is). A directed graph is used to find all the applicable materializations but we can also take advantage of this graph to organize them in the desired order.,21
Rename Method,Support stream joins Stream joins are used to relate information from different streams or stream and relation combinations. Calcite lacks (proper) support for stream-to-relation joins and stream-to-stream joins.stream-to-relation join like below fails at the SQL validation stage.select stream orders.orderId orders.productId products.name from orders join products on orders.productId = products.idBut if 'products' is a stream the query is valid according to Calcite even though the stream-to-stream join in above query is not valid due to unbounded nature of streams.,22
Extract Method,"Provide generic server metadata in responses Some follow on from CALCITE-903:The assumption in that work was that the common case in running behind a load-balancer is that a given client would continue to be routed back to the same avatica server instance. Sadly this is not necessarily reality.If the only load balancer technology available is only capable of an round-robin algorithm (or similar) we need to provide the information for a client to make a decision to return to the same server upon subsequent requests (e.g. fetching the next page of results).Thinking more generally the server which processed a given request is just general metadata. We could include things like the Avatica version the ""real"" JDBC version information etc.",23
Extract Method,Create separate SqlFunctionCategory values for table functions and macros This avoids trying to apply a table function in a context where it does not apply or a regular function where a table function is needed.,24
Extract Method,Support absolute path without scheme in loading ,25
Extract Method,Disk hotspot found during data loading # ScenarioCurrently we have done a massive data loading. The input data is about 71GB in CSV formatï_Œand have about 88million records. When using carbondata we do not use any dictionary encoding. Our testing environment has three nodes and each of them have 11 disks as yarn executor directory. We submit the loading command through JDBCServer.The JDBCServer instance have three executors in total one on each node respectively. The loading takes about 10minutes (+-3min vary from each time).We have observed the nmon information during the loading and findï__1. lots of CPU waits in the first half of loading;2. only one single disk has many writes and almost reaches its bottleneck (Avg. 80M/s Max. 150M/s on SAS Disk)3. the other disks are quite idel# AnalyzeWhen do data loading carbondata read and sort data locally(default scope) and write the temp files to local disk. In my case there is only one executor in one node so carbondata write all the temp file to one disk(container directory or yarn local directory) thus resulting into single disk hotspot.# ModificationWe should support multiple directory for writing temp files to avoid disk hotspot.Ps: I have improved this in my environment and the result is pretty optimistic: the loading takes about 6minutes (10 minutes before improving).,26
Extract Method,Added tableProvider to supply carbonTable wherever needed ,27
Extract Method,Use CarbonTableInputFormat in Presto Integration Use CarbonTableInputFormat in Presto Integration,28
Rename Method,CarbonData unsupport Boolean data type   Spark/Hive table support Boolean data type the internal table also should support Boolean data type.   Boolean data type Range: TRUE or FALSE. Do not use quotation marks around the TRUE and FALSE literal values. You can write the literal values in uppercase lowercase or mixed case. The values queried from a table are always returned in lowercase true or false.    In implementing this function we employ:#       endcoding: RLE#       data expression: byte array            CarbonData should support boolean data type in following aspects:#       create table: support Boolean data type#       insert into table values: support insert Boolean column #       insert overwrite#       insert into table select from another table#       select from a table#       load data: from a local csv file#       filter: including >= > = <= < = != in not in#       describle: should show boolean data typeWe also add some test cases in booleantype directory of spark2,29
Extract Method,Support timestamp more than 68 years Enhance NoDictionary Datatypes - int  long Problem:Current implementation supports timestamp as direct dictionary only. As dictionary is always integer only 68 years of range is supported.Solution:So this issue will support timestamp as default DICTIONARY_EXCLUDE. Allowing to store internally as unix long timestamp.Problem:Int and Bigint(long) types are supported only as Dictionary include or measure they are not allowed in dictionary exclude.Solution:Support Int and Bigint(long) as Dictionary exclude and also Sort columns support for intlong bigint.To be resolved - CARBONDATA-1485,30
Extract Method,Presto Integration - Performance Improvement Presto Integration : Performance ImprovementImplemented Better handling of Nulls in Stream ReadersAdded Short and Timestamp Reader to resolve it properlyOptimized Filters to ensure better push down.,31
Rename Method,Add Event Listener interface to Carbondata Add Event Listener interface to Carbondata. This will allow extending the current functionality of various commands to perform various other operations.Example: After completion of load process if any aggregate tables are created on that table then data load operation need to be done for the aggregate table also. In this case we can create a listener such as AggregateLoadListener and register it as an event bus. Then this listener can be called once the load operation is completed which will take care of loading the aggregate table.,32
Extract Method,Merging carbonindex files for each segment. HiProblem : The first-time query of carbon becomes very slow. It is because of reading many small carbonindex files and cache to the driver at the first time.  Many carbonindex files are created in below case Loading data in large cluster   For example if the cluster size is 100 nodes then for each load 100 index files are created per segment. So after 100 loads the number of carbonindex files becomes 10000. .It will be slower to read all the files from the driver since a lot of namenode calls and IO operations.Solution :Merge the carbonindex files in two levels.so that we can reduce the IO calls to namenode and improves the read performance.Merge within a segment.Merge the carbonindex files to single file immediately after load completes within the segment. It would be named as a .carbonindexmerge file. It is actually not a true data merging but a simple file merge. So that the current structure of carbonindex files does not change. While reading we just read one file instead of many carbonindex files within the segment.,33
Inline Method,Rename aggType to measureType There are many 'aggType' in the code but its meaning is not clear,34
Extract Method,Make ArrayType and StructType contain child DataType ,35
Rename Method,Clean up store path interface There are many getStorePath API it should be unified in one place,36
Extract Method,Count Star optimization Since carbon records number of row in metadata count star query can leverage it to improve performance,37
Extract Method,Create configuration from SparkSession for data loading Create configuration form SparkSession for data loading,38
Extract Method,Make FileOperations Pluggable 1. Refactor FileFactory based on FileType to support plug-gable file handlers so that custom file handlers can have their specific logic.Example : User can provide his own implementations by extending existing FileTypes,39
Extract Method,Should extract CarbonTable.buildUniqueName method and re-factory code to invoke this method Should  extract CarbonTable.buildUniqueName method and re-factory code to invoke this method,40
Extract Method,Reusing old row to reduce memory consumption In data converting process of data loading Carbondata will convert each row to another row by batch. Currently it will create a new batch to store the converted rows which I think can be optimized to reuse the old row batch's space thus will reduce memory consumption and GC overhead.,41
Rename Method,Refactor SortStepRowUtil to make it more readable Refactor and optimize `SortRowStepUtil` to make it efficient and more readable.,42
Extract Method,Add support for task/segment level pruning Add support for task/segment level pruning. Add code to compute task level min/max which can be helpful for task/segment level pruning,43
Extract Method,"Add dictionary path support to carbondata Add dictionary path support. Carbondata dictionary framework need to be further enhanced to support reading dictionary files from a given dictionary file location.Support new table property TABLE_PROPERTIES(''dictionary_path'""/XXX""). This will allow providing external dictionary path to a table.Ex: pre-aggregate tables can share dictionary path with main table so that dictionary need not be created again. ",44
Extract Method,Add Transaction support for pre-aggregation table load Currently the load process is like this:1. load main table 2. load preagg1 and write table status3. load preagg2 and write table status4. write table status for maintableImproved process:1. load main table2. load preagg13. load preagg24. write table status for preagg25. write table status for preagg16. write table status for maintable,45
Extract Method,Enhancement of merge index compaction feature to support creation of merge index file on old store where index file does not contain the blocklet info Enhancement of merge index compaction feature to support creation of merge index file on old store where index file does not contain the blocklet info.Old store created with carbondata 1.1 version does not contain the blocklet info in the index file. On that store if merge index file is created then blocklet information will not be present in the merge index file and for first time query again carbondata file footer will be read for blocklet information retrieval.Benefits:1. Support merge index file creation on old store2. Improve first time query performance.Note: First time query performance will be improved only if merge index file is created before running the first query,46
Rename Method,Optimization in data loading for skewed data In one of my cases carbondata has to load skewed data files. The size of data file ranges from 1KB to about 5GB.In current implementation carbondata will distribute the file blocks(splits) among the nodes to maximum the data locality and data evenly distributed we call it `block-node-assignment` for short.However the current implementation has some problems.The assignment is block number based. The goal is to make sure that all the nodes deal the same amount number of blocks. In the skewed data scenario described above the block of a small file and the block of a big file are very different from its size (1KB v.s. 64MB). As a result the difference of total data size assigned for each data node is very large.In order to solve this problem the size of block should be considered during the block-node-assignment. One node can deal more blocks than another as long as the total size of blocks are almost the same.,47
Inline Method,Add compaction listener ,54
Extract Method,Refactored code segregated process meta and process data in load command h1. Refactored code segregated process meta and process data in load and Compaction command class,55
Extract Method,during stream sometime carbontable is null in executor side ,59
Extract Method,"Access tablestatus file too many times during query * ProblemsCurrently in carbondata a single query will access tablestatus file 7 times which will definitely slow down the query performance especially when this file is in remote cluster since reading this file is purely client side operation.Â  * Â Steps to reproduce1. Add logger in `AtomicFileOperationsImpl.openForRead` and printout the file name to read.2. Run a query on carbondata table. Here I ran `TestLoadDataGeneral.test(""test data loading CSV file without extension name"")`.3. Observe the output log and search the keyword 'tablestatus'.Â Â Â ",61
Extract Method,Add a path into table path to store lock files and delete useless segment lock files before loading After [PR1984|https://github.com/apache/carbondata/pull/1984] merged it doesn't delete the lock files when unlock there are many useless lock files in table path especially segment lock files they grow after every batch loading.Solution :1. add a child path into table path called Locks all lock files will be stored in this path;2. Before loading get all useless segment lock files and delete them because just segment lock files will grow other lock files dosen't grow.,63
Rename Method,Collect SQL execution information to driver side ,65
Extract Method,Enhance compaction performance by enabling prefetch During compaction carbondata will query on the segments and retrieve a rowï_Œ then it will sort the rows and produce the final carbondata file.Currently we find the poor performance in retrieving the rows so adding prefetch for the rows will surely improve the compaction performance.In my local tests compacting 4Â segments each with 100 thousand rows costs 30s with prefetch and 50s without prefetch.In my tests in a larger cluster compacting 6 segments each with 18GB raw data costs 45min with prefetch and 57min without prefetch.,66
Extract Method,Page level uncompress and Query performance improvement for Unsafe No Dictionary *Page Level Decoder for query*Add page level on demand decoding in current code all pages of blocklet is getting uncompressed because of this memory footprint is too high and causing OOM Now added code to support page level decoding one page will be decoding when all the records are processed next page data will be decoded. It will improve query performance for example limit query.*Unsafe No Dictionary(Unsafe variable length)*Optimized getRow(for Vector processing) And putArray method,67
Extract Method,Add cache for DataMap schema provider to avoid IO for each read Add cache for DataMap schema provider to avoid IO for each read.Â ,68
Rename Method,Add Profiler output in EXPLAIN command More information should give in EXPLAIN command to show the effeteness of datamap,69
Extract Method,Add CG prune before FG prune CG prune before FG prune and passes the pruned segments and indexfiles to FG DataMap for further pruning.,70
Extract Method,Improve Lucene datamap performance by eliminating blockid while writing and reading index. Currently DataMap interface implementations use blockid and blockletid while writing index files Actually blockid is not needed to store in index files as it only requires blockletid. So it adds more memory and disk size to write index files.,71
Rename Method,Support visible/invisible datamap for performance tuning Invisible datamap will not be used during the query which can be used to verify whether to remove this datamap in the future.Â This feature is similar to `Invisible indexed` in mysql (https://dev.mysql.com/doc/refman/8.0/en/invisible-indexes.html).,72
Rename Method,Search mode support lucene datamap Carbon doesn's support now{code:java}18/04/23 06:12:14 ERROR CarbonSession: Exception when executing search mode: Error while resolving filter expression fallback to SparkSQL18/04/23 06:12:14 ERROR CarbonSession: Exception when executing search mode: Error while resolving filter expression fallback to SparkSQL{code}Carbon should support it.,73
Inline Method,Remove unnecessary TableProvider interface ,75
Extract Method,Support cache for bloom datamap Currently query using bloom filter datamap is slow. The root cause is that loading bloom filter index costs too much time. We can implement a driver side cache to accelerate the loading index procedure.,76
Extract Method,Improve Lucene datamap size and performnace. Improved lucene datamap size and performance by using the following parameters.New DM properties `flush_cache`: size of the cache to maintain in Lucene writer if specified then it tries to aggregate the unique data till the cache limit and flush to Lucene. It is best suitable for low cardinality dimensions.`split_blocklet`: when made as true then store the data in blocklet wise in lucene  it means new folder will be created for each blocklet thus it eliminates storing on blockletid in lucene. And also it makes lucene small chunks of data.,77
Extract Method,Chnage the bloom implementation to hadoop for better performance and compression The current implementation of bloom does not give better performance and compression And also it adds new guava dependency to carbon. So remove the guava dependency and add hadoopÂ bloom.,78
Rename Method,The order is different between write and read data type of schema in SDK The order is different between write and read data type of schema in SDK,79
Rename Method,Support StreamSQL for streaming job Currently carbon supports creating streaming job via Spark Streaming API this requires user to use spark-submit to create the streaming job. To make it easier for SQL users carbon should support StreamSQL to manage the streaming job.,81
Rename Method,"Introduce configurable Lock Path Currently table level lock files are prepared in table path and system level locks are created inside store.When supporting S3 locking cannot be done is using S3 files as S3 support onlyÂ  eventual consistency.So user can chose different HDFS location to configure locks so that data can be on S3 and locks can be on HDFS.User also chose Zookeeper LockÂ if does not have HDFS location.SoÂ require to support a configuration pathÂ ""carbon.lock.path""Â ",83
Rename Method,Support Avro datatype conversion to Carbon Format 1.Support Avro Complex Types: Enum Union Fixed with Carbon.2.Support Avro Logical Types: TimeMillis TimeMicros Decimal with Carbon.Â Please find the design document in the below link:https://docs.google.com/document/d/1Jne8vNZ3OSYmJ_72hTIk_5I4EeIVtxGNE5mN_hBlnVE/edit?usp=sharing,85
Extract Method,Should rename the methods of ByteUtil class to avoid the misuse the method toBytes will execute XOR operation on data.So the result is not the byte array of theÂ real value.Better to rename the methods of ByteUtil class to avoid the misuse,86
Rename Method,Adaptive encoding support for timestamp no dictionary and Refactor ColumnPageWrapper ,88
Move Method,Support Float and Byte Datatypes for SDK and DataSource CurrentlyÂ floatÂ is supported by internally storing the data as double and changing the data type to Double. This poses some problems while using SparkCarbonFileFormat for reading theÂ floatÂ type data.Internally as the data type is changed fromÂ FloatÂ to Double therefore the data is retrieved as a Double page instead ofÂ float.Â If the user tried to create a table using file format by specifying the datatype asÂ floatÂ for any column then the query will fail. User isÂ *restricted to use double to retrieve the data.*,89
Extract Method,Support dumping column chunk meta in CarbonCli ,90
Rename Method,Change bloom query model to proceed multiple filter values ,91
Rename Method,Data mismatch after compaction with measure sort columns problem: Data mismatch after compaction with measure sort columnsroot cause : In compaction flow (DictionaryBasedResultCollector) in ColumnPageWrapper inverted index mapping is not handled. Because of this row of no dictionary dimension columns gets data form other rows.Hence the data mismatchÂ solution: Handle inverted index mapping forÂ  DictionaryBasedResultCollector flow in ColumnPageWrapperÂ ,92
Inline Method,Refactor ColumnPageWrapper ,93
Extract Method,Added Performance statistics for query execution in driver and executor Added Performance statistics for query execution in driver and executor for each phase of query execution ,94
Rename Method,make zookeeper lock as default if zookeeper url is configured. make the lock type as zookeeper if zookeeper URL is present in the spark conf.if spark.deploy.zookeeper.url property is set in spark-default.conf then need to take the zookeeper locking.,95
Rename Method,"Entity Tab switching on new entity creation When a new Db|Obj Entity is created entity editor should switch to the ""Entity"" tab with ""Name"" field acquiring focus. This thing while minor causes me lots of irritation on new projects when lots of entities are created at once",96
Extract Method,Add confirmation dialog for delete actions. Apparently I'm foolish enough to constantly confuse the entity level and attribute/relationship level delete buttons.  As a result I constantly delete the wrong thing.  With no undo support this means closing the modeler reopening and redoing any work I hadn't save beforehand.  While I wholly acknowledge that a pop-up confirmation dialog may very well get ignored (a big chunk of my thesis revolved around this) adding that extra step may at least help prevent deletions due to inadvertent clicks.  If it became particularly annoying we could add a switch to globally disable the confirmations.,97
Rename Method,"Do something about to-many prefetch limitations http://cayenne.apache.org/doc/prefetching.html""PREFETCH LIMITATION: To-many relationships should not be prefetched if a query qualifier can potentially reduce a number of related objects resulting in incorrect relationship list.""This can bite an unsuspecting user... So we either 1. Address the core limitation by building a correct prefetch query (use subselect?)2. Detect these cases and throw an exception3. Detect these cases and silently drop a prefetchI should note that debugging these problems is very hard as they look totally random until you get to the cause.",100
Extract Method,Make PK metadata available via Obj* API. There are cases where it would be nice to have PK metadata available on the ROP client.  Since this data is currently only available via the Db* API this is an issue as the ROP client cannot access Db* items.  By making this data available via the Obj* API via a limited ObjAttribute the needed data could be handled on both server and client.  Likewise this could cleanup existing code that does Db* wrangling to get access to the metadata.Please see the following thread for more details:http://markmail.org/message/yvhyhlmi7kiasamg,103
Rename Method,Aligning query capabilities 1. EJBQLQuery should support DataRows pagination cache groups just like SelectQuery does.2. ProcedureQuery should support SQLResultSetMapping and 'setColumnNamesCapitalization' just like SQLTemplate,104
Rename Method,Add support for start index/offset of queries Cayenne already allows us to programatically set a query fetch limit.It would be nice if we could also specify a fetch index/fetch offset into a query to make it simple to fetch eg the 100th through the 150th results from the database.,105
Extract Method,"ObjRelationship Mapping Dialog Improvements [This is a GSoC 2008 task]The biggest complaint about the ObjRelationship mapping dialog is that it is often unclear how it operates especially to the new users. I.e. empty list of DbRelationships is displayed as a white area not giving any hints on what needs to be done to map a relationship. So that's confusing. Same thing when you add 1 path component there is no hint that you can chain more path components for the flattened relationship.At the minimum we may just add some hint text (""Select next DbRelationship"" in grey over the next available dropdown) but ideally we should implement a path browser similar to how the SelectQuery prefetch and ordering browsers operate (and similar to how OS X Finder does).",106
Extract Method,CayenneContext should support ThreadLocal operations CayenneContext lacks the ThreadLocal utility methods that DataContext has.  The plan is to add a BaseContext class that both derive from and make the functionality common to all subclasses.,107
Rename Method,Nested contexts on ROP Nested contexts should be avaliable via CayenneContext. This also leads to moving up some methods from DataContext to BaseContext or even ObjectContext,108
Rename Method,Generated DataMap classes should contain public constants for all query names The new DataMap classes are great for accessing named queries in a type-safe manner.  An additional improvement would be to add the public static final strings for every named query (not just object select queries) similar to how DataObjects are generated with constants for references property names of the entity.,110
Extract Method,"Default class generation folder is wrong Default class generation folder in the modeler (Tools > Generate Classes > Output directory) is selected to be the folder where cayenne.xml is stored. If a user changes it it is not saved in user preferences so on the next attempt to generate classes the old folder is shown.Another related improvement: In a project that has Maven folder layout cayenne.xml is often is src/main/resources. In this case we must select a default folder ot be src/mainjava (for ""src/test/resources"" it should be src/test/java). I.e. we should be smarter when we are dealing with predictable maven structures.",111
Move Method,Undo/Redo support in modeler Modeler should support Undo/Redo. History should include all add/remove/edit/sync operations and be reset after 'Major' operations like Reverse-Engineer.,112
Extract Method,Cayenne should support enum types in qualifier statements/expression handling Currently cayenne doesn't support java enums in its expression syntax and by extension in qualifier statements.  This makes it impossible to use an enum type as a class descriminator for inheritance.Enum expressions would look like:propertyName = some.package.Type.CONSTANTto make it consistent with JPA.,117
Extract Method,Implement qualifiers for DBEntities This is much like ObjEntities qualifier only applied on DBEntities level. This for instance means that it will be inserted in needed JOINs of any select thus allowing to use restricting qualifiers in e.g. middle table of many-to-many flattened relationship.This issue includes core changes and modeler support,119
Extract Method,"Allow providing custom INSERT UPDATE DELETE query builders This feature will allow user to incercept Cayenne delete insert update behavior and do something else that simple SQL-INSERTs UPDATEs DELETEs. For instance in many cases UPDATE which sets some 'deleted' field to true is more useful than simple ""DELETE FROM...""This issue includes core changes and modeler support",120
Rename Method,Add method to ExpressionFactory to match against the primary key of an object or list of objects It would be helpful to have a method in ExpressionFactory that build an expression using matchDbExp() to match an object's primary keys.This can be helpful in excluding a single or list of known objects from the result of a query.,121
Extract Method,Memorize user-selected column widths in preferences Modeler is using a few Swing tables most notably for attributes and relationships. The width of the columns in those tables are hardcoded and any user adjustments are reset back to defaults once selected entity is changed. We need to persist and restore the user preferences  per table type (DbAttribute DbRelationship ObjAttribute ObjRelationship). This will be based on the new preferences mechanism developed per CAY-1327,122
Move Method,Replace DefaultType with dedicated types We can shove off some milliseconds from large result set processing loops by replacing DefaultType which uses reflection on the ResultSet (ugh that was a bright idea) with dedicated per type extended types. This won't improve things dramatically however the change is straightforward and is definitely worth doing.,123
Rename Method,Use #result as optional directive for only few columns (not all) Here is few queries to show the problem:SELECT ARTIST_ID ARTIST_NAME FROM ARTIST  - working properlySELECT #result('ARTIST_ID' 'java.lang.Integer') #result('ARTIST_NAME' 'java.lang.String') FROM ARTIST - also working properlySELECT ARTIST_ID  #result('ARTIST_NAME' 'java.lang.String') FROM ARTIST- first column is returned as null!!! Not nice...,124
Extract Method,"Update Ordering to take enums instead of boolean flags. The Ordering class currently takes booleans to indicate ascending/descending and case sensitive/insensitive.  When reading the code it is impossible to know what ""true"" and ""false"" actually mean unless you are familiar with the API.  Update the class to use descriptive enums instead of boolean flags.",125
Rename Method,Add deleteObjects() to ObjectContext DataContext has a deleteObjects() method but ObjectContext doesn't.The documentation shows using deleteObjects:http://cayenne.apache.org/doc/deleting-objects.htmlThis is more of a hassle when using cayenneObject.getObjectContext().delete... instead of cayenneObject.getDataContext() because getDataContext() is deprecated in 3.0.,127
Rename Method,Implement memorized sorting of modeler columns Per CAY-1251 columns in various tables in the Modeler can now be resized and reordered with all user selections saved in local preferences. Here is another related improvement - allow users to click on the column headers and sort the items in the table. First click sorts ascending second - descending. Only one column can be used for sorting at a time. Sort selection is memorized in prefrences for each table type.Note that some tables are not sortable as the displayed ordering is significant in Cayenne. Namely all callback / listener methods and stored procedure parameters. Those should be left unchanged.,128
Inline Method,"Method ""readNestedProperty"" Should Resolve Through Iterative Invocations onto DataObject and Not Complete Within Cayenne.readNestedProperty (comes from email discussion with Andrus)Another thing which may help me implement this for my own use is if the ""readNestedProperty(..)"" method were to iteratively be applied to DataObjects rather than to eventually hit Cayenne.readNestedProperty(..) and iterate in there.  The entire nested property is currently parsed at the start but if it iterated through the ""readNestedProperty(..)"" methods of the data objects then just the first part of the nested property could be 'parsed off'.  The number of strings created from parsing the nested property would only be 2x the number compared to the current implementation.---I have taken a quick look at this but there is not so much impetus to implement this right now.  I will try to come back to it a bit later and submit a patch for it.---",131
Extract Method,"Deprecate DataContext.objectFromDataRow methods with unused ""refresh"" parameters. There are DataContext.objectFromDataRow methods that have a ""refresh"" parameter which isn't used.  Deprecate these methods and provide replacements and make internal code use the replacement versions.",133
Inline Method,QueryLogger to DI JdbcEventLogger migration Migration from deprecated QueryLogger to DI enabled JdbcEventLogger.,134
Extract Method,ObjectContext API to use varargs It is quite annoying to wrap objects in collections to invalidate objects... This is easily solved with varargs. For symmetry we can do the same with with deleteObjects. So we'll end up with:void invalidateObjects(Collection<?> objects);void invalidateObjects(Object... objects);void deleteObjects(Collection<?> objects);void deleteObjects(Object... objects);@Deprecated // redundantdeleteObject(Object object);,137
Extract Method,[PATCH] DataContextFactory is cumbersome to customize DataContextFactory is cumbersome to customize.  I want to use my own custom subclass of DataContext (which is probably the only customization ever needed right?) and in order to do so I have to copy the complete source of DataContextFactory and replace DataContext with MyContext.  It can be made much easier by refactoring the constructor call into an overridable method that does just that.  CayenneContextFactory has the same problem.,139
Extract Method,Get rid of shared locks in DataDomain metadata lookups We can improve performance by using concurrent collections instead of synchronizing the HashMap. This includes: DataDomain.properties DataDomain.nodesDataDomain.nodesByDataMapNameAlso instead of reindexing on failed lookups we can simply lazy-fill collection from a given DataMap,141
Extract Method,Injectable PkGenerator Sometimes it is useful to define PkGenerator independently from DbAdapter. Would be cool to have an ability to inject it (and maybe even decouple from DbAdapter). Of course need to preserve the defaults for each adapter if no override exists. ,142
Extract Method,"[PATCH] rename DataDomain.getNode to getDataNode ""DataDomain.getNode(String nodeName)"" should be called getDataNode instead for better consistency and discoverability.",143
Inline Method,"Split long DISJOINT_BY_ID prefetch query on several smaller queries It is improvement for CAY-1681. From Andrus' comment:And one more thing we will probably have to implement - breaking down OR query if it gets too long. This is a real problem which has been repeatedly mentioned in the context of the paginated queries and in fact solved in IncrementalFaultList. see IncrementalFaultList.resolveInterval - it checks the number of clauses in the qualifier against 'maxFetchSize'. We may need to make ""maxFetchSize"" a container property used by IncrementalFaultList as well as our prefetch strategy and take it into account in the later.",144
Extract Method,cdbimport improvements So I finally started using a DB-first (and hopefully ORM-modeling free) approach on a project. In other words - a maven profile that executes cdbimport and cgen to refresh the XML and Java classes from the current DB state. 3.1 version of 'cdbimport' is rather basic and we need to extend it significantly to produce reliable and complete results. This is a cover task for all improvements. Will open subtasks for individual things.,146
Extract Method,"Real support for DbEntity catalogs While DbEntity has a 'catalog' property we sort of ignore its presence don't include catalogs in generated SQL and advise users to enter it in the ""schema"" field. However there is a practical aspect of catalogs vs. schemas that became obvious when I started using 'cdbimport' with MySQL (see CAY-1759). MySQL only has a single notion of ""database"" as a table namespace unit within a given server instance. This maps to ""catalog"" in the MySQL JDBC driver (see [1] and [2] for explanation of schemas vs catalogs across DBs). While using schema in place of catalog was fine with manual mapping it became a problem with reverse-engineered DataMaps that left the schema empty. So all the generated SQL based on such DataMaps would use bare table names and if the catalog is not a part of JDBC URL a namespace conflict would occur.So we need to :1. honor catalog setting in generated SQL2. add ""default catalog"" DataMap and ""catalog"" to DbEntity fields in the Modeler[1] http://forums.mysql.com/read.php?39137564137629#msg-137629[2] http://stackoverflow.com/questions/7942520/relationship-between-catalog-schema-user-and-database-instance",152
Rename Method,Flatten object entities for many to many relationships on reverse engineering Need to remove temporary object entities for many to many relationship on reverse engineering. And make flattened relationships direct to both tables that take part in relation.,155
Extract Method,SelectQuery<T> for DataRows Consider this example:        SelectQuery<Artist> query = new SelectQuery<Artist>(Artist.class);        query.setFetchingDataRows(true);        List<Artist> objects = context.select(query);The query actually returns List<DataRow> but it appears that it returns List<Artist>... Looks like we need something like this here:        SelectQuery<DataRow> drQuery = SelectQuery.dataRowQuery(Artist.class);,156
Rename Method,Lock-free EntityResolver Need to improve EntityResolver to make it as lock-free as possible. There's no reason (other than its reuse in the Modeler) for keeping it synchronized. After the startup it is essentially read-only.,157
Rename Method,"Optimize Expression conversion to String and EJBQL Expression.toString() is pretty heavy: @Override    public String toString() {        StringWriter buffer = new StringWriter();        PrintWriter pw = new PrintWriter(buffer);        encodeAsString(pw);        pw.close();        buffer.flush();        return buffer.toString();    }We didn't bother much about it as it wasn't supposed to be called in runtime... Well it is sometimes:SelectTranslatorjava:433 String labelPrefix = pathExp.toString().substring(""db:"".length());And I am seeing this line occasionally in my app profiling reports. So we need to override ""toString"" at least for ASTObjPath and ASTDbPath with a lighter implementation",159
Rename Method,Make ResultIterator implement Iterable<T> create ObjectContext.iterate method 1. Make ResultIterator implement Iterable<T> to simplify its use in loops.2. Create ObjectContext.iterator method that is available both in ROP and server stacks. CayenneContext can simply do something stupid like iterating over a regular list. 3. Create a callback flavor - ObjectContextiterate(Select ResultIteratorCallback)4. Move ResultIterator to org.apache.cayenne - it should be available to all layers.5. Stop throwing CayenneException from all methods.,161
Extract Method,Make ConverterFactory extensible Make org.apache.cayenne.reflect.ConverterFactory extensible and injected via DI.In my app I'm looking to add support for Joda time classes and need to be able to extend this.,164
Extract Method,MySQL - allow specifying a length for TIMESTAMP and TIME columns For MySQL allow specifying a length for TIMESTAMP and TIME columns since MySQL 5.6.4 and up support it.  It determines the number of decimal places to use for fractional seconds.See http://dev.mysql.com/doc/refman/5.6/en/fractional-seconds.html,168
Extract Method,Porting to OSGi environment Cayenne framework doesn't run under OSGi environments as class-loading problems arise in the dynamic loading of autogenerated classes by Cayenne Modeler.,169
Extract Method,Allow DataNode name to be used as a root of SQLTemplate.  We should be able to route data-row SQLTemplate by DataNode name. If DataNode name is missing we should attempt to route to the default node... Current requirement to specify a DataMap root for adhoc queries is annoying.,170
Extract Method,Implement resolving Db paths for DataObjects Expressions using db paths don't support in-memory evaluation against DataObjects (there is a TODO in the code).,171
Extract Method,"DI: add support for decorators Decorators are a cheap alternative to AOP in DI. They have an advantage of being plain Java code (have you ever seen typical AOP interceptors ... insane unreadable mess) and having good performance (reading a ResultSet using a proxy-based strategy is going to be painful). So adding this simple construct that can be used in DI modules:binder.bind(I1.class).to(C1.class);       binder.decorate(I1.class).after(D1.class).before(D2.class);Each decorator declares access to a decorated ""delegate"" using normal @Inject annotation either in constructor or via a field. As mentioned before a decorator is simply a class implementing a given interface. Very clean.",173
Extract Method,"Improved Handling for Scalar Parameters Converting Expressions to EJBQL The ""toEJBQL()"" method on the Expression object works well for some cases but fails where parameter types are employed where there is no EJBQL literal that can be used to serialize the object as a string in the resultant EJBQL string.An obvious case of this is the use of a Date object.  Take the example of a comparison between an object path and a date parameter.  The current code is outputting _something_ like;{noformat}(a.something.createTimestamp > 25 Mar 2014 12:23:34){noformat}This ""toString"" on the Date object does not generate valid EJBQL and as far as I am able to ascertain there is no timestamp literal in EJBQL.  For this reason the ""toEJBQL()"" method on Expression is able to generate broken EJBQL strings.My solution to this will be to keep the existing ""toEJBQL()"" method but have that method fail with a runtime exception if it encounters a situation in which it is not able to serialize to EJBQL correctly.  Another method ""toEJBQL(List<Object> parameterAccumulator)"" will be provided.  This new method will populate the parameter accumulator each time it encounters a parameter that it is not able to express as a literal and instead it will use a positional parameter in the EJBQL string.  The caller can then use the 'captured' parameters to feed back into the EJBQLQuery object.",174
Rename Method,SQLSelect cleanup and omissions Will clean up and refactor SQLSelect API:Shorter style of chainable API :1. rename 'useLocalCache' to 'localCache' 2. rename 'sharedCache' to 'sharedCache'Misc:3. useLocalCache is void. Must return SQLSelect<T>4. 'cacheStrategy' should take cache groups vararg5. 'cacheGroups' should have a collection variant,175
Rename Method,"Variants of Property.like(..) : contains(..) startsWith(..) endsWith(..) http://markmail.org/message/cx7dzla46lcykkzqAn idea that I had while analyzing boilerplate code of the client Cayenne apps. An argument to Property.like(..) (or second argument to ExpressionFactory.likeExp(..)) requires a full pattern to match against. So people would often write their own utility code to wrap a String in ""%"" signs. Cayenne can easily take care of this via a few extra methods. In addition these new methods can do proper symbol escaping making ""like"" much safer to use. Property.contains(string); // same as Property.like(""%"" + string + ""%"");Property.icontains(string); // case insensitive versionProperty.startsWith(string); // same as Property.like(string + ""%"");Property.istartsWith(string); // case insensitive versionProperty.endsWith(string); // same as Property.like(""%"" + string);Property.iendsWith(string); // case insensitive version",176
Rename Method,Add support for iterators to Select ,180
Move Method,Saving a display state of Project ,181
Extract Method,"In-memory evaluation of DB expressions - non-id attributes This is a minor improvement to Cayenne DB expressions evaluation capabilities. When evaluating a ""db:"" expression against a Persistent object something like ""db:id"" currently returns a value of a PK column but if a DB column name is not a part of the ID (e.g. ""db:name"") it evaluates to NULL. There is a TODO in ASTDbPath to that extent. So let's support evaluation of path's corresponding to DbAttributes. ",182
Extract Method,cdbimport: detect when same FK constraint is defined twice I have a table that defines two constraints over the same exact FK/PK pair (this was by mistake and I have cleaned it up since but never the less MySQL treats this as a valid definition) :CREATE TABLE `origin` (  `ID` int(11) unsigned NOT NULL AUTO_INCREMENT  `A2_TEAM_ID` int(11) unsigned DEFAULT NULL  PRIMARY KEY (`ID`)  UNIQUE KEY `NAME` (`NAME`)  KEY `A2_TEAM_ID` (`A2_TEAM_ID`)  CONSTRAINT `origin_ibfk_1` FOREIGN KEY (`A2_TEAM_ID`) REFERENCES `a2_team` (`ID`)  CONSTRAINT `FK_ORIGIN_TO_TEAM` FOREIGN KEY (`A2_TEAM_ID`) REFERENCES `a2_team` (`ID`)) ENGINE=InnoDB AUTO_INCREMENT=175 DEFAULT CHARSET=utf8When doing cdbimport from Maven I got the following exception in the logs:[ERROR] Migration Error. Can't apply changes from token: Add Relationship (a2Team origin->a2_team.{ID ID})java.lang.IllegalArgumentException: An attempt to override relationship 'a2Team'at org.apache.cayenne.map.Entity.addRelationship(Entity.java:193)at org.apache.cayenne.merge.AddRelationshipToModel.execute(AddRelationshipToModel.java:43)at org.apache.cayenne.tools.dbimport.DbImportAction.execute(DbImportAction.java:218)at org.apache.cayenne.tools.dbimport.DbImportAction.execute(DbImportAction.java:118)at org.apache.cayenne.tools.DbImporterMojo.execute(DbImporterMojo.java:257)at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:106)at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)[INFO] Migration Complete.[WARNING] Migration finished. The following problem(s) were ignored.[WARNING] Validation failure for java.lang.IllegalArgumentException: Migration Error. Can't apply changes from token: Add Relationship (a2Team origin->a2_team.{ID ID})So the exception was caught and the build ultimately succeeded but the stack trace in the default Maven log made me think there was something wrong with the build. Perhaps we reduce the log level for the stack trace to DEBUG. The WARNING after it should be enough.So perhaps we don't log the stack trace in this situation ,183
Extract Method,"ServerRuntimeBuilder: use DataDomain name for the default DataNode When a DataNode is created implicitly by ServerRuntimeBuilder it is assigned a hardcoded name of ""cayenne"". Turned out this can be a problem sometimes. E.g. in LinkMove (a data copy framework at https://github.com/nhl/link-move) a separate Cayenne stack is used for each RDBMS source and then one extra stack is used for the data target. If we are using default configs for both source and target source stack binds its Transaction to the processing thread and then source connections leak into target operations because the corresponding DataNode names match. There are probably other ways to fix it (LinkMove should split producer and consumer into separate threads) but still a good idea to not hardcode the DataNode name.",185
Extract Method,"MappedSelect and MappedExec fluent query API We need a fluent query API to replace existing NamedQuery for calling queries mapped in the data map.MappedSelect query is designed to be used for mapped selecting queries:{code:java}  List<Artist> artists = MappedSelect.query(""ArtistSelectQuery"" Artist.class).param(""name"" ""artist1"").select(context);{code}MappedExec query is designed to be used for non selecting mapped queries:{code:java}  int[] updated = MappedExec.query(""ArtistUpdateQuery"").param(""name"" ""artist2"").update(context);{code}",188
Extract Method,Add supporting generated keys for PostgreSQL ,189
Extract Method,"cayenne-crypto: Lazy initialization of crypto subsystem I have a deployment scenario where secret keys are not available on app startup and users ""unlock"" the KeySource at a later time. Until then I'd still like my Cayenne stack to be operational for entities that do not require encryption. Currently this is not possible as DefaultValueTransformerFactory and DefaultBytesTransformerFactory are initialized eagerly and that requires a working KeySource. A possible solution to that is lazy initialization of DefaultValueTransformerFactory and DefaultBytesTransformerFactory which can be achieved via decoration in DI that injects decorated instance as a Provider.",190
Extract Method,DbLoader - allow loading DataMap without Obj layer When cdbimport loads a map to merge to an existing map it makes no sense to load obj later. Refactor DbLoader API to support such optimization.,191
Rename Method,ObjectNameGenerator refactoring - unifying relationship name generation Will need to refactor ObjectNameGenerator API mainly focusing on relationship name generation. The new API will generate both obj and db relationship names in a single method. We will no longer distinguish between the two as DbRelationship name does not correspond to any actual name found in the database and we might as well use the object layer name. The main visible change in the algorithm is that ObjRelationship name will be based on DbRelationship semantics not on DbRelationship name.In the future if this proves to be too limiting we might split a separate DbRelationship name generator. For now it will allow to avoid passing ExportedKey object to the strategy and make it private.As a part of this task we will remove LegacyObjectNameGenerator that uses different assumptions.,195
Extract Method,"Explicit ""contribution"" API for easier expansion of DI collections and maps Currently extending Cayenne via contribution to DI collections and maps is not easy as locating the corresponding map/collection is not transparent. It requires the knowledge of a String key for a given collection and doesn't tell the user the type of the objects in the collection. E.g.:{noformat}binder                .bindList(Constants.SERVER_DEFAULT_TYPES_LIST)                .add(new LocalDateType())                .add(new LocalTimeType())                .add(new LocalDateTimeType());{noformat}Let's wrap this in a static contribution API similar to what was developed in bootique.io. E.g.:{noformat}BQCoreModule.contributeExtendedTypes(binder).add(..).add(..){noformat}This way the users will have explicit API to access all module collections / maps and will know the type of objects they expect.",196
Extract Method,Extensible CacheInvalidationFilter logic We can improve CacheInvalidationFilter as it is done in Bootique to make it more flexible.See: https://github.com/bootique/bootique-cayenne/blob/master/bootique-cayenne-jcache/src/main/java/io/bootique/cayenne/jcache/invalidation/InvalidationFilter.java,202
Extract Method,"ObjectSelect improvement: columns as full entities *Changes in API*:# Add new Expression {{ASTFullObject}} that will be just a marker for the desired logic.  This expression can be later (_in post 4.0 versions_) used in {{where()}} and in {{orderBy()}} methods to act as ObjectId and  thus fill another gap where hacks like {{""db:OBJECT_ID""}} are used now.# Add new methods in Property:{code}    static <T extends Persistent> Property<T> createSelf(Class<? super T> type);    <T extends Persistent> Property<T> flat(Class<? super T> tClass){code}# Prohibit direct usage of Properties mapped on toMany relationships so that the following code will throw a {{CayenneRuntimeException}}{code}List<Object[]> result = ObjectSelect.query(Artist.class)        .columns(Artist.ARTIST_NAME Artist.PAINTING_ARRAY)        .select(context);{code}*Usage examples*:# Selecting root object plus some related fields:{code}Property<Artist> artistSelf = Property.createSelf(Artist.class);List<Object[]> result = ObjectSelect.query(Artist.class)        .columns(artistSelf Artist.ARTIST_NAME Artist.PAINTING_ARRAY.count())        .select(context);{code}# Selecting toOne relationship:{code}List<Object[]> result = ObjectSelect.query(Painting.class)        .columns(Painting.PAINTING_TITLE Painting.TO_ARTIST Painting.TO_GALLERY)        .select(context);{code}# Selecting toMany relationship the result will be as it will be in SQL query{code}Property<Artist> artist = Property.createSelf(Artist.class);Property<Painting> artistPainting = Artist.PAINTING_ARRAY.flat(Painting.class);Property<Gallery> artistPaintingGallery = Artist.PAINTING_ARRAY.dot(Painting.TO_GALLERY);List<Object[]> result = ObjectSelect.query(Artist.class)        .columns(artist artistPainting artistPaintingGallery)        .select(context);{code}",203
Extract Method,Add support for date/time components extraction in expression functions. Add support for the following date/time component-extracting functions:# {{year()}}# {{month()}}# {{week()}}# {{dayOfYear()}}# {{day()}} and/or {{dayOfMonth()}}# {{dayOfWeek()}}# {{hour()}}# {{minute()}}# {{second()}}However not all DB can support this functions e.g. sqlite has its own vision for date\time functions FrontBase don't support this functions completely.,204
Extract Method,"Update function support in expression parser We need following enhancements in expression parser:- add missing support for current date/time/timestamp functions- java-style ""camelCase"" naming for expression functions- make functions' names case sensitive (this one can break a little some code)",205
Rename Method,Rename PostCommit module and its content to CommitLog As postcommit module's name is not clarifies its purpose and clashes with {{POST_COMMIT}} callback it is proposed to change its name to {{commitlog}}.This requires some braking changes namely: - introduce new annotation {{@CommitLog}} instead of {{@Auditable}}- move all postcommit module's code into new packages ({{org.apache.cayenne.commitlog.*}})- rename {{Postcommit*}} classes,206
Extract Method,cgen: option to force run from maven/gradle Currently {{cgen}} checks file modification and skips class generation completely if data map file hasn't changed.  This can lead to minor annoyance in case you need to generate new classes when templates changed (or whatever reason user might have) plus there can be some bugs in this mechanics.Additionally this behavior should be mentioned in docs.Offered change is as simple as adding new option in {{cgen}} config that forces complete java code regeneration.,207
Rename Method,Remove commons-collections usage completely This task is final part of effort of removing all external dependencies from *cayenne-server*.*Why to do so*:* keeping cayenne free of outer dependencies will allow easier integration of Cayenne in projects reducing issues in case of dependencies incompatibilities* most of commons-collections code used by Cayenne can be replaced by plain java (this will require Java 8) so it will be easier to maintain* commons-collections v3.2.1 used now have security vulnerabilities (see this [issue|https://issues.apache.org/jira/browse/COLLECTIONS-580])*Negative impact*Cayenne use some tricky collections from that lib now and will require to deal with their replacementThose collections are:- {{LRUMap}} this will be seamlessly replaced by already used {{ConcurrentLinkedHashMap}}- {{CompositeCollection}} this will be copied into Cayenne code base as it has almost no dependencies and relatively small- {{ReferenceMap}} this should be implemented by Cayenne as copying this will lead to copying significant part of commons-collections code base luckily Cayenne actually use only two variants of this map: strong keys + weak values and strong keys + soft values.,208
Extract Method,Clean up build scripts and code after support for Java 7 will be dropped # {{cayenne-java8}} module should be included into core# remove conditional compilation of java8 modules (tutorials rop?)# setup java 8 support in all modules# check deprecation (MacOS Modeler),210
Extract Method,cdbimport: add option to skip user-defined relationships There is a problem in {{cdbimport}} tool when user define relationships that are not backed by foreign key in DB. This may be required to link table with view or just in case some DB-related optimizations. Currently there is no option to keep this relationships as they will be deleted by next {{cdbimport}} run.Workaround may be excluding all tables with this relationships or add relationships at runtime.,211
Rename Method,cdbimport: add option to create project file *cdbimport* tools are already pretty advanced and stable but you still need Modeler to create new project. It is really slows down start of new project and moreover complicates new users transition into Cayenne world.New option in *cbimport* config can be like this:{code:xml}<configuration>    <cayenneProject>${project.basedir}/src/main/resources/cayenne/cayenne-project.xml</cayenneProject>    <map>${project.basedir}/src/main/resources/cayenne/datamap.map.xml</map>    <cdbimport>        <!-- ... -->    <cdbimport></configuration>{code}And the logic should be like this: * without {{cayenneProject}} option result will be same as now * if {{cayenneProject}} is set but no file exists it will be created and DataMap linked to it * it {{cayenneProject}} is and file already exists then DataMap should be linked to it (if it is new) or update existing one,212
Rename Method,Add prefetch-related API to SQLSelect This one is self-descriptory. There is just no prefetch capabilities in {{SQLSelect}} query.Â ShouldÂ be pretty straightforwardÂ as underlying {{SQLTemplate}} already supports it.,213
Extract Method,Add prefetch type support for SQLTemplate query and SelectQuery Add prefetch type support for the SQLTemplate query and SelectQuery: * add prefetch type in Modeler * add prefetch type support to dataMap,214
Rename Method,Make SqlTemplate and SqlExec possible to return generated keys. Make SqlTemplate and SqlExec possible to return generated keys.https://lists.apache.org/list.html?user@cayenne.apache.orgÂ ,215
Extract Method,"Implement Quoting of identifiers Say a table ""t0"" has an attribute ""my attrib 0"".You correct the obj-attribute to for instance ""my_attrib_0"" but the db-attribute is still ""my attrib 0"".When you try to run a query on ""t0"" the generated query looks like:SELECT ... t0.my attrib 0 ... FROM dbo.t0 AS t0This obviously can't possibly work.The correct SQL would be:SELECT ... t0.[my attrib 0] ... FROM dbo.[t0] AS t0Notice the square brackets arround the attributes and table names: that makes the string a valid attribute or table name valid.A further improvement might be to add the database name:SELECT ... t0.[my attrib 0] ... FROM [dbname].dbo.[t0] AS t0I mention it because I use the quantum plugin as a database access plugin and it complained about a table called ""dbo.tablename"" (as I believe it should) and all was well when I used mydbname.dbo.tablename.",216
Extract Method,"Collections aren't supported with in-memory filtering It would be good to be able to filter objects based on a property where the property is a collection.i.e. be able to filter on the 'name' of the ANOtherClass.class ClassToBeFiltered{  ArrayList<ANOtherClass> list = new ArrayList<ANOtherClass>();}class ANOtherClass{  String name = ""test"";}",217
Extract Method,"DataObjectUtils 'objectForPK' should work on the client While DataObjectUtil.objectForPK takes ObjectContext as an argument using it on the remote client is not possible. The following exceptions happen on various overloaded calls:1. This will require method signature change:DataObjectUtils.objectForPK( c1new ObjectId(""MtTable1"" ""TABLE1_ID"" 1));java.lang.ClassCastExceptionat org.objectstyle.cayenne.DataObjectUtils.objectForPK(DataObjectUtils.java:276)at org.objectstyle.cayenne.remote.ClientChannelEventsTst.testSyncSimpleProperty(ClientChannelEventsTst.java:93)2. This should issue a query instead of doing DbEntity lookup:   DataObjectUtils.objectForPK(                c2                ClientMtTable1.class                1);org.objectstyle.cayenne.CayenneRuntimeException: [v.@CAYENNE_VERSION@ @CAYENNE_BUILD_DATE@] No DbEntity for ObjEntity: MtTable1at org.objectstyle.cayenne.DataObjectUtils.buildId(DataObjectUtils.java:352)at org.objectstyle.cayenne.DataObjectUtils.objectForPK(DataObjectUtils.java:167)     ",218
Extract Method,"Modeler doesn't manage FK constraints with MYSQL HelloI used CayenneModeler to design my tables and classes and would like to generate the corresponding SQL to update my MySQL database. But unfortunately the ""create FK support"" option doesn't make any difference and no foreign key sql statement is generated.Here is an example of a relation i want to appear in my DB :   <db-relationship name=""toCountry"" source=""address"" target=""country"" toMany=""false"">       <db-attribute-pair source=""country"" target=""id""/>   </db-relationship>   <db-relationship name=""toState"" source=""address"" target=""state"" toMany=""false"">       <db-attribute-pair source=""state"" target=""id""/>   </db-relationship> ",219
Extract Method,"SQLTemplate improvement - API to control the capitalization of the data row labels ""SELECT *""  SQLTemplates sometimes mess up the capitalization of labels in a returned DataRow so it can't be converted to an object properly. Most users would follow a naming convention for the table names (ALL_UPPERCASE or all_lowercase). Unfortunately such mapping will not work consistemtly across different databases and SELECT * queries. So what was suggested on the mailing list [1] to address this problem (and avoid using #result workaround) is an API to force capitalization of the result set labels.I did a few preliminary tests and this solution works extremely well.[1] http://objectstyle.org/cayenne/lists/cayenne-user/2007/06/0044.html",220
Extract Method,"Split Expressions By default the Cayenne qualifier translator removes ""duplicate"" joins.   Add syntax to the Cayenne expression parser and qualifier translator to indicate which expressions should be ""split expressions"" and not be removed as duplicates.",221
Extract Method,IncrementalFaultList performance improvements It's been reported that paginated queries are slow on very big lists:http://objectstyle.org/cayenne/lists/cayenne-user/2007/06/0168.htmlhttp://objectstyle.org/cayenne/lists/cayenne-devel/2007/05/0058.htmlThis issue will be used to track various optimizations that we make.Optimization #1 - removing synchronization on elements array from fillIn method.,224
Extract Method,"Improve readNestedProperty() to handle to-many relationships in the path. Improve readNestedProperty() to handle to-many relationships in the path.  This will resolve this documented limitation:Read to-many relationship in the middle of the path (throws exception):String name = (String)artist.readNestedProperty(""paintingArray.paintingName"");And allow:List names = (List) artist.readNestedProperty(""paintingArray.paintingName"");to succeed.  I'm not sure if this feature should be added to writeNestedProperty() [see https://issues.apache.org/cayenne/browse/CAY-815 for improvement feature] though.",225
Rename Method,EJBQL Delete Statement Support Implement support for EJBQL Delete Statement,227
Move Method,EJBQL Update Statement Support Implement support for EJBQL Update Statements,228
Rename Method,EJBQL Subquery support Support for EJBQL Subqueries ANY ALL and EXISTS constructs,231
Rename Method,Remove arbitrary reverse relationship mapping limitations  From the mailing list post:Remove We have two rules related to relationship mapping that we can really do well without:1. A DbRelationship always requires a reverse DbRelationship.2. A to-many ObjRelationship without a reverse to-one is effectively read only.I've done some work on a project where we've used generic persistent classes and it occurred to me that while the two things above are indeed a property of Cayenne runtime users don't have to worry about such low level details. Cayenne can automagically add missing reverse relationships in runtime to the corresponding entities without user ever noticing. That simple - don't know why nobody thought of that before :-)BTW what makes (2) painless is CayenneDataObject that can store arbitrary data in it so a back pointer from toOne side to the toMany site can be stored. This won't work in case of POJO's (without extra enhancement) but for normal Cayenne we get that functionality out of the box.,232
Extract Method,EJBQL Support for Functional Expressions Support String Arithmetic and Datetime functions in EJBQLQuery.Will be committing shortly all functions except a few that'll require more work:* SIZE (requires a correlated subquery)* TRIM char (requires cross-db testing; there's no JDBC standard syntax),233
Move Method,Support for mapping to-many as Maps and Sets and Collections Per JPA spec we should support mapping to-many relationships as Lists Collections Sets and Maps (we currently only do Lists). Need to add that stuff to Cayenne classic and map to JPA.  I see the following subtasks:* Support explicit to-many semantics mapping in ObjRelationship (collection class; map key for Maps)* In the Modeler allow to specify the choices in ObjRelationship Inspector* In class generation template use correct collection type (I guess for maps the add/remove semantics can be the same as for lists ... not sure if we need removeFrom(Object key)??)* Runtime support including reverse relationships* Support for prefetching* Testing* Bridging JPA mapping ,234
Rename Method,Deprecate EventManager.getDefaultManager() and stop using it EventManager.getDefaultManager()  is used by the map package to update mapping on dependent mapping object changes. The rest of Cayenne runtime uses EventManager that belongs to Configuration. We should deprecate 'getDefaultManager' and stop using the singleton in runtime.,238
Inline Method,CayenneModeler free-text search When working with huge models finding needed entities/attributes can be challenging in the Modeler. It would be nice to have a search field (either in the bottom of the frame FireFox style or in the top right corner next to the menu bar) with Ctrl-F shortcut (Command-F on Mac). The search should present a list of selectable matched model objects automatically jumping to the first one.  Project scanning and navigation to the search results can probably be copied from the validation action (except that it would be nice to avoid a modal popup window).,239
Extract Method,merge changes between model and db I want to be able to migrate schema changes between a DataMap and a database. Mainly for two reasons. 1) Make it easier for DBAs and developers to keep track of DB related changes in a project. 2) Make it simpler for developers to keep the db schema in sync with the model.,240
Extract Method,"CM Usability: Object Select Query Improvements Object Select Query UI should be smarter and more usable:#1 Qualifier field shouldn't be just a dummy text filed. It should be smarter with validations or even some sort of completionor live checking so that the user quickly can do what he wants and be sure that it's OK. Because this dummy field is very error prone many CM users avoid ""Named Queries"". I must admint - even myself - I always type something wrong there.#2 When selecting a QueryRoot -> e.g to Person than the Query Name should be changed too to ""PersonQuery"" but only in the case the user hasn't manually changed that field to something else than the default generated by CM at dialog open. This is very user friendly (many IDEs offer such ""variable suggestion"").#3 Orderings and Prefetches tab panes should use a JSplitPane to separte the upper and the lower zones. For entities with many fields the users always have to scroll because one can't simply drag a split pane to adapt the size.",241
Extract Method,"CM Usability: Welcome Screen (Panel) Add a Welcome Screen (Panel) to CM when no Project is open.This makes any application look professional and it gives new users confidence (in contrast to a big blank green screen) and it's of course all about the ""first impression"" :).This method is practiced even by IDEs (Eclipse IntelliJ but also by any desktop application that wants to look professional e.g. ""http://www.productiveme.com/doc/tutorial/1.png"" )",242
Extract Method,CM Usability: JComboBox Autocompletion Please use JComBox with autocompletion in CM at least in places that are very repetitive like selecting Field Types.It is very simple to implement it and there are several extremly well documented examples how to do it. E.g.:http://www.orbital-computer.de/JComboBox/,243
Rename Method,Refactoring class generator classes I am going to do some renaming and code refactoring to make class generator code easier to extend. Here is a few points I am planning to work on:1. Naming. CayenneGenerator ClassGenerator MapClassGenerator are all independent classes none inheriting from each other. Just by looking at the name  it is impossible to tell that CayenneGenerator is an ant task ClassGenerator is a template processor and MapClassGenerator is a controller for multiple templates2. Version 1.1 vs. version 1.2... Ideally we should get rid of the versions but for now it would be nice to may be nice to just split different version handlers into different subclasses... Don't have a clear idea yet how to do it...,244
Extract Method,"Add extended enumeration support Current Cayenne support for Java 1.5 enumerations is relatively simplistic.  Given:enum Color { RED GREEN BLUE }Cayenne will store 0 1 and 2 (respectively) for a numeric column or ""RED"" ""GREEN"" and ""BLUE"" for a string column.  There is no way to specify your own values for the enumerations which is especially important when mapping to an existing schema.  Also in the case of numeric types the order they are declared is fragile.  If someone later sorts the colors to be BLUE GREEN RED then the values of BLUE and RED will be swapped (and incorrect) on future reads from the database.Cayenne needs to be able to support explicitly mapping the enumeration to the database value.",250
Rename Method,CuratorFramework.Builder should allow adding multiple auths Currently one can add a single authentication scheme/bytes when building CuratorFramework. It would be handy to add multiple.,251
Extract Method,TreeCache should implement maxDepth ,252
Extract Method,SharedValue has limited utility but can be improved Currently SharedValue has limited utility as the internally managed version is always used for trySetValue. A good improvement would be a) add an API to get the current value AND current version and b) add an alternate trySetValue that takes a new value AND an expected version. ,253
Rename Method,Make ZKPaths accept more than one child ZKPaths currently only accepts one parent and one child nodes. It would be useful to be able to create paths with more depth.,254
Extract Method,"The current method of managing hung ZK handles needs improvement In v1.3.0 a ""major change"" was added whereby ""when the Curator state changes to LOST a flag will be setso that the next time Curator needs to get the ZooKeeper instance the current instance will be closed and a new ZooKeeper instance will be allocated (as if the session had expired).""This has turned out not to be optimum. Instead if the session timeout elapses before a SysConnected is received treat it as a failed session and dispose and reallocate the ZooKeeper handle. This has be shown to be superior internally at Netflix.",255
Inline Method,"Optimize TreeCache fix possible concurrency issue Hi I have been looking at the TreeCache impl and have some questions.It doesn't look right to me that there's separate atomic refs for a node's data and stat. It seems the stat in a ChildData object obtained from getCurrentData() might not correspond to the data that it's with. This could be problematic when doing conditional state changes given assumptions about that data.An obvious and simple solution to this would be to have a single AtomicReference<ChildData> field instead which would have the additional significant benefit of eliminating ChildData obj creation on every cache access. PathChildrenCache works this way but my understanding was that TreeCache is intended to be a (more flexible) replacement.Furthermore I'd propose that the data field of ChildData be just a final byte[] instead of an AtomicReference. This would avoid needing two volatile reads to get to the data and mean that ""sharing"" these (per above) is a bit safer. The ChildData byte[] AtomicReference is only used by PathChildrenCache.clearDataBytes() (not currently used by TreeCache at all) and that capability could be easily maintained by having PathChildrenCache use it's own simple subclass of ChildData containing the atomic reference.If similar capability were to be added to TreeCache I'd suggest it would be better to just replace the node's ChildData object with a copy that has the byte[] field nulled out (but same stat ref).I'm fairly new to the code so apologies if there's something I've missed/misunderstood! But if there is agreement I'd also be happy to prepare a PR.RegardsNick",256
Extract Method,Expose extra metrics in TracerDriver Currently the TracerDriver exposed the latency of ZK operations in multi-tenant environment extra metrics are required to help tracing and monitoring:* the bytes being sent and received so we can monitor the client usage scenarios.* which ensemble participant the client is talking to used to find out the problematic Zk server when the issue happened.* the z-node path to easily find out which z-node caused the problem like high load etc.,257
Extract Method,Handle graceful close of ZookKeeper client waiting for all resources to be released Ths idea is to leverage the new ZooKeeper#close(timeoutMs) method introduced with ZOOKEEPER-2697.This new method waits for the internal threads to finish this way the client is sure that all internal resources handled but low-level ZooKeeper client have been released.This is very useful in tests because the user can wait for the test environment to be cleared.In some cases you want to return from the 'close' method as soon as possibile. In ZooKeeper a new specific method as been added in order to let the user ask for a specific behaviour.,259
Extract Method,"Include curator framework state in error messages While looking into [Apache Fluo #1004|https://github.com/apache/fluo/pull/1004]Â I am seeing error messages like the following.Â Â Â {noformat}2018-01-22 02:57:23383 [leader.LeaderSelector] ERROR: The leader threw an exceptionjava.lang.IllegalStateException: instance must be started before calling this methodat com.google.common.base.Preconditions.checkState(Preconditions.java:149)at org.apache.curator.framework.imps.CuratorFrameworkImpl.getData(CuratorFrameworkImpl.java:363)at org.apache.fluo.core.oracle.OracleServer.takeLeadership(OracleServer.java:426)TRUNCATED{noformat}Â When I see this error message I know theÂ  [CuratorFrameworkState|https://curator.apache.org/apidocs/org/apache/curator/framework/imps/CuratorFrameworkState.html]Â is not STARTED.Â  However I don't know if its LATENT or STOPPED.Â  From a debugging standpoint this information would be very useful.Â  An example of the code that generates this error messages is at [CuratorFrameworkImpl.java line 408|https://github.com/apache/curator/blob/3f7b610ad5c5a0c6a7b0331f02294bd433a54554/curator-framework/src/main/java/org/apache/curator/framework/imps/CuratorFrameworkImpl.java#L408].Â  This code could be changed to call an internal method like the following :Â {code:java}private void checkState() {  CuratorFrameworkState state = getState(); //store state in local var to avoid race conditions since it may be read twice  Preconditions.checkState(state == CuratorFrameworkState.STARTED ""instance state must be %s before calling this method however its %s"" CuratorFrameworkState.STARTED state);}{code}Â Â ",260
Extract Method,Operation- and EnpointInterface- Description metdata improvements Improvements to the metadata processing including- Package protect methods accessing the annotations directly- Provide defaults (per JAX-WS spec) for annotations- Provide getters for metadata associated with JAX-WS annotationsAlso provide additional tests and refactoring of ProxyDescriptor based on the above.I have a patch I will submit shortly.,261
Extract Method,"Java2Wsdl needs ""extra classes"" support In Axis 1 Java2Wsdl has support for ""extraclasses"" which allows the wsdl to generate complex types for subclasses that are possible return types for methods that return abstract classes or interfaces.  For example:One might have an abstract class ""com.myCo.fruit.Fruit.java"" that has the subclasses ""com.myCo.fruit.FruitApple.java"" and ""com.myCo.fruit.FruitOrange.java"".  If there is a method in our service interface:public Fruit getUsersFavoriteFruit(User user){   return fruitService.getFavoriteFruit(user);}I want my WSDL to have definitions of not just Fruit but also FruitOrange and FruitApple so my client will be able to handle those types.  In Axis 1 you could add something to you ant task like:extraclasses=""com.myCo.fruit.FruitApple.java                            com.myCo.fruit.FruitOrange.java""Is there the possibility of getting this feature in Axis2?ThanksMatt",262
Move Method,Add metadata support for WebServiceProvider and related annotations on provider-based service implementation class Add support in the metadata layer and refactor the EndpointController and JavaBeanDispatcher and ProviderDispather classes for provider-based annotations: WebServiceProvider ServiceMode BindingType.I am working on a patch which I will submit shortly.,263
Move Method,Adding support to handle bare wsdl in a Java Bean Endpoint I am adding support to java bean endpoint so it can handle a non wrap or bare wsdl. I am also adding test cases created from a wrapped wsdl and using binding file with enableWrapperStyle false.,264
Move Method,WSDL2Java should not create ExtensionMapper in package axis2.apache.org WSDL2Java creates a class called ExtensionMapper which belongs to the package axis2.apache.org. Can this please be changed so that it is created in the same package as the classes for the types aka in an application-specific package? Or at the very least change the package to org.apache.axis2 instead of axis2.apache.org. However I can't think of a reason why ExtensionMapper should not belong to an application-specific package because the class actually is specific to the application for which it was created.Thanx!,265
Extract Method,"Provide a way to expose interface to client instead of concrete class There may be instance where service author has interface that he need to generate wsdl  while he has separate service impl class. With current Axis2 impl there is no way to achieve this goal. I think we can solve this problem by adding one more optional parameter to services.xml<parameter name=""ServiceInterface"">qualified name of the service interface </parameter> in addition to <parameter name=""ServiceClass"">qualified name of the service class </parameter> ",266
Move Method,Improved yahooservices sample I have improved the yahooservices sample to facilitate a UI for user convenience,267
Extract Method,Need to save and restore the AXIS2 MessageContext There is an AXIS2 requirement to save the message context at some point in the handler chain store it away and restore it later.  This requirement also includes the need to let handlers manage message-specific data when the message context is saved and restored.                                In particular this feature can be used by a WS-ReliableMessaging implementation to persist and re-send messages and to recover from a server restart. The idea being to save a message context then later restore the message context and pick up the message processing at the point at which the message context was saved (that is without having to completely restart the message from the beginning).Refer to the wiki page    http://wiki.apache.org/ws/FrontPage/Axis2/MessageContextSaveRestorefor a description of a proposal on how to accomplish this feature.,269
Move Method,Small change to the debug information formatting I am making a small change to the debug formatting.  I will commit the change soon.,270
Extract Method,Add SOAP 1.2 support for JAXWS dynamic proxies The current dynamic proxy client implementation does not support creating a SOAP 1.2 request message.  The infrastructure is there in the message model we just need to wire it up in the proxy.This also requires fixing the metadata layer to read the soap binding transport URL from the WSDL.  I am working on this and should have some code to drop later on this afternoon.,271
Move Method,JAX-WS Cache or improve the @XMLRootElement and related annotation lookups The JAX-WS doc/lit wrapped marshalling code needs to query annotations such as @XmlRootElement @XmlList @XmlType etc.This defect is opened to save/cache the resulting information on the OperationDescription or possibly a static WeakHashtable to improve performance.,272
Rename Method,fix and enable the test added in jira AXIS2-1830 fix and enable the test added in jira AXIS2-1830,273
Rename Method,SAAJ 1.3 implementation Two patches attached (for modules saaj & saaj-api) include part of saaj 1.3 implementation. Integrated & tested this with latest source from trunk. ,275
Extract Method,Reading class array from Jar file for creating JAXBContext I am adding code to update JAXBUtils to read classes from jar file in order to collect list of classes to create a JAXBContext.So as per current logic we will read context path if anvObjectFactory is defined in a given package. if not we will read all the classes from the file system directory and jar files for a given package.,277
Extract Method,wsdl generation should work for all message receivers Currently ?wsdl only works for a specific set of message receivers. IMO it should work for all message receivers- and if no info is available about the schemas of the messages then generate just xs:any for those messages. That way you always get a WSDL for any service by doing ?wsdl. Of course the usefulness of the WSDL will vary depending on how much info is available at the time.,278
Extract Method,Reduce the trace messages for object serialization to improve performance with trace enabled  There is extensive tracing being performed by the message context serialization related code.  While this has beenhelpful in quickly debugging problems the amount of trace messages needs to be reduced to improve performancewhen tracing is enabled.Since much of the read/write to streams is done in a utility class the improvement can be made there.   ,279
Extract Method,Allow WS-* protocol faults to be thrown to client code with clearer messages The WS-A Spec defined reason strings are confusing to the majority of users. It would be good if the AxisFaults thrown when they are received included more information.,280
Extract Method,A method in HTTPSender to send GET messages With the new message formatter concept in Axis2 SOAPOverHTTPSender (this name will be changed to something else) has become the common sender for all formats including REST. So the RESTSender will be removed soon and there will be a message formatter for REST too. Other than this there are some situations where JSON responses are sent using GET method.Because of these reasons there should be a method in HTTPSender to send messages using GET method. So I have implemented this and the patch is attached....,281
Extract Method,Improve trace correlation for message context serialization In debugging some of the more complicated application scenarios it has become clear that having better correlation in the trace messages among some of the objects in the message context object graph is very helpful in determining failure points.The following needs to be done:- Make use of the logCorrelationIDString in the MessageContext- Add a logCorrelationIDString to the OperationContext- Add a logCorrelationIDString to the Options- Add some try..catch blocks around the use of ObjectStateUtils.writeObject(...) for some objectsIn addition clean up the following- remove commented out System.out.println lines - move the check of the NeedsToBeReconciled flag to a method ,283
Rename Method,"The class org.apache.axis2.uitl.Builder can be refactored to improve usability improve object orientation and slightly improve performance The method ""public static OMBuilder getBuilder(InputStream inStream String charSetEnc String soapNamespaceURI)"" in the class org.apache.axis2.uitl.Builder takes three parameters but when you look at the code last two parameters are optional. A more object oriented way of handling this would be to have this method overloaded to handle the difference scenarios.Right now this method has two null checks which get executed every time due to this. When I navigate through the calling stack I realize every time the caller is well aware of whether the optional parameters are available or not and he is passing null in such scenarios since there is no overloaded method. At the external interfaces where a parameter is really required implementors are passing a default value making the null check redundant.Also there is one overloaded method which takes a parameter of type Reader which provides the same functionality. IMO this method is again redundant since the only difference is how you handle the input which should be the responsibility of the caller not the utility class like this.",284
Extract Method,"JAXWS Should cache the @PostConstruct methods to unnecessary reflection. Currently the EndpointLifecycle manager is performing reflection on the service instance to determineif it has a method with @PostConstruct.  We should do this one time and save the information away in the ""injection description"".I am currently testing a patch.This problem was discovered by David Strite who is part of IBM's performance analysis team.  Thanks David !",285
Extract Method,Annotation-based support in metadata layer Doc/Lit/Bare body-based routing For Doc/Lit/Bare routing of an incoming message to an operation the input AxisMessage element QName must be set to the part name and then the operation needs to be added to the AxisService MessageElementQNameToOperationMapping.  This is not being done by the metadata layer for AxisOperations it created based on annotations.,286
Rename Method,Change JAXWS MessageContext to use EndpointDescription The JAXWS MessageContext should hold a reference to the EndpointDescription rather than the ServiceDescription.  This JIRA will be used to change all of the places where that is referenced.This is also needed by AXIS2-2218 for integration of the handlers.,287
Rename Method,Add implementation of LogicalMessageContext/LogicalMessage for use in handlers We need an implementation of the APIs that are used in a LogicalHandler flow.  I have some of this completed already and will be contributing.,289
Rename Method,handler integration tests enabled Handlers are more fully integrated some tests are enabled.,290
Extract Method,ListingAgent should support service name with composite paths Currently ListingAgent assumes only the last part of the URI correspond to the servicename. e.g. GET /axis2/services/serviceName Attached is a patch that allows the ListingAgent to function properly with composite service names.e.g. GET /axis2/services/path/to/serviceComposite names are useful to expand the name-space of possible endpoints.   Composite service names are common in other projects such as Apache Ode (BPEL engine).,292
Move Method,Add progress bar to IDEA AXIS2 WSDL2Java plugin currently axi2 eclipse plugin have the progressbar . Attaching the patch to the Idea plugin to add progressbar.,293
Extract Method,Minor extensibility improvements Factored out some calls into separate methods in certain classes to make it easier to extend them and change behavior.,294
Extract Method,Service lifecycle http://www.nabble.com/java.lang.NoSuchMethodException-tf3886972.html,295
Extract Method,Override HandlerChain annotation The attached patch enables for the HandlerChain annotation to be overridden. That's needed by app servers where the handlers can be specified in a deployment descriptor.,296
Extract Method,Callback Interface changes Discussion here: http://marc.info/?t=117858969600039&r=1&w=2,297
Rename Method,[PATCH] Upgrades 'Simple' HTTP and NIO HTTP transports to HttpCore 4.0-alpha6  There has been a number of improvements and bug fixes Axis2 'simple' and NIO HTTP transports should pick up [1]. I am attaching a patch that upgrades Axis2 to the latest HttpCore API.All test cases pass for me.Oleg[1] http://www.apache.org/dist/jakarta/httpcomponents/httpcore/RELEASE_NOTES.txt,299
Extract Method,"Control whether a WSDL is returned when  ?wsdl comes in - both at service level and global level Need the ability to suppress the returning of WSDL document when a ?wsdl or ?wsdl2 URL comes into the Axis2 engine.  Mainly for security grounds.The proposal would be to have the ability to do this globally (i.e. via the axis2.xml file) and at the service level (i.e. in services.xml file) where the services.xml file overrides the global switch.  I do not see a problem with naming the property the same in both configuration files something like ""exposeWSDLDocument"" with the value being ""true"" or ""false"".",300
Extract Method,Provide enabling support  in JAXWS for SOAP/JMS Currently when validating the description hierarchy we throw a WebServiceException if we encounter a transport other than 'HTTP'. This feature will allow SOAP/JMS as well ,301
Extract Method,Support JAX-WS and Metadata client-side sparse composite to override certain annotation members Add support for a JAX-WS client to use a sparse DescriptionBuilderComposite to override certain annotation values specified in the client artifacts on a per-SerivceDelegate basis.  This support is the basis for supporting additional client-side metadata such a deployment descriptor information.  Added additional tests to verify the new functionality.,302
Rename Method,JAX-WS 2.1: Support @RespectBinding and RespectBindingFeature RespectBinding is a feature added in JAX-WS 2.1 that allows an endpoint to ignore the binding defined in a wsdl:binding element.  In other words an endpoint could support both SOAP 1.1 and SOAP 1.2 even though the WSDL may indicate only SOAP 1.1 support for the endpoint.  Here's a quick summary of the work to be done:1. Update the metadata APIs to expose the RespectBinding data available2. Update the annotation processing code in the DescriptionBuilder to process the @RespectBinding annotation.3. Update the WebServiceFeature processing code to account for the RespectBindingFeature.  4. Change the EndpointController to have a toggle point that checks against a RespectBinding property.  5. Update Provider processing code (ProviderDispatcer) to handle the scenario where the return type is invalid according to the input.  This is described in the check that exists in the EndpointController.,303
Move Method,Merge inbound WS-Addressing handlers into a single handler Currently we have an AddressingInHandler class plus two subclasses i.e. AddressingFinalInHandler and AddressingSubmissionInHandler. These subclasses don't really do very much so their functionality can be moved to the AddressingInHandler class.,304
Extract Method,"Make the definition of custom ""anonymous"" URIs configurable The WS-Addressing specifications define ""anonymous"" URIs that can be used in the wsa:ReplyTo or wsa:FaultTo headers to indicate that a response should be sent synchronously using the backchannel of the two-way transport. Occasionally the need may arise to define additional URIs to have the same semantics as described above. One such occasion is implementing WS-RM. It has defined it's own ""anonymous"" URI and this is currently supported in Axis2 by hardcoding the value in the kernel code. It would be better to allow such URIs to be configured in a more dynamic way.",305
Extract Method,AxisServlet:  abstracting initialization of ConfigurationContext I was wondering if the current maintainer of AxisServlet could review the following changes to AxisServlet.   The refactoring is aimed at improving the extensibility of AxisServlet by allowing the override of initConfigContext().The use-case I am trying to support is to obtain a ConfigurationContext from JNDI (instead of directly loading from file system) to share ConfigurationContext between loosely-coupled components in different classloading hierarchies and/or where the configuration repository is not deployed in the servlet's /WEB-INF directory.I'm open to other approaches if anyone has better ideas on how to abstract the initialization of the ConfigurationContext such as enhancing the ConfigurationContextFactory.  I was just going for the simplest solution to support this kind of extensibility. ,306
Extract Method,Plugin in JAXWS RI's wsgen to generate the dynamic wsdl/xsd's When one deploys a jaxws annotated service using the PojoDeployer the dynamic wsdl and xsd generated don't use all the annotations present in the classes Quick way is to plugin the wsgen tool from JAXWS. if the JAXWS RI jar's are in the classpath then we pick them up and use them to generate the wsdl. This tactic is already used quite successfully by the Geronimo folks. thanksdims,307
Extract Method,Introduce support in JAX-WS for OASIS XML Catalogs. Note the following in JSR-109:JAX-WS requires support for a OASIS XML Catalogs 1.1 specification to be used when resolving any Webservice document that is part of the description of a Web service specifically WSDL and XML Schemadocuments. Refer to section 4.4 of JAX-WS specification.I will take a crack at this.,308
Extract Method,JAXB CustomBuilder support in JAXWS layer. Problem: We want to unmarshall the soap body payload as an OMSourcedElement backed by a JAXB object.Solution:The proposed solution is to create a new class JAXBCustomBuilder.The JAXBCustomBuilder is registered on the StAXOMBuilder and it will automatically demarshall the payload into an OMSourcedElement backed by a JAXB object.Here is a quick summary1) Receive first message.2) The JAXWS unmarshalling code builds a JAXBContext and unmarshalls the payload.3) The JAXWS unmarshalling code builds a JAXBCustomBuilder with the same JAXBContext.4) The JAXBCustomBuilder is placed on the ServiceContext.5) Receive second message6) The Dispatch code associates the ServiceContext with a MessageContext.6a) This event is intercepted and triggers the JAXBCustomBuilder to be registered on the StAXOMBuilder.7) When the StAX events of the payload are pulled the JAXB object is automatically unmarshalled.Note that this defect requires changes that are being made as part of WSCOMMONS-303.ThanksRich,309
Extract Method,Refactor the ThreadContextMigratorUtil class to use parameters instead of properties. Implementations of the ThreadContextMigrator interface are currently managed using the ThreadContextMigratorUtil class. As these implementations represent static configuration information not just runtime state it would be better to store them as parameters on the AxisConfiguration. This would also allow us to extend the AxisConfigurationBuilder to recognize ThreadContextMigrators in the axis2.xml file.,310
Extract Method,Add support for @Action and @FaultAction Complete support for the new annotations.,312
Inline Method,LocalTransportSender can be used on multithread. I was changed LocalTrasportSender for multithread. We can LocalTransportSender on mutlithread by using attached code.Please check attached codes.I hoped that this enhanced added to next release.thanks,313
Rename Method,Fixing the interop tests in itest Most of the Interop Tests in Integration/itest fail due to various reasons and I've started fixing those.These interop tests have used XMLComparatorInterop to compare the returned Soap Envelope with the expected response. Since this class contains errors we decided not to use it anymore. Instead the /compare /method in WhiteMesaInterop class was changed and a new method /compareXML /is added. So should we remove XMLComparatorInterop since it is not used anymore?,314
Rename Method,"JAXWS: Support Binding Property to Access SOAPHeaders  Background:------------------The JAX-WS specification defines properties to set/get attachments on the dispatch/proxy.  For example:        // Create Dispatch for the payload        Dispatch<String> dispatch =                 svc.createDispatch(portName String.class Service.Mode.PAYLOAD);        // Get the request context        Map<String Object> requestContext = dispatch.getRequestContext();        // Get the attachments (non-payload) that should also be sent.        Map<String DataHandler> attachmentMap = new HashMap();        attachmentMap.put(""javax.xml.ws.binding.attachments.outbound"" myDataHandler);        // Attach the attachments to the request context        dispatch.getRequestContext().put(""javax.xml.ws.binding.attachments.outbound"" attachmentMap);The ""javax.xml.ws.binding.attachments.*"" properties make it convenient to get/receive attachments.Proposal:----------The proposal is to add the same kind of functionality to get/set SOAP Headers.  For example:        // Create Dispatch for the payload        Dispatch<String> dispatch =                 svc.createDispatch(portName String.class Service.Mode.PAYLOAD);        // Get the request context        Map<String Object> requestContext = dispatch.getRequestContext();        // Create a new outbound header        Map<QName List<String>> headerMap = new HashMap();        List<String> myHeadersList = new ArrayList<String>();        myHeadersList.add(""<pre:sample xmlns:pre=""http://sample"">hello</pre:sample>"";        QName myHeaderQName = new QName(""http://sample"" ""sample"" ""pre"");        headersMap.put(myHeaderQName myHeadersList );        // Attach the headers map to the request context        dispatch.getRequestContext().put(""jaxws.binding.soap.headers.outbound"" headerMap);Details:--------Proposed names:  ""jaxws.binding.soap.headers.outbound"" and ""jaxws.binding.soap.headers.inbound"".  This is similar naming convention as the existing attachment propertiesProposed value:   Map<QName List<String>>     QName is the qname of the header(s)     List<String> is a list of xml values (normally one)         String is the xml string for a single header.            (The object is a String.  Most JAXWS users will not be familiar with OM and may not want to build a SAAJ SOAPHeader).Semantics for ""jaxws.binding.soap.headers.outbound"":  Prior to the dispatch/proxy invocation:       The customer sets the outbound map on the RequestContext.  During the dispatch/proxy invocation:       The outbound jaxws engine adds the headers to the message.Semantics for ""jaxws.binding.soap.headers.inbound""  During the dispatch/proxy invocation:       The engine will provide a map<QName List<String>> on the ResponseContext.  After the dispatch/proxy invocation:       The customer accesses the inbound headers map from the ResponseContext. ",315
Extract Method,Extract initTransport() method from AxisServlet::init( ServletConfig ) Extracting initTransport() method allows more accurate subclassing,316
Move Method,Axis2 kernel currently has a direct dependency on Commons HttpClient 3.1 (1) This seems conceptually wrong to me. The kernel ought not have any dependencies on a transport specific libraries(2) A more practical reason: it is just a matter of time HttpClient 3.1 will be superceded by HttpClient 4.0 and support for commons HttpClient will be discontinued. If there is an agreement this is indeed an issue which should be resolved I will happily invest time looking into what it takes to decouple HttpClient from Axis2 kernel.Oleg,317
Extract Method,JAXWS: Store the exception thrown from a web method implementation in a property so that it can be queried by an outbound jaxws handler Scenario:A JAXWS webservice on the server throws an exception (checked or unchecked).The JAXWS engine converts the exception into a message containing a SOAP Fault.An outbound JAXWS application handler is installed.  In the handleFault method the customer wants to do some work related to the exception. For example the customer may want to log exception information in a database.Problem:The conversion of the Exception -> SOAP Fault is lossy.  Some of the information about the Exception is lost.For example the stack trace of the exception is not captured (and should not be captured) in the SOAP Fault.Other java information is also lost.Solution:Store the Exception on a new property.The outbound JAXWS application handler can then access the exception directly to query java specific information.Next Step:I have coded the  solution and verification tests.  The changes are minimal and I will be committing the changes later today.,318
Rename Method,Refactored the changes made when improving Faulty Services handling. 0,319
Rename Method,Implicit SEI restiction on Static and Final Method exposure as webservice JAX-WS 2.2 specification restricts exposure of static and Final method on Implicit SEI as a webservice. I am making a change that will allow us to be compliant with this requirement I am also adding new test cases to prove restriction on Static and Final operations.,320
Inline Method,JAX-WS: Support an option to allow invalid xml characters to be removed from an outbound JAXB serialization Background:The JAX-WS engine uses JAXB data objects.The JAXB data objects are marshaled to xml using a JAXB provided Marshaler.  The Marshaler writes information to an XMLStreamWriter (which will be the axiom provided MTOMXMLStreamWriter).Problem:If a customer populates the JAXB bean with non-xml characters (0x15) the JAXB Marshaler will write the illegal characters without errors.However the SOAP node receiving the message will fail.Solution:Provide a JAX-WS property that a customer can set (using the @WebServiceContext) to recognize and remove illegal characters.This solution will have an axiom and axis2 contribution.,321
Extract Method,JAX-WS: Minor performance improvement scrub I am scrubbing the JAX-WS code looking for minor performance improvements related to property access.,322
Rename Method,Support new SOAP/JMS binding ID The W3C SOAP/JMS binding specification (located here: http://www.w3.org/2002/ws/soapjms/) is nearing its first approved version and has defined a more up to date binding ID for use in the @BindingType annotation (for JAX-WS endpoint impl classes) and within the soap:binding transport attribute within a WSDL document.    Previously the SOAP/JMS spec defined this binding ID:   http://www.example.org/2006/06/soap/bindings/JMS/but that was just a placeholder and the spec has amended that to this new binding ID:   http://www.w3.org/2010/soapjms/I've made some minor changes to the jax-ws and axis2 code to reflect this new binding ID and I've also run some tests and found a few problems and fixed those as well.    I'm attaching a patch file with the changes.The purpose of the JIRA is to request that someone apply the changes since I'm not a committer.,323
Extract Method,"Small improvements to avoid unnecessary map lookups Background:The Axixs2 engine is based on a ""Context"" programming model.  Thus the code highly depends on hierarchical getProperty and setProperty calls.  Problem:Doug Larson (IBM) has identify several minor issues in the JAX-WS and kernel code where unnecessary getProperty calls are being performed.  This causesunnecessary map calls.  Removing the calls provides small boosts in performance.",324
Extract Method,Provide support for complex object types. It is not possible to pass complex type objects via a web service invocation. The following error is thrown while attempting to do so.[ERROR] Exception occurred while trying to invoke service method echoArrayorg.apache.axis2.AxisFault: Unknow type {http://www.w3.org/2001/XMLSchema}stringat org.apache.axis2.databinding.utils.BeanUtil.deserialize(BeanUtil.java:349)at org.apache.axis2.databinding.utils.BeanUtil.processObject(BeanUtil.java:827)at org.apache.axis2.databinding.utils.BeanUtil.ProcessElement(BeanUtil.java:717)at org.apache.axis2.databinding.utils.BeanUtil.deserialize(BeanUtil.java:655)at org.apache.axis2.rpc.receivers.RPCUtil.processRequest(RPCUtil.java:153)at org.apache.axis2.rpc.receivers.RPCUtil.invokeServiceClass(RPCUtil.java:206)at org.apache.axis2.rpc.receivers.RPCMessageReceiver.invokeBusinessLogic(RPCMessageReceiver.java:117)at org.apache.axis2.receivers.AbstractInOutMessageReceiver.invokeBusinessLogic(AbstractInOutMessageReceiver.java:40)at org.apache.axis2.receivers.AbstractMessageReceiver.receive(AbstractMessageReceiver.java:110)at org.apache.axis2.engine.AxisEngine.receive(AxisEngine.java:181)at org.apache.axis2.transport.http.HTTPTransportUtils.processHTTPPostRequest(HTTPTransportUtils.java:172)at org.apache.axis2.transport.http.HTTPWorker.service(HTTPWorker.java:296)at org.apache.axis2.transport.http.server.AxisHttpService.doService(AxisHttpService.java:281)at org.apache.axis2.transport.http.server.AxisHttpService.handleRequest(AxisHttpService.java:187)at org.apache.axis2.transport.http.server.HttpServiceProcessor.run(HttpServiceProcessor.java:82)at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)at java.lang.Thread.run(Thread.java:662),325
Extract Method,Runtime processing and schema generation is not consistent for java.util.List Axis2 generate schema for java.util.List correctly and it's identical to Array schema but run time expect additional wrapping element for lists. The impact is client generated from above WSDL file is not usable with the service hence this need to be changed. It is possible to use same Schema for both Lists and Arrays and  return list/array instances depend on actual service method signature.,326
Extract Method,Add getSimpleTypeObject(Class parameter String text) method to org.apache.axis2.databinding.typemapping.SimpleTypeMapper Currently org.apache.axis2.databinding.typemapping.SimpleTypeMapper class has getSimpleTypeObject(Class parameter OMElement value) method. Only usage of the OMElement parameter of this method is to retrieve the text encapsulated in it by calling value.getText() method. getSimpleTypeObject(Class parameter String text) would be a more API-wise cleaner version of this method.Introducing getSimpleTypeObject(Class parameter String text) is useful since converting a String into a simple type (int long etc.) is a common requirement. In fact in many places of Apache Synapse project this sort of conversions are done using newly written code. If getSimpleTypeObject(Class parameter String text) method is introduced to SimpleTypeMapper class then it can be reused by number of places including some code in Apache Synpase project without writing new code for the same purpose.,327
Move Method,"ServiceBuilderExtension for Axis2 Deployers Refer the following discussion[1]  to find objectives of this idea.ServiceBuilderExtension can take following API.public interface ServiceBuilderExtension {public void init( ConfigurationContext configurationContext);public Map<StringAxisService> buildAxisServices(DeploymentFileData deploymentFileData) throws DeploymentException;;}and possible to register them with deployers as follows.<deployer extension="""" directory="""" class=""""><serviceBuilderExtension name =""jwsbuilderExt"" class=""org.apache.axis2.jaxws.framework.JAXWSServiceBuilderExtension""></deployer> * One deployer can have number of ServiceBuilderExtensions and will invoke them in the order they defined in the axis2.xml file. * For a given deploymentFile if all ServiceBuilderExtensions fail to create AxisService then base deployer will take care about the deployment of that particular deploymentFile.* For a given deploymentFile if a ServiceBuilderExtension could create a AxisService then stop execution of other  ServiceBuilderExtensions registered and return the AxisService immediately to the base deployer for the further processing.   * Axis2 AbstractDeployer implement necessary helping methods for this idea so that extended Deployer from  AbstractDeployer can easily utilise  ServiceBuilderExtension concept. * Immediate goal is to support JAX-WS artefacts through ServiceDeployer . [1] - http://axis.markmail.org/thread/kvhvcvfufpo6zfe3[2] - http://axis.apache.org/axis2/java/core/api/org/apache/axis2/deployment/AbstractDeployer.html",328
Inline Method,fix small issue causing null pointer exception fix typos add javadoc comments. - fix spelling (axis2Steram -> axis2Stream)- if axisurl != null case would not set the axisConfig gave null pointer error on old line #81- fix spelling (unableHttp -> enableHttp)- add warning message when could not find the axis2.xml anywhere and using default one in classpath resource.- externalize init parameter strings as constants and provide documentation.- add javadoc comments to class and other methods.,330
Extract Method,Allow the WS-Addressing action to be set in the client without also setting the SOAP action Modify the Options class to allow a user to set the WS-Addressing action without also setting the soap action. This is important as according to the WS-Addressing 1.0 SOAP Binding Spec it will allow the ability to obscure the action through SOAP-level security mechanisms without having to resort to transport level security mechanisms. Patch to follow.,331
Rename Method,JAXWS: Update AxisInvocationController to use the OperationClient instead of the ServiceClient For the purposes of greater flexibility the AxisInvocationController should use the Axis2 OperationClient API rather than using the ServiceClient.  I've started a little bit of this work in a sandbox and will hopefully have patches to post later on today.Also this work is dependant upon the patch that I posted for JIRA issue 909.,332
Extract Method,Need a way to build AxisServices for all ports/services in a wsld. WSDL 11 allows multiple services per wsdl file and multiple ports per service. Currently unless a service and port name are specified WSDL11ToAxisServiceBuilder will only return the first port on the first service. I would like to provide an extension to WSDL11ToAxisServiceBuilder that would take a wsdl file and return a List of AxisService objects one for each port in the wsdl.  To make this more efficient I will need to make some minor changes to org.apache.axis2.description.WSDL11ToAxisServiceBuilder.java  i.e.   - restructuring in the populateService method so that all the processing that is not specific to an AxisService is only done one time.  Move this code to a new method setup().  Setup code will include reading in the wsdl file processing policies imports etc.   -  make some methods and fields protected instead of private so they can be accessed by subclasses.The new extension is proposed to be org.appache.axis2.description.WSDL11ToAllAxisServicesBuilder.  It has a public method popluateAllServices() which operates as follows  - calls the setup() method on the parent.    - iterates through all the services and all the ports in the wsdl setting serviceName and portName on the parent.   - calls up to populateService() on the parent.  This will return an AxisService specific to the service and port name specified.    - changes the name on the AxisService to the port name instead of the service name so it can be uniquely identified.  - returns the List of AxisService objects one for each port in the wsdlI will also make corresponding changes for WSDL 2.0 in org.apache.axis2.description.WSDL20ToAxisServiceBuilder.java and create WSDL20ToAllAxisServicesBuilder.java.,333
Extract Method,Initial annotation processing in metadata abstraction layer and associated refactoring JAXWS Proxy The JAXWS proxy was doing some annotation processing directly.  Move that annotation processing to the metatdata abstraction layer (i.e. the description package).  Also provide an associated test to verify the metdata layer annotation processing for an SEI class.This is an initial pass at adding metadata processing to the description package.  There needs to be more refactoring to create server-side metadata remove annotation class references from Proxy and then move to WSM for JSR-181 (and other) annotation processing.  That work will be done under subsequent JirasI will attach a patch file shortly.,334
Move Method,Add description test utility class and refactor existing tests Refactor utility methods used in the Description tests (such as accessing private attributes on ServiceDelegate to verify test results) into a utility class.I will attach a patch shortly.,335
Rename Method,Adding Simple Content Restriction to ADB Here I submitted the patch regarding the implementation of simple content restriction.Any comments are appreciated ...,336
Extract Method,Move IBM contrib code for War to Wab conversion into trunk to provide a (default) BundleConverter for War files ,337
Extract Method,Override subsystem name & description ,341
Extract Method,Small refactorings ,343
Extract Method,Upgrade subsystem tests to pax exam 3 ,344
Extract Method,Support jpa 2.0 and 2.1 with the same code base ,346
Extract Method,Support JPA2 features ,350
Extract Method,Support jpa annotations on method and class ,351
Inline Method,getInstance method on the core Activator shows up on jvisualvm sampling during performance analysis ,352
Extract Method,"""Make equals and hashCode comparisons within the header ",353
Rename Method,Compute service requirements and capabilities once in BundleRevisionResource. ,354
Extract Method,Provide a more efficient implementation of a system repository. ,355
Rename Method,Maven plugin no longer includes non-bundle artifacts ,358
Move Method,AriesApplicationResolver backed by OBR ,359
Extract Method,Separate out the Isolated and Shared bundle content in DeploymentMetadata.java ,361
Extract Method,Add JPA oersistence layer to the blog sample ,362
Rename Method,Separate the blueprint integration from the container context management ,363
Rename Method,Update FrameworkMBean API method names ,367
Extract Method,Improve filter generation in ManifestHeaderProcessor ,371
Extract Method,Add support for precedence when multiple transaction elements are selected ,373
Extract Method,Implement Framework MBean ,378
Extract Method,Implement Bundle State MBean ,380
Extract Method,MBean exceptions ,381
Extract Method,Override Application-Version in eba-maven-plugin ,386
Extract Method,Update OBR Application Resolver to use OBR 1.6 ,387
Rename Method,Update tranaction strategy to transaction attribute in blueprint transaction project ,389
Extract Method,Web itests - clean-up ,391
Rename Method,"""Make jndi proxy creation more flexible and ",392
Extract Method,Implement an isolated application runtime ,393
Rename Method,Provisioning changes required to support application isolation ,394
Rename Method,Move Subsystems to use org.apache.felix.bundlerepository instead of org.osgi.service.obr ,397
Extract Method,Add update function and plug point to application api ,398
Rename Method,Injecting an entity manager using factory method ,400
Extract Method,Add Generics to JNDI and allow more flexible mechanism to plug URL ObjectFactories in. ,401
Extract Method,Efficiency problem in the method listBundles() of class org.apache.aries.jmx.framework.BundleState ,403
Extract Method,Invoke namespace handler for custom scope elements. ,404
Extract Method,Allow to use different blueprint bundle tracker customizer based on different osgi framework ,405
Extract Method,Use of PlatformRepository in resolvers needs to be more flexible ,411
Extract Method,Make optional references sane ,414
Rename Method,Provide hook point for different Blueprint transaction interceptor similar to JPA hook point for persistence units ,415
Extract Method,Improve proxy support for final classes and final methods ,419
Extract Method,Have blueprint extender process bundles associated with composite bundle when detecting the CompositeBundleFactory service ,422
Extract Method,Improvements to IFile API ,425
Extract Method,Make Aries bundle modelling API consumable by non-OSGi clients ,427
Extract Method,Custom Component Metadata support ,428
Rename Method,Allow plugins to extend the Aries Application Modeller ,432
Extract Method,EJB support in Apache Aries ,433
Extract Method,Extend Parser and ParserService to also accept an InputStream ,437
Extract Method,Usage of a Configuration Admin service within an isolated application framework ,438
Extract Method,Implement extender for detecting persitence units. ,440
Move Method,Provide persistence unit metadata parser ,445
Move Method,Upgrade to asm 4 for java 7 support ,446
Rename Method,Modify default of Export-EJB: header to be ALL when empty string ,465
Extract Method,Add field injection to Blueprint ,466
Rename Method,Implement application support ,467
Inline Method,Improve performance  of saving entities by using insert if possible Mongo Template has this method:  that does a bulk insert Basic repository interfaces that extend from MongoRepository does not have it. They have: this is considerably much slower. saving using  for 6000+ items took a few minutes when write concern is set to journaled expected ability to save it in matter of milliseconds by using workaround: I created a CustomImpl that for my repository class that calls mongoOperation. this is much faster. I am hoping that it .insert would be a built-in feature on the next release.,474
Extract Method,Correctly evaluate CONTAINS keyword on collection properties Currently we just create a regex for the keyword containing. Once the property is collection like we could also go on and check if the requested parameter is contained in the collection of values of the document.,475
Extract Method,"Sort can not use the metamodel classes generated by QueryDSL When working with QueryDSL I would like to be able to sort queries in a type-safe way and not just when building queries. To be able to sort by the metamodel classes generated when using {{QueryDSL}} the method {{getKeyForPath}} of class must be improved in order to allow dealing with metadata names like : customer. lastName customer.metadata.creationDate and so on (where metadata is an embedded document). These metadata names are generated by using the metamodel classes as follows : (metamodel generated from sample project Support in sorting through embedded documents would also be welcomed note that special care must be taken because QueryDSL creates intermediate ""useless"" classes that must be ignored when creating the real path of the desired field to be used.",477
Rename Method,Add DbObject to GeoJson Converter The Spring Data MongoDB project provides some Geo Converters and especially a Geo Json To Db Object Converter. But Db Object To Geo Json Converter is not provided. I have write a custom Spring Converter to manage this feature but i think it could be useful if Spring Data MongoDB provide it. Maybe it could be also a good idea to move the class in the Spring Data Commons project. Other Spring projects could use these types.Thank you.,478
Rename Method,Support partial filter expressions for indexing introduced in MongoDB 3.2 MongoDb 3.2 introduced an new index option which allows to index only documents where a given expression matches.See It would be nice to have the {{@Indexed}} extended to this.,480
Extract Method,"Support missing aggregation pipeline operators in expression support. Most of the operators are not present in Method Reference Node see  Note that pipeline aggregation stages and group accumulator operators"" are already supported by the DSL. I need more operators to support the group by week of year where the week starts on Monday ",482
Rename Method,Add new MongoDB  aggregation operators. like index Of Array reverse Array reduce,484
Rename Method,Add replace Root aggregation stage ,485
Rename Method,Not able to set server Selection Time out on Mongo Client Options using Mongo Client Options Factory Bean Hello I'm using spring-data-mongo XML namespace to configure my mongo client used in my application. I'd like to set the server Selection Timeout  attribute of the underlying Mongo Client Options. But there is no way to set this attribute using the xml namespace. So that we have to deal with the default value (30s) of the plain Mongo Client Options which is way too long in our use-case Could it be possible to add a {{server-selection-timeout}} attribute on XML client-options tag server Selection Timeout method on Mongo Client Options Factory Bean  so that this timeout can be set ?Thanks,486
Rename Method,Not able to set server Selection Timeout on Mongo Client Options using Mongo Client Options Factory Bean Hello I'm using spring-data-mongo XML namespace to configure my mongo client used in my application. I'd like to set the server Selection Timeout  attribute of the underlying Mongo Client Options. But there is no way to set this attribute using the xml namespace. So that we have to deal with the default value (30s) of the plain Mongo Client Options which is way too long in our use-case Could it be possible to add a {{server-selection-timeout}} attribute on XML client-options tag server Selection Timeout method on Mongo Client Options Factory Bean  so that this timeout can be set ?Thanks,487
Rename Method,Allow usage of projection interfaces in Fluent Mongo Operations on read we currently only allow mapping Document back into DTO types. By using Projection Factory within Mongo Template can enable support for interfaces and dynamic projections.,488
Extract Method,Query By Example Find One probe type Mongo Example Mapper inspects the probe type and writes type restrictions according to known types in  Mapping Context. Types assignable to the probe get included in the  operator. My probe type is different than the documents in the mongo collection.,490
Extract Method,Add support for aggregation operators date From String date From Parts and date To Parts ,491
Rename Method,Username password authentication support for Mongo Log Appender java The SD Mongo DB Log Appender currently only supports unauthenticated DB links.Let's change that.,493
Rename Method,Refactor Entity Metadata access in Mongo Query Method Currently the Mongo Query Method uses an Entity Information Creator to create an Entity Information instance being used by the query execution engine top determine the collection to query. As the collection can be determined by the sole inspection of mapping metadata the get Collection Name should rather be on an Entity Metadata extension which would result in a simpler lookup of the metadata and the Entity Information Creator API being obsolete entirely.,494
Rename Method,Polish Bean Definition Parsers to avoid warnings in STS ,495
Extract Method,Remove performance hotspots ,496
Extract Method,Allow usage of Criteria within Update. Usage of Criteria should be possible in {{Update}} to allow more fine grained statements.,498
Extract Method,Improve cycle detection for DbRef's ,499
Inline Method,Improve performance  of saving entities by using insert if possible Mongo Template has this method:  that does a bulk insert Basic repository interfaces that extend from Mongo Repository does not have it. They have this is considerably much slower actual saving using items took a few minutes when write concern is set to journaled expected ability to save it in matter of milliseconds by usingMongo Operation insert(Collection extends Object batch To Save Class entity Class workaround I created a Custom Impl that for my repository class that calls mongo Operation. this is much faster. I am hoping that it .insert would be a built-in feature on the next release.,500
Rename Method,Sort can not use the metamodel classes generated by Query DSL When working with QueryDSL I would like to be able to sort queries in a type-safe way and not just when building queries. To be able to sort by the meta model classes generated when using Query DSL the method get Key For Path of class  must be improved in order to allow dealing with metadata names like  customer Ast Name customer metadata creation Date and so on where metadata is an embedded document. These metadata names are generated by using the meta model classes as follows : meta model generated from sample project Support in sorting through embedded documents would also be welcomed note that special care must be taken because creates intermediate useless classes that must be ignored when creating the real path of the desired field to be used.,502
Rename Method,Add support for min Distance to Near Query ,504
Extract Method,Add support for geometry to support Geo JSON queries. allow Shape s to be converted into Geo JSON format using the $geometry operator within queries.,505
Extract Method,Use org bson types instead of com mongo db. use org bson Document This means incomplete list for queries for queries for queries for Basic DB Object Collection insert for insert Collection replace for replacing update Collection update for updates Update Options for eg. specifying upsert,506
Extract Method,Deprecate non Mongo Client related configuration options in XML namespace We should deprecate all XML elements that currently create {{Mongo}} instances in favor of the ones creating Mongo Client instances attribute to create a Simple Mongo Db Factory with a Mongo Client Uri Mongo DUtils should skip authentication for instances.,509
Extract Method,Provide a collectionName in Mongo Mapping Events I use repository events to update my search index. Search index names correspond to collection names in my database. Often I'm able to recognize the collection name based on the object being saved. But I'm unable to do so when someone pulls the object directly from Mongo Template changes and saves it back.  IMHO events should contain info about collection name where the object is being saved to  removed from. It does not look that difficult see attachment.,510
Rename Method,Add an option to specify the cursor batch Size for repository methods returning streams It would be great if you provide an option to set the  In case of ETL where you process a lot of GB streaming results is already heaven on earth compared to paging. In the Mongo DB Cursor default implementation is set to 0 which means the database chooses it. In my configuration the batch Size seems to be very small. I could observe that when I fetch data from a remote database. Java Mongo DB Driver Batch Size I couldn't verify that overriding the batch Size gives the expected performance boost.,511
Extract Method,Add support for lookup to aggregation. Performs a left outer join to another collection in the same database.,512
Extract Method,Add converter for Currency ,514
Extract Method,Add support for out operand for Aggregation There is a special operator to save my aggregation data into an output collection But to get it to work in Spring Data I do the following trick I think a new Aggregation Operation is required for this. I am going to fix it by myself.,515
Extract Method,Add overload of Mongo Operations stream to take an explicit collection name Right now I am playing with Spring Data MongoDB and I need to have multiple collections consisting of object of the same class. All of the collections can be possibly large so I can not even use Mongo Template because Mongo Template  stream Query Class also does not allow me to specify the collection name. Also it also means that the stream method definition is inconsistent with find All which has a variant allowing you to specify the collection.,516
Rename Method,Reactive support in Spring Data Mongo DB Investigate on reactive paradigms support in Spring Data MongoDB using Reactive-Streams. Create a prototype that includes native support for the MongoDB Reactive-Streams driver.,517
Move Method,Add support for any match mode in query by example ,519
Rename Method,Add Db Object to Geo Json Converter The Spring Data MongoDB project provides some Geo Converters and especially a Geo Json To Db Object Converter. But Db Object To Geo Json Converter is not provided. I have write a custom Spring Converter to manage this feature but i think it could be useful if Spring Data MongoDB provide it. Maybe it could be also a good idea to move the class in the Spring Data Commons project. Other Spring projects could use these types. Thank you.,520
Rename Method,Support partial filter expressions for indexing introduced in Mongo DB Mongo Db 3.2 introduced an new index option which allows to index only documents where a given expression matches. See It would be nice to have the Indexed extended to this.,521
Extract Method,Abstract Mongo Configuraton should allow multiple base package for Document scanning Upon enabling Enable Mongo Auditing I was getting the following failure: Unsupported entity com. Get on risk  registration. Registration Could not determine Is New Strategy. This was happening because the Abstract Mongo Configuration get Initial Entity Set only allows for the scanning of one package. As my packages have been designed feature based rather then layer based I had POJO's with the Document annotation in various packages.  I solved the issue by overriding the Abstract Mongo Configuration get Initial Entity Set with The main change is the addition of get All Mapping Base Packages. As the community moves towards a package by feature design I think this change would be benefit a lot of people.,522
Rename Method,Add support for no Cursor Timeout in Query MongoDB provides the cursor option cursor no Cursor Timeout which allows a cursor to continue until exhausted or closed beyond Mongo's default 10 minutes timeout option.The option could be exposed to the Query and Meta classes and then added to the cursor's options in Query Cursor Preparer prepare,523
Rename Method,Add new MongoDB 3.4 aggregation operators. like index Of Array reverse Array reduce,526
Rename Method,Add Template Wrapper to reduce method overloads on Mongo Template. Use indirections to provide meaningful fluent API calls like,528
Extract Method,Add TemplateWrapper to reduce method overloads on MongoTemplate. Use indirections to provide meaningful fluent API calls like,529
Rename Method,Not able to set serverSelectionTimeout on Mongo Client Options using Mongo Clien tOptions Factory Bean Hello I'm using spring-data-mongo XML namespace to configure my mongo client used in my application. I'd like to set the server Selection Timeout  attribute of the underlying Mongo Client Options. But there is no way to set this attribute using the xml namespace. So that we have to deal with the default value (30s) of the plain Mongo Client Options which is way too long in our use-case Could it be possible to add a server selection timeout attribute on XML client-options tag server Selection Timeout method on Mongo Client Options Factory Bean  so that this timeout can be set ?Thanks,530
Extract Method,Add support for aggregation result streaming It would be great if there was option to stream the result set from aggregation operations Stream option is there but only for find operations I have worked on this and it's working Issues limitations explain option can't be used for streaming result No custom option support for streaming result Out operation works correctly but also returns the stream which can be used to fetch results whereas in original methods  Mapping Results are empty. It would be great if you could review my code and tell me if anything can be changed improved or what's wrong. creating pull request with same id. EDIT created pull request  Thanks,531
Extract Method,Add partial index support to ReactiveIndexOperations. ,532
Extract Method,Add fluent alternative for Reactive Mongo Operations I am a big fan of the new fluent API provided as part of DATAMONGO-1563 and I think such API would be very useful on Reactive Mongo Operations as well. I can contribute the related Kotlin extensions That could allow me to demonstrate the new fluent API in application since it would be perfect for such use case.,533
Inline Method,Allow usage of projection interfaces in Fluent Mongo Operations on read we currently only allow mapping {{Document}} back into DTO types. By using Projection Factory within Mongo Template can enable support for interfaces and dynamic projections.,535
Rename Method,Move to fluent API for repository query execution Moving to the new fluent API on Mongo Operations we should be able to optimize a few things in the repository query execution Being able to define which type to read (for query mapping) and which type to produce should allow us to get rid of the additional step of DTO mapping after read The fluent API returning new instances for intermediate steps should allow us to set up the broad coordinates which type to read e and keep those around rather than deciding about that very late in the execution lifecycle.,537
Rename Method,Query B yExample Find One probe type Mongo Example Mapper inspects the probe type and writes type restrictions according to known types in  Mapping Context. Types assignable to the probe get included in the  operator. My probe type is different than the documents in the mongo collection.,539
Move Method,Support bitwise query operators The criteria builder should support MongoDB [bitwise query operators introduced with 3.2 which allow to match a field against given bitmasks. This feature would be a good addition to the existing,543
Rename Method,Allow document replacements via Mongo Collection findOne And Replace Mongo Collection has no find And Modify method and it seems that find One And Update was used as a replacement. Find And Modify allows field value in the update but find One And Update does not. This breaks the ability to do find And Modify with a replacement document.,544
Rename Method,Add support for mapReduce to ReactiveMongoOperations ,546
Extract Method,Decouple reactive mongo bits from blocking MongoClient ,547
Rename Method,Provide additional options for setting Write Concern on a per operation basis With default Write Concern  NORMAL - do not wait for server errors  update method will return lazy version of Write Result - i.e. we can get update result after another call to database.  Thus we can not do update result check in one call to database - performance problem If application works with Write Concern. NORMAL by default then most effective way to do update with result checking is to call it with Write Concern. SAFE,548
Move Method,Support geting index information on a collection or mapped class. See Getting a List of Indexes on a Collection for the JSON data structure that is returned from this query. That can be mapped to a Java class for easy access/type-safe usage.,549
Extract Method,Allow Collections as parameters in Query When using the Query annotation in a Repository I'd like to do something like this:Currently it tries to run this query: which obviously fails because the strings are not quoted. I believe this could be easily fixed by adding some code in to properly handle Collections specifically Collections of Strings,551
Move Method,Enums can't be used in Criteria This will give json object serialization error message To make the criteria work I have to use the name method of the enum ThanksDonny,552
Extract Method,The nin and all methods on Criteria should take a collection like the in method Criteria's in method takes both variable arguments as well as a collection. However the nin and all methods only accept variable arguments which makes them less useful when for things like finding a list lost values and then using them in a query.,553
Extract Method,Lazy Load for DbRef DbRef's appear to be loaded eagerly Would be nice if there was support for storing DbRef's on a document but being able to lazy (or manually) load them.,554
Extract Method,Adapt new entity instantiation API from Spring Data Commons ,556
Rename Method,Support After and Before keywords for query creation ,557
Extract Method,Improve MongoDbUtils API Mongo Db Api still uses the low level String and char parameters to capture username and password. It should rather take a User Credentials instance.,558
Rename Method,Username password authentication support for Mongo Log Appender java The SD Mongo DB Log Appender currently only supports unauthenticated DB links Let's change that.,559
Extract Method,Grid Fs Template is not setting the file Content Type Hello I can't find a way to set a value for the content type key in the db fs files collection using the Grid Fs Template class. It should be possible setting this value with an overloaded version for the store method something like this: Does it make sense? Thanks Carlo Micieli,562
Extract Method,Unify usage of Sort APIs in Query API The Query API currently works with a dedicated Sort class that is very different from the way the Sort class works in Spring Data Commons. We should allow the Query API being used with the Sort of Spring Data Commons and deprecate the custom one for now and remove entirely in a next major release.,563
Extract Method,Add background attribute to Indexed and Compound Index MongoDB accept since version 2.2 a background indexation. Please add this function to annotation Indexed and Compound Index.,564
Extract Method,Provide support for remove Bydelete By methods like for find By on repository interfaces On the repository interfaces I can define methods like If I want to have the same method for deleting e.g. I have to implement a custom Repository for this. Please provide also automatic interface method support for  like you do already for ,566
Rename Method,Refactor Entity Metadata access in Mongo Query Method Currently the Mongo Query Method uses an Entity Information Creator to create an Entity Information instance being used by the query execution engine to determine the collection to query. As the collection can be determined by the sole inspection of mapping metadata the get Collection Name should rather be on an Entity Metadata extension which would result in a simpler lookup of the metadata and the Entity Information Creator API being obsolete entirely.,567
Rename Method,Add support for new Aggregation Framework Mongo 2.2 has some new aggregation framework features that we could add query criteria support for.,569
Extract Method,Repository  find By Field Ignore Case doesnt work I create a simple user document with an email field all getter setter provided Here my repository its working - i find a user document with  its NOT working  i don’t find a user document with I also try with the first name it also doesn’t work Here my logs I am using spring-data-mongo db 1.3.1 I try to debug but it is in spring-data-common where it is handle the find By strategy lookup right now I am using Query DSL to handle this,573
Extract Method,Allow usage of Criteria within Update. Usage of Criteria should be possible in Update to allow more fine grained statements.,576
Extract Method,Add new field naming strategy and make it configurable through XML Java config My company needs to insert records into Mongo from a Java app and read from a node.js app. In keeping with our existing data models in Mongo we'd like to keep field names in lower case with underscores snake case. I've added XML and Java config attributes for  Field Naming  Strategy and created the class Lower Case With Underscores Field Naming Strategy A pull request is forthcoming via GitHub. I'll be happy to make any changes you see fit.,578
Inline Method,Overhaul automatic index creation Index creation did not consider the properties path when creating the index. This lead to broken index creation when nesting entities that might require index structures. Assume the following structure.  As a proposed solution only those entities annotated with Document shall be inspected. By traversing properties down the structures the path used for index creation can be resolved.,579
Extract Method,Add support for creating text index. ,581
Extract Method,Add support for query modifiers Can we have support for query modifies across the board. For eg I would like to limit all queries to X ms.,582
Extract Method,Allow to pass options to the Aggregation Pipeline Sometimes it is necessary to pass additional configuration into the Aggregation Pipeline.As an example: to circumvent the memory limit of 100 mb for aggregation pipeline stages as of this writing one can pass the allow Disk Use true option to allow sorting on disk for larger datasets.,583
Rename Method,Add support for SpEL expressions in Query We should adapt the SpEL support that we have in Spring Data JPA in Spring Data MongoDB as well.,584
Rename Method,"Enable directory scanning with java 7 feature - WatchService  I would like to offer my implementation to DefaultDirectoryScanner.I extended this class In ""listEligibleFiles"" I'm listening to os events with java 7 feature ""WatchService"" . In that way 1.I can be sure that I'm handling file only this time.(there will be only one ""create event"" for each file.2.A very reliable way to pick files from directories.3.For existing files in directory I implemented init-method that scan all directories with ""Files.walkFileTree"" (also a java 7 feature).the first time i call ""listEligibleFiles"" I'm handling the existing files. From Next time on - I will treat only ""create events"" that came from WatchService.4.In same init-method Iregistered all subdirs of root directory ( that has been supplied through ""directory"" property) with java 7 feature ""Files.walkFileTree"".",585
Rename Method,Add Ability to Customize the SpEL Evaluation Contexts Used Throughout the Framework  For example custom {{PropertyAccessors}}. ,586
Rename Method,JMS: DSL/XML Inconsistent receiveTimeout  The XML parser sets the default to no-wait; the DSL leaves it at the default (infinite wait).They should be consistent but I don't think no-wait is correct for the reasons discussed at the end of the SO answer. Infinite wait is incorrect too. Maybe 1000?On second thought - perhaps no-wait (-1) if we have a {{CachingConnectionFactory}} with {{cacheConsumers}} otherwise 1000?,587
Rename Method,filter defined in scanner gets overwritten by the one defined in FileReadingMessageSource  As described here http://stackoverflow.com/questions/28087753/cant-set-a-different-filter-in-fileinbound-channel-adapter-and-scanner  and confirmed by Artem we can't define 2 different {{filters}} in a {{scanner}} and a {{<inbound-channel-adapter>}} .when you look at the documentation its misleading because you have the feeling you use 2 independent components while actually {{FileReadingMessageSource}} overwrites the scanner's filter config. ,588
Extract Method,"Add pseudo-transaction support to pollers for inbound channel adapters that are not inherently transactional  1) In almost every business case it is a requirement never to lose a message. 2) When an adapter polls for input messages you usually want to persist the progress so when the server restarts it doesn't reprocess everything from scratch. prevent-duplicates=""true"" is currently useless because progress is not persisted.A way of achieving both of these is to delete an input file/email/whatever AFTER it has been committed by outbound-adapter or messageStoreAfterCommit input-deletion by TransactionSynchronization is proposed by Mark Fisher here: But it is difficult to setup and should be supported directly by SI. There are many ways to this - here are a couple of examples:Full example of how it could be easily expressed in a file poller:",589
Extract Method,Add Support to TCP Endpoints to Send/Receive Entire Message  Add attribute 'extract-payload' (default=true) similar to JMS endpoints to tcp connection factories supporting the transfer of the complete message.Notes:If extract-payload is false switch default (de)serializer from ByteArrayCrLfSerializer to Default(De)Serializer (java serialization).Assign a new message id when deserializing a complete message; put the id from the transmitted message into a header tcp_remote_message_id.Support for UDP is covered under a separate JIRA (INT-1808) because it's more complex; those adapters don't currently support pluggable serializers.,590
Extract Method,MessageHistory is not applied for components registered at run time e.g. dynamic Integration Flows  ,591
Extract Method,Make ErrorMessageSendingRecoverer more generic and introduce ErrorMessageStrategy for customization  ,592
Inline Method,Add Scripting Support for <inbound-channel-adapter/>  <inbound-channel-adapter/> currently supports expression ref or nested bean; it does not support a nested <script/> element. SpEL doesn't support escape characters; it would be useful to be able to use: Work-around:,593
Extract Method,"Cannot use Kotlin with Spring Integration 5.0.4 while Java still working (Integration Flows DSL)  All those notations compiled but fail at run time while Spring Integrationcannot find method to use as Route. If config is rewritten in Java - all working.One thing is that Kotlins lambdas are not of synthetic class while Java's areso Spring Integration not sees them as lambdas.At another hand it worked before and some DSL things as `transform` and some other still work with Kotlin lambdas.It can be avoided with explicit method name :While it's very ugly.Does anyone knew solution except ""apply"" or shifting @Configuration with Sp Int to Java? ",594
Extract Method,add ObjectToJsonTransformer.ResultType.BYTES for downstream  Add mode for better efficient with the downstream components who is based on the like ,595
Rename Method,Expose Message ID generation strategy  ,596
Rename Method,Add MetadataValueStrategy to allow to determine the value for MetadataStore not only key  ,597
Extract Method,Set AbstractPersistentAcceptOnceFileListFilter as default filter for AbstractRemoteFileStreamingMessageSource and AbstractInboundFileSynchronizer  Right now there is no default in the therefore on each poll we try to consider **all** the remote files as candidates for local copy ,598
Extract Method,AMQP Outbound - Support the ContentTypeDelegatingMessageConverter  The converter is invoked with an empty object.Use the mapped properties when invoking the message converter so the delegating converter can decide which converter to use.Change the method to do the conversion in the adapter instead of the template (as is the case for the method). ,599
Extract Method,"JPA Adapter: Improve ""flush"" behavior  The typical flush behavior with JPA operations is to executes the flushes (actual execution of the SQL query) at transaction commit. The current implementation is not sufficient in that regard for certain scenarios. For example we want to poll a database for records and at the same time we want to delete the fetched data. If the ""SourcePollingChannelAdapter"" is configured with a Transaction Interceptor the transctional behavior will be wrapped around the *doPoll()* method of the *SourcePollingChannelAdapter*. This may or may not be an issue for a users architecture but the flush (state synchronization with the database) will currently occur AFTER the message was sent to the outputChannel. Thus this may lead to unexpected behavior. This issue is even more pronounced when using DirectChannels (same thread) which means a Transaction commit (including flush) may not occur until after the execution of 1 or more components further downstream. In order to improve the situation and to provide flexibility to the user we need to implement the following additional logic:For the JPA Adapter add an attribute *flush* which defaults to *true*. What this means is that the JPA Adapters will automatically synchronize the persistence state with the database before the actual commit will happen. I think this flush shall happen *once* on the adapter level (not for each individual JPA Operations - e.g. in the case of a Select+subsequent delete: only one flush shall occur). I believe that this attribute should be added as in cases where users want to use multiple JPA Adapters in a flow they most likely don't want to flush the session for each adapter as this may lead to inefficiencies. We need to make sure that we document the behavior and various use-cases exhaustively in the reference manual as well. ",601
Rename Method,Add support for @CompileStatic (and for other compiler options) into Groovy Script components  The is supported already now as a method-level hint.Instead of:To make it a bit easier and less verbose we could utilize the  as an out-of-the-box feature for Groovy script components to attempt to compile them statically.See more information in the Blog Post: With that our scripts can still be as a one line but based on the Java runtime not Groovy already:,602
Extract Method,Improve Messaging Annotations handling for the AbstractReplyProducingMessageHandler in case of  ,603
Extract Method,"Add ""outputChannel"" Late Binding support for the  See related discussion in the Java DSL: Where assumes the channel auto-creation during the phase the doesn't do that with its Even if the feature is more closer to the *consumer* component that should not hurt if we will create the same channel from the Inbound Channel Adapter perspective. That should be treated as an based channel auto-creation logic for the XML  component. ",604
Rename Method,The Jackson2JsonObjectMapper should be consistent with similar from Spring Core around ObjectMapper configuration  The JavaDocs from the As well as the code from the So the should configure its default the same way. ,605
Extract Method,Move spring-integration-mqtt from Extensions to Core  Move module polish add docs ,606
Extract Method,Provide @EnablePublisher support  Although {{PublisherAnnotationBeanPostProcessor}} is defined by Integration infrastructure (for {{@Publisher}}) by default there is no hook to configure {{defaultChannel}} ruther then configure it manually from {{@Bean}} definition. ,608
Extract Method,Add InputStream support to FileWritingMessageHandler  It could be useful if the FileWritingMessageHandler accept InputStream DataInput ByteBuffer and Path as message payloud. ,609
Extract Method,Add iterator ability to the XPathMessageSplitter  ,610
Extract Method,Implement Closeable/Flushable on PropertiesPersistingMetadataStore  ,611
Rename Method,Add support for MessageGroupStore to Redis module  ,613
Extract Method,Match Exception Hierarchy in ErrorMessageExceptionTypeRouter  ,615
Move Method,Introduce @EnableMessageHistory  ,616
Extract Method,Make ExpressionEvaluatingRequestHandlerAdvice DSL-Friendly Add support for channel names.,618
Extract Method,Extend Integration MBean Exports To Include Other Endpoints  The IntegrationMBeanExporter currently exposes MBeans for MessageSource MessageHandler and MessageChannel objects. Certain gateways can behave in a similar fashion to MessageSource objects when configured not to return a reply. This gives an inconsistent picture via JMX; for example switching from to an means one loses the ability to start/stop the adapter using jconsole.Endpoint control via the works fine in both cases because it uses a different mechanism to reach the Lifecycle methods.It would be nice if we could also expose all AbstractEndpoint instances as first class citizen MBeans even if just the Lifecycle methods. ,619
Extract Method,Allow to configure Messaging Annotations method level for MessagaSource MessageHandler components (e.g. FileReadingMessageSource)  ,621
Extract Method,"GatewayProxy does not keep track which method triggered the message flow  I want to try and avoid having to manually define each method in a gateway bean. My current setup is:Each gateway method is multiplexed to use the same HTTP outbound gw in order to unnecessarily create a bunch of HTTP gateways. To make this happen I need to pass along a URL which is unique per method. The usual way to do this would be to define each method in the XML config setup a static header (there's no way to create a static header via annotation) and then read that off in the http outbound gateway but that would leave me defining each method which is messy. In my interface I want to be able to just say something like:and in a later header enricher read that annotation off. There's only one thing missing: the gateway bean does not keep track which method trigged the message. If I had a {""originatingMethodCall"": methodInvocation} reference then it would be trivial. Thoughts? Aside #1: I worked around it by making an aspect and storing what I wanted in a thread-local but that looks an unclean approachAside #2: This would not have been necessary if it were possible to define static headers per method as an annotation.",622
Rename Method,DSL: Consider Making a Final .log Terminal If a flow ends with perhaps it could be just a logging adapter rather than a wireTap (or automatically set the output channel to,623
Extract Method,Use AnnotatedElementUtils instead of AnnotationUtils wherever feasible  This issue is the Spring Integration equivalent of a Spring Framework issue. The following is copied from Status QuoSpring Framework 4.2 introduced support for explicit annotation attribute overrides via however due to historical reasons the Spring code base typically uses the lookup methods in which do not support attribute overrides. h2. DeliverableMigrate from lookups in to methods such aswherever feasible. ,624
Extract Method,Add support for @BridgeFrom and @BridgeTo MessageChannel @Bean level annotations for @Configuration classes  ,625
Move Method,Provide a Mechanism to Configure SpEL PropertyAccessors and Functions with Java Configuration  .you can add PAs to the  but doing that in user code would be too late for many cases where components have already gotten their evaluation context. It would have to be done in a that runs early in the application context lifecycle. I will open a JIRA issue but for now you might find it easier to a small XML snippet into one of your classes.,627
Extract Method,FTP Outbound Gateway should support NLST operation for ls command  If FTP Outbound Gateway command is used with (file names only) as option ftp command should be supported as an alternative way to get file names.The issue is my production FTP server could list directories/files if command is used will returns empty directory / file list.According to my understanding in implementation the command is only a post operation that extracts list of FTP file name after retrieved the file list based on command. In my case this will return nothing and no file name could be extracted.,628
Move Method,Global Wire Tap Pattern Matching Improvements  Options: Add regex support  Add negation to the simple pattern (like was done recently in the and subsequently the Add property with simple patterns or a combination of the above. ,629
Rename Method,In org.springframework.integration.ws.MarshallingWebServiceOutboundGateway the field 'uri' doesn't work the same way the 'deafultUri' works in org.springframework.ws.client.core.WebServiceTemplate  In WebServiceTemplate the field 'defaultUri' can understand URI for both HTTP and But in the field 'uri' only understands HTTP URI. If one needs to use JMS URI then he needs to implement his own But this is not properly documented. It would be great if we could make 'uri' field understands JMS (or any other) URIs update the documentation so that users can easily find out that they need to implement DestinationProvider ,631
Extract Method,There is no way for TcpOutboundGateway to use a concept of pool of TCP connections  In certain cases there is a need to connect to TCP server and keep some number of connections for subsequent use similar to the way JDBC pools work. Despite the fact that tcp-connection-factory has single-use attribute set to false tcp-connection-factory has one and only one TCP connection therefore no connection pooling is possible.Besides that TcpOutboundGateway for each request acquires a semaphore (in handleRequestMessage method) which means that only one thread can send outgoing TCP request at a single point of time.The proposal is to introduce a concept of connection pooling in tcp-connection-factory which would let users to define pool parameters such as initial and maximum number of connections wait timeout etc. The factory might use Commons Pool or some other techniques to implement the pooling.At the moment in order to use more than one TCP connection we came up with a workaround by creating 2 instances of tcp-outbound-gateway and 2 instances of tcp-connection-factory. Since both tcp-outbound-gateways listen on the same DirectChannel round-robin balancing does the trick. ,632
Extract Method,Cover new ApplicationEvent support in the Spring Framework 4.2  ,633
Extract Method,JMX counters use AtomicInteger which is too small and rolls over leading to inaccurate statistics  The use of the AtomicInteger class in the JMX o.s.i.monitor package means that for applications that consume a lot of events the counter can roll over in a relatively short space of time AtomicLong would be a better replacement. ,634
Extract Method,SFTP Adapters Docs  The SFTP Adapters documentation could use an enhancement to the SFTP Outbound Channel Adapter section (as described in the Stackoverflow reference URL) to include a working example of the setRemoteDirectoryExpression() method which shows how one can dynamically compute the directory path based on data in the message (either from payload or headers). ,637
Extract Method,HttpRequestExecutingMessageHandler and Content-Type headers  HttpRequestExecutingMessageHandler (via http:outbound-channel-adapter) populates the Content-Type header on all HTTP requests even when it is not applicable (e.g. GET). As Content-Type is meant to identify the message body for POST and PUT requests it does not make sense to populate it for other HTTP methods (GET HEAD DELETE TRACE optional for OPTIONS). ,638
Extract Method,TCP Add a Mechanism to Allow Connection Subclasses  Provide something like for s so a user can return a subclass for example to wrap the socket's input stream in a ,639
Rename Method,JVM NPE in Selector.close()  Under some circumstance (perhaps an unregistered selector) the JVM throws an NPE when closing a selctor.We currently catch an Check the selector is registered and change the catch to ,640
Rename Method,Implement ConcurrentMetadataStore  Extend as by adding the and methods.Change the to implement Change the to use If returns an existing object (file timestamp) check the timestamp against the remote file timestamp. If they match ignore this file. If they don't match use the method update the key if the value hasn't changed again.This will enable (S)FTP inbound adapters to work in a clustered environment such that only one of the instances will process a file. ,642
Rename Method,ChannelInterceptorAware - Support Removal of an Individual Interceptor  See XD Perhaps 2 methods:,643
Rename Method,"Make user-defined prefix configurable in DefaultHttpHeaderMapper  When setting custom headers http:outbound-gateway is appending ""X-"" for all the headers before making the call. We should have a boolean variable which enables/disables this behaviour. ",644
Rename Method,"Extend groovy:script tag to support a ScriptVariableGenerator strategy  When setting custom headers http:outbound-gateway is appending ""X-"" for all the headers before making the call. We should have a boolean variable which enables/disables this behaviour. ",645
Rename Method,"JmsMessageDrivenEndpoint contains method getComponetType() rather than getComponentType()  The  class contains method a method named I believe that methodname contains a typo and should instead be (note the additional ""n"" before the final",653
Rename Method,TCP: Provide Option to Separate Connection Establishment from Message Flow  Summary was: Provide options to configure TCP client to receive data only (without sending any data to server) ,654
Rename Method,DefaultConfiguringBeanFactoryPostProcessor should not create default beans if they are provided by a parent application context  This would make it easier to provide resources such as a shared thread pool or an error channel in a parent context. Currently this can be achieved by declaring an alias in the child context causing to report true. ,655
Extract Method,Add INFO message when P2P channel has more then one subscriber  ,657
Move Method,jms:channel to support context:property-placeholder properties for concurrency settings  parses the value on the concurrency property so that it can assign both concurrent-consumers and max-concurrent-consumers in the underlying Container. However this happens before PostProcessors can run and therefore the property-placeholder hasn't yet been substituted. As a possible solution can the jms:channel have the same 'concurrent-consumers' and 'max-concurrent-consumers' properties so that they can be passed straight through to Container (allowing for later substitution of properties by ,658
Extract Method,"Add pseudo-transaction support to pollers for inbound channel adapters that are not inherently transactional  In almost every business case it is a requirement never to lose a message. When an adapter polls for input messages you usually want to persist the progress so when the server restarts it doesn't reprocess everything from scratch. prevent-duplicates=""true"" is currently useless because progress is not persisted.A way of achieving both of these is to delete an input file/email/whatever AFTER it has been committed by outbound-adapter or messageStoreAfterCommit input-deletion by TransactionSynchronization is proposed by Mark Fisher here: But it is difficult to setup and should be supported directly by SI. There are many ways to this - here are a couple of examples:Full example of how it could be easily expressed in a file poller:",659
Extract Method,ByteArrayLengthHeaderSerializer should accept two-byte headers  When connecting to legacy systems that use a TCP socket at least in my experience a two-byte header is the norm for message delimitation (despite the provided 4-byte-length being more powerful.)For testing SI I patched the ByteArrayLengthHeaderSerializer adding a constructor that supports that way (patch attached.)IMHO it would be really useful to have that functionality in the distribution. ,660
Extract Method,Polish tests that intermittently break CI  Failed again with the same error on  there seem to be intermittent failure where Fixed,661
Extract Method,Add wildcards support for header-filter  The header filter component can only be configured by providing an array of String Wildcards are currently not allowed in ,670
Inline Method,HttpRequestExecutingMessageHandler and Content-Type headers  populates the Content-Type header on all HTTP requests even when it is not applicable (e.g. GET). As Content-Type is meant to identify the message body for POST and PUT requests it does not make sense to populate it for other HTTP methods (GET HEAD DELETE TRACE optional for OPTIONS). ,671
Extract Method,Add support for MessageStore to Redis module  ,672
Extract Method,Simplify Cookie Handling in HttpOutbound Gateway  Consider the following...server returns a set-cookie header containing Currently the DefaultHttpHeaderMapper maps the headers if possible; this means that without intervention the cookie will be sent back in a set-cookie: header instead of cookie: header.If an integration flow wishes to implement this work flow it would have to use a custom header mapper or otherwise manipulate the headers to move the for the next request. Consider making cookies first class citizens in this scenario by providing an automated (configurable) mechanism - perhaps a subclass of the DHHM to handle cookies appropriately.,673
Rename Method,Add namespace support for Gemfire Inbound/Outbound Channel Adapters  ,674
Extract Method,Add ability to limit connections in FtpSessionFactory  The destination FTP server has a limit for concurrent FTP connections. I use task:executor to parallelize file transfers as I have a router and a number of different destination servers. Thus it's unacceptable for me to limit concurrent connections on task:executor. As a result I'm getting the following exceptions:Caused by: F response 421 received. Server closed connection. It'd be great to have maximal concurrent ftp connections limit on the side of ,675
Extract Method,Add SSL Support for the Tcp ConnectionFactories  JDK SSL Support is built around blocking -so this will only be provided for the implementations SSLEngine can (and will) be used to support SSL over NIO.,676
Extract Method,IP Module Code Improvements  Continued improvements on code style ,677
Extract Method,FileTransferringMessageHandler - Add support for writing files to a temporary directory  The current implementation writes files with a configurable suffix in the same directory.Our current SFTP customers/users prefer to use a temporary directory. The file must be moved from the temporary directory to the destination directory when it's written successfully. To handle this case I created a customized FileTransferringMessageHandler. If you are interested in the changes I have made please let me know. ,678
Inline Method,Improve SimpleMessageStore to address some of the concurrency issues that resulted in INT-2221  Just a placeholder now will provide more details later but should be a simple fix ,679
Extract Method,Come up with the strategy for handling serialization for temp channels  See more comments ,681
Extract Method,AMMR - Add @ManagedOperation to setChannelMappings()  It would be useful if the could be used to replace the mappings in a dynamic router.Consider is not supported by this command processor. If using the Control Bus consider adding Simply adding to enables this capability (using a custom converter).I have the fix and tests; not sure if we want to include (a more robust version of) the converter. Perhaps this converter is a candidate for core? ,683
Rename Method,"Add ""container-class"" option into message-driven-channel-adapter container definition (jms module)  ",684
Rename Method,Tcp Client ConnectionFactory Failover  Provide a mechanism say a comma delimited list of to allow a client tcp-connection-factory to fail over to alternative hosts. If the option is missing use the port from the port attribute. ,685
Extract Method,in the file outbound-channel-adapter add the ability to append to a file if the filename given already exists  We have an application that reads XML from a message queue and writes to a flat file. The naming convention we have to use for the flat file limits us to writing one file per second but the file is allowed to have multiple comma-delimited lines. The filename included the date/time to the second and is dictated by a third party application.It's not practical to rewrite the entire application in Spring Batch.Can a change be made to configure file:outbound-channel-adapter to allow me to append to file with the same name as generated by FilenameGenerator? ,686
Extract Method,Increase Timeout/Diagnostics For When GemFire Won't Start for Tests  ,687
Extract Method,Support ObjectName Patterns in JMX <notification-listening-channel-adapter/>  Currently the listens on a single MBean. Given that the payload of messages produced by such an adapter is a Notification object that normally contains the ObjectName of the source object a single adapter *could* create listeners for multiple MBeans using the method to find all MBeans matching the supplied pattern(s). ,688
Extract Method,Provide a way to pass variables to in line scripts  Currently there is no way to pass custom variables to bind to an inline script. The reason is because if the  element contains a value it cannot also have  children. Currently if you need to bind variables other than payload and header you have to create an external script and reference it. This would be convenient for injecting resolved property placeholders and beans into simple scripts. I'm thinking of something along the lines of the p namespace:Note: The component is Groovy Support but should be Scripting Support,689
Extract Method,Add transaction management support for DelayHandler's schedule tasks  ,690
Extract Method,"Use WebServiceTemplate marshalling methods in MarshallingWebServiceOutboundGateway  While testing a Spring Integration application that uses the I was getting a ""Unable to internalize message"" error on the service the requests were being sent to. I was able to fix this error by overriding the method and call the 's method. In addition to fixing the problem I was having this would also eliminate the need for the classes which are essentially duplicating the functionality that is already available in the ",691
Rename Method,"Improve the File Overwrite Handling for the File Outbound Channel Adapter  We recently added support for file appends with. In order to improve the behavior further we will remove the ""append"" attribute and replace it with a new enumeration called mode providing the following options: Replace - The Default SettingAppend - Same functionality as Fail - Raise an exception when the file already exists (new)Ignore - Silently ignore the message payload file if the destination file already exists (new)",692
Rename Method,Add support for Uri Templating to the <ws:outbound-gateway> for all transports supported by Spring WS  ,693
Move Method,Expose serializer settings for Redis Outbound Channel Adapter  ,694
Rename Method,"Add support for adding/removing individual recipients to the RecipientListRouter  We need to support adding and removing recipients dynamically as well as possibly support RLR with no recipients at its initial state. A typical use case is ""registering an interest in something that is part of the Message"". For example a dynamic Recipient could be created with the following selector expression:Also copying some email content regarding this:_Seems like the simplest thing we could do is provide an addRecipient(..) method. Although we'd probably want to expose it so that it takes a *string* expression and a *string* channel name (as opposed to the current Recipient object which pairs a MessageChannel instance with a MessageSelector instance. Having just Strings would be better if we also add  and let it be driven via Control Bus (although maybe we'd even want to parse a single string for that). Or maybe we should *extend* RecipientListRouter with a new DynamicRecipientListRouter class and provide that one with an explicit channel for the dynamic configuration commands (probably need to support *removal* from the list also)._As far as RLR with 0 Recipients I think all messages would have to be dropped the same way they are dropped in pub-sub channel if there is no subscribers. Essentially RLR plays a role of selector-based pub-sub channel. But because it has logic (selector) its not a channel. ",697
Extract Method,Improve TCP Socket Timeout Handling When Used with a Gateway  Gateway sockets may time out prematurely.Consider an outbound gateway with a connection factory that creates sockets with a soTimeout of 10 seconds and a gateway remoteTimeout of 10 seconds.At T0 a client sends a message and immediately receives the response.At T5 a client sends a message but the server takes 6 seconds to respond.The SO_TIMEOUT will cause the socket to throw a SocketTimeoutException at T10 and the client will timeout at T15 - the reply at T11 will fail to be written.When used for request/reply scenarios the TcpConnection should wait 2 SO_TIMEOUT intervals if the last send on the socket occurred within the first interval.,698
Extract Method,Support local-directory-expression in (S)FTP Outbound Gateway  Currently you can use the gateway (LS) to recursively list remote files but the GET command will flatten the directory locally.By adding support for expressions in the localDirectory we could support reconstructing the directory tree locally. ,699
Extract Method,Conditional expression evaluation within a logger-channel-adapter  It will be better to evaluate an expression in LoggingHandler only if the corresponding log level is enabled. ,700
Extract Method,Enhance 'Dispatcher has no subscribers for channel xxx' to Include Context Id  When an application uses multiple contexts with the same channel name it can be challenging to determine in which context the exception occurred.Adding the context id to the message will allow easier debugging (if the application sets the context id). ,701
Rename Method,Make AbstractReplyProducingMessageHandler.onInit() final and have custom method like such as doCustomInit()  Make final and have custom method like such as (with a NoOp in the abstract class) to prevent mistakes created by user when extending ,702
Extract Method,Consider Adding a Mechanism to Control the number of subscribers to a DirectChannel Globally  A number of features might benefit from the ability to set default behaviors at the or even application level.For example see INT-2285 - globally limit the number of subscribers to a to 1 instead of (effectively) infinity.Also - default behavior when 'late' replies arrive at a ,704
Extract Method,Add Redis Queue Channel Adapters  ,705
Extract Method,The http outbound gateway is missing a URI variable provider  Currently the http outbound gateway expects a hard-coded map of uri variables. When multiplexing from different messages there is currently no way of declaring such variables dynamically by invoking an SPeL expression or providing a factory method.The scenario here is:Gatewaymethod1 setting header URL to both feeding into a http outbound gateway. The current system requires the http outbound gateway to preset both uriA and uriB.An improved version would make the http-gateway ask a helper method to provide the uri variables dynamically at run time in a fashion similar to being able to craft the uri via a url-expression.,707
Extract Method,AbstractRemoteFileOutboundGateway - Add support for Put  Currently the upload of files is not supported. Implement put and mput. ,708
Move Method,imap-idle-channel-adapter : is there any possibility to add a new feature in imap-idle-channel-adapter to throw an exception/error if re-connection is not happned before (mail.imaps.timeout) given timeperiod  we are writing an integration component which reads emails from 2010 exchange server and creates service tickets on behalf of customers.initially we used inbound-channel-adapter to read emails from exchange server its working finethese days we are facing some issues.some times exchange server is not available due to infrastructure issuessome times we are seeing connection issues because of exchange server cluster configurations.above two situations channel adapters are facing issues to read messages from INBOX.until we see the logs we are not in a position to say what went wrong. support team doesn't know what happening.to over come this we added below configurationusing this errorChannnel message we are able to notify the support team.because of imap-idle-channel-adapter rich features we started using imap-idle-channel-adapter. using this we are not able to recive errormessage when connection is dropped. imap-idle-channel-adapter currently its writing a WARN message and trying to reconenct is there any possibility to add new feature in imap-idle-channel-adapter to throw an exception/error if re-connection is not happned before given timeperiod ,709
Inline Method,Provide a hook to easily configure custom PropertyAccessors for SpEL EvaluationContext  Even if it looks very easy to add custom via there is no need to configure it at all as it is registered with AC by framework.And there is need a hook to inherit custom from parent AC. ,711
Inline Method,Improve GroovyScriptExecutingMessageProcessor performance: avoid synchronization where it is possible  ,712
Extract Method,Add Simple get() to RemoteFileOperations  The current method requires a message (and callback).Add for cases where a message is not available rather than forcing the creation of a message just for the purpose of ,714
Extract Method,AbstractMessageChannel/UnicastingDispatcher Improvements  Channel: Move payload conversion and interceptors inside try block so exceptions (if any) are wrapped in a MessagingException if necessary.Do not iterate over empty datatypesDo not iterate over empty interceptorsDispatcher:Remove the read/write lock around the load balancer; it is not neededOptimize for a single handler - skip load balancer ,715
Extract Method,Add Reference Documentation for JDBC i-c-a select-sql-parameter-source  Currently undocumented (aside from the schema). ,716
Rename Method,Provide @EnableIntegrationMBeanExport support  Similat to but for the registration of bean. ,720
Extract Method,Improve Aggregator Performance  Avoid fetching all messages for consideration by the release strategy.Subclass the aggregator and either wire it up as a and reference it from a or consider using a to swap out the class name in the bean definition.,723
Extract Method,Consider lazy resolving of MessageChannels for Integration components from Lifecycle start() not afterPropertiesSet()  ,724
Rename Method,"The PointToPointSubscribableAmqpChannel should not declare Queue if ""queueName"" is provided  ",725
Extract Method,Add a status-code-expression attribute to an http inbound-channel-adapter  Add a status-code-expression attribute to an HTTP inbound-channel-adapter that would allow a developer to override the default 200 being returned from the channel adapter. ,726
Rename Method,Add Reactor's Promise<?> Gateway's return type support  ,727
Extract Method,Remove final Modifier AbstractEndpoint.stop(Runnable callback)  The callback is invoked immediately preventing a subclass from deferring the callback invocation. ,728
Extract Method,JMX counters use AtomicInteger which is too small and rolls over leading to inaccurate statistics  The use of the AtomicInteger class in the JMX o.s.i.monitor package means that for applications that consume a lot of events the counter can roll over in a relatively short space of time AtomicLong would be a better replacement.,730
Rename Method,Adding support to PacketExtension for the XMPP outbound adapter  Currently the format of an XMPP message is hard coded in handleMessageInternal method of ChatMessageSendingMessageHandler. It would be useful to expose support for custom formatting of the message currently Smack support this by PacketExtension.A workaround is to exploit the behaviour of handleMessageInternal  which will do a simple pass through if the Message payload is of type org.jivesoftware.smack.packet.Message however this is not explicitly cited in the documentation and it may change in the future,731
Extract Method,"Add Expression setters to components where only String ""expression"" variants exist  ",732
Extract Method,Optimize performance of FileWritingMessageHandler  The performance of the slow when setting the.It would be helpful if that class could be optimized to perform better for the use case where user always wants to write to the same file or a timestamp-based file name for multiple messages on a given flow.More info of the specific use case and performance timings can be found here on the reference URL of this JIRA. A few things to perhaps consider offering Expose a  attribute to let users tweak optimization themselves Expose a attribute  Expose a attribute  Should there be a global setting for a default value of  and ? Documentation should be added to the method and to any newly exposed methods that make the user aware of the risk of data loss in the event of a power failure when you have data buffered in memory. ,734
Rename Method,Publish aTcpServerConnectionExceptionEvent From Server Socket Errors  Currently s are published when exceptions occur on established sockets but there is no even published on server socket errors (such as bind errors).Add a new event type and publish is from exceptions. ,736
Inline Method,"Remove ""Self-Closing"" TCP Behavior  Move ""single-use"" connection closing from the connection to the user of the connection (endpoints). ",739
Move Method,Add a Process Barrier Component  Several use cases have come up where it is desirable to suspend a thread (e.g. a container thread to delay message acknowledgement) until some async activity has completed. See the referenced stack overflow question/answer for one such use case. Consider adding a new component (e.g. whereby a thread can be suspended until some message with the same correlation arrives. In most cases the suspended thread will not need a message but we could say send a collection of the two messages to the output channel (which can be of course). In the cited use case the output of an aggregator would be sent to the barrier to release the container thread. We need a mechanism for the signaling message to cause the suspended thread to throw an exception - perhaps if the payload of the releasing message is a ,740
Rename Method,BridgeHandler - Don't Create a New Message  should override  returning false,741
Extract Method,Http Inbound Gateway Timeout  Currently a on the HTTP inbound gateway returns a 200 OK. Invoke the error flow (if present) for a timeout Return a 500 by default but allow customization - e.g. to 504 ,744
Extract Method,"FtpSession does not allow list or listNames without a path  requires a path be passed to both the ""list"" and ""listNames"" methods. The path must have text.The underlying Apache FTP client allows a file listing to be performed without a path (so it is performed in the default folder after logging in to the ftp server). The FTP protocol also does allow a ""LIST"" command to be performed without an explicit folder path.I am currently integrating with two commercial FTP servers that require LIST to be performed without a folder path but I can't achieve that with the current version of FTPSession.I'm happy to make the code enhancement myself and perform a pull request. For reference I would change lines 66 to 75 to be:",746
Extract Method,int-http:inbound with no Content-Type  Hello I imported spring-int-basic-http example are ran the request using Fiddler. Here is my URL and request header SNAPSHOT/receiveGatewayThe response is blank. If I add a request header:Content-Type: text/plainI get response from server. Please help me with this. I have a client application which is hitting the URL without passing Content-Type and hence I am unable to read the request body. ,747
Extract Method,Add preserve-timestamp to FileWritingMessageHandler  ,748
Extract Method,Expose the Runtime Object Model as a Map  See the barrier sample.Add getters as needed to avoid the need for reflection,749
Extract Method,Update remote file permission through sftp file adapter  We are using the spring integration sftp outbound adapter to sftp a file to a remote server. The receiver has mentioned that the file we ftp needs to have read-write permission. This is currently not supported in spring integration ,750
Rename Method,Get rid of explicit script class name and rely on the GroovyClassLoader logic when the class name isn't provided  ,751
Extract Method,LoggingHandler: add Level variant ctor and provide an Expression variant setter for better JavaConfig support  ,752
Extract Method,@GlobalChannelInterceptor and Property Placeholders  The adds the bean definition for the wrapper after the PPC has run.Ensure runs before the properties are resolved or document the limitation and how to use SpEL instead - see the referenced SO question.,753
Rename Method,The directory should not be repeatly defined in both WatchServiceDirectoryScanner and <int-file:inbound-channel-adapter />  The requires the be defined as constructor. If this scanner be injected in the  attribute has to be defined again in once more. I think the checking rule should be relaxed such that developer only has to defined once. ,754
Extract Method,Consider Adding ErrorChannel Sources to the Object Map  Gateways MessageProducers Pollers,755
Extract Method,Let WireTap to resolve the target message channel later via the provided channelName isntead of its instance  ,756
Extract Method,Add Router Support to Graph  ,757
Rename Method,RmiOutboundGateway needs to facilitate the propagation of the security context  Spring Security Remoting provides the classes and The factory (or a similar custom implementation) can be injected into the so that the Spring Security Context is propagated from the RMI client to the remote RMI server thread.should allow the configuration of its so that the security context is propagated. ,758
Extract Method,AbstractInboundFileSynchronizer doesn't restore the remote path for local file  In case of AWS S3 protocol we may have tree-like structure:Where actually all the nested paths are artificial and we have a complex key with  for the target file. Anyway that would be good to let to restore such a structure locally even if we can't do that with (S)FTP because there we scan only the current directory without recursion. The solution looks like fully similar to the one in the In case of AWS S3 protocol we may have tree-like structure:Where actually all the nested paths are artificial and we have a complex key with  for the target file. Anyway that would be good to let to restore such a structure locally even if we can't do that with (S)FTP because there we scan only the current directory without recursion. The solution looks like fully similar to the one in the ,759
Extract Method,File Synchronizer - Fetch One File Per Poll  The remote file synchronizer pulls all files when polled.For large files it would be helpful if the synchronizer can be configured to limit the number of files it retrieves on each poll. ,760
Extract Method,IMAP Message Rendering Consistency  A pop3 just renders the mail body.An imap renders some headers:Since 2.2 we have wrapped messages in an this is done to eagerly fetch the message but causes an IMAP mail rendering to be just the body (like with POP3). This is exacerbated by the new 4.3 feature to use a header mapper which causes on the original message to be performed on the mime message rendering the additional information in the payload. Consider delegating to the field to restore the normal IMAP behavior to IMAP messages.We should also add an option e.g. to the mail receiver to allow reverting to the current state - there is some value in providing consistency across POP3 and IMAP - if this flag is set we should also change the rendering with a header mapper to be consistent with this mode as well.,761
Extract Method,SPCA Source Proxy Applied to All Methods  If the source is already a proxy we only advise the method. If it's not we advise all methods which is incorrect. It would be an improvement to consistently use a and remove the conditional method name test in the ,762
Extract Method,Pagination for mongodb inbound adapter  Would like to have a new numeric property 'page-size' on to set the limit on the find query. Default: -1  no limit. Would you welcome such pull request? ,763
Rename Method,Make ExpressionEvaluatingRequestHandlerAdvice DSL-Friendly  Add support for channel names. ,766
Rename Method,FTP Inbound Adapter/Outbound Gateway supports Spel for filename-regex  In my use case the filename regular expression is constructed on the fly.The existing attribute and could only allowed static patternIs it possible to support Spel expression attribute for FTP Inbound Adapter and FTP Outbound Gateway similar to the following?,767
Extract Method,Compiled SpEL Improvement for MMIH  In XML one can define a {{exception-type-router}} to create an easy routing based on the type of exception.This is currently not as easy when using the Spring Integration Java DSL (haven't checked the Groovy and Scala versions!). I expected to be able to use the default route but that was problematic.This is problematic as the {{Exception}} is wrapped in 1 or more {{MessagingException}} s depending on the level it occurred on. To fix one would have to manually define a and pass it to the function.Although it isn't that hard to do it would be nicer if we could use the DSL for it with a for instance function. ,769
Move Method,Provide a Mechanism to Configure SpEL PropertyAccessors and Functions with Java Configuration  you can add PAs to the  but doing that in user code would be too late for many cases where components have already gotten their evaluation context. It would have to be done in a that runs early in the application context lifecycle. I will open a JIRA issue but for now you might find it easier to a small XML snippet into one of your classes.,770
Rename Method,Add JSON Representation of FileInfo (File Streaming Inbound Adapters)  ,771
Extract Method,JsonPropertyAccessor cannot return array values  The can be used to retrieve specific indexed values from a JSON array but doesn't really support returning the whole array (e.g. to be used in a subsequent expression or iteration) because the result is not returned as a list. Specifically: wraps with but that doesn't offer any 'list-like' functionality. If it was wrapped in an instead then it seems to work (and the workaround of specifying array indexes as strings isn't needed any more). Note that I found this when trying to use the with a Thymeleaf so this might not be much of a concern to Spring Integration anyway.Prototype fix (I can create a cleaned-up pull request with tests if you like):,772
Extract Method,Support Custom Bean Naming  When configuring a flow such as I would like to be able to set the bean name of the generated listener container bean and/or have it related to the flow bean name.does not allow me to do that and we end up with a bean name like...,773
Rename Method,Provide DSL version of <int:exception-type-router />  In XML one can define a to create an easy routing based on the type of exception.This is currently not as easy when using the Spring Integration Java DSL (haven't checked the Groovy and Scala versions!). I expected to be able to use the default route but that was problematic.This is problematic as the is wrapped in 1 or more {{MessagingException}} s depending on the level it occurred on. To fix one would have to manually define a and pass it to the function.Although it isn't that hard to do it would be nicer if we could use the DSL for it with a for instance function. ,775
Extract Method,Provide Stream and Flux splitting support  JDK 9 supports It would be great for SI to support this.,776
Extract Method,Allow additive header black listing  This is a small enhancement over  the current solution allows to set the headers but not to add headers to an existing configuration so a user can only overwrite the existing list. At the very least it should be possible to consult the existing set of excluded headers so the user can merge the additional headers in the existing set. ,777
Extract Method,Allow additive header black listing  This is a small enhancement over the current solution allows to set the headers but not to add headers to an existing configuration so a user can only overwrite the existing list. At the very least it should be possible to consult the existing set of excluded headers so the user can merge the additional headers in the existing set. ,778
Extract Method,Support Configuring AbstractInboundFileSynchronizingMessageSource with a Custom Scanner  For example allow injection of a custom or expose it so the scanner can be changed. ,779
Extract Method,Allow a user to set the id of the DefaultLockRepository via constructor  Currently the id of the is set to a random at instance initialization time. A good feature is to allow the id to be set via the constructor. This allows the user of the lock repository to set the to something meaningful to them. ,780
Extract Method,Provide a mechanism to configure the MessageHandlerMethodFactory  Currently  is created in   for each annotated operation while it only needs a single instance to create (s) for each annotated operation. So this in itself could be a nice improvement to SI.should probably also be exposed as a bean to allow other frameworks (especially in the world of Spring Boot) to interact and/or override it. For example one of the frameworks that builds on SI is Spring Cloud Stream and since it defines its own set of annotations for data handling operations it too creates an instance of with its own set of which do not match the ones provided by SI at times producing different results simply based on how operation is annotated So it would be nice if SI were to either expose or look for (or both) bean thus allowing it to be shared ,782
Extract Method,Apply MetricsFactory for late-bound components.  Apply perations to components registered after context ,783
Extract Method,"Cannot use Kotlin with Spring Integration 5.0.4 while Java still working (Integration Flows DSL)  All those notations compiled but fail at run time while Spring Integrationcannot find method to use as Route. If config is rewritten in Java - all working.One thing is that Kotlins lambdas are not of synthetic class while Java's areso Spring Integration not sees them as lambdas.At another hand it worked before and some DSL things as `transform` and some other still work with Kotlin lambdas.It can be avoided with explicit method name :While it's very ugly.Does anyone knew solution except ""apply"" or shifting ",784
Extract Method,MessageHeaders.ID could be automatically mapped to AmqpHeaders.MESSAGE_ID  Currently  sent through an AMQP outbound adapter the value of these two fields gets discarded.Altought IDs could be fixed by turning on implicit ID generation on the descendants of  that comes with a side effect which is difficult to debug.The ID of the sent and received messages will defer and it takes some time to debug the call hierarchy from  understand why.That said I'd propose to map ID and TIMESTAMP fields automatically and add the option of defining them as transient fields. ,785
Extract Method,Consider use of unlink methods from RedisTemplate in the doRemove method of RedisMessageStore  In there is this method which is called when keys are removed from Redis upon releasing a MessageGroup.For removing keys form Redis there are an alternative RedisTemplate's unlink methods which allow more performant operation which can be used for the same purpose.However unlink operation is only available from Redis 4.0 and above so if backward-compatibleness is prioritized this can stay as is but preferably unlink if it is supported by the Redis in use. ,787
Rename Method,JMS: DSL/XML Inconsistent receiveTimeout  The XML parser sets the default to no-wait; the DSL leaves it at the default (infinite wait).They should be consistent but I don't think no-wait is correct for the reasons discussed at the end of the SO answer. Infinite wait is incorrect too. Maybe 1000?On second thought - perhaps no-wait (-1) if we have a   ,788
Extract Method,Enhancement for Spring Integration DSL to be able to declaratively handle contentType when it is used to define handlers in Spring Cloud Stream environment.  When Spring Cloud Stream performs deserialization of byte at consumer endpoints the method argument for a handler annotated with is used to determine which class type it converts the byte received. The determined class type becomes an argument to a Converter which is also selected by the value of contentType of a Message header at runtime and the actual deserialization is performed by the Converter. For mime types that are unknown to vanilla setup Converters for those mime types need to be provided by means of defining as Bean in a Configuration or through auto configuration process.  from spring-cloud-stream-schema is one of those that is auto configured. When handlers are implemented using Java Config style programming one can write  annotated methods with an argument which is used by the framework for registering components that perform the byte to Object conversion behind-the-scenes. Since what developer needs to take care is to define  method and nothing else other than properly setting up the  spring-cloud-stream-schema module and relevant properties one can say that Converter is defined and configured in a declarative manner. However when using Spring Integration DSL style Converters need to be configured imperatively. Here is an example of using With Spring Integration DSL building the mechanics of message flow has become much more intuitive from the perspective of programmers and  it has become more preferable way to define message flows than Java Config. This request is to enhance the Spring Integration DSL to handle the mimeType conversion declaratively. ,789
Rename Method,Roo generated Spring MVC apps should be deployable on GAE Currently GAE raises a compilation exception of the Roo generated jspx files in a GAE environment. The problem is the use of jasper runtime in the current version of GAE alongside geronimo . This means that GAE have an API which allows JSP and an implementation of Jasper which only supports JSP 2.0. Spring Roo generated MVC apps use a number of JSPfeatures mostly EL related which are supported by all current Web containers but not GAE As such GAE deployments are currently not supported by Roo generated Spring MVC applications. However we are in contact with Google to resolve this issue as soon as possible. As a workaround Roo could ship a custom tag library (potentially with reduced functionality to support JSP  Another issue which needs to be resolved is the full support of the default GAE data store. This will require changes outlined in to fully support references between domain objects.,790
Extract Method,Ensure exception description is always rendered when development is false Roo's DefaultProcessManager is responsible for rendering exception messages that occur when a process is executing. The messages are highly detailed when development mode is true When development mode is false currently the result of Throwable get Message is rendered. However if Throwable getMessage is null or empty no text is displayed This can result in confusion for users who may encounter a bug in an add-on particularly Null Pointer Exceptions,791
Rename Method,Allow separate installation of new languages for a MVC/JSP scaffolded application Based on the comments in it is desirable to install additional languages into a MVC/JSP scaffolded application via a separate command The implementation should be flexible enough to support easy addition of support for new languages.,792
Extract Method,Roo shell should inform user of add-ons which may offer missing commands The work was in preparation for being able to display more meaningful messages to the user should they invoke a command which is not installed locally but may be available from a specific add-on known via the Roo add-on repository This task is to implement this search feature.,793
Rename Method,Request Factory naming convention tweaks After watching Bob's confusion figuring out RequestFactory he's suggested some name changes. I would like to get these into but only after RayC and Amit s big changes land The last is the informal name for the objects that hang right off the Request Factory and mostly shows up in generated java doc and the source for Request Factory Generator. E.g. in requests employee find All Employees  the employee all returns what should be called a request builder.,794
Rename Method,Move OBR-related services into dedicated module Currently the SimpleParserComponent loads the OBR indexes and uses them for automatic resolution of available commands. It is desirable this functionality be moved into a dedicated module so it becomes available for other search-related use cases.,795
Extract Method,Support multi module Maven projects This is the feature I would like to see in the future Below are the layout I can think for now Initially Roo will create parent core and webapp projects parent project This is pom project and the place where we call the Roo shell and use its command to create modules. Module's name format is something like  project name module- or  project name plugin  The modules list in this project pom is handled by Roo generator The list contains: core all module and webapp projects  All the modules inside will inherit the parent project The pom of this project also has dependencies of application frameworks like spring  jpa Maybe we can use this project to build and run the full tests. core project It contains main application contexts and reusable codes shared between module projects: helpers utils base classes general auditing aspects like loggers module project can be one or many jar  The module project contains the main components like controllers models services daos templates and resources which belong to the controllers of the module. Module project has the core project as its default dependency. The dependencies between module projects are managed by the developers webapp project war It contains web configurations It doesn't contain any component codes templates But it contains the web resources that are shared between module projects. The dependencies in webapp are automatically handled by Roo generator during module project creation. The dependencies in webapp includes: core and all module projects. During the packaging all module projects will be jar-ed and stored in WEB-INF lib of the war. ,796
Extract Method,Labels in application.properties getting overwritten at Roo startup Info from the forum thread I have got an entity that is named AI Document to avoid collisions with the DOM Document class. All is fine and dandy but the labels that Roo has come up with for the controller scaffolding are not what I want displayed in the UI It's displaying AI Document instead of Document When I manually go and update the label text in the application.properties file Roo doesn't complain  but when Roo starts up the next time it scans the project and I see this line displayed Aargh. Roo just went and reverted the labels.I have ve noticed this behavior on several labels in the application properties file not just the label for AIDocument.,814
Rename Method,Implement support for reference fields in entities for addon-gwt ,815
Extract Method,Post code refactor and clean up ,816
Extract Method,Consolidate the 3 public subclasses of RequestObject There can be only one. Death to RecordListRequest long live Request Object List Employee Proxy Bonus points if it gets named Request instead of RequestObject,817
Extract Method,Integration of Roo with bundlor install bundlor and associated commands The attached patch extends roo with a Bundlor add on. The root command is install bundlor which generates a template mf file and updates the pom to include use of Bundlor during the packaging step Once bundlor is installed for a project there is a set of commands that become available to manage the template  Full command list install bundlor bundlor nonexported packages to control which packages should be privatebundlor version package exports bundlor configure imports bundlor add explicit import ome notes on the patch Adding bundlor to a project required not only updating dependencies but also updating build plugins. I've extended the core project model to cope with this.The current version of bundlor drags in snapshot dependencies for which I had to add new repositories to the master pom file When bundlor gets to M5 these should no longer be needed The addon bundlor project depends on a couple of external libraries outside of the Spring Roo project. I had to explicitly add these as dependencies in the bootstrap project in order for the roo-dev classpath to be properly generated. Was there a better way The bundlor commands update the template mf file. Some of the time when they execute roo puts out a Managed ROOT template mf message but other times it doesn't. The file is always correctly updated. Not sure why this is Bundlor currently doesn't work due to maven snapshot hell if you run mvn package you will get a NoClassDefFoundError. The roo addon is doing the right thing this is an issue with Bundlor itself that will be fixed in M5. This patch is against revision 156 of trunk Note in this build of roo I also noticed that roo create project foo followed by mvn package causes a build failure because there is no web xml file,820
Extract Method,DBRE multiple schema support At the moment we allow only one database schema per project in DBRE What is the problem to support multiple For instance At the moment second script command wipes out all generated artifacts created by first command.,821
Inline Method,DBRE to generate proper nullable length precision and scale for all @Column We already have this information available in dbre xml  We just need to push it accordingly to appropriate Column annotation attributes.,822
Inline Method,Add addon to deploy and run Roo projects on Google App Engine infrastructure ,823
Extract Method,Allow creating multiple flow definitions in the Web Flow add-on ,824
Rename Method,Make File Converter respect Shell get Home convention reports an issue caused because File Converter does not respect the Shell get Home conventions required by STS' embedded Roo feature. File Converter should be changed accordingly.,825
Extract Method,RooBot client The new RooBot client addon provides an interface with the RooBot add-on registration service for Spring Roo add-ons Several new commands are introduced This add on also contributes a Add On Finder implementation which supports suggestions of add-ons for currently unknown commands.,828
Inline Method,Introduce all JPA type annotations via AspectJ ITDs Currently when adding a new entity both the Entity and Roo Entity annotations as well as the Table and other type annotations are created in the java file This is unncessary confusing and adds clutter to the java file This change will cause the Roo Entity annotation to trigger the creation of the JPA type annotations in the entity ITD instead. New attributes will be added to the Roo Entity annotation.,829
Rename Method,GWT's ValueBoxEditorDecorator isn't ready for primetime The EditViews shouldn't use it.,831
Extract Method,Add support for Firebird Database (jaybird) This issue is to posible support for Firebird Database.I make tests to include manually but i can´t work properly. I create a project with hibernate and mysql support Change pom.xml I put jaybird (is Maven Central library in replacement mysql lib.I add dependency in pom This dependency is correctly added en WEB-INF lib Next chage in persistence.xml changes database propertiesI add this lines in database properties database password masterkey Compile the project succefull and run but i have much errors.Thanks,832
Extract Method,Add DBRE support for Firebird database To use DBRE with a Firebird database the driver needs to be OSGi wrapped. Note also that the latest version of Firebird in maven central is and it requires or above due to the dependency on java sql SQL Client Info Exception class which is not part of the JDK 5 API.,834
Extract Method,Post RELEASE code refactor and clean up ,835
Extract Method,SimpleParser should report error if user presents a command option unwanted by the command It is currently possible for a user to enter commands with command options that the command doesn't actually use. For example this is an illegal command Simple Parser should detect options presented that the resolved command doesn't use and report an error to the user without executing the command. The message should probably suggest they use tab assist and or the help command to view the legal options for the command Initially assigned to Alan as he has done recent Simple Parserrelated work,837
Extract Method,Enhance binding information in JavaType It is useful to store additional metadata within Java Type concerning whether a resolved Java Type represented a known type variable or not.,838
Extract Method,"scope option in ""dependency add"" command A very simple and obvious feature request. Being able to apply standard maven dependency configuration Add the optional scope parameter to the dependency add command in the roo shell Default value of course compile optional other values test provided runtime No need to manually set the scope of test libraries in the pom",839
Extract Method,Improve display of add-on information for various addon commands Improve display of add on information for various add on commands such as  addon list and addon info,840
Extract Method,Improve display of add-on information for various addon commands Improve display of add-on information for various addon commands such as addon list and  addon info,841
Rename Method,Add 'addon search' command to allow flexible addon discovery The new 'addon search' command should have the following attributes to facilitate Roo add-on discovery requiresDescription comma separated list of search terms  required linesPerResult maximum number of lines per add-on optional max Results maximum number of results option trustedOnly display only trusted add-ons in search results optional compatibleOnly display only compatible add-ons in search results (optional requires Command display only add-ons which offer the specified command optional,842
Extract Method,Roo core changes to facilitate  There are a number of changes that have had to be made to Roo core in order to complete  These include but aren't limited to inner types initializers parameter varargs more complete import model parsing type directly from a String nested types.,843
Rename Method,Clean up GWT addon source The GWT addon source formatting is all over the place and should be reformatted.,844
Extract Method,DBRE to map MySQL tinyint(1) and or bit SQL types to java.lang.Boolean for GWT DBRE maps MySQL tinyint and bit types to boolean requires java lang Boolean for Request Factory Request an option to support this scenario.,845
Extract Method,Boost performance of persistence setup command The persistence setup command completes in an unacceptable time due to the number of xml read and write operations. This improvement request will improve the efficiency of said operations.,847
Rename Method,ItdType Details Providing Metadata Item and Physical Type Metadata to adopt a parameterized T extends Member Holding Type Details> superclass with get Member Holding Type Details():T method ,848
Rename Method,Add a MetadataCache.put(MetadataItem) method for use by Roo infrastructure types ,849
Move Method,Improve metadata tracing features ,850
Extract Method,Infinite metadata loop detection should retry at completion of current metadata retrieval stack ,851
Extract Method,Introduce notify For Generic Listene r(String) method to allow Abstract Itd Metadata Provider to gracefully handle generic listeners ,852
Move Method,Conversion Service should observe metadata immutability and dependency injection conventions ,853
Move Method,Add BeanInfoUtils with static methods to assist eliminate use of Bean Info Metadata ,854
Extract Method,Modify org.springframework.roo.support.util.WebXmlUtils.addFilterAtPosition to allow adding dispatcher tags to filter-mappings As a Roo Addon developer I want an easy way to add dispatcher tags to filter-mappings Please modify Web Xml Utils add Filter At Position to allow this would generate:,855
Extract Method,Improve Roo messages generated from persistence setup command While Roo outputs messages like Updated ROOT pom.xml not all these messages indicate what actually changed. This improvement will add the actual type of change to the message for example Updated ROOT pom.xml Added filter,856
Extract Method,Abstract Itd Metadata Provider should permit more flexible class-level downstream dependency resolution Currently AbstractItdMetadataProvider supports dependency registrations where the downstream is Instance specific meaning the target downstream MID for a notification is known in advance and presented as part of the metadata notification from the Metadata Dependency Registry A null downstream dependency can be presented via a generic listener and handled by the new mechanism. However this approach means all metadata events in the system pass through the Metadata Provider which is inefficient if only a particular subset of known in advance metadata notifications are desired. A class level downstream dependency of the MetadataProvider get Provides Type if and only if the upstream dependency is a PhysicalTypeIdentifier notification. In this case the AbstractItdMetadataProvider unwraps the upstream notification's MID (ie a Physical Type Identifier and converts it into a local Metadata Provider instance specific MID. Sometimes it is desirable to listen for all metadata notifications of a particular type just as we do for Physical Type Identifier. If every metadata notification will pass to an instance specific MID the first approach listed above works fine. If the upstream metadata is a Physical Type Identifier the last approach above will accommodate it. However it is not presently handled if the upstream is a non Physical Type Identifier and the downstream is not instance-specific. We should add a new method to AbstractItdMetadataProvider that subclasses can override to resolve a class-level downstream into an instance-specific downstream if the third scenario above is occurring. The AbstractItdMetadataProvider code should handle the PhysicalTypeIdentifier use case and be available for super calls if desired.,858
Extract Method,Introduce add-on / component upgrade functionalities to Roo shell Currently it is fairly inconvenient for a user to determine if a newer version of any of the installed add-ons components is available through RooBot. A set of newly introduced commands addon upgrade should address this as follows Upgrade a specific Spring Roo Add-on Component Upgrade a specific Spring Roo Add-on / Component from a search result ID  Upgrade all relevant Spring Roo Add-ons / Componentsaddon upgrade available  List available Spring Roo Add-on Component upgrades addon upgrade settings Settings for Add-on upgrade operations this allows the user to set his preferred add-on stability level Furthermore the Roo shell should list the number of upgradable add-ons upon startup if roobot xml zip was successfully downloaded.,859
Extract Method,Refactor JspMetadata Listener handle i18n properties more efficiently Refactor Jsp Metadata Listener handle  properties more efficiently.,860
Extract Method,Composite primary key support in MVC scaffolding table relation in db i build roo from git ok then show Spring Roo 1.1.2.BUILD-SNAPSHOT mvn tomcat:run it run ok i show in web list other entity table ok but tee table error show Data access failure Sorry a problem occurred while accessing the database Exception Message Exception Stack Trace java lang Thread run my database struct in attachment open18.sql.,861
Extract Method,Java Parser Mutable Class Or InterfaceTypeDetails.updateTypeAnnotation(..) should not flush changes to disk twice during normal operation Invocation of the above method causes two consecutive Updating filename java operations  and associated messages if in development mode This is inefficient from a disk writing perspective and also confusing for add-on developers in development mode.,862
Extract Method,Abstract Itd MetadataProvider to offer create Local  method It is difficult to create a local MID meaning a MID specific to the Metadata Provider from a Member Holding Type Details object such as an Type Details. It would be useful if this were on the superclass which is particularly useful for Abstract Member Discovering Itd Metadata Provider subclasses.,863
Move Method,Make DBRE Database class fully immutable The destination package mutator in Database java will be removed and the field introduced via the constructor,864
Move Method,Remove MetadataService and Class Path Operations from Java Bean Metadata and change back to BIM for now To make Java Bean Metadata immutatable references to Metadata Service and Claspath Operations need to be pushed back to the Java Metadata Bean Provider.,866
Extract Method,Data-on-demand and integration tests to support composite primary keys This ticket is to combine existing related tickets into one,867
Extract Method,Boost performance of gwt setup command Similar to the changes in  the Gwt Operations Impl class will be changed to add all dependencies etc at once rather than the expensive operation of one at a time,869
Extract Method,Provide overloaded constructors to Abstract  Identifiable Annotated Java Structure Builder instances To construct a new Field Metadata Builder with all the attributes of an existing field except for the declared By Metada tId the builder must be constructed first with the id and then all the attributes copied manually. A new set of constructors will be added to easily facilitate this eg Field Metadata Builder declared By MetadatId existing These constructors will have immediate use with the GWT add-on.,870
Inline Method,Improve consistency of Roo code base ,871
Extract Method,Add extra Column attributes to composite key fields in RooIdentifier ITD Currently attributes such columnDefinition are added to the Roo Db Managed ITD but not to the fields in the Roo Identifier ITD. This change will add these attributes,872
Move Method,Move test integration and dod commands into addon-test and addon-dod respectively Now that addon-test and addon-dod do not have dependencies on addon-entity the test integration command can be moved to its rightful place in addon-test and the dod command to addon-dod.,874
Extract Method,Remove dependency on EntityMetadata in DBRE This change will remove references to EntityMetadata from addon-dbre. The dependency on addon-entity will still remain as Dbre Database Listener Impl implements Identifier Service from addon entity,875
Extract Method,Custom Data tags get lost when ITD supplied fields or methods get pushed in to the corresponding java sources If a Metadata producing type tags a field or method and the member is already available in the corresponding governor the tagged MD is lost in favour of the original MD most likely JavaParser...MD. The tagged MD is lost in Abstract Member Holding Type Details Builder,876
Extract Method,Create EntityAnnotation Values for consistency with other add-ons The Roo Entity annotation has many attributes and these should be stored and referenced in its own Abstract Annotation Values implementation.,877
Rename Method,"Modification of Roo commands to enhance usability and consistency between add-ons Roo's current command names have evolved over a lengthy period. New add-ons have been created and existing add-ons have addressed additional requirements. In addition a wider audience of people have tried Roo and reported their experience in learning commands and intuitively understanding what the present commands mean. Invariably this evolution has resulted in a better understanding of usage patterns and provides considerable scope for revisiting the existing command names and improving them for consistency expressiveness and memorisation ease.This issue will result in existing command names changing. Command options (ie those portions of a command prefixed by a double-hyphen) on the other hand will not be reviewed as part of this task. This is because command options are not nearly as critical for learning Roo as are command names as once a command name has been established the tab-completing shell interface guides the user through the mandatory and optional command options in an intelligent and easy-to-use manner anyway.The main area of intended improvement is to identify the desired output artifact at the beginning of the command. For example create controller"" would become controller create (or similar). In addition where possible the differentiation between installation and post-installation setup is to be removed. As such install jpa would become ""persistence setup"" and the ""update jpa"" would be removed.This task has been deferred until now so that a detailed understanding of version 1.0.0 commands would be reached.",879
Rename Method,Turbo charge performance of persistence setup command ,880
Extract Method,Enhance File Manager to create and update XML files Currently clients are required to create a MutableFile instance to read an XML file to convert to a DOM Document File Manager and XmlUtils will be enhanced to do the work itself reducing the need for so much code in callers.,881
Inline Method,findXXX(id) should use query.getSingleResult() instead of query.getResultList() Currently in order to get a single result findXXX gets a resultList and gets the first element from the list. This requires the method to be transactional because if it was not transactional results.size would throw an exception stating that the session manager has been closed Using query.getSingleResult() is more efficient because it does not require a transaction Instead of this being generated currently This should be generated A NonUniqueResultException (- if more than one result) should propagate out not be masked because if such an error occurs it would identify a lack in database integrity. As it is currently such a lack of integrity would be hidden.,887
Extract Method,Web view and JSON view inconsistent. JSON view should use deepSerialize() or allow option to example However this view is inconsistent with the normal GET as the normal GET is deep (aka you can see all the users  One To Many properties such as Authorities etc This is because the generated JSON code in the underlying object Otherwise ROO should provide a mechanism for doing this However to be consistent you might want to consider deep being the default. Otherwise the same data is not being delivered as the default GET view.,888
Extract Method,Remove Maven project listener artifacts The listeners such as Dependency Listener Plugin Listener etc will be removed from the project module as they are redundant and are of no use if the user edits the pom manually.,889
Extract Method,Performance enhancements relating to Member Details Scanner and Type Location Service Impl ,890
Rename Method,Change TypeLocationService.getSrcMainJavaTypes to getProjectJavaTypes(Path path) Pass a Path attribute to method to make more generic  eg allow test java types to be returned if required,891
Rename Method,Upgrade of SolrJ driver version to 1.4.1 Upgrade of SolrJ driver version to ,892
Extract Method,Improve performance of Roo shell with large projects For projects with more than entities and web controllers Roo takes a long time to load,893
Extract Method,Create JavaBeanAnnotationValues for consistency with other add-ons ,895
Rename Method,Creating a java.util.Set of URLs is slow and unreliable  returns a Set of URLs. Sets rely upon the equal and hashCode methods of the contained objects to be performant and correct. The URL class fails on both counts as documented here  Notably these methods require a DNS lookup slow and can return different results depending upon whether an internet connection is available  inconsistent We should change this method to return a Set of either plain Strings or URIs (which do not have this problem) based on the needs of the calling code.,896
Extract Method,Minor changes for Add-on RooBot operations API to accommodate STS requirements Minor changes for Add-on RooBot operations API to accommodate STS requirements,898
Extract Method,"Controllers shouldn't call findAll on each request As explained in the forum thread and using the Pet entity as an example the controller code generated by Roo calls {{findAllPets()}} upon every request thanks to the presence of this method This is wasteful given that Some of the controller's request handling methods show and delete don't even use the returned list of Pets The list method obtains its own list of Pets usually via The create and update methods only need the list of all Pets if the Pet class has a Pet (or Pets) field. We can make the scaffolded controllers more performant by only obtaining the list of all Pets when it's actually required In the meantime the workaround is as follows Push in the populate Pets method and have it return null (saves time in the ""list"" method) If the entity has a field of its own type (or a collection thereof) push in the create and update methods and modify them to explicitly put the pets list into the model for example showing the create methods",899
Extract Method,Improve support for classifier element in Maven dependency element As the classifier element is part of the equals hashCode and compareTo methods in Dependency it makes sense to add it as a option in the dependency add and remove commands. This will enable users to be able to add and remove dependencies for sources jars for example.,901
Extract Method,Allow FileManager to explain why it's deleting a file Currently the  method only takes a canonical file path. As noted by a  in AbstractItd Metadata Provider it would be useful to have an overloaded version of this method that also took a reason for deletion with this message being displayed on the console appended to the existing rather laconic message This would be a boon for anyone debugging Roo or its addons,902
Extract Method,Display Id as first item when using @RooEntity and @RooToString Since all JPA entities have an Id it would be nice to display the id as first item in the generated to String method.,903
Extract Method,"""field jms template"" command should add a JmsOperations not a JmsTemplate The Roo command  adds a field to the specified Java class as follows In the spirit of programming to interfaces this field should be declared as an  instead. This will allow users to deploy an alternative  implementation in their  if desired.",904
Extract Method,"Improve thread safety of mail messages If you run this script and this bean definition There are two problems with this Because the bean is a singleton every entity class with a Simple Mail Message field will receive the same instance and because this class is stateful it's possible that different threads will interfere with each other e.g. one thread will set a ""from"" address that will be used by a different thread  Even if the bean is made a prototype a given instance of a given entity might conceivably be used by multiple threads at once causing the same thread safety issue. The solution is to use the field as a genuine template as follows The workaround is to make the above change manually.",905
Rename Method,Upgrade Database.com (VMFORCE) JPA provider Roo is using an old version of the Database om (VMFORCE) JPA provider. We should upgrade the version as well as the JPA configuration.,919
Extract Method,Make test transaction integration optional Make test transaction integration optional. The transaction integration is not desired for all persistence providers so making its use optional in the generated tests will allow other providers to leverage integration test support.,925
Rename Method,Provide a mechanism to see if the ITDs associated with a specific type have changed In Roo a representation of the ITDs in the project is not maintained and can only be retrieved from the metadata item. This is troublesome when trying to maintain a cache which takes into account changes to ITDs on disk such as PersistenceMemberLocatorImpl. Another potential improvemen an ITD store would have is in MemberDetailsScannerImpl as a reduction in mindless metadata retrieval could be achieved by using the already produced ItdTypeDetails from subsequent metadata production. An initial prototype was developed for Member Details Scanner Impl which reduced metadata requests by 80% but more work is needed before this is ready for prime time.,929
Extract Method,Issue warnings for deprecated MVC commands. Issue warnings for deprecated MVC commands.,930
Rename Method,Don't update database upon switching JPA provider if any entities are DBRE-managed The internal  bike shop test project uses MySQL with Hibernate. If after creating the project you change the JPA provider e.g. as follows:persistence setup  database MYSQL provider ECLIPSELINK then in  the provider is configured to update the database schema This is not desirable if project has any DBRE-managed entities as by definition the corresponding tables should never be updated. It would be better for the above property to be set so as not to update the schema none in the case of EclipseLink While this would mean not creating or updating the required tables for non-DBRE entities this is the lesser of two evils.,931
Extract Method,DataOnDemand lookup from IntegrationTestMetadataProvider is too rigid Integration Test Metadata Provider bases its search for an entity s Data On Demand based on the types name Roo knows everything about the project so the lookup can be made to be much more flexible by inspecting a type's RooDataOnDemand annotation.,964
Inline Method,Create add-on for displaying a pretty-print representation of a class A new add-on addon displayname will be created to provide a method returning a string representation of the owning class. This is similar to addon-tostring however the new add-on will produce a string that can be used for display in UIs as well as to aid in conversion. The method name and the exact fields can be customised via the new Roo Display Name annotation,965
Extract Method,Standardise how Roo annotation values are read While most metadata providers read Roo annotations using a subclass of Abstract Annotation Values these two do not We should harmonise them to use the same pattern as the others the metadata provider reads the annotation values into a subclass of Abstract Annotation Values and passes that instance to the metadata class.,967
Inline Method,Enhance addon-tostring to use a commons-lang builder and remove addon-displaystring Having two add-ons to generate toString-style methods is not ideal and so this improvement will modify the output of Roo To String to be more compatible for display in UI views as well as removing the need to have the addon-displaystring. The JSF and MVC add-ons can simply use the to String method to display entity data in tables as well as for JSF Converters.The to String add-on will now make use of the commons-lang ReflectionToStringBuilder to generate the to String method which can handle nulls collections etc without the need for the add-on itself to handle such conditions. Of course for each entity the to String  method can be pushed in and customized.,972
Extract Method,Allow DBRE to make DB connection via JNDI ,979
Rename Method,Decouple PrimeFaces references in JsfOperationsImpl Currently Prime Faces is hard coded in the Jsf Operations Impl class. This improvement will add a new optional library option to the web jsf setup command and treat Prime Faces similarly to the user-selectable implementation option.,981
Inline Method,Choose between data access patterns At this time the Aspect J generated aspects and entity classes follow the Active Record pattern. It would be nice if we could choose between this pattern and Data Access Object with or without Spring Dao Support. The way to configure would be persistence setup  database AccessPattern Active Record DAO,985
Extract Method,Use Apache Commons libraries in Roo Until now Roo has internally used copies of classes that are available in the Apache commons lang commons io commons collections and commons codec libraries. This ticket will add the commons dependencies to Roo and remove the Roo managed equivalents and their test classes such as String Utils. The commons libraries are small well tested and are already OSGi compatible.,1005
Rename Method,Add Selenium Add-on Roo should generate Selenium test cases for MVC applications.,1012
Extract Method,Provide option for Spring Roo managed XML configuration for Services Controllers GWT Locators etc. For some environments notably GAE it is preferable to use XML configuration instead of annotations. Doing so reduces the amount of time required for applications to start up. In an environment like GAE where applications are frequently created and destroyed using XML configuration can greatly increase performance.,1013
Extract Method,Make GAE and GWT version properties When setting up GWT or GAE the version are hard coded throughout the pom.xml file. It would easier to upgrade to new versions if the version was a property.,1015
Extract Method,Generate JSON methods to de/serialize dates in the ISO 8601 format I understand that the JSON spec does not explicitly call for dates to be serialised in accordance with ISO 8601 but the JavaScript toJSON method does make that assumption. Offering that option would be very helpful to many languages and also to human reading / debugging. To support backwards compatibility I propose an addition command line and annotation option that will by default preserve the existing serialisation as millis since start of the epoch. When enabled the new option will modify the generation performed by Json Metadata class.,1016
Extract Method,Improve method level security of Spring Roo services by including a PermissionEvaluator Add the ability to use PermissionEvaluators with services managed by Spring Roo.,1017
Rename Method,Spring Roo doesn't start with JDK 1.8 I am using Spring Roo  and java version When I try to start roo.sh I get a ton of unresolved constraints (log file attached). After searching the web I found this  and tried to replace the felijar in the Spring Roo distribution and could start the shell successfully.,1018
Rename Method,MenuOperations should make use of fileManager.findMatchingAntPath(..) to update message bundles Currently MenuOperations updates messages roperties files individually to insert label names for menu items. This requires us to register every new messages properties file there. It makes sense to use fileManager find Matching Ant Path antPath  where the ant path matches all messages properties.,1019
Rename Method,Roo Addon Suite Support Include Roo Addon Suite Support on Spring Roo Shell. Related with  A Roo Addon Suite is a great way to package and distribute a set of add-ons together for example if you want to distribute Roo custom distributions. Roo Addon Suite is based on OSGi R5 Subsystems that provides a really convenient deployment model without compromising the modularity of Roo.,1024
Rename Method,Roo Commands Refactoring After analyze Sping Roo components is really necessary to apply the following changes:addon create wrapper command must not appear after project setup.equals command must not appear until project setup Remove flash test command and its implementationfocus command must not appear until project setupmaven command must not appear until project setupmetadata command must not appear until project setup Rename project command with project setupRename poll command with project scanproject scan command must not appear until project setup process manager command must appear only with development modereference guide command must appear only with development modeweb mvc json setup command must not appear until project setupRename osgi framework command with Remove Spring Roo osgi commandsaddon install id must be deletedaddon info id must be deletedCreate new command addon install url* to install bundles/addons by URL.  Create new command addon repository introspect that lists installed addons. Adds repository parameter to list addons from an specific repository,1025
Rename Method,Search wrappings on default OSGi repository Roobot is not implemented on   now components are searched on OSGi R5 repositories Update all wrapping searchs (jdbc drivers libraries etc...) to use repository structure instead of roobot structure. Repository Structure to Spring Roo Addons:,1026
Rename Method,Create visual component to manage Spring Roo Repositories and addons Manage OSGi repositories and addons are an important feature of The goal of this task is to create a new visual application (using JavaFX) to manage Spring Roo Repositories and addons using graphical mode. With a simple Spring Roo command like addon repository manager this graphical application will be launched and will allow developers to manage their installed repositories and addons more easily than using commands.,1027
Rename Method,Create new command that make push in of declared methods and fields from ITDs to Java files Create new command that make push-in of declared methods and fields from ITDs to Java files. Commands to create will be push in all package class method,1028
Rename Method,Annotation based configuration Use Spring Boot Move to class configuration in spite of XML configuration. For those unfamiliar with Configuration classes you can think of them as a pure Java equivalent to Spring XML files. Since Spring  the Configuration approach provides a truly first-class option for those who wish to configure their applications without XML. Spring Boot is well suited for web application development:It makes it easy to create stand-alone production-grade Spring based Applications.It favors Java-based configuration. It generally recommends that your primary source is a Configuration class.Spring Boot auto-configuration attempts to automatically configure your Spring application based on the jar dependencies that you have added.Auto-configuration is noninvasive at any point you can start to define your own configuration to replace specific parts of the auto-configuration.You are free to use any of the standard Spring Framework techniques to define your beans and their injected dependencies.It allows you to externalize your configuration so you can work with the same application code in different environments.Spring Profiles provide a way to segregate parts of your application configuration and make it only available in certain environments.Spring Boot uses Commons Logging for all internal logging but leaves the underlying log implementation open.If Spring Security is on the classpath then web applications will be secure by default with ‘basic’ authentication on all HTTP endpoints.If you work in a company that develops shared libraries or if you work on an open-source or commercial library you might want to develop your own auto-configuration. Auto-configuration classes can be bundled in external jars and still be picked-up by Spring Boot.Spring Boot includes a number of additional features to help you monitor and manage your application when it’s pushed to production.,1030
Rename Method,Generate commands to include properties on default configuration file Develop new commands to include Spring Boot or others properties on default application config file By default this configuration file will be located on  but it could be modified using Spring Roo Shell Configuration features These commands should delegate on ApplicationConfigService These commands should take in mind force parameter These commands should take in mind profile parameter,1037
Extract Method,Create new indicator CliOptionAutocompleteIndicator Auto completing command option values will take in count dependencies between same command options in order to provide right values,1038
Rename Method,Update Repository JPA commands Update Repository JPA commands to support new Spring Boot project structure. Check starter-data-jpa instead of persistence Doesn't include configuration files,1040
Move Method,Generate repositories for read Only and read And Write entities When developer tries to generate new repository using repository commands he needs to define for which entity wants to generate the repository. Take in mind that this entity could be  read Only or read And Write ReadOnly entities:Roo generates an interface Entity Custom Repository to include dynamic queries.Roo creates an empty implementation of previous interface that extends of Query Dsl Repository Support.Roo generates an interface called Read Only Repository  that extends Repository like the following Roo creates an interface that extends interface Entity Custom Repository and Read Only Repository Will be annotated with email protectedRead And Write entities Roo generates an interface EntityCustomRepository to include dynamic queries.Roo creates an empty implementation of previous interface that extends of Query Dsl Repository Support.Roo generates an interface that extends Entity Custom Repository and JpaRepository Will be annotated with email protected,1041
Rename Method,Remove Active Record support on Spring Roo finders Active record support was removed on Update finders generation to new Spring Data repositories system Update finders names to use the following Spring Data nomenclature:,1045
Rename Method,"Refactoring of ""service"" commands offers some commands to manage service layer Generates new services for all entities of current project Generates new service for an specific entity Update these command and its parameters to follow Spring Roo analysis New commands should be like the following Generates new services for all entities of current project",1046
Move Method,Update AspectJ to 1.8.8 uses Spring IO Platform to manage spring and third party dependencies Spring IO Platform provides the last version of AspectJ  as you could see on version appendix Update generated  to include last version of AspectJ and AspectJ Maven Plugin.,1047
Extract Method,Install Spring Security using Spring Boot autoconfiguration Install Spring Security on generated project using Spring Boot autoconfiguration. Include new commands Remove deprecated commands Include new annotations,1048
Rename Method,Generate Controllers with different responseTypes Generate new Controllers inside current project Add parameter on provided command to allow developers to select the responseType for that new controller,1051
Rename Method,Generate Formatters Generate formatters inside Spring Roo project These formatters will be used on presentation layer to format registered entities.,1056
Rename Method,Generate views files using FreeMarker template engine Now a days Spring Roo generates views files with an static format and structure. This format and structure is not editable by developer so developer doesn't take any decision about view generation should provide system based on template engine that allows developers to customize view generation with its own content format and structure. Free Marker will be the first template engine included on Spring Roo shell. This template engine will read developers custom templates and it will process them to generate final views .,1058
Rename Method,Use JQuery Datatables on list views Spring Roo shell will use FreeMarker template engine to generate views. Include all necessary elements and configuration to be able use JQuery Datatables components on list views instead of static tables.,1059
Inline Method,Generate master-detail view by default using Datatables component Generate master-detail views by default using Datatables component. Datatables component has been implemented on This feature will allow users to display relations between entities.,1060
Extract Method,Maintain generated thymeleaf views A first version of thymeleaf view generation has been included on Spring  but is necessary to improve it including maintenance of generated views. Check if the new view to generate exists on current project. If not exists include the new one. If exists merge the content of the old one with the new one. Take in mind  attribute. This attribute will allow Spring Roo shell if is necessary to update element or not. ,1061
Rename Method,Include dependency with database driver on repository modules Spring Roo includes a maven dependency with the database driver on application modules during  execution. However is necessary to include this driver in every modules that contains a repository when command is executed. This dependency will be included for test purposes so it should be included using the scope,1063
Rename Method,Create FieldCreatorProvider implementation for Embeddable classes To allow the addition of new fields to Embeddable classes is necessary to create new implementation of the Field Creator Provider interface that works with Embeddable classes and cover Embeddable classes requirements.,1064
Rename Method,"Update addon-dto to generate ""Projection"" classes Update the addon addon-dto included on Spring Roo  to register a new command that separates Projections generation and DTO generation.",1065
Rename Method,Update finder commands for using DTO's and Projections Update finder command so finders can return Entities and Projections and/or receive DTO's and Entities.,1066
Extract Method,Improve findAll and finder method generation with support class for QueryDsl Repository Custom implementations should use a more clear way for querying. Redo the implementations for find All finBReference and finders to make use the new methods of Query Dsl Repository Support Ext.,1067
Extract Method,Implement command to generates detail controllers Implement command to generates detail controllers independlty of  web mvc controller command,1068
Rename Method,Improve relationship management model level Improve entity relationship management,1071
Extract Method,Improve relationship management repository level Improve entity relationship management.,1076
Extract Method,Add default values to JavaPackage parameters when possible Modify commands using Java Package types to autocomplete with default values when possible. Mandatory JavaPackage parameters should be turned to optional when possible as well.,1077
Extract Method,Include dependency spring-tx Include dependency spring-tx and check that Transactional annotation is generated correctly,1078
Rename Method,Improve I18n commands Improve  commands to be able to select a default language for the current application.,1079
Extract Method,Improve relationship management service level Improve relationship management service level,1080
Rename Method,Move Finder into JPA repository Move finders feature into Repository JPA add on as current implementation just support this type of repository. Also makes easier manage generation.,1083
Rename Method,Add support to send and receive emails Add support to send and receive emails using the service Java Mail Sender and the project springlets,1084
Extract Method,Generate SOAP Web Services using Spring Roo shell Include new commands in the Spring Roo shell to be able to generate new SOAP web services and to generate new SOAP web service clients.,1085
Extract Method,Add support to send and receive JMS messages Add support to send and receive JMS messages,1086
Extract Method,Update integration test support Update integration test support to use spring boot improvements,1088
Extract Method,Improve help system usability The help info is confusing it combines user info with internal Roo info which is not understand by the user. On the other hand the text format is hard for reading. There are help systems like Bash man command which writes more readable help texts Improve the Roo help system to be similar to man help system in terms of readability and usability. We take man as example to avoid to reinvent the wheel.,1089
Extract Method,Implement Controller URL generation using new support from Springlets Due to the new use of Thymeleaf utility to create the application links and the behavior described in  it is needed another way to easily generate URL's to controllers in generated views using Springlets support.,1090
Rename Method,New entity visualization support using a new format annotation Modify generated code to implement a new visualization system for entities in datatables and selectors.,1091
Extract Method,Specify views where details should be displayed Should be great that developer could specify the master views where the new detail will be displayed. Include  parameter into the web mvc detail command to specify a separated comma list with the master views where to include the new detail. Only list show and views will be valid to include the detail list.,1092
Rename Method,All included fields should be private All included fields should be private and the methods should use accessor and mutator methods instead of use the field reference directly.,1093
Rename Method,Generated reports should only show projection fields when defined Report column builder is not building projection fields when selected it shows all entity fields.,1094
Extract Method,"Prefers define ""implements"" in java files than aj files Sometimes AspectJ can't compile some classes which implements declaration is defined in a related  files So where ever is possible generate implements declaration in the file.",1095
Extract Method,Improve JavaBean addon Improve JavaDoc add-on return this in setters Improve javadoc,1096
Extract Method,Improve readability of message resources managed by Roo Currently Roo uses the default JDK java util Properties API to handle its message resources. This implementation is however a little limiting with regards to formatting it does not preserve formatting nor order of the key value pairs It may be useful to handle properties files through a simple Util class so we can preserve formatting and order.,1097
Extract Method,Change shell prompt to provide current entity as a 'path' Interesting idea from a guy that attended my Roo demo today in Stockholm:change the Roo shell prompt to show you what the current entity is that you're working with as you now have to memorize that when you start adding fields and such. The prompt could provide you with feedback on 'where you are' just like a normal shell prompt.I liked the idea so I'm logging it as a Jira issue before I forget about it.,1098
Extract Method,Expose generated methods from the dynamic finder add-on to the controller and view artifacts Currently the dynamic finder add-on will integrate custom finder methods into domain objects but leave it to the developer to expose them through the controller to the view layer. This could be automated.,1099
Extract Method,"Introspect an existing database to support simple automatic JPA entities Most real world enterprise applications are built in one of two ways Starting with the relational schema and then building the application on top of it Starting with the required web views and then building the application to support them It would be nice if Roo offered extended support for as many people have existing databases they wish to introspect and then web expose. The usage scenario would be Roo commands like:In this example database reverse would scan all tables in the localhost my db database and create an entity for each table. Unlike a normal entity though the fields would be added to a Entity Name Roo Reverse ITD The benefit of adding the fields into an ITD is ""database reverse can automatically remove or update the Entity Nam Roo Reverse with each subsequent execution This is very useful if the database is changing",1100
Rename Method,"Introspect an existing database to support simple automatic JPA entities Most real world enterprise applications are built in one of two ways Starting with the relational schema and then building the application on top of it Starting with the required web views and then building the application to support them It would be nice if Roo offered extended support for as many people have existing databases they wish to introspect and then ""web expose"". The usage scenario would be Roo commands like:In this example database reverse would scan all tables in the localhost my db database and create an entity for each table. Unlike a normal entity though the fields would be added to a Entity Name Roo Reverse ITD The benefit of adding the fields into an ITD is ""database reverse can automatically remove or update the Entity Nam Roo Reverse with each subsequent execution This is very useful if the database is changing",1102
Extract Method,Need support for @EmbeddedId - a way to specify this in @RooEntity I want to use EmbeddedId in order to use composite primary key. e.g. I want to use the following No I want to have roo managed entity Edge that looks like the following now I want it in the following way In the RooEntity - it is great to have something called embeddedIdentifier with true and false options false being the default option.,1113
Extract Method,JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,1115
Extract Method,Simplify web.xml builder Editing web.xml could be simplified as suggested in the above forum post. Maybe something like This would seem to resolve a number of hickups we encountered during development of add-ons to date like filter/filter-mapping ordering semantics It might be better to put this into a metadata object instead of provide a builder. It's also appropriate to review the web.xml fragmentation capabilities being considered in Servlet Spec 3 if building a metadata object.,1146
Extract Method,ITDs should avoid superflous whitespace after end of each generated line Currently Roo generates ITDs which contain various indent-related spaces at the end of most lines. While this is not an error it creates source code that is unnecessarily larger than it needs to be in terms of bytes and it looks unpleasant to a developer if they edit the file. Roo should not put whitespace at the end of each line.,1147
Rename Method,"Reduce duplicate reference data code in generated controllers Imagine a domain in which a Car has a Make Model and Colour In the generated Car Controller Roo Controller file the createForm create updateForm and update methods all contain the same code for populating the form's reference data This violation of the means that if you want to tweak what reference data goes into the model for example only offer non-metallic colours you have to ""push in"" all four of those public methods. If Roo could instead generate a resuable private addReferenceData Model Map method that was called by those four methods then a developer wanting to tweak the reference data need only push in that one private method. In the above example it would look like this by default and of course the four public methods listed above would also need to be changed to for example",1148
Extract Method,Scaffolded controllers should use plural when referring to a collection of resources Scaffolded controllers map resources to a singular path Owner Controller maps owner. Instead they should use plural when referring to a collection of resources owners and owners,1149
Rename Method,"Improvements for addReferenceData() method in genrated Controllers Instead of one add Reference Data method that adds all reference data there could by one mehtod for each reference data. Those methods don't have to be called by the other controller methods but could just be annotated with Model Attribute like in the PetClinic example This would allow for better customization of how to get reference data. For example sometimes it is not appropriate to use ""findAll"" for some reference data but to do something like this This kind of customization is not possible with the current add Reference Data method.Another issue is that the adReferencData method seems to get too much data For example for a Pet entity I always get Why does a Pet Controller need all Pets as reference data? Usually there is no Pet select box in a pet form",1150
Extract Method,[patch] Move CompilationUnit creation into its own method in JavaParserMutableClassOrInterfaceTypeDetails Right now CompilationUnit creation and associated physical file creation is code Java Parser Mutable Class Or Interface Type Details createType File Manager fileManager final Class Or Interface Type Details cit String fileIdentifier I would like to reuse CompilationUnit creation to obtain a string with the java code. That way I can compare that string with a existing file My use case I want to generate some metadata class associated with original class. I created a Metadata Provider that listens to source code modifications and if some condition is met creates or updates some java file. If the file already exists I compare its contents with my generated java code. If it's different the file will be updated. To generate that java code I want to reuse code in Java Parser Mutable Class Or InterfaceType Details.I attach a patch to Java Parser Mutable Class Or Interface Type Details file with my solution. It's about moving code from createType into public static String get Output Class Or Interface Type Details cit and then reusing this method inside of createType.,1151
Extract Method,mappedsuperclass implementation I think Roo's implementation of mappedsuperclass could be improved seems to be the suggested way to create a mappedsuperclass inheritance structure. However this results in problems The mappedsuperclass itself is generated with an entity annotation. I don't think this is correct as it results in the ORM creating a DB table for the mappedsuperclass this doesn't seem right and is never populated with any data.Also the sun docs say a mappedsuperclass shouldn't have an entity annotation The tests fail as Alex observes in  This is an AssertError and it kind of kills the advantage of auto generated tests when you can't build the project in mvn. Something which may be preferable at times.Possible solutions could be  Maybe the mappedsuperclass option should be moved to the Roo class command It may be a better fit as a mappedsuperclass isn't a standard entity Manually remove the entity annotation from the mappedsuperclass after Roo makes it This makes the tests work again and hibernate no longer makes the redundent superclass table Suggestions welcome.,1152
Rename Method,Add integration with enum types I would d love to be able do something like this: new java enum -name ~.domain.EntreeOption add value CHICKEN add value STEAK add value VEGETARIAN new persistent class jpa domain Guest add field enum jpa entreeOption -type domain.EntreeOptionThis would create the enum EntreeOption the JPA entity Guest and add a field in Guest of type EntreeOption:,1153
Rename Method,Google Web Toolkit GWT Integration Roo should have support for GWT,1154
Rename Method,Review templating approach to be more flexible and allow better custom branding of generated applications The generated view artifacts do currently allow for customization of the generated application via CSS the header jsp and the footer jsp but there are some opportunities to simplify end-user branding.,1156
Extract Method,Provide ability for AbstractItdMetadataProvider to discover Java types for non-PhysicalTypeIdentifier notifications Presently Abstract Itd Metadata Provider does not support being notified of class-level notifications to be received unless the upstream dependency is a Physical Type Identifier.A fallback model should apply where a class-level notification is received that does not have a Physical Type Identifier upstream dependency. The fallback will be to locate all *.java files in the file system and request the subclass provide the metadata.In practical terms this allows the subclass to be notified of project-wide metadata changes such as:,1159
Extract Method,Refactor addons dependencies xml files to be more generic so to allow maven plugins  repositories  plugin repositories and properties Addons with a dependencies xml file will be changed to configuration xml and other maven artifacts will be added such as repositories,1160
Rename Method,"Require explicit enabling of targetUrlParameter parameter in AbstractAuthenticationTargetUrlRequestHandler It's possible that an AuthenticationSuccessHandler or other class extending from this class may be used in scenarios where it isn't desirable to have a parameter used to determine the redirect location though when it is invoked during the login request this shouldn't be a problem. A user might create a subclass or use SimpleUrlAuthenticationSuccessHandler without realising that the parameter-based functionality exists. It would therefore probably be preferable to require that the functionality is explicitly enabled (as with the use of the ""referer"" header). ",1164
Extract Method,Support naming of filter chains in the namespace to allow for easier integration with external services like OAuth Since the namespace now supports multiple filter chains this will allow an id to be added for each list of filters in the bean registry which will make it easier for other services such as OAuth to specify which filter chain they want to integrate with. ,1165
Rename Method,"Excessive (and misleading) logging in DelegatingMethodSecurityMetadataSource If you switch on global method security Spring Security adds a custom pointcut matcher and delegates to the. This code in that class logs *every* method in ebery bean in the context 9as far as I can tell) whether or not it is going to be intercepted:So 99.99% of these logs have attributes=[] (empty) which according to the matcher means it does not match.Could the log level be changed to TRACE and also the message changed to ""Analyzing"" or ""Matching"" instead of ""Adding""? ",1166
Extract Method,Defensively invoke SecurityContextHolder.clearContext() in FilterChainProxy In situations where applications try to obtain the globally it may cause a memory leak if the application uses security=none and the is even read from. A similar situation can occur if users manually create the filter chain and do not properly add the to the. In order to be defensive about memory leaks it would be good to call in the itself. ,1168
Extract Method,Code cleanup on bcrypt implementation In situations where applications try to obtain the globally it may cause a memory leak if the application uses security=none and the is even read from. A similar situation can occur if users manually create the filter chain and do not properly add the to the. In order to be defensive about memory leaks it would be good to call in the itself. ,1169
Rename Method,Support for Servlet 3.0/3.1 asynchronous request processing ,1171
Rename Method,"BasicAuthenticationFilter should not invoke on ERROR dispatch I have configured the following security element: I have set up custom error pages in my web.xml:If for example an authentication fails the 401 page should be displayed. Now if a client sends an invalid Basic value the filter chain finds that and calls the 401 page through Tomcat. Unfortunately the filter chain proxy does not know that this is one request with an error forward only. It fires the entire chain again Now the maintains an property which tells him to observe checks only once. The does not have such a property which treats a  whole new request. The response gets reset twice and no output it written to the client.I could set the error pages to security=""none"" but I want to maintain the security context on all pages whether they are protected or not.     ",1172
Extract Method,"Adding new constructor with advanced parameter rootDN to class ActiveDirectoryLdapAuthenticationProvider We have different ""domainname"" and dc=infodc=localAnd was meet problem with auth domain users.After modifiyng ActiveDirectoryLdapAuthenticationProvider with adding new contructor this work now.We need a possibility to set different domainName and rootDN.This work properly for us with next code:",1173
Rename Method,Update BCrypt to be best practice The tutorial and JavaDoc should be updated to reflect that using BCrypt is the recommended best practice ,1174
Rename Method,Handling Multiple AuthenticationEntryPoint defaults For example we don't handle the following very well:We should also handle cases where OAuth is involved. ,1175
Rename Method,http.authorizeUrls() to http.authorizeRequests() This is more descriptive of what is happening (we don't necessarily need to match on the URL) ,1176
Move Method,AbstractSecurityWebApplicationInitializer should allow registration of Java Config ,1177
Move Method,Spring Security should provide HandlerMethodArgumentResolver for Authentication.getPrincipal() ,1178
Rename Method,Update Java Configuration Samples to use @Autowired AuthenticationManagerBuilder This mechanism uses the Global so that it easier to share authentication with other configurations (i.e. and other instances). ,1179
Rename Method,CsrfAuthenticationStrategy to add valid token to HTTP request after clearing the one in HTTP session clears the token stored in HTTP session after successful authentication:But if the configured in the application is not a client redirect (HTTP 301/302) the token still stored in the request is invalid.should store a new in current request after clearing the on in the session. Something like:,1180
Rename Method,Convert Java Config samples to thymeleaf and tiles JSPs are easier for users to get started with but prevents reuse in the samples. is an older technology that has not had updates in quite some time and the additional filter causes more complex setup for Spring Security (save this for a specific lesson) ,1183
Extract Method,Restriction on LdapAuthenticationProviderConfigurer with DefaultLdapAuthoritiesPopulator When using the to define the LDAP Authentication (with () method) we can't define a. In the build() of the is hardcoded. In my context we use the LDAP to process the authentication and a database to process the Role. That was passible in the XML configuration with a but in it's not possible. ,1184
Extract Method,Support csrf protection of logout URL with xml configuration CSRF protection works fine with java configuration because an is configured that only accepts POST.However in our application we are using XML configuration with and and that configures a  which does not take the request method into account. We could not find a way to change the configuration to ignore GET requests to the logout URL.This might just be a documentation problem but looking at the code I suspect that this is currently not supported at all. ,1185
Extract Method,Improve StandardPasswordEncoder Performance My web application uses a to match passwords in Basic Authentication.Under heavy load (700 Http requests per second) becomes a major source of contention inAt 700 HTTP requests per seconds this synchronized block has an average contention of 247 milliseconds with maximums of 1.18 seconds.At 500 HTTP requests per seconds this synchronized block has an average contention of 28 milliseconds with maximums of 291 milliseconds. I wonder if there is a non synchronized alternative to ,1187
Extract Method,LdapAuthenticationProviderConfigurer should find available port We can try the default port but if it is not available we should configure an available port. ,1190
Extract Method,Support static nested groups in LDAP ,1192
Rename Method,Support authorization rules by SimpMessageType The initial security only allows protecting a destination without taking into account the message type. When using a like STOMP subscriptions and messages should be protected independently: a user may be allowed to subscribe to a destination in order to receive messages but not allowed to send messages to that destination. ,1193
Extract Method,Support configuring SecurityExpressionHandler<Message<Object>> Description We should allow easily configuring via Java Configuration and XML configuration. Original DescriptionI work on a project starting to use websockets and we plan to secure it with spring-security-messaging.We have custom security expressions (_all examples are bogus_) ieMy websocket security config class extends but I can't find a method either in or in where I can inject my instance of ,1194
Extract Method,Add support for AES/GCM to Crypto module It's nice to get something sensible out of the box but the default AES/CBC/PKCS5Padding is not the best choice any more if AES/GCM is available. Obviously existing apps will break if we change the default but it would be nice to add an extra option to tweak the algorithm assuming it's supported in Java. ,1195
Extract Method,SecurityContextHolderAwareRequestFilter RequestFactory on property change We should update the when properties change to ensure a consistent state. ,1197
Extract Method,Add AbstractPreAuthenticatedProcessingFilter principalChanged(HttpServletRequest Authentication) This will allow implementations to override a method to determine if the principal has changed. ,1198
Rename Method,Make type() call in projections optional (or remove it) I think the type() call should be optional. The compiler can also cast the data set directly and the result type is computed from the input types anyways.,1201
Extract Method,Add aggregations for streaming Add support for summation mimimum and maximum aggregations based on the reduce functionality.,1203
Extract Method,No createCollectionsEnvironment in Java API In the Scala API the ExecutionEnvironment has the method createCollectionEnvironment but not in the Java API. We should stick to one approach in both APIs.,1211
Extract Method,Support multi-character field delimiters in CSVInputFormats The CSVInputFormat supports multi-char (String) line delimiters but only single-char (char) field delimiters.This issue proposes to add support for multi-char field delimiters.,1213
Rename Method,[GitHub] Rework Configuration Objects Currently the configurations are implemented hacky. Everything is represented as a serialized string and there is no clean interface such that different flavors of configurations (global- delegatin- default) are inconsistent.I propose to rework the configuration as a map of objects which are serialized on demand with either a serialization library or default serialization mechanisms. Factoring out the interface of a Configuration allows to keep all flavors consistent.---------------- Imported from GitHub ----------------Url: https://github.com/stratosphere/stratosphere/issues/12Created by: [StephanEwen|https://github.com/StephanEwen]Labels: enhancement Created at: Mon Apr 29 23:43:11 CEST 2013State: open,1216
Extract Method,Use program source line of invocation  as the default function name Right now the default function name is {{getClass().getName()}}.In many cases this is not really very revealing. Using line in the source code where the function is invoked is more helpful.This can be easily obtained by{code}Thread.getCurrentThread().getStackTrace()[0]{code},1217
Extract Method,Improve File Input Split assignment While running some DFS read-intensive benchmarks I found that the assignment of input splits is not optimal. In particular in cases where the numWorker != numDataNodes and when the replication factor is low (in my case it was 1).In the particular example the input had 40960 splits of which 4694 were read remotely.  Spark did only 2056 remote reads for the same dataset.With the replication factor increased to 2 Flink did only 290 remote reads. So usually users shouldn't be affected by this issue.,1219
Extract Method,"Add static code analysis for UDFs Flink's Optimizer takes information that tells it for UDFs which fields of the input elements are accessed modified or frwarded/copied. This information frequently helps to reuse partitionings sorts etc. It may speed up programs significantly as it can frequently eliminate sorts and shuffles which are costly.Right now users can add lightweight annotations to UDFs to provide this information (such as adding {{@ConstandFields(""0->3 1 2->1"")}}.We worked with static code analysis of UDFs before to determine this information automatically. This is an incredible feature as it ""magically"" makes programs faster.For record-at-a-time operations (Map Reduce FlatMap Join Cross) this works surprisingly well in many cases. We used the ""Soot"" toolkit for the static code analysis. Unfortunately Soot is LGPL licensed and thus we did not include any of the code so far.I propose to add this functionality to Flink in the form of a drop-in addition to work around the LGPL incompatibility with ALS 2.0. Users could simply download a special ""flink-code-analysis.jar"" and drop it into the ""lib"" folder to enable this functionality. We may even add a script to ""tools"" that downloads that library automatically into the lib folder. This should be legally fine since we do not redistribute LGPL code and only dynamically link it (the incompatibility with ASL 2.0 is mainly in the patentability if I remember correctly).Prior work on this has been done by [~aljoscha] and [~skunert] which could provide a code base to start with.*Appendix*Hompage to Soot static analysis toolkit: http://www.sable.mcgill.ca/soot/Papers on static analysis and for optimization: http://stratosphere.eu/assets/papers/EnablingOperatorReorderingSCA_12.pdf and http://stratosphere.eu/assets/papers/openingTheBlackBoxes_12.pdfQuick introduction to the Optimizer: http://stratosphere.eu/assets/papers/2014-VLDBJ_Stratosphere_Overview.pdf (Section 6)Optimizer for Iterations: http://stratosphere.eu/assets/papers/spinningFastIterativeDataFlows_12.pdf (Sections 4.3 and 5.3)",1221
Rename Method,Rework Constant Field Annotations Constant field annotations are used by the optimizer to determine whether physical data properties such as sorting or partitioning are retained by user defined functions.The current implementation is limited and can be extended in several ways:- Fields that are copied to other positions- Field definitions for non-tuple data types (Pojos)There is a pull request (#83) that goes into this direction and which can be extended.,1222
Extract Method,"Allow setting custom file extensions for files created by the FileOutputFormat A user requested the ability to name avro files with the ""avro"" extension.",1223
Extract Method,"Command-line interface verbose option & error reporting Let me run just a basic Flink job and add the verbose flag. It's a general option so let me add it as a first parameter:> ./flink -v run ../examples/flink-java-examples-0.8.0-WordCount.jar hdfs:///input hdfs:///output9Invalid action!./flink <ACTION> [GENERAL_OPTIONS] [ARGUMENTS]  general options:     -h--help      Show the help for the CLI Frontend.     -v--verbose   Print more detailed error messages.Action ""run"" compiles and runs a program.  Syntax: run [OPTIONS] <jar-file> <arguments>  ""run"" action arguments:     -c--class <classname>           Class with the program entry point (""main""                                      method or ""getPlan()"" method. Only needed                                      if the JAR file does not specify the class                                      in its manifest.     -m--jobmanager <host:port>      Address of the JobManager (master) to                                      which to connect. Use this flag to connect                                      to a different JobManager than the one                                      specified in the configuration.     -p--parallelism <parallelism>   The parallelism with which to run the                                      program. Optional flag to override the                                      default value specified in the                                      configuration.Action ""info"" displays information about a program.  ""info"" action arguments:     -c--class <classname>           Class with the program entry point (""main""                                      method or ""getPlan()"" method. Only needed                                      if the JAR file does not specify the class                                      in its manifest.     -e--executionplan               Show optimized execution plan of the                                      program (JSON)     -m--jobmanager <host:port>      Address of the JobManager (master) to                                      which to connect. Use this flag to connect                                      to a different JobManager than the one                                      specified in the configuration.     -p--parallelism <parallelism>   The parallelism with which to run the                                      program. Optional flag to override the                                      default value specified in the                                      configuration.Action ""list"" lists running and finished programs.  ""list"" action arguments:     -m--jobmanager <host:port>   Address of the JobManager (master) to which                                   to connect. Use this flag to connect to a                                   different JobManager than the one specified                                   in the configuration.     -r--running                  Show running programs and their JobIDs     -s--scheduled                Show scheduled prorgrams and their JobIDsAction ""cancel"" cancels a running program.  ""cancel"" action arguments:     -i--jobid <jobID>            JobID of program to cancel     -m--jobmanager <host:port>   Address of the JobManager (master) to which                                   to connect. Use this flag to connect to a                                   different JobManager than the one specified                                   in the configuration.What just happened? This results in a lot of output which is usually generated if you use the --help option on command-line tools. If your terminal window is large enough then you will see a tiny message:""Please specify an action"". I did specify an action. Strange. If you read the help messages carefully you see that ""general options"" belong to the action.> ./flink run -v ../examples/flink-java-examples-0.8.0-WordCount.jar hdfs:///input hdfs:///output9For the sake of mitigating user frustration let us also accept -v as the first argument. It may seem trivial for the day-to-day Flink user but makes a difference for a novice.",1224
Extract Method,Add strictly local input split assignment 0,1225
Inline Method,Make Akka timeout configurable in YARN client. 0,1226
Rename Method,Add option to switch between Avro and Kryo serialization for GenericTypes Allow users to switch the underlying serializer for GenericTypes.,1228
Rename Method,Add option to pass Configuration to LocalExecutor Right now its not possible for users to pass custom configuration values to Flink when running it from within an IDE.It would be very convenient to be able to create a local execution environment that allows passing configuration files.,1229
Extract Method,Add pre-aggregator for time windows Currently there is only support for pre-aggregators for tumbling policies.A pre-aggregator should be added for time policies.,1230
Rename Method,Add more tests for Kafka Connectors The current {{KafkaITCase}} is only doing a single test.We need to refactor that test so that it brings up a Kafka/Zookeeper server and than performs various tests:Tests to include:- A topology with non-string types MERGED IN 359b39c3- A topology with a custom Kafka Partitioning class MERGED IN 359b39c3- A topology testing the regular {{KafkaSource}}. MERGED IN 359b39c3- Kafka broker failure MERGED IN cb34e976- Test with large records (up to 30 MB) MERGED IN 354922be- Flink TaskManager failure,1231
Rename Method,"Rename the function name from ""getCurrentyActiveConnections"" to ""getCurrentActiveConnections"" in  org.apache.flink.runtime.blob I think the function name ""getCurrentyActiveConnections"" in ' org.apache.flink.runtime.blob' is a wrong spelling it should be ""getCurrentActiveConnections"" is more better and also I add some comments about the function and the Tests.",1232
Inline Method,Add type hints to the streaming api The streaming operators currently don't support type hints as the batch api does with the `returns(...)` method. The functionality is essentially provided already by the setType method but it should be replaced and modified to fully provide the necessary functionality.,1234
Rename Method,Refactor streaming scala api to use returns for adding typeinfo Currently the streaming scala api uses transform to pass the extracted type information instead of .returns. This leads to a lot of code duplication.,1235
Rename Method,Split SubmitTask method up into two phases: Receive TDD and instantiation of TDD A user reported that a job times out while submitting tasks to the TaskManager. The reason is that the JobManager expects a TaskOperationResult response upon submitting a task to the TM. The TM downloads then the required jars from the JM which blocks the actor thread and can take a very long time if many TMs download from the JM. Due to this the SubmitTask future throws a TimeOutException.A possible solution could be that the TM eagerly acknowledges the reception of the SubmitTask message and executes the task initialization within a future. The future will upon completion send a UpdateTaskExecutionState message to the JM which switches the state of the task from deploying to running. This means that the handler of SubmitTask future in {{Execution}} won't change the state of the task.,1236
Rename Method,Make python tests less verbose Currently the python tests print a lot of log messages to stdout. Furthermore there seems to be some println statements which clutter the console output. I think that these log messages are not required for the tests and thus should be suppressed. ,1237
Extract Method,"Task Failures and Error Handling This is an issue to keep track of subtasks for error handling of task failures.The ""design doc"" for this can be found here: https://cwiki.apache.org/confluence/display/FLINK/Task+Failures+and+Error+Handling",1238
Extract Method,String delimiter for SocketTextStream The SocketTextStreamFunction uses a character delimiter despite other parts of the API using String delimiter.,1239
Move Method,Simplify Gelly Jaccard similarity example The Gelly Jaccard similarity example can be simplified by replacing the groupReduceOnEdges method with the simpler reduceOnEdges.,1241
Move Method,Configure number of vcores Currently the number of vcores per YARN container is set to 1.It is desirable to allow configuring this value. As a simple heuristic it makes sense to at least set it to the number of slots per container.,1243
Rename Method,Fail YARN application on failed single-job YARN cluster Users find it confusing that jobs submitted in single-job YARN cluster mode leave the Flink YARN application in state SUCCEEDED after the job fails.,1244
Extract Method,Add named attribute access to Storm compatibility layer Currently Bolts running in Flink can access fields only by index. Enabling named index access is possible for whole topologies and Tuple-type as well as POJO type inputs for embedded Bolts.,1246
Rename Method,Rename OperatorState methods to .value() and .update(..) We should rename OperatorState methods to .value() and .update(..) from getState and updateState to make it more clear.,1247
Extract Method,Clean up StreamRecord and Watermark types StreamRecords and Watermarks can be returned from streaming inputs alternatingly. They should have a common super type to make the code cleaner and allow for proper generic typing.,1254
Extract Method,Add type hint with TypeExtactor call on Hint Type As per discussion with [~aljoscha]A very good and type safe way to supply type hints would be the following:Define a hint class that takes a generic parameter.{code}public abstract class TypeHint<T> {}{code}The hints would have the following method:{code}public DataSet<T> returns(TypeHint<T> hint);{code}It would be used like this:{code}DataSet<Sting> data = ...;data  .flatMap( (String str Collector<Integer> out) -> out.collect(str.length()) )  .returns( new TypeHint<Integer>() {}){code}This would create an inline subclass of the hint from which the type could be extracted. The generics would ensure that it is typesafe.,1256
Extract Method,Support blocking job submission with Job Manager recovery Submitting a job in a blocking fashion with JobManager recovery and a failing JobManager fails on the client side (the one submitting the job). The job still continues to be recovered.I propose to add simple support to re-retrieve the leading job manager and update the client actor with it and then wait for the result as before.As of the current standing in PR #1153 (https://github.com/apache/flink/pull/1153) the job manager assumes that the same actor is running and just keeps on sending execution state updates etc. (if the listening behaviour is not detached).,1257
Extract Method,Remove old web interface and default to the new one 0,1258
Rename Method,Gelly API improvements During the Flink Forward Gelly School training I got some really valuable feedback from participants on what they found hard to grasp or non-intuitive in the API. Based on that I propose we make the following improvements:-  rename the mapper in creation methods to {{VertexInitializer}} so that its purpose is easier to understand.- add a {{fromTuple2DataSet}} method to easily create graphs from {{Tuple2}} datasets i.e. edges with no values.- in {{joinWith*}} methods it is hard to understand what are the parameters in the mapper and what will be the output. I suggest we flatten them try to give intuitive names and improve the javadocs.- in neighborhood methods it is hard to understand what are the arguments of the {{EdgeFunction.iterateEdges}} and {{ReduceEdgesFunction.reduceEdges}}. Javadocs and parameter names could be improved here too.,1259
Rename Method,"Create database state backend The goal is to create a database state backend that can be used with JDBC supporting databases.The backend should support the storage of non-partitioned states and also the storage of Key-value states with high throughput. As databases provide advanced querying functionality the key-value state can be implemented to be lazily fetched and should scale to ""arbitrary"" state sizes by not storing the non-active key-values on heap.An adapter class will be provided that can help bridge the gap between different sql implementations.",1260
Rename Method,Add periodic offset commit to Kafka Consumer if checkpointing is disabled Flink only writes the offsets from the consumer into ZK if checkpointing is enabled.We should have a similar feature to Kafka's autocommit in our consumer.Issue reported by user: http://stackoverflow.com/questions/33501574/flink-kafka-why-am-i-losing-messages,1261
Extract Method,Integrate the Either Java type with the TypeExtractor Integrate the Either Java type with the TypeExtractor so that the APIs recognize the type and choose the type info properly.,1262
Move Method,[py] Remove combiner The current combiner implementation in the PythonAPI is quite a mess. It adds a lot of unreadable clutter is inefficient at times and can straight up break in some edge cases.I will revisit this feature after FLINK-2501 is resolved. Several changes for that issue will make the reimplementation easier.,1263
Extract Method,Make ApplicationMaster/JobManager akka port configurable Similar to the BlobServer the YARN ApplicationMaster should allow starting it on a specified list or range of ports.In cases where only certain ports are allowed by a firewall users can specify a range of ports where they want the AM to allocate its RPC port,1264
Rename Method,Rename Either creation methods to avoid name clash with projection methods Currently the method signatures for creating Either values `Either.left(left)` and the projection methods `either.left()` only differ in the parameters. This makes it awkward to use with lambdas such as: 'eitherStream.filter(Either:isLeft).map(Either::left)'The above code is currently impossible.I suggest to change the creation methods to `Either.createLeft(left)` and `Either.createRight(right)` and also to directly expose the Left Right classes.,1265
Inline Method,Allow setting custom start-offsets for the Kafka consumer Currently the Kafka consumer only allows to start reading from the earliest available offset or the current offset.Sometimes users want to set a specific start offset themselves.,1266
Rename Method,Decouple restart strategy from ExecutionGraph Currently the {{ExecutionGraph}} supports the following restart logic: Whenever a failure occurs and the number of restart attempts aren't depleted wait for a fixed amount of time and then try to restart. This behaviour can be controlled by the configuration parameters {{execution-retries.default}} and {{execution-retries.delay}}.I propose to decouple the restart logic from the {{ExecutionGraph}} a bit by introducing a strategy pattern. That way it would not only allow us to define a job specific restart behaviour but also to implement different restart strategies. Conceivable strategies could be: Fixed timeout restart exponential backoff restart partial topology restarts etc.This change is a preliminary step towards having a restart strategy which will scale the parallelism of a job down in case that not enough slots are available.,1268
Move Method,Remove Unused ProcessingTime EventTime and AbstractTime These are leftovers only having Time should suffice for specifying time.,1269
Rename Method,Implement DataSet.count using a single operator {{DataSet.count}} is currently implemented using a {{FlatMapFunction}} followed by a {{DiscardingOutputFormat}}. As noted by [~StephanEwen] in [FLINK-2716] this can be done with only a {{RichOutputFormat}}.This change is also applicable to {{DataSet.collect}} and {{Utils.CollectHelper}}.,1270
Extract Method,Add option to eagerly deploy channels Intermediate partitions are consumed via input channels. These channels are instantiated lazily when the partition producer emits the first record. This can lead to higher latencies for the first records passing the pipeline.In order to decrease this latency we can deploy channels eagerly.I would like to add this as a flag to the intermediate results of the execution graph. On deployment the task can then deploy the input channels as soon as the intermediate stream has been registered at the partition manager.,1271
Move Method,Add example for reading and writing to Kafka The Kafka connector of Flink is the most used streaming connector. I would like to add two examples showing how to read and write data into a Kafka topic.,1273
Inline Method,[py] Generalize OperationInfo transfer A set number of arguments is transferred whenever a user defines an operation. For a CSV Source for example these are delimiters/filepath for a map function only the set ID'S are transferred. As such for all operators a separate routine is defined that governs which arguments are transferred.While working on FLINK-3275 I realized that adding a new argument/parameter in this case parallelism is not as straightforward as it could be. Most newly added operators will require a new routine; whereas adding new arguments may require the modification of multiple routines. Over times this is bound to become a big mess.All arguments are stored in an OperationInfo object which also contains default values for all unused arguments. I want to generalize the whole affair by transferring all arguments used or not. This will reduce clutter make it easier to add new parameters (only 4 new lines needed 2 for defining new fields inside Java/Python OperationInfo Classes; 1 each for sending/receiving the new argument) and will make the transfer consistent across all operations.,1274
Extract Method,"Create constructors for FsStateBackend in RocksDBBackens I think no user understands what they are supposed to do when they should define a ""backing state backend"".The most common thing will probably be that the RocksDB snapshots are backed up to HDFS and that this is also the backing state backend.A simple constructor with a single URI/Path/String that configures both at the same time should be added to make this accessible to users.",1275
Rename Method,Scramble HashPartitioner hashes The {{HashPartitioner}} used by the streaming API does not apply any hash scrambling against bad user hash functions.We should apply a murmor or jenkins hash on top of the hash code similar as in the {{DataSet}} API.,1278
Extract Method,Add SBT build tool description to quickstart documentation Add documentation about the SBT quickstart templates to the quickstart documentation.,1281
Rename Method,Replace random NIC selection heuristic by InetAddress.getLocalHost Currently the {{ConnectionUtils.findAddressUsingStrategy}} method returns the first {{NetworkInterface}} whose address is not a loop back address not a link local address and an {{Inet4Address}}. Before returning this address it is retried to connect to the {{JobManager}} using the {{InetAddress.getLocalHost}} address a last time.The heuristic if not choosing the {{InetAddress.getLocalHost}} often makes no sense because it returns a random {{NetworkInterface}} address. It would be better to simply return the {{InetAddress.getLocalHost()}} instead.,1283
Rename Method,Re-enable Table API explain The Table API explain was temporarily disabled to port the Table API on top of Calcite. It should be re-enabled before merge the changes back to the master branch.,1284
Rename Method,Change access of DataSetUtils.countElements() to 'public'  The access of DatasetUtils.countElements() is presently 'private' change that to be 'public'. We happened to be replicating the functionality in our project and realized the method already existed in Flink.,1285
Rename Method,"Remove Nephele references There still exist a few references to nephele which should be removed:{code}flink\docs\setup\local_setup.md:   79  $ tail log/flink-*-jobmanager-*.log   80  INFO ... - Initializing memory manager with 409 megabytes of memory   81: INFO ... - Trying to load org.apache.flinknephele.jobmanager.scheduler.local.LocalScheduler as scheduler   82  INFO ... - Setting up web info server using web-root directory ...   83: INFO ... - Web info server will display information about nephele job-manager on localhost port 8081.   84  INFO ... - Starting web info server for JobManager on port 8081   85  ~~~   ..  118  $ cd flink  119  $ bin/start-local.sh  120: Starting Nephele job manager  121  ~~~{code}{code}flink\flink-runtime\src\main\java\org\apache\flink\runtime\operators\TaskContext.java:   70: AbstractInvokable getOwningNepheleTask();{code}{code}flink\flink-runtime\src\main\java\org\apache\flink\runtime\operators\BatchTask.java: 1149   * @param message The main message for the log. 1150   * @param taskName The name of the task. 1151:  * @param parent The nephele task that contains the code producing the message. 1152   * 1153   * @return The string for logging. .... 1254   */ 1255  @SuppressWarnings(""unchecked"") 1256: public static <T> Collector<T> initOutputs(AbstractInvokable nepheleTask ClassLoader cl TaskConfig config 1257  List<ChainedDriver<? ?>> chainedTasksTarget 1258  List<RecordWriter<?>> eventualOutputs{code}",1286
Extract Method,Scala API for CEP Currently The CEP library does not support Scala case classes because the {{TypeExtractor}} cannot handle them. In order to support them it would be necessary to offer a Scala API for the CEP library.,1287
Rename Method,Consolidate TimestampAssigner Methods in Kafka Consumer On {{DataStream}} the methods for setting a TimestampAssigner/WatermarkEmitter are called {{assignTimestampsAndWatermarks()}} while on {{FlinkKafkaConsumer*}} they are called {{setPunctuatedWatermarkEmitter()}} and {{setPeriodicWatermarkEmitter()}}.I think these names should be matched also the name {{setWatermarkEmitter}} does not hint at the fact that the assigner primarily assigns timestamps.,1288
Rename Method,Introduce key groups for key-value state to support dynamic scaling In order to support dynamic scaling it is necessary to sub-partition the key-value states of each operator. This sub-partitioning which produces a set of key groups allows to easily scale in and out Flink jobs by simply reassigning the different key groups to the new set of sub tasks. The idea of key groups is described in this design document [1]. [1] https://docs.google.com/document/d/1G1OS1z3xEBOrYD4wSu-LuBCyPUWyFd9l3T9WyssQ63w/edit?usp=sharing,1289
Inline Method,RabbitMQ Source/Sink standardize connection parameters The RabbitMQ source and sink should have the same capabilities in terms of establishing a connection currently the sink is lacking connection parameters that are available on the source. Additionally VirtualHost should be an offered parameter for multi-tenant RabbitMQ clusters (if not specified it goes to the vhost '/').Connection Parameters===================- Host - Offered on both- Port - Source only- Virtual Host - Neither- User - Source only- Password - Source onlyAdditionally it might be worth offer the URI as a valid constructor because that would offer all 5 of the above parameters in a single String.,1291
Move Method,Add a Kafka TableSink with JSON serialization Add a TableSink that writes JSON serialized data to Kafka.,1292
Extract Method,Add possiblity for the RMQ Streaming Source to customize the queue This patch adds the possibilty for the user of the RabbitMQStreaming Connector to customize the queue which is used. Thereare use-cases in which you want to set custom parameters for thequeue (i.e. TTL of the messages if Flink reboots) or thepossibility to bind the queue to an exchange afterwards.The commit doesn't change the actual behaviour but makes itpossible for users to override the newly create `setupQueue`method and cutomize their implementation. This was not possiblebefore.,1294
Rename Method,"Change the name of ternary condition operator  'eval' to  '?'   The ternary condition operator in Table API is named {{eval}} for example: {{(42 > 5).eval(""A"" ""B"")}} leads to ""A"".  IMO the eval function is not well understood. Instead the ""?"" is a better choice I think which is used in Java for condition operator. It will be clearer and more literal understood e.g.{{(42 > 5).?(""A"" ""B"")}} or {{(42 > 5) ? (""A"" ""B"")}}If it make sense I will pull a request.",1296
Inline Method,Do not start Metrics Reporter by default By default we start a JMX reported that binds to a port and comes with extra threads. We should not start any reported by default to keep the overhead to a minimum.,1298
Extract Method,"Metric naming improvements A metric currently has two parts to it:  - The name of that particular metric  - The ""scope"" (or namespace) defined by the group that contains the metric.A metric group actually always implicitly has a map of naming ""tags"" like:  - taskmanager_host : <some-hostname>  - taskmanager_id : <id>  - task_name : ""map() -> filter()""We derive the scope from that map following the defined scope formats.For JMX (and some users that use JMX) it would be natural to expose that map of tags. Some users reconstruct that map by parsing the metric scope. JMX we can expose a metric like:  - domain: ""taskmanager.task.operator.io""  - name: ""numRecordsIn""  - tags: { ""hostname"" -> ""localhost"" ""operator_name"" -> ""map() at X.java:123"" ... }For many other reporters the formatted scope makes a lot of sense since they think only in terms of (scope metric-name).We may even have the formatted scope in JMX as well (in the domain) if we want to go that route. [~jgrier] and [~Zentol] - what do you think about that?[~mdaxini] Does that match your use of the metrics?",1299
Extract Method,CsvTableSource does not support reading SqlTimeTypeInfo types The Table API's {{CsvTableSource}} does not support to read all Table API supported data types. For example it is not possible to read {{SqlTimeTypeInfo}} types via the {{CsvTableSource}}.,1300
Extract Method,Add possiblity for the RMQ Streaming Sink to customize the queue This patch adds the possibilty for the user of the RabbitMQStreaming Sink to customize the queue which is used. This adopts the behavior of [FLINK-4025] for the sink.The commit doesn't change the actual behaviour but makes itpossible for users to override the `setupQueue`method and customize their implementation. This was only possible for the RMQSource before. The Sink and the Source offer now both the same functionality so this should increase usability. [FLINK-4025] = https://issues.apache.org/jira/browse/FLINK-4025,1301
Rename Method,"Rename ""recovery.mode"" config key to ""high-availability"" Currently HA is configured via the following configuration keys:{code}recovery.mode: STANDALONE // No high availability (HA)recovery.mode: ZOOKEEPER // HA{code}This could be more straight forward by simply renaming the key to {{high-availability}}. Furthermore the term {{STANDALONE}} is overloaded. We already have standalone cluster mode.{code}high-availability: NONE // No HAhigh-availability: ZOOKEEPER // HA via ZooKeeper{code}The {{recovery.mode}} configuration keys would have to be deprecated before completely removing them.",1302
Rename Method,"New Flink-specific option to set starting position of Kafka consumer without respecting external offsets in ZK / Broker Currently to start reading from the ""earliest"" and ""latest"" position in topics for the Flink Kafka consumer users set the Kafka config {{auto.offset.reset}} in the provided properties configuration.However the way this config actually works might be a bit misleading if users were trying to find a way to ""read topics from a starting position"". The way the {{auto.offset.reset}} config works in the Flink Kafka consumer resembles Kafka's original intent for the setting: first existing external offsets committed to the ZK / brokers will be checked; if none exists then will {{auto.offset.reset}} be respected.I propose to add Flink-specific ways to define the starting position without taking into account the external offsets. The original behaviour (reference external offsets first) can be changed to be a user option so that the behaviour can be retained for frequent Kafka users that may need some collaboration with existing non-Flink Kafka consumer applications.How users will interact with the Flink Kafka consumer after this is added with a newly introduced {{flink.starting-position}} config:{code}Properties props = new Properties();props.setProperty(""flink.starting-position"" ""earliest/latest"");props.setProperty(""auto.offset.reset"" ""...""); // this will be ignored (log a warning)props.setProperty(""group.id"" ""..."") // this won't have effect on the starting position anymore (may still be used in external offset committing)...{code}Or reference external offsets in ZK / broker:{code}Properties props = new Properties();props.setProperty(""flink.starting-position"" ""external-offsets"");props.setProperty(""auto.offset.reset"" ""earliest/latest""); // default will be latestprops.setProperty(""group.id"" ""...""); // will be used to lookup external offsets in ZK / broker on startup...{code}A thing we would need to decide on is what would the default value be for {{flink.starting-position}}.Two merits I see in adding this:1. This compensates the way users generally interpret ""read from a starting position"". As the Flink Kafka connector is somewhat essentially a ""high-level"" Kafka consumer for Flink users I think it is reasonable to add Flink-specific functionality that users will find useful although it wasn't supported in Kafka's original consumer designs.2. By adding this the idea that ""the Kafka offset store (ZK / brokers) is used only to expose progress to the outside world and not used to manipulate how Kafka topics are read in Flink (unless users opt to do so)"" is even more definite and solid. There was some discussion in this PR (https://github.com/apache/flink/pull/1690 FLINK-3398) on this aspect. I think adding this ""decouples"" more Flink's internal offset checkpointing from the external Kafka's offset store.",1303
Extract Method,Add Offset Parameter to WindowAssigners Currently windows are always aligned to EPOCH which basically means days are aligned with GMT. This is somewhat problematic for people living in different timezones.And offset parameter would allow to adapt the window assigner to the timezone.,1304
Extract Method,"Allow uploaded jar directory to be configurable  I notice sometimes it's preferable to have uploaded jars to be put into a configurable directory location instead only have it at runtime. In this case we can pre-load the directory with jars in a docker image and allows us to leverage the jobmanager restful interface to start/kill jobs.WebRuntimeMonitor.javaString uploadDirName = ""flink-web-upload-"" + UUID.randomUUID();this.uploadDir = new File(getBaseDir(config) uploadDirName);",1305
Rename Method,Unify CheckpointCoordinator and SavepointCoordinator The Checkpoint coordinator should have the functionality of both handling checkpoints and savepoints.The difference between checkpoints and savepoints is minimal:  - savepoints always write the root metadata of the checkpoint  - savepoints are always full (never incremental)The commonalities are large  - jobs should be able to resume from checkpoint or savepoints  - jobs should fall back to the latest checkpoint or savepointThis subsumes issue https://issues.apache.org/jira/browse/FLINK-3397,1306
Extract Method,Enable RollingSink to custom HDFS client configuration Optimizing the configuration of hdfs client in different situation such as {{io.file.buffer.size}}  can make rolling sink perform better. ,1307
Rename Method,Provide support for asynchronous operations over streams Many Flink users need to do asynchronous processing driven by data from a DataStream.  The classic example would be joining against an external database in order to enrich a stream with extra information.It would be nice to add general support for this type of operation in the Flink API.  Ideally this could simply take the form of a new operator that manages async operations keeps so many of them in flight and then emits results to downstream operators as the async operations complete.,1309
Rename Method,Make the ExecutionGraph independent of Akka Currently the {{ExecutionGraph}} strongly depends on Akka as it requires an {{ActorSystem}} to create the {{CheckpointCoordinatorDeActivator}}. Furthermore it allows {{ActorGateways}} to register for job status and execution updates.In order to improve modularization and abstraction I propose to introduce proper listener interfaces. This would also allow to get rid of the {{CheckpointCoordinatorDeActivator}} by simply implementing this interface. Furthermore it will pave the way for the upcoming Flip-6 refactoring as it offers a better abstraction.,1310
Extract Method,Remove ForkableFlinkMiniCluster After addressing FLINK-4424 we should be able to get rid of the {{ForkableFlinkMiniCluster}} since we no longer have to pre-determine a port in Flink. Thus by setting the ports to {{0}} and letting the OS choose a free port there should no longer be conflicting port requests. Consequently the {{ForkableFlinkMiniCluster}} will become obsolete.,1311
Rename Method,Introduce SlotProvider for Scheduler Currently the {{Scheduler}} maintains a queue of available instances which it scans if it needs a new slot. If it finds a suitable instance (having free slots available) it will allocate a slot from it. This slot allocation logic can be factored out and be made available via a {{SlotProvider}} interface. The {{SlotProvider}} has methods to allocate a slot given a set of location preferences. Slots should be returned as {{Futures}} because in the future the slot allocation might happen asynchronously (Flip-6). In the first version the {{SlotProvider}} implementation will simply encapsulate the existing slot allocation logic extracted from the {{Scheduler}}. When a slot is requested it will return a completed or failed future since the allocation happens synchronously.The refactoring will have the advantage to simplify the {{Scheduler}} class and to pave the way for upcoming refactorings (Flip-6).,1312
Extract Method,Guard Flink processes against blocking shutdown hooks Resource managers like YARN send the JVM the {{SIGTERM}} signal to kill the process if it wants to terminate a process.With {{SIGTERM}} the JVM shutdown hooks run and may cause the process to freeze up on shutdown. Especially since all dependencies (like Hadoop) may install shutdown hooks (and do so) it is not in Flink's control to make sure all Shutdown hooks are well behaved.I propose to add a guard that forcibly terminates the JVM if clean shutdown does not succeed within a certain time (say five seconds).,1314
Move Method,Separate configuration parsing from MetricRegistry In order to decouple the {{MetricRegistry}} object instantiation from the global configuration we could introduce a {{MetricRegistryConfiguration}} object which encapsulates all necessary information for the {{MetricRegistry}}. The {{MetricRegistryConfiguration}} could have a static method to be generated from a {{Configuration}}. ,1315
Inline Method,TaskManager should commit suicide after cancellation failure In case of a failed cancellation e.g. the task cannot be cancelled after a given time the {{TaskManager}} should kill itself. That way we guarantee that there is no resource leak. This behaviour acts as a safety-net against faulty user code.,1316
Rename Method,Port WebFrontend to new metric system While the WebFrontend has access to the metric system it still relies on older code in some parts.The TaskManager metrics are still gathered using the Codahale library and send with the heartbeats.Task related metrics (numRecordsIn etc) are still gathered using accumulators which are accessed through the execution graph.,1317
Extract Method,"Implement Mini Cluster This task is to implement the embedded mini cluster (similar to the {{LocalFlinkMiniCluster}} based on the new components developed in ""Flink Improvement Proposal 6""",1318
Rename Method,Add MapState for keyed streams Many states in keyed streams are organized as key-value pairs. Currently these states are implemented by storing the entire map into a ValueState or a ListState. The implementation however is very costly because all entries have to be serialized/deserialized when updating a single entry. To improve the efficiency of these states MapStates are urgently needed. ,1319
Rename Method,Don't block on buffer request after broadcastEvent  After broadcasting an event (like the checkpoint barrier) the record writer might block on a buffer request although that buffer will only be needed on the next write on that channel.Instead of assuming that each serializer has a buffer set we can change the logic in the writer to request the buffer when it requires one.,1320
Rename Method,Introduce non async future methods Currently Flink's {{Futures}} support only async methods. This means that each chained operation is executed potentially by a different thread. In some cases this is not necessary and the context switches inflict additional costs. Therefore I propose to add non async methods to the {{Futures}.,1323
Extract Method,Expose input/output buffers and bufferPool usage as a metric for a Task We should expose the following Metrics on the TaskIOMetricGroup: 1. Buffers.inputQueueLength: received buffers of InputGates for a task 2. Buffers.outputQueueLength: buffers of produced ResultPartitions for a task 3. Buffers.inPoolUsage: usage of InputGates buffer pool for a task 4. Buffers.outPoolUsage: usage of produced ResultPartitions buffer pool for a task,1324
Extract Method,GenericWriteAheadSink: Decouple the creating from the committing subtask for a pending checkpoint So far the GenericWriteAheadSink expected thatthe subtask that wrote a pending checkpoint to the state backend will be also the one to commit it tothe third-party storage system.This issue targets at removing this assumption. To do this the CheckpointCommitter has to be able to dynamicallytake the subtaskIdx as a parameter when asking if a checkpoint was committed and also change thestate kept by the GenericWriteAheadSink to also include that subtask index of the subtask that wrote the pending checkpoint.This change is also necessary for making the operator rescalable.,1325
Extract Method,Add Scala API for KeyedStream.flatMap(TimelyFlatMapFunction) 0,1326
Rename Method,Allow the AbstractStreamOperatorTestHarness to test scaling down Currently the AbstractStreamOperatorTestHarness allows for testing an operator when scaling up through snapshot and restore. This is not enough as many interesting corner cases arise when scaling down or during arbitrary combinations of scaling up and down.This issue targets to add this functionality so that an operator can snapshot its state restore with different parallelism and later scale down or further up.,1327
Rename Method,Rename Methods in ManagedInitializationContext We should rename {{getManagedOperatorStateStore()}} to {{getOperatorStateStore()}} and  {{getManagedKeyedStateStore()}} to {{getKeyedStateStore()}}. There are no unmanaged stores and having that extra word there seems a bit confusing plus it makes the names longer.,1328
Rename Method,Provide Timestamp in TimelyFlatMapFunction Right now {{TimelyFlatMapFunction}} does not give the timestamp of the element in {{flatMap()}}.The signature is currently this:{code}void flatMap(I value TimerService timerService Collector<O> out) throws Exception;{code}if we add the timestamp it would become this:{code}void flatMap(I value Long timestamp TimerService timerService Collector<O> out) throws Exception;{code}The reason why it's a {{Long}} and not a {{long}} is that an element might not have a timestamp in that case we should hand in {{null}} here.This is becoming quite look so we could add a {{Context}} parameter that provides access to the timestamp and timer service.,1329
Extract Method,add more efficient isEvent check to EventSerializer -LocalInputChannel#getNextBuffer de-serialises all incoming events on the lookout for an EndOfPartitionEvent.-Some buffer code de-serialises all incoming events on the lookout for an EndOfPartitionEvent(now applies to PartitionRequestQueue#isEndOfPartitionEvent()).Instead if EventSerializer offered a function to check for an event type only without de-serialising the whole event we could save some resources.,1330
Move Method,Make all Testing Functions implement CheckpointedFunction Interface. Currently stateful functions implement the (old) Checkpointed interface.This is issue aims at porting all these function to the new CheckpointedFunction interface so that they can leverage the new capabilities by it. ,1331
Extract Method,make the BlobServer use a distributed file system Currently the BlobServer uses a local storage and in addition when the HA mode is set a distributed file system e.g. hdfs. This however is only used by the JobManager and all TaskManager instances request blobs from the JobManager. By using the distributed file system there as well we would lower the load on the JobManager and increase scalability.,1332
Rename Method,Improved resource cleanup in RocksDB keyed state backend Currently the resources such as taken snapshots or iterators are not always cleaned up in the RocksDB state backend. In particular not starting the runnable future will leave taken snapshots unreleased.We should improve the releases of all resources allocated through the RocksDB JNI bridge.,1333
Extract Method,Extending AllWindow Function Metadata Following the logic behind [12] ProcessAllWindowFunction can be introduced in Flink and AllWindowedStream can be extended in order to support them. [1] https://cwiki.apache.org/confluence/display/FLINK/FLIP-2+Extending+Window+Function+Metadata[2] https://issues.apache.org/jira/browse/FLINK-4997,1334
Rename Method,Make the production functions rescalable (apart from the Rolling/Bucketing Sinks) This issue targets porting all the functions in the production code to the new state abstractions. These functions are:1) StatefulSequenceSource2) MessageAcknowledgingSourceBase3) FromElementsFunction4) ContinuousFileMonitoringFunction,1335
Rename Method,Consolidate and harmonize Window Translation Tests The tests that check whether API calls on {{WindowedStream}} (both Java and Scala) result in the correct runtime operation are scattered across {{TimeWindowTranslationTest}} and {{WindowTranslationTest}} and the test coverage is not the same for Scala and Java.We should ensure that we test all API calls and that we also test the Scala API with the same level of detail.,1336
Rename Method,Properly Close StateBackend in StreamTask when closing/canceling Right now the {{StreamTask}} never calls {{close()}} on the state backend.,1338
Extract Method,Make AbstractUdfStreamOperator aware of WrappingFunction Right now when using a {{WrappingFunction}} as happens for {{WindowFunction}} and also for some functions in the Scala API then using custom interfaces is not possible. These custom interfaces are for example the checkpointing functions such as {{ListCheckpointed}} and {{CheckpointedFunction}}.We should teach {{AbstractUdfStreamOperator}} about {{WrapingFunction}} so that it can correctly handle the case where wrapped user functions implement on of these interfaces.Also in the Scala API we have some custom functions that mimic {{WrappingFunction}} behaviour. These should be moved to use {{WrappingFunction}} if possible.,1339
Rename Method,Introduce state handle replication mode for CheckpointCoordinator Currently the {{CheckpointCoordinator}} only supports repartitioning of {{OperatorStateHandle}}s based on a split-and-distribute strategy. For future state types such as broadcast or union state we need a different repartitioning method that allows for replicating state handles to all subtasks.This is the first step on the way to implementing broadcast and union states.,1341
Extract Method,Improve Task and checkpoint logging  The logging of task and checkpoint logic could be improved to contain more information relevant for debugging.,1342
Rename Method,User-provided hashes for operators We could allow users to provided (alternative) hashes for operators in a StreamGraph. This can make migration between Flink versions easier in case the automatically produced hashes between versions are incompatible. For example users could just copy the old hashes from the web ui to their job.,1343
Extract Method,Try to reuse the resource location of prior execution attempt in allocating slot Currently when schedule execution to request to allocate slot from {{SlotPool}} the {{TaskManagerLocation}} parameter is empty collection. So for task fail over scenario the new execution attempt may be deployed to different task managers. If setting rockDB as state backend the performance is better if the data can be restored from local machine. So we try to reuse the {{TaskManagerLocation}} of prior execution attempt when allocating slot from {{SlotPool}}. If the {{TaskManagerLocation}} is empty from prior executions the behavior is the same with current status.,1344
Extract Method,Remove Mesos dynamic class loading Mesos uses dynamic class loading in order to load the {{ZooKeeperStateHandleStore}} and the {{CuratorFramework}} class. This can be replaced by a compile time dependency.,1345
Extract Method,remove unused KvStateRequestSerializer#serializeList KvStateRequestSerializer#serializeList is unused and instead the state backends' serialisation functions are used. Therefore remove this one and make sure KvStateRequestSerializer#deserializeList works with the state backends' ones.,1346
Move Method,Storm LocalCluster can't run with powermock Strom LocalCluster can't run with powermock. For example:The codes which commented in WrapperSetupHelperTest.testCreateTopologyContext,1347
Rename Method,"Add an Option to Deactivate Kryo Fallback for Serializers Some users want to avoid that Flink's serializers use Kryo as it can easily become a hotspot in serialization.For those users it would help if there is a flag to ""deactive generic types"". Those users could then see where types are used that default to Kryo and change these types (make them PoJos Value types or write custom serializers).There are two ways to approach that:  1. (Simple) Make {{GenericTypeInfo}} threw an exception whenever it would create a Kryo Serializer (when the respective flag is set in the {{ExecutionConfig}})  2. Have a static flag on the {{TypeExtractor}} to throw an exception whenever it would create a {{GenericTypeInfo}}. This approach has the downside of introducing some static configuration to the TypeExtractor but may be more helpful because it throws exceptions in the programs at points where the types are used (not where the serializers are created which may be much later).",1348
Extract Method,Clean up FileSystem The {{FileSystem}} class is overloaded and has methods that are not well supported. I suggest to do the following cleanups:  - Pull the safety net into a separate class  - Use the {{WriteMode}} to indicate overwriting behavior. Right now the {{FileSystem}} class defines that enum and never uses it. It feels weird.  - Remove the {{create(path overwrite blocksize reolication ...)}} method which is not really supported across file system implementations. For HDFS behavior should be set via the configuration anyways.All changes have to be made in a non-API-breaking fashion.,1350
Extract Method,Make handlers aware of their REST URLs The handlers in the WebRuntimeMonitor are currently unaware of the actual REST URL used. The handlers are simply registered under a given URL without any guarantee that the handler can actually deal with that URL.I propose to let handlers themselves specify under which URL's they are supposed to be reachable. This provides are tighter coupling between URL and handler.,1351
Extract Method,Add support for secure YARN clusters with Kerberos Auth The current YARN client will throw an exception (as of https://github.com/stratosphere/stratosphere/pull/591) if it detects a secure environment.---------------- Imported from GitHub ----------------Url: https://github.com/stratosphere/stratosphere/issues/592Created by: [rmetzger|https://github.com/rmetzger]Labels: enhancement YARN Created at: Sun Mar 16 11:05:07 CET 2014State: open,1352
Extract Method,Add support for null values to the java api Currently many runtime operations fail when encountering a null value. Tuple serialization should allow null fields.I suggest to add a method to the tuples called `getFieldNotNull()` which throws a meaningful exception when the accessed field is null. That way we simplify the logic of operators that should not dead with null fields like key grouping or aggregations.Even though SQL allows grouping and aggregating of null values I suggest to exclude this from the java api because the SQL semantics of aggregating null fields are messy.---------------- Imported from GitHub ----------------Url: https://github.com/stratosphere/stratosphere/issues/629Created by: [StephanEwen|https://github.com/StephanEwen]Labels: enhancement java api Milestone: Release 0.5.1Created at: Wed Mar 26 00:27:49 CET 2014State: open,1354
Extract Method,Extend JarFileCreator to automatically include dependencies We have a simple {{JarFileCreator}} which allows to add classes to a JAR file as follows:{code:java}JarFileCreator jfc = new JarFileCreator(jarFile);jfc.addClass(X.class);jfc.addClass(Y.class);jfc.createJarFile();{code}The created file can then be used with the remote execution environment which requires a JAR file to ship.I propose the following improvement: use [ASM|http://asm.ow2.org/] to extract all dependencies and add create the JAR file automatically.There is an [old tutorial|http://asm.ow2.org/doc/tutorial-asm-2.0.html] (for ASM 2) which implements a {{DependencyVisitor}}. Unfortuneately the code does not directly work with ASM 5 but it should be a good starting point.,1355
Rename Method,Throw exception if solution set is CoGrouped on the wrong key. Co Grouping with the solution set on a key other than the solution set is not possible. The program however does not cause an error but simply causes the program to not correctly match elements.I have a patch coming up...,1357
Extract Method,Add a yarn session parameter for the number of task slots per task manager In order to have a transparent and elegant YARN setup the shell needs a parameter to specify how many task slots each task manager should offer. If that flag is not specified then each taskmanager will offer as many slots as the JVM determines system cores. We may think about changing that to a default of one to make it more predictable.,1358
Extract Method,NettyConnectionManager does not close connections Network connections created via NettyConnectionManager are not closed.,1359
Extract Method,Aggregators are not exported Currently aggregator values cannot be saved after a Giraph job.  There should be a way to do this.,1360
Extract Method,Add Blocks Framework This will be a set of diffs adding Blocks Framework summary on each diff will explain it's contents.More context in:http://mail-archives.apache.org/mod_mbox/giraph-dev/201506.mbox/%3CCABJ-n3v-24YLzgNmrT3TZT6R8t4Vw1hrBcWWTghG_XgaC%3DYqrg%40mail.gmail.com%3E,1361
Extract Method,Allow IPs for Worker2Worker communication For worker2worker communication we use O(n^2) DNS lookups where n is the number of workers. If network infrastructure is less reliable combined with a large number of workers these DNS lookups occasionally fail. Fortunately we don't always need DNS lookups as in many clusters hosts can communicate with each other directly using IP addresses. Let's add a configuration option to use IP address instead of the host name. ,1363
Extract Method,Remove limit on the number of partitions Currently we have a limit on how many partitions we can have because we write all partition information to Zookeeper. We can instead send this information in requests and remove the hard limit.,1365
Extract Method,Use pool of byte arrays with InMemoryDataAccessor Have a pool of byte arrays with InMemoryDataAccessor to save on byte array creation and initialization.,1366
Extract Method,"Port of the HCC algorithm for identifying all connected components of a graph Port of the HCC algorithm that identifies connected components and assigns a componented id (the smallest vertex id in the component) to each vertex.The idea behind the algorithm is very simple: propagate the smallest vertex id along the edges to all vertices of a connected component until convergence. The number of supersteps necessary is equal to the length of the maximum diameter of all components + 1The original Hadoop-based variant of this algorithm was proposed by Kang Charalampos Tsourakakis and Faloutsos in ""PEGASUS: Mining Peta-Scale Graphs"" 2010http://www.cs.cmu.edu/~ukang/papers/PegasusKAIS.pdf",1367
Rename Method,Clarify messages behavior in BasicVertex initialize() can receive a null parameter for messages (at least that's what EdgeListVertex does). We should avoid that and pass an empty Iterable instead. That should be cheap for us inside of the InputFormat just passing a static immutable empty list.setMessages(Iterable<M>) should be changed to putMessages(Iterable<M>). the set prefix suggests an assignment while setMessages is used to transfer the messages to the internal datastructure the user is responsible for. putMessages() should clarify this.,1369
Rename Method,Changes to ExtendedByteArrayDataOutput AddingÂ a new stream type which extends fromÂ com.esotericsoftware.kryo.io.Input/Output so thatÂ it can be used with kryo serializer.Â It improves the performance of kryo by faster read/writes (unsafe IO) and alsoÂ eliminatesÂ the need for interim buffers to convertÂ fromÂ DataInput and DataOutput.,1370
Rename Method,Adding faster serialization classes. These changes add two new kryo serialization classes (KryoSimpleWritable and KryoSimpleWrapper)Â which disable reference tracking at the expense of not supporting recursive and nested structures. Disabling reference tracking significantly improves the serialization performance.Â One a sample pipeline the running time reduced from 75 minutes to 5 minutes.,1371
Extract Method,De-duplicate pagerank implementation in PageRankBenchmark Currently in PageRankBenchmark we have the code for pagerank duplicated in each of the implementations of Vertex:{noformat}    public static class PageRankHashMapVertex extends HashMapVertex<            LongWritable DoubleWritable DoubleWritable DoubleWritable> {        @Override        public void compute(Iterator<DoubleWritable> msgIterator) {            if (getSuperstep() >= 1) {                double sum = 0;                while (msgIterator.hasNext()) {                    sum += msgIterator.next().get();                }                DoubleWritable vertexValue =                    new DoubleWritable((0.15f / getNumVertices()) + 0.85f *                                       sum);                setVertexValue(vertexValue);            }            if (getSuperstep() < getConf().getInt(SUPERSTEP_COUNT -1)) {                long edges = getNumOutEdges();                sendMsgToAllEdges(                    new DoubleWritable(getVertexValue().get() / edges));            } else {                voteToHalt();            }        }    }    public static class PageRankEdgeListVertex extends EdgeListVertex<            LongWritable DoubleWritable DoubleWritable DoubleWritable> {        @Override        public void compute(Iterator<DoubleWritable> msgIterator) {            if (getSuperstep() >= 1) {                double sum = 0;                while (msgIterator.hasNext()) {                    sum += msgIterator.next().get();                }                DoubleWritable vertexValue =                    new DoubleWritable((0.15f / getNumVertices()) + 0.85f *                                       sum);                setVertexValue(vertexValue);            }            if (getSuperstep() < getConf().getInt(SUPERSTEP_COUNT -1)) {                long edges = getNumOutEdges();                sendMsgToAllEdges(                        new DoubleWritable(getVertexValue().get() / edges));            } else {                voteToHalt();            }        }    }{noformat}This code can be consolidated into private class and the two implementations just extend that.,1372
Extract Method,Refactor BspServiceWorker::loadVertices() Currently BspServiceWorker::loadVertices() is more than 200 lines and convoluted. I found it difficult to grok while debugging today.,1373
Extract Method,Random Walks on Graphs Implementing RWR on Giraph should be a very simple modification of the SimplePageRankVertex code.{code}if ( myID == sourceID )      DoubleWritable vertexValue = new DoubleWritable((0.15f + 0.85f * sum);else      DoubleWritable vertexValue = new DoubleWritable(0.85f * sum);{code}It would be nice to make it as configurable as possible by using parametric damping factors preference vectors strongly preferential etc...More or less along these lines:http://law.dsi.unimi.it/software/docs/it/unimi/dsi/law/rank/PageRank.html,1374
Rename Method,Add secure authentication to Netty IPC Gianmarco De Francisci Morales asked on the user list:bq. I am getting the exception in the subject when running my giraph programbq. on a cluster with Kerberos authentication.This leads to the idea of having Kerberos authentication supported within GIRAPH. Hopefully it would use our fast GIRAPH-37 IPC but could also interoperate with Hadoop security.,1376
Rename Method,Make iteration over edges more explicit Is there any particular reason why BasicVertex implements Iterable?It seems to me that doing{code:java}for (I neighbor : vertex){code}is not that explicit and{code:java}for (I neighbor : this){code}gets even obscure (which may be why all examples in the codebase explicitly instantiate an iterator and call next()).What I propose is a more explicit{code:java}Iterator<I> outEdgesIterator(){code}and also a convenient{code:java}Iterable<I> outEdges(){code}so for example an algorithm can use{code:java}for (IntWritable neighbor : outEdges()){code},1377
Extract Method,Add metrics system into Giraph Currently a lot of Giraph's operations are not transparent. As a Hadoop job the Giraph logging is at the mercy of Hadoop's logging system and can disappear when one encounters a memory issue.  ,1378
Extract Method,"Partitioning outgoing graph data during INPUT_SUPERSTEP by # of vertices results in wide variance in RPC message sizes This relates to GIRAPH-247. The unfortunately named ""MAX_VERTICES_PER_PARTITION"" fooled me into thinking this value was regulating the size of initial Partition objects as they were composed during INPUT_SUPERSTEP from InputSplits each worker reads.In fact this configuration option only regulates the size of the outgoing RPC messages stored locally in Partition objects but decomposed into Collections of BasicVertex for transfer to their eventual homes on another (or this) worker. There they are combined into the actual Partitions they will exist in for the job run.By partitioning these outgoing messages by # of vertices metrics load tests have shown the size of the average message is not well regulated and can create overloads on either side of these transfers. This is important because:1. Throughput and memory are at a premium during INPUT_SUPERSTEP.2. Only one crashed worker in a Giraph job causes cascading job failure even in an otherwise healthy workflow.This JIRA renames the offending variables/config options and further regulates outgoing graph data in INPUT_SUPERSTEP by the # of edges and THEN the # of vertices in a candidate for transfer. This much more effectively regulates message size for typical social graph data and has been show in testing to greatly improve the amount of load-in data Giraph can handle without failure given fixed memory and worker limits.",1379
Extract Method,Netty optimization to handle requests locally whenever possible  Adds optimization to handle requests locally when possible rather than go over the network(Original description follows below: see related JIRAs for each item besides the above)* Makes netty the default instead of HadoopRPC* Added optimization to handle requests locally when possible rather than go over the network* Added TimedLogger to print only within a given time period* Added optimization for using multiple channels between clients/servers when bandwidth is limited per connection* Added ByteCounter to track bandwidth across Netty (can be later integrated with GIRAPH-232).* Upgraded rat to 0.8 and excluded iml files (Intellij Idea),1380
Extract Method,Add option to limit the number of open requests As I mentioned in the discussion on the mailing list GIRAPH-45 patch itself wasn't enough to run jobs with any amount of messages data. The thing which can still happen is that we have too many send messages requests to which we didn't get replies yet so these requests use all our memory. Adding limit on the number of open requests will fix that last case.,1381
Rename Method,Add thread and channel pooling to NettyClient and NettyServer Add thread and channel pooling on NettyClient and NettyServer.Instead of a NettyClient's addressChannelMap being:  address => Channelit is:  address => ChannelRotaterOriginally part of GIRAPH-262 extracted here.,1382
Rename Method,"InputSplit Reservations are clumping leaving many workers asleep while other process too many splits and get overloaded. With recent additions to the codebase users here have noticed many workers are able to load input splits extremely quickly and this has altered the behavior of Giraph during INPUT_SUPERSTEP when using the current algorithm for split reservations. A few workers process multiple splits (often overwhelming Netty and getting GC errors as they attempt to offload too much data too quick) while many (often most) of the others just sleep through the superstep never successfully participating at all.Essentially the current algo is:1. scan input split list skipping nodes that are marked ""Finsihed""2. grab the first unfinished node in the list (reserved or not) and check its reserved status.3. if not reserved attempt to reserve & return it if successful.4. if the first one you check is already taken sleep for way too long and only wake up if another worker finishes a split then contend with that worker for another split while the majority of the split list might sit idle not actually checked or claimed by anyone yet.This does not work. By making a few simple changes (and acknowledging that ZK reads are cheap only writes are not) this patch is able to get every worker involved and keep them in the game ensuring that the INPUT_SUPERSTEP passes quickly and painlessly and without overwhelming Netty by spreading the memory load the split readers bear more evenly. If the giraph.splitmb and -w options are set correctly behavior is now exactly as one would expect it to be.This also results in INPUT_SUPERSTEP passing more quickly and survive the INPUT_SUPERSTEP for a given data load on less Hadoop memory slots. ",1383
Rename Method,Hide the SortedMap<I Edge<IE>> in Vertex from client visibility (impl. detail) replace with appropriate accessor methods As discussed on the list and on GIRAPH-28 the SortedMap<I Edge<IE>> is an implementation detail which needs not be exposed to application developers - they need to iterate over the edges and possibly access them one-by-one and remove them (in the Mutable case) but they don't need the SortedMap and creating primitive-optimized BasicVertex implementations is hampered by the fact that clients expect this Map to exist.,1384
Extract Method,Improve ZooKeeper issues Currently if the ZooKeeper process fails we have little information on why and what happened.  This patch addresses this by keeping the last 100 log lines and dumps when the map fails under a RuntimeException.Here is an example of a master task failure when there is an invalid JVM argument passed to ZooKeeper.  The error is much for obvious now.2012-10-04 15:05:28916 WARN org.apache.giraph.zk.ZooKeeperManager: logZooKeeperOutput: Dumping up to last 100 lines of the ZooKeeper process STDOUT and STDERR.2012-10-04 15:05:28916 WARN org.apache.giraph.zk.ZooKeeperManager$StreamCollector: Unrecognized option: -BadOpt2012-10-04 15:05:28916 WARN org.apache.giraph.zk.ZooKeeperManager$StreamCollector: Could not create the Java virtual machine.2012-10-04 15:05:28919 INFO org.apache.hadoop.mapred.TaskLogsTruncater: Initializing logs' truncater with mapRetainSize=-1 and reduceRetainSize=-12012-10-04 15:05:28959 WARN org.apache.hadoop.mapred.Child: Error running childjava.lang.IllegalStateException: run: Caught an unrecoverable exception onlineZooKeeperServers: Failed to connect in 5 tries!                                 at org.apache.giraph.graph.GraphMapper.run(GraphMapper.java:591)                                 at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:763)                                 at org.apache.hadoop.mapred.MapTask.run(MapTask.java:369)                                 at org.apache.hadoop.mapred.Child$4.run(Child.java:259)                                 at java.security.AccessController.doPrivileged(Native Method)                                 at javax.security.auth.Subject.doAs(Subject.java:396)                                 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)                                 at org.apache.hadoop.mapred.Child.main(Child.java:253)Caused by: java.lang.IllegalStateException: onlineZooKeeperServers: Failed to connect in 5 tries!       at org.apache.giraph.zk.ZooKeeperManager.onlineZooKeeperServers(ZooKeeperManager.java:721)       at org.apache.giraph.graph.GraphMapper.setup(GraphMapper.java:328)       at org.apache.giraph.graph.GraphMapper.run(GraphMapper.java:573)       ... 7 more2012-10-04 15:05:28963 INFO org.apache.hadoop.mapred.Task: Runnning cleanup for the task,1385
Rename Method,Ensure that subclassing BasicVertex is possible by user apps Original assumptions in Giraph were that all users would subclass Vertex (which extended MutableVertex extended BasicVertex).  Classes which wish to have application specific data structures (ie. not a TreeMap<I Edge<IE>>) may need to extend either MutableVertex or BasicVertex.  Unfortunately VertexRange extends ArrayList<Vertex> and there are other places where the assumption is that vertex classes are either Vertex or at least MutableVertex.Let's make sure the internal APIs allow for BasicVertex to be the base class.,1387
Rename Method,Cleaner MutableVertex API Currently MutableVertex requires the user to instantiate a vertex before creating the request.Using instantiateVertex() also requires a downcast to MutableVertex.What we can do instead is pass id and value (and optionally edges) to addVertexRequest() and the instantiation/initialization will happen internally and safely.,1389
Extract Method,Improve the way we keep outgoing messages As per discussion on GIRAPH-357 in standard application chances that we get to use client-side combiner are very low. I experimented with benefits which we can get from not having the client-side combiner at all. It turns out that having a lot of maps in SendMessageCache and then collection inside each of them really hurts the performance. ,1390
Extract Method,MasterObserver for user post-application customization https://reviews.apache.org/r/7981/,1391
Extract Method,Max message request size in bytes initialize buffers to expected size Now that all message requests are kept in serialized format we can limit the size of request by the number of bytes instead of number of messages in it. Sizing the message buffers to expected size gave about 10% speedup on a large application run.,1392
Extract Method,Improve the way to keep outgoing messages As discussed in GIRAPH-12(http://goo.gl/CE32U) I think that there is a potential problem to cause out of memory when the rate of message generation is higher than the rate of message flush (or network bandwidth).To overcome this problem we need more eager strategy for message flushing or some approach to spill messages into disk.The below link is Dmitriy's suggestion.https://issues.apache.org/jira/browse/GIRAPH-12?focusedCommentId=13116253&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13116253,1393
Extract Method,Cleanup MapFunctions MapFunctions is a very simple enum and the code using it is convoluted. Instead we can use some java magic making the enum smarter and cleanup the code that uses it.,1394
Extract Method,Observer for job launch lifecycle 0,1396
Extract Method,WorkerObserver 0,1398
Extract Method,When having open requests log which workers are they sent to When waiting for requests to be processed it's useful to know which workers are open requests sent to.,1400
Extract Method,Make it possible to use different out-edges data structures for input and computation In some cases the properties we want in the VertexEdges implementation during input may differ from the ones we want during computation.Two examples:1) During input we want to keep only the top K edges according to weight so we use a fixed-size min-heap. During computation our algorithm needs fast random access so we use a hash-map.2) We have a VertexEdges implementation that's optimized for space and/or iteration speed but has slow insertion. We can then use a different data structure that has fast insertion during input.We can add an option to specify a different VertexEdges class to be used in EdgeStore during input.,1401
Extract Method,Initialize compute OutEdges directly from input OutEdges Compute VertexEdges can for example be immutable or have better performance when adding all edges than when adding one by one.,1404
Extract Method,Add support for multiple Vertex/Edge inputs For example sometimes we want to read input from several tables which is currently not possible. We should allow any number of vertex and edge input formats to be set.,1405
Move Method,Allow IO formats to add parameters to Configuration Currently we heavily rely on some runners (HCatGiraphRunner and HiveGiraphRunner) to prepare Configuration before application starts and we have no way of using hcat/hive io without these runners. It would be better and more flexible if io formats would add what's needed for underlying io to Configuration themselves.https://reviews.apache.org/r/10690/,1406
Rename Method,Input superstep should support aggregators like any other superstep add aggregator for VertexReader to allow user to aggregate values at Input SuperStep,1407
Extract Method,Specialized message stores I was investigating with where the time/CPU is going in some applications and receiving messages on server side turned out to be one of the most expensive things we do. We should provide better implementations using primitive maps whenever that's possible.,1408
Extract Method,Allowing plain computation with types being configurable All types are configurable via VERTEX_ID_CLASS and other constants in GiraphConstants but they are still required to match Computation types (unless code is JYthon).It should be allowed to pass if provided types are subtypes of Computation types.Also it should be possible to change incoming and outgoing message classes without need to have explicit computation class with appropriate compile time defined types.,1409
Extract Method,Add an option to dump only live objects to JMap We should be able to dump only live objects with JMap.,1411
Move Method,Large-memory improvements (Memory reduced vertex implementation fast failure added settings)  Current vertex implementation uses a HashMap for storing the edges which is quite memory heavy for large graphs.  The default settings in Giraph need to be improved for large graphs and heaps of >20G.,1412
Extract Method,Fix checkpointing We need to make checkpoiting in Giraph functional again - it misses a lot of data because of many additions we've been making to Giraph (like information from WorkerContext/MasterCompute proper integration with per superstep output etc).,1413
Extract Method,Allow having state in aggregators Currently aggregators are specified via Class<Aggregator> and so are created with empty constructor which makes it extremely hard to have:ArrayAggregator(int n Aggregator<A> elementAggregator)but each specific instance needs to be defined as a class.Goal is to allow that,1414
Extract Method,More command line logging Add more logging to command line:- if a worker fails log which worker failed- if some worker is low on memory warn user about it,1417
Rename Method,Change CassandraClient#init() to CassandraClient#initialize() for consistency with other Datastores.  This makes drawing comparisons between the different Datastores easier and should have been kept consistent from the start anyway.,1419
Extract Method,Creates org.apache.gora.cassandra.serializers package in order to clean the code of store and query packages and to support additional types in future. It has been discussed at GORA-81 and a kind of fix has been committed as a part of GORA-138 patch.Since it is one of the main functionalities of gora-cassandra to handle Serializers it seems a good idea to separate those code from store and query packages.I will be attaching my patch shortly.1) Utf8Serializer makes simple to handle Type.STRING;2) GoraSerializerTypeInferer is a single point go get Serializer and it is extensible to support additional types in future;3) Type.BOOLEAN is supported.,1421
Rename Method,Removal of _g_dirty field from _ALL_FIELDS array and Field Enum in Persistent classes In auto-generated persistent classes we create an array field called ALL_FIELDS as you know. But this array also contains _g_dirty field which is not a stored field at all. Maybe we should remove _g_dirty field from the array since the array is used for getting all fields in the stored table. We can also remove it from Field enum so the users do not know about the _g_dirty field.,1423
Rename Method,Upgrade HBase to 0.98 HBase 0.98 release is the current stable release.Gora should be built based on HBase 0.98.,1424
Inline Method,Remove Unused Method Parameters Remove unused method parameters and make API signatures clean.,1425
Extract Method,Avoid HBase MiniCluster restarts to shorten gora-hbase tests Currently our hbase tests are taking forever and a day. We should shorten the time by avoiding MiniCluster restarts.Just implement the cluster as a singleton and clean up the tables inbetween test by doing a scan and deletes for all rows. It's muchfaster than restarting the cluster.For code referenece please see the implementation here[1]. The class isHBaseClusterSingleton. It needs some refactoring but I think it'senough to speed your tests.Thanks Ioan for the heads up.[1] http://svn.apache.org/repos/asf/james/mailbox/trunk/hbase/src/test/java/org/apache/james/mailbox/hbase/,1426
Extract Method,Upgrade to Apache Avro 1.7.x I am not sure what this involves as of yet but I have a small feeling that it's going to be some reasonably major work...  ,1428
Extract Method,"COMPUTE STATS for only new partitions/columns {{COMPUTE STATS}} is absolutely necessary for my {{JOIN}} queries to even finish. Its runs are extremely long which is completely understandable given that my table have 1000s of partitions and 100s of columns.Now the frustrating part is that when running {{COMPUTE STATS}} twice the second run is not really faster than the first.For my use-case where I only add a few partitions every day and very rarely change the schema it would be absolutely lovely to be able to optionally specify an {{INCREMENTAL}} option to {{COMPUTE STATS}}. The behavior should then be ""Skip computing stats for partitions/columns which already have computed stats. In short I would like a way to tell Impala _""Trust me the stats you have computed in the past are still valid I only added some data""_.",1429
Extract Method,Pull out common conjuncts from disjunctions I see this in tpch and tpcds where predicates look like this:where   ( col = 1 AND ... AND ...) or   ( col = 1 AND ... AND ...) or   ( col = 1 AND ... AND ...)It's an OR of ANDs and there are predicates that are in each OR (col = 1). This could be pulled out.,1430
Extract Method,Netezza compatibility functions: math Math functions for improved Netezza compatibility:* {{COT}}* {{DCEIL}}* {{DEXP}}* {{DFLOOR}}* {{DLOG10}}* {{DPOW}}* {{DROUND}}* {{DSQRT}}* {{DTRUNC}}* {{FACTORIAL}}* {{FPOW}}* {{RADIANS}}See attachment for links for more information.,1431
Extract Method,Expand CTAS to allow partition clauses 0,1432
Rename Method,Impala should support CROSS JOIN Getting this error message when attempting a CROSS JOIN: ERROR: com.cloudera.impala.common.NotImplementedException: Join requires at least one equality predicate between the two tables.,1433
Extract Method,Provide support for registering permanent udfs Hive already implements it via HIVE-6047. So we have the metastore support for it ready. We just need to tune our catalog to read and load them during startup. Otherwise all the UDFs are lost during catalogd restart.,1434
Rename Method,"Consider replacing constant exprs with literal equivalents Evaluating expressions that are the same for every row can be very expensive (note these numbers are taken from the debug build so maybe this is not an issue in release?):{code}(15:33:02@desktop) ~/src/cloudera/impala (cdh5-trunk) $ impala-shell.sh -q ""SELECT 1.2345678 FROM tpch.lineitem"" -B 2>&1 > /dev/null...Fetched 6001215 row(s) in 61.33s(15:34:06@desktop) ~/src/cloudera/impala (cdh5-trunk) $ impala-shell.sh -q ""SELECT CAST('1.2345678' AS DOUBLE) FROM tpch.lineitem"" -B 2>&1 > /dev/null...Fetched 6001215 row(s) in 74.86s{code}",1435
Rename Method,"Support INSERT and LOAD DATA to S3 Impala can query from S3 but not write to S3.  A few mechanical changes are needed to make the insert/load data code use the correct filesystem rather than the table's base dir (see also IMPALA-1816) however the bigger issue that needs to be decided is:These operations both rely on FileSystem.rename() at the coordinator to narrow the crash consistency window. But S3 doesn't natively support rename and rename() will actually do a copy and delete which is too slow and prone to failure defeating the purpose.  A couple options that can be considered/explored: (a) simply document that S3 doesn't give the same amount of crash consistency (b) try to use multi-part uploading to simulate the same effect (since parts are ""hidden"" until the multi-part is committed) or (c) enhance the metastore so that it can atomically add files (i.e. S3 objects) to tables.Also some regions provide eventual consistency though maybe that's only for writes rather than object creation so maybe not an issue.",1436
Inline Method,"Bad plan choices due to incorrect number of estimated hosts. The number of hosts a plan node is estimated to run on during plan generation and the actual number of hosts the query will run on could be different in the following scenarios:1. Queries accessing tables on non-HDFS remote storage e.g. S3 or Isilon2. Queries with aggressive partition pruning3. Large clusters relative to the size of the tables scannedThis discrepancy between the number of estimated hosts and the actual number of hosts can lead to suboptimal plan choices in particular a bad join strategy (broadcast vs. partitioned).To determine whether a query is running slow due such a bad planning decision you can examine the ""hosts"" field in the query plan from the profile and contrast it with the actual hosts from the execution summary.",1437
Extract Method,"Implement ""SHOW CREATE"" for functions Request is to implement ""SHOW CREATE"" for user defined functions.This originated as a customer request.",1438
Extract Method,"Support for null-safe equal/IS [NOT] DISTINCT FROM ANSI SQL has a IS [NOT] DISTINCT FROM comparison operator (which in mysql is called ""null-safe equal"" or <=>) that in essence implements =/<> but with NULLs.a IS NOT DISTINCT FROM bis the same asa = b OR (a IS NULL AND b IS NULL)a IS DISTINCT FROM bis the same as(a <> b OR a IS NULL OR b IS NULL) AND NOT (a IS NULL AND b IS NULL)We should add support for all variants.https://wiki.postgresql.org/wiki/Is_distinct_fromhttp://dev.mysql.com/doc/refman/5.0/en/comparison-operators.html#operator_equal-to",1439
Extract Method,Make idle_session_timeout a query option We currently have a process-wide session timeout (see --idle_session_timeout) but it would be useful to also have the ability to set per-session timeouts. We can do this by taking a parameter in the HS2 OpenSession() call (it already has a map of properties) and changing the session timeout logic in impala-server.cc.This could be set to a lower value by some clients e.g. Hue that do not close queries automatically and want to ensure queries aren't left open for a long time without requiring that all clients have the same session timeout.,1440
Inline Method,"Add PURGE option to DROP TABLE Prior to using HDFS Encryption to create an encryption zone that is used to hold data files for an Impala table we would see the expected behaviour of DROP TABLE removing the data files for the table. Now that encryption is used the drop table command succeeds however the data files are not removed and the Hive Metastore log shows ""can't be moved from an encryption zone.""Attempting to remove the data files using ""hdfs dfs -rm"" produces the same message as above however adding -skipTrash allows the files to be removed.On the basis of the above adding a PURGE option to DROP TABLE (and ALTER TABLE ... DROP PARTITION) that results in -skipTrash been used for the deletion of the data files would be very useful.",1441
Rename Method,"Add endtime to impalad lineage output Currently we just include start time as ""timestamp"" in the lineage. endtime should be useful too.",1442
Extract Method,"Introduce CLUSTERED plan hint for insert statements h4. Add a new ""clustered"" plan hint for insert statements.Example:{code}CREATE TABLE dst (...) PARTITIONED BY (year INT month INT);INSERT INTO dst PARTITION(yearmonth) /*+ clustered */ SELECT * FROM src;{code}The hint specifies that the data fed into the table sink should be clustered based on the partition columns.For now we'll use a local sort to achieve clustering and the plan should look like this:SCAN -> SORT (yearmonth) -> TABLE SINKh4. Syntax and behavior{code}INSERT INTO dst PARTITION(yearmonth) /*+ clustered */ SELECT * FROM src;{code}- We will not support the legacy-hint style with brackets {code}[clustered]{code}- The hint should be obeyed if the target table is a partitioned HDFS or Kudu table. Otherwise it should be ignored with a warning.- For Kudu tables the sorting should be done on the primary keys.h4. Making clustered the default planEventually we want to make the ""clustered"" plan the default because it is more robust with large inserts into many partitions. With that in mind we should also add a corresponding ""noclustered"" hint that removes the sort. Of course that hint will not do anything until we change the default behavior but we should add it nevertheless to have the hints complete.",1443
Extract Method,Runtime filter forwarding between operators Although the planner does much to reduce the output cardinality of operators where possible (e.g. by predicate pushdown) there are some situations where the planner cannot apply a selectivity-based optimisation but the execution engine can based on the output from operators at runtime. One example is 'dynamic partition pruning' where a filter is computed on a join key which is a partition column by the build side of a hash table and then propagated to the probe side scan. The scan can then simply skip those partitions which don't appear in the filters.There are other opportunities for optimisation based on runtime-computed filters. This JIRA tracks adding a general mechanism for filter computation and propagation in the backend and the corresponding work in the planner to identify valid optimisation opportunities.,1444
Extract Method,Complete 'ALTER' DDL command support for Kudu Supported operations should include:* add column** nullable columns may have null or non-null defaults* drop column* rename column* rename table* add/drop range partitions,1445
Rename Method,&raw param for debug web UI /varz and others similar to what query profiles have Raw text only no html param &raw would make it easier for people to copy and paste from the debug web UI without formatting problems. I would personally like to see this on /varz especially metrics would be useful too but is probably worth adding to all /urls.,1447
Extract Method,Consider changing arithmetic conversions to produce decimal in more cases The conversion rules for mixing decimals and integers are a little confusing. {code}select 10.0 / 3{code}results in a double because it has a decimal literal and non-decimal literal argument.But{code}select 10.0 / 3.0{code}results in a decimal because it has two decimal literal arguments.Part of the justification for this is that decimal was significantly slower than double so we didn't want users to see sudden performance regressions. However we've since improved decimal performance a lot so we should consider revisiting this behaviour.The relevant code is Expr.convertNumericLiteralsFromDecimal(),1448
Move Method,Kudu Connector: use new Scan Token API Kudu 0.9 will include a new feature called Scan Tokens which will simplify planning on the Java side and creating scanners on the C++ side.  The API is designed with Impala in mind so it has a built in serialization method so that scan tokens can be 'created' on the Java side and then serialized and rehydrated into a scanner.  See the [design doc|https://github.com/apache/incubator-kudu/blob/master/docs/design-docs/scan-tokens.md] for a high level intro to the API and the [MR input format|https://github.com/apache/incubator-kudu/blob/master/java/kudu-mapreduce/src/main/java/org/kududb/mapreduce/KuduTableInputFormat.java] and [Spark RDD|https://github.com/apache/incubator-kudu/blob/master/java/kudu-spark/src/main/scala/org/kududb/spark/kudu/KuduRDD.scala] for usage examples.,1449
Extract Method,Range based pruning for in-predicate For in-predicates with long in-lists such as TPCDS-Q8 we may benefit by finding the min and max in the list and use them to prune out unqualified scan ranges or filter out rows which don't fall into the range.,1450
Extract Method,Support WITH clause to factor out subqueries out of main query. Here is an example of using the WITH clause:{code}WITH t1 as (select c1 c2 from x)     t2 as (select a b c from y)SELECT * FROM t1 t2 WHERE t1.c1 = t2.a{code}t1 and t2 are essentially subqueries written in the WITH clauseWe will not support recursive queries.,1451
Rename Method,Tuning parameters for Impala HBase Integration Impala's hbase integration does not tuning of the query sent to HBase in particular in particular the scan cache size and disabling of the block cache.   Both parameters are documented has having a performance on queries and also the wider HBase platform.   Disabling the block cache results prevents churn on the region server heap the scanner cache reduces round trips.  This is normally set via the setCacheBlocks and setCaching methods on the scanner API.,1452
Inline Method,CatalogServiceCatalog.extractFunctions should log the underlying exception when failing to load a function The code path here:https://github.com/cloudera/Impala/blob/cdh5-trunk/fe/src/main/java/com/cloudera/impala/catalog/CatalogServiceCatalog.java#L437-L442does not log the reason for failure to copy. Looks likehttps://github.com/cloudera/Impala/blob/cdh5-trunk/fe/src/main/java/com/cloudera/impala/common/FileSystemUtil.java#L374-L382swallows the exception. This makes it harder to find root cause.,1454
Extract Method,"Introduce SORT BY clause in CREATE TABLE statement This issue is intended as a usability improvement for IMPALA-4163 where the SORT BY columns can be specified directly in the table definition like this:{code}CREATE TABLE t (day INT hour INT)PARTITIONED BY (year INT month INT)SORT BY (day hour);{code}The above table creation has the effect that all inserts into the table have an implicit ""sortby(dayhour)"" plan hint applied. See IMPALA-4163 for details on the hint.Just like with the ""sortby"" hint the SORT BY clause can only contain non-partition columns for HDFS tables and non-primary key columns for Kudu tables.This has the following benefits:* Users will not have to remember to put the sortby hint in all insert statements.* The SORT BY columns are a physical design choice so it makes sense to store them as part of the table metadata.* This is a convenience feature. It has the same effect as the sortby() hint for INSERT statements but doesn't require the user to remember to include the hint for every INSERT statement.Challenges:* The Hive Metastore has no SORT BY concept so we'll need to store the information in the generic TBLPROPERTIES map.* No other engines (Hive Spark) will understand this table property. That means that data written by those engines will require an explicit sorting hint (as far as that's available).",1455
Extract Method,Support insert plan hints for CREATE TABLE AS SELECT In order to tune ETL processes we should also support the same plan hints for CREATE TABLE AS SELECT (CTAS) statements that we already support for INSERT statements. Otherwise the convenience of a CTAS may be somewhat lost or the generated data may not be as efficient as it could be with CREATE + INSERT + hints.The placement of the hint should be as in Oracle's syntax:{code}CREATE /*+ clustered  sort(dayhour) */ TABLE t ASSELECT * from src;{code},1456
Rename Method,Don't abort Catalog startup quickly if HMS is not present If the catalog daemon can't contact the HMS on startup it will fail out of the {{Catalog}} constructor in {{MetaStoreClientPool.addClients()}}.We might consider not doing so but instead retry for a longer time to allow the catalog and the HMS to be started concurrently.,1457
Extract Method,Add dictionary filtering to Parquet scanner To efficiently process a highly selective scan query just using partition pruning is too coarse grain (due to the limit on the number of partitions). For selective scan very often Impala can simply check the values in the Parquet dictionary page and determine that the whole row group can be thrown out.,1458
Rename Method,ImpalaD should not open 21000 and 21050 Ports till Catalog is Received Currently ImpalaD's open the frontend connections and this results in query failures. The preferred behaviour would be that the ports remain closed till the catalog is received and for any reason is the SS connectivity is not established after reasonable attempts and timeouts then the impalad to simply shut down.{code}impalad.INFO:I1216 17:39:40.437333 10463 jni-util.cc:166] com.cloudera.impala.common.AnalysisException: This Impala daemon is not ready to accept user requests. Status: Waiting for catalog update from the StateStore.impalad.INFO:I1216 17:39:40.438743 10463 status.cc:112] AnalysisException: This Impala daemon is not ready to accept user requests. Status: Waiting for catalog update from the StateStore.impalad.INFO:I1216 17:39:40.918184 10464 jni-util.cc:166] com.cloudera.impala.common.AnalysisException: This Impala daemon is not ready to accept user requests. Status: Waiting for catalog update from the StateStore.impalad.INFO:I1216 17:39:40.918994 10464 status.cc:112] AnalysisException: This Impala daemon is not ready to accept user requests. Status: Waiting for catalog update from the StateStore.impalad.INFO:I1216 17:39:44.129482 10465 jni-util.cc:166] com.cloudera.impala.common.AnalysisException: This Impala daemon is not ready to accept user requests. Status: Waiting for catalog update from the StateStore.{code}This will help especially when we have multiple impala'd behind a LB and connections can be directed to daemons with the catalog when some servers/impala services are been restarted for any reason.,1459
Extract Method,Simplify the code for file/block metadata loading by manually calling listLocatedStatus() for each partition. The fix for IMPALA-4172/IMPALA-3653 uses Hadoop's Filesystem.listFiles() API to recursively list all files under an HDFS table's parent directory. We then map each file to its corresponding partition. However the use of listFiles() and the associated code for doing the file-to-partition mapping does not really make sense because listFiles() is just a recursive wrapper around listLocatedStatus(). So for a table with 10k partitions there will be 10k RPCs doing listLocatedStatus().We should simplify our code to just loop over all partitions and call listLocatedStatus(). This has the following benefits:* Simper code. Would have avoided bugs like IMPALA-4789.* Faster code. No need to map files to partitions.* Easier to parallelize in the future.* Easier to decouple table and partition loading in the future.Keep in mind that for S3 tables we do want to use the listFiles() API to avoid being throttled by S3.Relevant links:https://github.com/apache/hadoop/blob/branch-2.6.0/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java#L1720https://github.com/apache/hadoop/blob/branch-2.6.0/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java#L766,1460
Extract Method,Evaluate parquet::Statistics to skip data in nested types IMPALA-2328 adds read support for min/max statistics for scalar columns. We should also add support for skipping row groups based on predicates on nested fields.,1461
Extract Method,Add support for dictionary filtering on nested fields Parquet dictionary filtering currently supports only non-nested data. It would be useful to be able to filter on nested data. Filtering can only happen if the value of the nested data is required to be non-empty.,1462
Extract Method,"Missed opportunities for static partition pruning with COALESCE() TBH I'm not sure how general this issue is so rather than attempting to express it as abstractly as possible I'll just start with a specific example (apologies in advance for formatting...horrible JIRA artist here):{code}# create two tables -- the main point here is that t1 has 2 partitions> create table t1 (i int) partitioned by (part int) ;> create table t2 (i int) partitioned by (part int) ;> insert into t1 (part i) values (1 1) (2 2) ;> insert into t2 (part i) values (1 1) (2 2) ;# query 1: coalesce t1 partition column with literal value â€” partition pruning kicks in> explain with results as ( select coalesce(t1.i t2.i) as i coalesce(t1.part 666) as part from t1 full outer join t2 on (t1.i = t2.i) ) select * from results where part = 1;Query: explain with results as ( select coalesce(t1.i t2.i) as i coalesce(t1.part 666) as part from t1 full outer join t2 on (t1.i = t2.i) ) select * from results where part = 1+------------------------------------------------------------------------------------+| Explain String                                                                     |+------------------------------------------------------------------------------------+<SNIP>| 00:SCAN HDFS [default.t1]                                                          ||    partitions=1/2 files=1 size=2B                                                  |+------------------------------------------------------------------------------------+# ^^^note # partitions scanned^^^# query 2: coalesce t1 partition column against dynamic value â€” partition pruning does NOT kick in> explain with results as ( select coalesce(t1.i t2.i) as i coalesce(t1.part t2.part) as part from t1 full outer join t2 on (t1.i = t2.i) ) select * from results where part = 1;Query: explain with results as ( select coalesce(t1.i t2.i) as i coalesce(t1.part t2.part) as part from t1 full outer join t2 on (t1.i = t2.i) ) select * from results where part = 1+------------------------------------------------------------------------------------+| Explain String                                                                     |+------------------------------------------------------------------------------------+<SNIP>| 00:SCAN HDFS [default.t1]                                                          ||    partitions=2/2 files=2 size=4B                                                  |+------------------------------------------------------------------------------------+# ^^^note # partitions scanned^^^{code}In both of those queries we're applying a filter predicate to a column that is defined as follows:coalesce(t1.part _)Since t1.part is a partition column in t1 every row in t1 has a non-null value for t1.part. Furthermore because t1.part appears as the first parameter to coalesce every row in the result set that contains _any_ data from t1 will get its ""part"" value from t1. Thus all t1 data in the result set will be subject to the filter predicate in t1.part -- i.e. all t1 partitions other than (part=1) could have been pruned during query planning.Question: what is wrong with the above reasoning? am I missing something fundamental/obvious here?Thanks!",1463
Rename Method,Reduce number of Kudu clients that get created Creating Kudu clients is very expensive as each will fetch metadata from the Kudu master. We can reduce the load on the Kudu master by reusing Kudu clients when possible. To start we can use a single client for the entire BE and another for the entire FE.This is dependent on a metadata invalidation improvement from Kudu (https://gerrit.cloudera.org/#/c/6719/),1464
Extract Method,Simplify remaining constant conditionals A recent change (IMPALA-1861) added SimplifyConditionalsRule which simplifies some conditional functions with constant conditions (e.g. if (true 0 1) -> 0).This rule currently only covers IF OR AND CASE and DECODE. We should extend it to cover further conditionals such as COALESCE IFNULL IS(TRUE/FALSE) etc.,1465
Extract Method,Coalesce chains of OR conditions to an IN predicate. Would be nice to implement an ExprRewriteRule that coalesces multiple compatible OR conditions to an IN predicate e.g.:{code}(c=1) OR (c=2) OR (c=3) OR (c=4) ...->c IN (1 2 3 4...){code}Long chains of OR are generally unwieldy and transforming them to IN has the following benefits:* IN predicates with long value lists are evaluated with an O(log n) lookup in the BE* It is easier to extract min/max values from an IN predicate for Parquet min/max filtering* The IN predicate may be faster to codegen than a deep binary tree or ORsNote that this new rule complements existing rules to yield interesting improvements e.g.:{code}(c1=1 AND c2='a') OR (c1=2 AND c2='a') OR (c1=3 AND c2='a')->c2='a' AND c1 IN (1 2 3){code}I've attached a relevant query profile from one of Mostafa's experiments.,1466
Extract Method,Add query option to control join strategy when tables have no stats In IMPALA-5120 the join strategy was changed from bcast to shuffle when tables have no stats.  Adding a query option to specify the behavior lowers the risk that users may have come to rely on this behavior.  This would allow them to revert back to the previous behavior.Query option proposal:{noformat}default_join_distribution_mode = [ broadcast | shuffle ] {noformat}Ideally the default would be shuffle but in the spirit of preserving existing behavior it will stay broadcast. We should re-evaluate this choice in a compatibility-breaking release.,1467
Extract Method,"Consider automatically disabling codegen for entire query based on planner estimates We should consider automatically disabling codegen for the entire query if there are relatively few rows processed. This would improve response time of queries that are too big for the ""single node optimisation"" but don't benefit from codegen.",1468
Extract Method,Improve join cardinality estimation with a more robust FK/PK detection This JIRA is for tracking improvements to our join-cardinality estimation. In particular we should improve the handling of many-to-many joins and multi-column joins. It is understood that some cases cannot be reliably detected with our limited metadata and statistics but we should try our best given those limitations.Further improving cardinality estimation with new metadata and statistics is tracked elsewhere e.g.:IMPALA-2416IMPALA-3531,1469
Extract Method,Join inversion should avoid reducing the degree of parallelism The degree of inter-node parallelism for a join is determined by its left input so when inverting a join the planner should be mindful of how the inversion affects parallelism.For example the left join input may have been reduced by joining with several dimension tables so much that it becomes smaller than the right hand-side (another small dimension table). By inverting such a join the degree of parallelism may be reduced to one or very few nodes based on how many nodes the right-hand size is executed on.,1470
Extract Method,"Disallow managed Kudu table to explicitly set Kudu tbl name in CREATE TABLE There's no reason to allow this behavior. Managed tables create Kudu tables with the name (in Kudu) ""impala::db_name.table_name"". Renaming (in Impala) a managed Kudu table results in renaming the underlying Kudu table e.g. rename table_name to new_table name results in changing the Kudu table to ""impala::db_name.new_table_name"". But allowing a new table to specify the kudu table name is inconsistent with the renaming behavior and just introduces opportunities for confusion.{code}  private void analyzeManagedKuduTableParams(Analyzer analyzer) throws AnalysisException {    // If no Kudu table name is specified in tblproperties generate one using the    // current database as a prefix to avoid conflicts in Kudu.    // TODO: Disallow setting this manually for managed tables    if (!getTblProperties().containsKey(KuduTable.KEY_TABLE_NAME)) {      getTblProperties().put(KuduTable.KEY_TABLE_NAME          KuduUtil.getDefaultCreateKuduTableName(getDb() getTbl()));    }{code}",1471
Rename Method,Support for ORC format files In Hulu we have supported ORC format files in our version of Impala. Will you accept this feature? Weâ€™re willing to contribute it to the community.,1472
Extract Method,CREATE TABLE should validate directory permissions/existence as part of LOCATION path analysis checks CREATE TABLE statement does not validate existence or permission on the LOCATION path. If the path does not exist or if Impala cannot access it it's likely a typo in the DDL stmt.,1473
Rename Method,"Don't synthesize block metadata in the catalog for S3/ADLS Today the catalog synthesizes block metadata for S3/ADLS by just breaking up splittable files into ""blocks"" with the FileSystem's default block size. Rather than carrying these blocks around in the catalog and distributing them to all impalad's we might as well generate the scan ranges on-the-fly during planning. That would save the memory and network bandwidth of blocks.That does mean that the planner will have to instantiate and call the filesystem to get the default block size but for these FileSystem's that's just a matter of reading the config.Perhaps the same can be done for HDFS erasure coding though that depends on what a block location actually means in that context and whether they contain useful info.",1474
Extract Method,"End-to-end compression of metadata The metadata of large tables can become quite big making it costly to hold in the statestore and disseminate to coordinator impalads. The metadata can even get so big that fundamental limits like the JVM 2GB array size and the Thrift 4GB are hit and lead to downtime.For reducing the statestore metadata topic size we have an existing ""compact_catalog_topic"" flag which LZ4 compresses the metadata payload for the C++ codepaths catalogd->statestore and statestore->impalad.Unfortunately the metadata is not compressed in the same way during the FE->BE transition on the catalogd and the BE->FE transition on the impalad.The goal of this change is to enable end-to-end compression for the full path of metadata dissemination. The existing code paths also need significant cleanup/streamlining. Ideally the new code should provide consistent size limits everywhere.",1475
Rename Method,Update DESCRIBE statement to respect column level privileges Currently if a user is granted select on a subset of columns on a table the DESCRIBE command will show them all columns and the DESCRIBE FORMATTED/EXTENDED is not allowed.This change would update the DESCRIBE command that if a user has select on a subset of columns it will only show the data from the columns the user has access to.Â  For DESCRIBE FORMATTED/EXTENDED if a user has some column access but not all columns the Location and View * Text would be removed from the additional metadata.The purpose of this change is to increase consumability by allowing tools that allow users to browse data such a for creating reports to present only columns they have access to.Â  There is also a security aspect to this fix by not exposing additional data.Â  Other statements such a SHOW COLUMN STATS will be handled by a separate Jira to be opened.Â ,1476
Extract Method,"The profile of all statements should contain the Query Compilation timeline Some statements do not seem to include the ""Query Compilation"" timeline in the query profile.Repro:{code}create table t (i int);describe t; <-- loads the table but no FE timeline in profileinvalidate metadata t;alter table t set tbproperties('numRows'='10'); <-- loads the table but no FE timeline in profile{code}All statements should include the planner timeline.",1477
Extract Method,Support for SELECT * FROM (VALUES(11)(22)) We might consider Postgres' support for {{SELECT * FROM (VALUES <list of tuples>) AS T}} as a way to avoid writing out laborious UNION ALL clauses. This issue can probably be implemented as a parser rewrite of https://issues.cloudera.org/browse/IMPALA-66,1478
Rename Method,"Add JVM Pause Monitor to Impala Processes In IMPALA-3114 we added a pause monitor for Impala. In addition to that we should port/borrow Hadoop's JvmPauseMonitor [https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/JvmPauseMonitor.java.]Â I believe that when the JVM is aggressively GCing the C++ threads will continue to get scheduled (and won't log) but the Java ones will log. (I've definitely seen JvmPauseMonitor be accurate many times.)[~bharathv] when you were testing this were you able to reproduce it triggering when the JVM half was in ""GC hell""?",1480
Rename Method,"Include number of required threads in explain plan Impala has an internal notion of ""required threads"" to execute a fragment e.g. the fragment execution thread and the first scanner thread. It's possible to compute the number of required threads per fragment instance based on the plan.We should include this in the resource profile and expose it in the explain plan. This could then be a step toward implementing something like IMPALA-6035.",1481
Move Method,Remove IMPALA_MINICLUSTER_PROFILE=2 Based on the discussion here: https://lists.apache.org/thread.html/49f9b68ed3d6d2c0fdee16a877b259922545e4824e1233479227a657@%3Cdev.impala.apache.org%3E,1482
Extract Method,Automatically choose mem_limit based on estimate clamped to range We should add admission control support for intelligently choosing how much memory to give a query based on the memory estimate. Currently we require that you set a single mem_limit per pool. Instead it would be better if we allowed configuring min/max guardrails within which admission control chose an amount of memory that will allow good performance.Initially I think mem_limit will be the same for all backends. Eventually it could make sense to have different limits per backend depending on the fragments.,1483
Extract Method,Intern HdfsStorageDescriptors Every partition currently has an HdfsStorageDescriptor attached. In most cases the number of unique storage descriptors in a warehouse is pretty low (most partitions use the same escaping file formats etc). For example in the functional test data load we only have 24 unique SDs across ~10k partitions. Each object takes 32 bytes (with compressed oops) or 40 (without). So we can get some small memory/object-count savings by interning these.,1484
Extract Method,at least one equality predicate error message needs improvement We get this error message on joins with no equality predicate:{{com.cloudera.impala.common.NotImplementedException: Join requires at least one equality predicate between the two tables. }}There are two problems.# This query {{select a.bool_col from alltypessmall a alltypessmall b where (a.int_col = b.int_col or a.bool_col = b.bool_col);}} has 'at least one equality predicate' but since it's part of a disjunction it's not accepted by Impala.# It would be good to print out the names of the tables since multi-way joins are often complex and pinpointing the missing predicate can be tricky.,1485
Extract Method,Make num retries for InconsistentMetadataFetchException configurable Currently hardcoded to 10 (INCONSISTENT_METADATA_NUM_RETRIES),1486
Extract Method,Improve partition pruning time Impala can roughly prune 10k partitions per sec. For huge tables it might have >30k partitions. For queries that have tight SLA the partition pruning time can be significant.Hive's partition pruning is faster than Impala.,1487
Extract Method,Clean RiotReader RiotReader has lots of deprecated functions but for historical reasons is visible in the public API package.1. migrate out deprecated functions to RiotParsers in o.a.j.riot.lang.2. Use RDFDataMgr where ever possible3. Clean tests up.,1489
Extract Method,Clean RIOT and IRI handling. General cleaning up of the code.,1491
Extract Method,"Add support for SPARQL 1.1 ""list sugar"" format. Add the ability to process a where clause like ( <a> ?b 'c') <foo> <bar>as per http://www.w3.org/TR/2013/REC-sparql11-query-20130321/#collections ",1497
Rename Method,Iter - add document add takeWhile clean up. Some improvements for {{Iter}} and related Iterators.* Documentation* {{takeWhile}} {{takeUntil}} {{dropWhile}} {{dropUntil}}* Deprecate operations on {{Collection}}s better done with Java8 streams.* Rework {{IteratorTruncate}} to use {{IteratorSlotted}} not have it's own equivalent machinery.,1498
Rename Method,Remove DatasetGraphCaching This class seems to do nothing much but does add complexity.  There is caching in {{DatasetImpl}} that competes with this caching.  Nowadays Jena's storage systems are using {{GraphView}} which is very lightweight so caching the creation of graphs is not beneficial and the cache has to be concurrent and transaction safe.The internal class {{DatasetGraphCaching.Helper}} is used only in SDB.  {{Helper}} could be moved to into SDB.,1499
Move Method,Improve jena-base lib.Tuple Tuples which are immutable value-equality fixed length sequences of instances of the same type accessed by index.They are like {{T[]}} but immutable with .equals based on contents and could be improved to work independently of their original use in TDB.Proposal:#  Make Tuple an interface (simpler than current) with special implementations for low number of elements.# {{ColumnMap}} ==> {{TupleMap}} and sort out the naming as much as possible to use as a way to manage index mapping not just rewriting Tuples.An intermediate step is to move {{ColumnMap}} into TDB.This is not a proposal to add it to {{Triple}} or {{Quad}}.{{Triple}} is not the same as a 3-length {{Tuple<Node>}}.  Triple elements are accessed by name ({{getSubject}} etc) not index.  SPO is just one possible order in which to think about triples but it has no special status. ,1505
Extract Method,Support alternative QueryParsers in jena-text Jena-text is currently hardwired to use Lucene QueryParser. This parser is (intentionally) limited so that it doesn't analyze wildcard queries. Instead they will be expanded directly.This is a problem if you want to do accent-insensitive wildcard queries (using ASCIIFoldingFilter) or other wildcard queries which rely on a special analyzer. However Lucene offers an alternate parser AnalyzingQueryParser that could be used in such cases.I'd like to extend jena-text with a configuration parameter that allows using AnalyzingQueryParser instead of the standard QueryParser. For example the configuration could look like this:{noformat}<#indexLucene> a text:TextIndexLucene ;    text:directory <file:Lucene> ;    text:queryParser text:AnalyzingQueryParser ;    text:queryAnalyzer [        a text:ConfigurableAnalyzer ;        text:tokenizer text:KeywordTokenizer ;        text:filters (text:ASCIIFoldingFilter text:LowerCaseFilter)    ]     text:entityMap <#entMap> ;{noformat}I've written some very preliminary code to implement this but I'm not yet satisfied with it. It's a bit problematic because the parser cannot be constructed in advance but must be dynamically created separately for each query (because it needs parameters that can differ between queries). Thus the TextIndexConfig must store information about which parser variant to use but not the actual QueryParser/AnalyzingQueryParser instance. This isn't rocket science though maybe some kind of Factory pattern would work.For some background for why this is needed see this Skosmos issue:https://github.com/NatLibFi/Skosmos/issues/424,1508
Move Method,"Provide unionDefaultGraph for all graphs. Provide ""unionDefaultGraph"" for all graphs.",1509
Inline Method,riot command : provide exit codes when validating. See discussion on user@https://lists.apache.org/thread.html/f2ec45ff2ddf91b39270b2c23062b92af1817c8d0ac42af928cbdb5b@%3Cusers.jena.apache.org%3E,1511
Extract Method,Provide an embedded Fuseki server for use from java. The command line standalone server can be launched from Java - that is a full server.An embedded server would be without dependencies on the filing system (the {{run/}} area). It will run without UI web pages admin servlets or external configuration files. In other words - just the services on datasets configured from java.,1512
Extract Method,Control the flushing of the commit log based on journal size. See discussion in thread: https://lists.apache.org/thread.html/b992db7e843e1444d2c4835ac9ece41bee2b251a60785d8881452b95@%3Cusers.jena.apache.org%3ETDB batches commits to save on overheads of small updates. However if the application is continually making large commits then batching saves little and causes more temporary resource consumption (RAM especially). It may also cause jitters in performance due to a large updates to the main database.,1513
Extract Method,Add methods on Resource class that look for Properties in a specific language It would be very convenient to have in the {{org.apache.jena.rdf.model.Resource}} interface methods such as:{code}    public Statement getProperty( Property p String lang );    public Statement getRequiredProperty( Property p String lang );{code}They'd work like very much like {{org.apache.jena.rdf.model.ModelCon.listStatements(Resource Property String String)}}.This would simplify queries to a Resource's language-specific properties.,1516
Inline Method,Upgrade text search to latest Lucene We are currently at Lucene 4.9.1 which is quite outdated compared to latest Lucene which is 6.2.1 .Note that there is project to add a simple completion feature in addition to existing simple search.But it would be better to do that on an updated Lucene dependency .,1517
Rename Method,Add support to VALUES in SelectBuilder It seems that QueryBuilder is lacking on support for building VALUES block or there is no obvious method to do that.,1518
Extract Method,Elastic Search Support for Apache Jena Text  This Jira tracks the development of Jena Text ElasticSearch Implementation.The goal is to extend Jena Text capability to index at scale in ElasticSearch. This implementation would be similar to the Lucene and Solr implementations.We will use ES version 5.2.1 for the implementation.The following functionalities would be supported:* Indexing Literal values* Updating indexed values* Deleting Indexed values* Custom Analyzer Support* Configuration using Assembler as well as Java techniques.,1519
Inline Method,"Provide detailed setup for RIOT parsing with a parser builder. Provide a parser builder for detailed setup of RDFParser.  This is a new low level interface to the parsing process. It replaces and extends the machinery hidden inside {{RDFDataMgr}} ({{process}} and {{getReader}}) and {{RDFParserRegistry.ReaderRIOTLang}}.It aligns with the changes to {{HttpOp}} to have a specific optional {{HttpClient}} (JENA-576 and related work) and so allows applications to control the HTTP setup without resorting to direct use of {{HttpOp}}.More detailed control can be exposed including language specific and specialized needs for example [PR#211 ""preserve id of blanknodes in JSON-LD""|https://github.com/apache/jena/pull/211].{{RDFDataMgr}} functions involving a Context can be can be deprecated.",1520
Extract Method,Language-specific collation in ARQ As [discussed|http://markmail.org/message/v2bvsnsza5ksl2cv] on the users mailing list in October 2016 I would like to change ARQ collation of literal values to be language-aware and respect language-specific collation rules.This would probably involve changing at least the [NodeUtils.compareLiteralsBySyntax|https://github.com/apache/jena/blob/master/jena-arq/src/main/java/org/apache/jena/sparql/util/NodeUtils.java#L199] method.It currently sorts by lexical value first then by language tag. Since the collation order needs to be stable across all possible literal values I think the safest way would be to sort by language tag first then by lexical value according to the collation rules for that language.But what about subtags like {{@en-US}} or {{@pt-BR}}? Can they have different collation rules than the main language? It would be a bit strange if all {{@en-US}} literals sorted after {{@en}} literals...It would be good to check how Dydra does this and possibly take the same approach. See the message linked above for further backgound.I've been talking with [~kinow] about this and he may be interested in implementing it.,1522
Extract Method,Provide detailed setup for RIOT output with a writer builder. This is a companion to JENA-1306 ({{RDFParserBuilder}}).The current writing process is not as flexible as parsing. Initially this is a basic placeholder.,1523
Rename Method,QueryIterRoot should not be overloaded for initial bindings. {{QueryIterRoot}} is also used for initial bindings but the code sometimes assumes that the root binding is the join identity iterator (one row no columns).{{QueryIterRoot}} should be reserved for for this case.{{QueryIterator}} could have a method {{.isJoinIdentity()}} so that an iterator can be tested without peeking and without assuming the java class.,1524
Extract Method,Fuseki maintenance 0,1525
Extract Method,Embedded Fuseki for testing usage Provide {{ServerCtl}} (see the full server and Fuseki1) for test cycle support.Enable the possibility of security in embedded Fuseki for testing purposes.,1526
Extract Method,Performance regression in Model.remove(Model m) method The Model.remove(Model) works very slow on large models as it propagates to GraphUtil.deleteFrom(Graph Graph) which computes size of the target graph by iterating over all triples. This computation takes nearly 100% of the time of the Model.remove(Model) operation.It seems this commit introduced the issue: https://github.com/apache/jena/commit/781895ce64e062c7f2268a78189a777c39b92844#diff-fbb4d11dc804464f94c27e33e11b18e8Due to this bug deletion of a concept scheme on a large ontology may take several minutes. ,1527
Rename Method,Transaction promotion Expose the transaction promotion capabilities of TIM TDB and TDB2.,1528
Rename Method,Allow setting of RDF/XML Reader properties when using RIOT. RIOT does not currently have a mechanism for setting the ARP (Jena's RDF/XML parser) properties. Of all the parsers only ARP has properties to set. The RIOT/ARP bridge  {{ReaderRIOTRDFXML}} has no mechanism for passing property/value settings to ARP.,1531
Extract Method,Support bz2 compression for parsing and loading. 0,1532
Extract Method,Include a Javacc based Turtle parser in RIOT Turtle is the basis for some additional languages (RDF* SHACL and ShEX compact forms).The main RIOT Turtle parser is written for speed with the tuned tokenizer and directly written java grammar parser. This makes it harder to reuse and extend.This ticket proposes including another RDF 1.1 compliant Turtle parser based on JavaCC to provide an easier route for additional languages by providing all the details of Turtle such as the tokens and prefix name handling in a form more suitable as a base for the new language. It will still be by being a copy of the parser system not class inheritance.)RDF 1.1 Turtle and SPARQL 1.1 were aligned by the working groups and share tokens and several grammar rules.This would not be active by default (i.e. not a registered {{Lang}} and it's parser factory but registered by automatic initialization). It's test suite would be run in the build and pass the RDF 1.1 Turtle test suite.Â There is non-RDF1.1 Javacc Turtle parser in jena-core is based on the pre-RDF1.1 state of Turtle. It is sufficient for the assembler tests that read turtle files. It could be moved into the test area except there appear to be some legacy applications that only use jena-core.Â ,1533
Extract Method,RDFConnectionRemote : Pass original string to remote server where possible. Not all places are passing the string end-to-end.Â ,1534
Extract Method,Support dynamic function invocation in ARQ Per my suggestion to the SPARQL Working Group I am proposing a new built in CALL() which can be used in SPARQL queries for dynamic function invocation.CALL with no argument returns nullCALL with some arguments evaluates the first argument sees if the result is a URI (if not errors) and if so tries to generate a function from it and invoke it with the arguments to that invocation being the remaining arguments to CALLI have implemented a prototype implementation of this in the ARQ language which I have committed to Trunk others with more knowledge of expression and function evaluation in ARQ may have suggestions on how to improve the implementation,1535
Extract Method,Support HTTP auth for SPARQL Update requests ARQ supports HTTP Basic Authentication for read queries (setBasicAuthentication() in QueryEngineHTTP).I can't find authentication support for SPARQL Update though. The responsible class UpdateProcessRemote uses HttpOp as an abstraction and HttpOp in turn encapsulates HttpClient from Apache HTTP Components.So I guess a way to go would be to extend HttpOp to support auth or to expose the HttpClient object in order to set auth options as described in [1].I'm happy to help if I can get some hints about the preferred way to include auth support.[1] http://hc.apache.org/httpcomponents-client-ga/tutorial/html/authentication.html,1537
Rename Method,Remove DAML+OIL support Remove (legacy) DAML+OIL support.,1538
Extract Method,Include Fuseki request count in header When Fuseki is logging requests it uses a unique number for each query. It would be useful if this number was included in the headers for the query response. This would make it slightly easier to trace back errors to the Fuseki logs,1539
Rename Method,ARQ should be able to optimize implicit joins and implicit left joins There is a class of useful optimizations that currently ARQ does not even attempt to apply which are usually referred to as implicit joins.A trivial example is as follows:SELECT *WHERE{  ?x ?p1 ?o1 .  ?y ?p2 ?o2 .  FILTER(?x = ?y)}Currently this requires us to compute a cross product and then apply the filter even with streaming evaluation this can be extremely costly.  The aim of this optimization is to produce a query like the following:SELECT *WHERE{  ?x ?p1 ?o1 .  ?x ?p2 ?o2 .  BIND(?x AS ?y)}This optimization can also be applied to some left joins where the implicit join applies across the join e.g.SELECT *WHERE{  ?x ?p1 ?o1 .  OPTIONAL  {    ?y ?p2 ?o2 .    FILTER(?x = ?y)  }}This can be thought of as a generalization of TransformFilterEquality except covering the case where both items are variables.  Since both things are variables we need to be careful about when we apply this optimization since when = is used we need to guarantee that substituting one variable for the other does not alter the semantics of the query.I believe the optimization is safe to apply providing that we can guarantee (as far as possible) that one variable is non-literal.  This can be done by inspecting the positions in which the mentioned variables are used and ensuring that at least one of the variables occurs in the graph subject or predicate position.Safety for left joins is a little more complex since we must ensure that at least one of the variables occurs in the RHS and we can only make the substitution in the RHS as otherwise we change the join semantics.,1540
Extract Method,Unify HTTP usage and authentication mechanisms in ARQ Currently ARQ uses a mixture of HttpClient and HttpURLConnection to perform various HTTP operations e.g. SPARQL Queries SPARQL Updates and SPARQL Graph Store Protocol.This has the effect of making the code somewhat awkward to maintain and makes certain operations like authentication more complex than they need to be because different parts of the system support different modes of authentication.For example currently SPARQL queries only support Basic Auth and they always pre-authenticate so they cannot do proxy auth or use any other kind of auth method.  On the other hand SPARQL updates use HttpClient which is capable of performing Basic Digest NTLM SPNEGO and Kerberos for both normal and proxy auth but is never pre-authenticates.This task proposes unifying all HTTP operations in ARQ to use Apache HttpClient since it is more flexible and introducing a more extensible framework for doing authentication.In terms of HTTP unification we need to convert the following:- HttpQuery should use HttpClient- LocatorURL should use HttpClientIn terms of HTTP Authentication my idea is as follows introduce a new interface HttpAuthenticator which provides an apply(AbstractHttpClient client URI target) method.  All systems that may permit HTTP auth will allow use of an authenticator providing a generic interface for authenticators will allow us to introduce authenticators for other auth schemes e.g. form based logins.We can also provide authenticators that leverage existing mechanisms e.g. storing credentials in a service context which would be used by default.  Existing methods that accept username and password would use simpler authenticators.,1541
Extract Method,Add initial binding support to update queries SPARQL update queries lost the ability to specify initial bindings.  The patch will add that functionality (note that initial bindings are only meaningful for INSERT/DELETE/WHERE and DELETE/WHERE update queries.Note that this changes the API for 3rd party UpdateEngine implementers.,1544
Extract Method,Improve Fuseki/TDB transaction memory usage TDB has to buffer in memory all of the modified blocks for a transaction before committing it.  This causes out of memory exceptions when attempting to add a large number of statements in a single transaction.An easy way to fix this would be to copy the write block contents into a memory mapped file instead of heap memory ^â€ ^.  We can provide three user specified options for controlling the location of the temporary blocks:# JVM heap (default and what we currently use)# Direct memory (process heap but not in the JVM)# Memory mapped temporary fileSee this [Jena thread| http://markmail.org/thread/ckeevvhl2luevixw] for some additional discussion.^â€ ^ _The harder way would involve writing the old blocks to the journal then writing the new blocks directly to the indexes with a tombstone pointing to the old block in the journal so that readers could still retrieve the old version.  This however would seem to require a substantial refactor as well as a change to the on-disk database format._,1545
Rename Method,Add read(Reader...) to the general ReaderRIOT interface. 0,1546
Inline Method,Possible optimisation for FILTER(?var != <constant>) I have an idea for a possible optimisation for queries of the following general form:{noformat}SELECT *WHERE{  # Some Patterns  FILTER(?var != <http://constant>)} {noformat}This pattern crops up surprisingly often in real SPARQL workloads since it is often used to either limit a variable to exclude certain possibilities or to avoid self referential links in the data.In some cases it seems like this could be safely rewritten as follows:{noformat}SELECT *WHERE{  # Some Patterns  MINUS { BIND(<http://constant> AS ?var) }}{noformat}Or perhaps in a more generalised form like so:{noformat}SELECT * WHERE{  # Some patterns  MINUS { VALUES ?var { <http://constant/1> <http://constant/2> } }}{noformat}Which would nicely deal with the case of stating that a variable is not equal to multiple constant values.As I pointed out earlier this would not apply in every case specifically I think at least the following must be true:- The variable must be guaranteed to be bound (similar to existing filter equality and implicit join optimisations)There is also the potential to spot cases where the variable will always be unbound and thus the expression is always an error and replace the entire sub-tree with {{table empty}} as we already do for equality and implicit join filters.I plan on taking a look at implementing this in the new year if anyone has any thoughts on this (especially wrt to restrictions that should apply to when the optimisation is considered safe) then please comment.,1547
Rename Method,Do constant folding as part of query optimisation Currently Jena does not automatically simplify expressions by constant folding even though the function API actually has support for this already baked into it.This issue will track work to integrate a transform for this into the standard optimiser.,1548
Extract Method,Generate JSON from SPARQL directly. The capability to generate JSON directly from a SPARQL (or extended SPARQL) query would enable the creation of JSON data API over published linked data.This project would cover:# Design and publication of a design.# Refinement of design based on community feed# Implementation including testing.# Refinement of implementation based on community feedSkills required: Java some parser work design and discussion with the user community basic understanding of HTTP and content negotiation.,1549
Extract Method,Optimization OrderByDistinctApplication can enable better TopN optimization. There is a new opportunity for transformation of slice/order to a TopN execution.Currently {{OrderByDistinctApplication}} is done after TopN.If {{OrderByDistinctApplication}} is done before TopN optimization then the algebra may have the form slice/order/distinct/project which is can be executed as a TopN query. This form does not appear in algebra expressions directly from queries; the modifier order is slice/distinct/project/order which in general is not safe for TopN execution.However {{OrderByDistinctApplication}} picks out a condition for it's own optimization that leaves the resultant algebra as further transformable to TopN.Currently:{noformat}SELECT DISTINCT  ?zWHERE  { ?s ?p ?z }ORDER BY ?zLIMIT   5{noformat}Compiled to algebra:{noformat}(slice _ 5  (distinct    (project (?z)      (order (?z)        (bgp (triple ?s ?p ?z)))))){noformat}After optimization: TopN did not apply:{noformat}(slice _ 5  (order (?z)    (distinct      (project (?z)        (bgp (triple ?s ?p ?z)))))){noformat}but this is possible:{noformat}(top (5 ?z)  (distinct    (project (?z)      (bgp (triple ?s ?p ?z))))){noformat},1550
Extract Method,Wire the XSD duration accessor operations to SPARQL keyworks. ARQ supports xsd:durations and the functions {{fn:(years|months|days|hours|minutes|seconds)-from-duration}} but these are accessed by fn: URI.  The functions  YEARS HOURS etc etc can be overloaded to provide these functions when called on xsd:durations.,1551
Extract Method,Use an alternative implementation of a FRuleEngineI Give the possibility to use an alternative {{FRuleEngineI}} implementation.My proposition is to introduce a singleton factory {{FRuleEngineIFactory}}.{code:title=com.hp.hpl.jena.reasoner.rulesys.impl.FRuleEngineIFactory.java|borderStyle=solid}public class FRuleEngineIFactory {    private static FRuleEngineIFactory instance = new FRuleEngineIFactory();    public static void setInstance(FRuleEngineIFactory instance) { FRuleEngineIFactory.instance = instance; }    public FRuleEngineIFactory getInstance() { return instance; }        public FRuleEngineI createFRuleEngineI(ForwardRuleInfGraphI parent List<Rule> rules boolean useRETE) {        FRuleEngineI engine;        if (rules != null) {            if (useRETE) {                engine = new RETEEngine(parent rules);            } else {                engine = new FRuleEngine(parent rules);            }        } else {            if (useRETE) {                engine = new RETEEngine(parent);            } else {                engine = new FRuleEngine(parent);            }        }        return engine;    }}{code}This factory will be used by existing classes.{code:title=com.hp.hpl.jena.reasoner.rulesys.BasicForwardRuleInfGraph|borderStyle=solid}    @Overrideprotected void instantiateRuleEngine(List<Rule> rules) {    engine = FRuleEngineIFactory.getInstance().createFRuleEngineI(this rules false);}{code}{code:title=com.hp.hpl.jena.reasoner.rulesys.FBRuleInfGraph|borderStyle=solid}@Overrideprotected void instantiateRuleEngine(List<Rule> rules) {    engine = FRuleEngineIFactory.getInstance().createFRuleEngineI(this rules useRETE);}{code}{code:title=com.hp.hpl.jena.reasoner.rulesys.RETERuleInfGraph|borderStyle=solid}    @Overrideprotected void instantiateRuleEngine(List<Rule> rules) {    engine = FRuleEngineIFactory.getInstance().createFRuleEngineI(this rules true);}{code}And i could replace the factory instance by my own instance :{code}FRuleEngineIFactory.setInstance(new CustomFRuleEngineIFactory());{code}I will propose a patch soon.,1552
Extract Method,Read payload for errors in HttpOp Some sparql servers (virtuoso) will return a http entity with an error message when commands go wrong. In combined setups using both fuseki and virtuso the jena client cannot be used since it ignores proper errors from virtuoso.The enclosed patch adds the response payload to the error the existing behaviour just throws away the payload upon errors.,1554
Extract Method,Make QueryExecution implement (Auto)Closeable In JDK 7 {{Closeable}} can be used with try-with-resources for more convenient programming structure. (and is compatible with JDK 6 if so desired)However {{Closeable.close() throws IOException}} so I hope this can be approved for at least the next major Jena ARQ version (2.12).,1555
Rename Method,"SPARQL replace [users@ email report ""Replace doesn't error on patterns that match zero length strings""|http://mail-archives.apache.org/mod_mbox/jena-users/201407.mbox/%3CCA%2BQ4Jn%3DqOS85yCPGexqX%2BLFkX4Dw8QSQmBLq2kT05%2Btvj0xksA%40mail.gmail.com%3E].{{replace(""abc"" "".*"" ""x"")}} returns {{""xx""}} because it uses Java's Matcher.replaceAll - it's one match for the whole of ""abc"" (it's a greedy pattern) and one match for the trailing empty string. Java returns {{""x""}} for an empty string and {{""xx""}} for any non-empty string in place of ""abc"". F&O calls out this case and makes it an error.http://www.w3.org/TR/xpath-functions/#func-replace{quote}An error is raised \[err:FORX0003\] if the pattern matches a zero-length string that is if the expression fn:matches("""" $pattern $flags) returns true. It is not an error however if a captured substring is zero-length.{quote}",1556
Extract Method,Add StreamRDFWriter as a place where stream-based writers can be found. Provide a place to get streaming writers.  RDFDataMgr does not guarantee this not know which writers are streaming.,1559
Rename Method,"Filter placement should be able to break up extend The following query demonstrates a query plan seen internally which is considered sub-optimal.Consider the following query:{noformat}SELECT DISTINCT ?domainName{  { ?uri ?p ?o }  UNION  {    ?sub ?p ?uri    FILTER(isIRI(?uri))  }  BIND(str(?uri) as ?s)  FILTER(STRSTARTS(?s ""http://""))  BIND(IRI(CONCAT(""http://"" STRBEFORE(SUBSTR(?s8) ""/""))) AS ?domainName)}{noformat}Which ARQ optimises as follows:{noformat}(distinct  (project (?domainName)    (filter (strstarts ?s ""http://"")      (extend ((?s (str ?uri)) (?domainName (iri (concat ""http://"" (strbefore (substr ?s 8) ""/"")))))        (union          (bgp (triple ?uri ?p ?o))          (filter (isIRI ?uri)            (bgp (triple ?sub ?p ?uri)))))))){noformat}Which makes the query engine do a lot of work because it computes the both the {{BIND}} expressions for lots of possible solutions that will then be rejected when for many of them it would only be necessary to compute the first simple {{BIND}} function.It would be better if the query was planned as follows:{noformat}(distinct  (project (?domainName)    (extend (?domainName (iri (concat ""http://"" (strbefore (substr ?s 8) ""/""))))      (filter (strstarts ?s ""http://"")        (extend (?s (str ?uri))          (union            (bgp (triple ?uri ?p ?o))            (filter (isIRI ?uri)              (bgp (triple ?sub ?p ?uri))))))))){noformat}Essentially when we try to push a filter through an {{extend}} if we determine that we cannot push it through the extend we should see if we can split the {{extend}} instead thus resulting in a partial pushing.Note that a user can re-write the original query to yield this plan if they make the second {{BIND}} a project expression like so:{noformat}SELECT DISTINCT (IRI(CONCAT(""http://"" STRBEFORE(SUBSTR(?s8) ""/""))) AS ?domainName){  { ?uri ?p ?o }  UNION  {    ?sub ?p ?uri    FILTER(isIRI(?uri))  }  BIND(str(?uri) as ?s)  FILTER(STRSTARTS(?s ""http://""))}{noformat}",1560
Extract Method,Handling simple literalslanguage literals and xsd:string in RDF 1.1 In RDF 1.1 simple literals (no language tag no mentioned datatype) a have datatype xsd:string.  Literals with language tag havedatatype rdf:langString.Output should not explicitly use ^^xsd:string (or datatype=)NodeFactory should catch no datatype and set xsd:string or rdf:langString as appropriate.,1561
Extract Method,"Make the cache of LPBRuleEngine bounded to avoid out-of-memory The class ""com.hp.hpl.jena.reasoner.rulesys.impl.LPBRuleEngine"" uses an in-memory cache named ""tabledGoals"" which has no limit as to the size/number of entries stored.{noformat}    /** Table mapping tabled goals to generators for those goals.     *  This is here so that partial goal state can be shared across multiple queries. */    protected HashMap<TriplePattern Generator> tabledGoals = new HashMap<>();{noformat}We have experienced out-of-memory issues because of the cache being filled with millions of entries in just a few days under normal query usage conditions and a heap memory set to 3GB.In our setup we have a dataset containing multiple graphs some of them are actual data graphs (backed by TDB) and then there are two which are ontology models using a ""TransitiveReasoner"" and an ""OWLMicroFBRuleReasoner"" respectively. A typical query may run over all the graphs in the dataset including the ontology ones (see below for a query template). Eventhough the ontology graphs would not yield any additional results for data queries (which is fine) the above mentioned cache would still fill up with new entries.{noformat}SELECT ?p ?oWHERE {  GRAPH ?g {    <some resource of interest> ?p ?o .  }}{noformat}As there is no upper bound to the cache soon or later all available heap memory will be consumed by the cache giving rise to an out-of-memory criticial error.",1562
Rename Method,Use Java8 constructs with jena-core iterators For example: https://github.com/apache/jena/pull/54,1563
Extract Method,Add BIND support to the QueryBuilder where clause QueryBuilder does not currently support the BIND statement this is to add that functionality.,1565
Move Method, rename the UpdateDeniedException As noted in a discussion on the dev list between myself and Andy this update is to rename the current UpdateDeniedException to AccessDeniedException and extend it from a newly created OperationDeniedException.AddDeniedException and DeleteDeniedException will extend AccessDeniedException.jena-permissions will extend AccessDeniedException to create:ReadDeniedException -- for read restrictionsUpdateDeniedException -- for update restrictions (modifying triples that already exists as opposed to adding new triples)This will allow Fuskei to properly respond to the case where jena-permissions is in place and there are update restrictions in place.  Currently Fuseki returns this as a 500 error.  Once we have a common permission denied exception we can return either authentication required or access denied as appropriate.,1566
Rename Method,"Remove "".json"" from registration of RDF/JSON. To avoid confusion with JSON-LD  remove "".json"" from registration of RDF/JSON.",1568
Extract Method,Merge kafka.utils.Time and kafka.common.utils.Time We currently have 2 different versions of Time in clients and core. These need to be merged.It's worth noting that `kafka.utils.MockTime` includes a `scheduler` that is used by some tests while `o.a.kafka.common.utils.Time` does not. We either need to add this functionality or change the tests not to need it anymore.,1569
Extract Method,Add KafkaConsumer pause capability There are some use cases in stream processing where it is helpful to be able to pause consumption of a topic. For example when joining two topics you may need to delay processing of one topic while you wait for the consumer of the other topic to catch up. The new consumer currently doesn't provide a nice way to do this. If you skip calls to poll() or if you unsubscribe then a rebalance will be triggered and your partitions will be reassigned to another consumer. The desired behavior is instead that you keep the partition assigned and simply One way to achieve this would be to add two new methods to KafkaConsumer:{code}void pause(TopicPartition... partitions);void resume(TopicPartition... partitions);{code}Here is the expected behavior of pause/resume:* When a partition is paused calls to KafkaConsumer.poll will not initiate any new fetches for that partition.* After the partition is resumed fetches will begin again. * While a partition is paused seek() and position() can still be used to advance or query the current position.* Rebalance does not preserve pause/resume state.,1570
Extract Method,Allow certain Sensors to be garbage collected after inactivity Currently metrics cannot be removed once registered. Implement a feature to remove certain sensors after a certain period of inactivity (perhaps configurable).,1571
Move Method,New consumer should commit before every rebalance when auto-commit is enabled If not then the consumer may see duplicates even on normal rebalances since we will always reset to the previous commit after rebalancing.,1572
Extract Method,Add a metric that records the total number of metrics Sounds recursive and weird but this would have been useful while debugging KAFKA-2664,1573
Extract Method,KIP-42: Add Producer and Consumer Interceptors This JIRA is for main part of KIP-42 implementation which includes:1. Add ProducerInterceptor interface and call its callbacks from appropriate places in Kafka Producer.2. Add ConsumerInterceptor interface and call its callbacks from appropriate places in Kafka Consumer.3. Add unit tests for interceptor changes4. Add integration test for both mutable consumer and producer interceptors (running at the same time).,1574
Rename Method,KIP-31/KIP-32 clean-ups During review I found a few things that could potentially be improved but were not important enough to block the PR from being merged.,1575
Rename Method,"Improve protocol type errors when invalid sizes are received We currently don't perform much validation on the size value read by the protocol types. This means that we end up throwing exceptions like `BufferUnderflowException` `NegativeArraySizeException` etc. `Schema.read` catches these exceptions and adds some useful information like:{code}throw new SchemaException(""Error reading field '"" + fields[i].name +""': "" +(e.getMessage() == null ? e.getClass().getName() : e.getMessage()));{code}We could do even better by throwing a `SchemaException` with a more user friendly message.",1576
Rename Method,Support session windows The Streams DSL currently does not provide session window as in the DataFlow model. We have seen some common use cases for this feature and it's better adding this support asap.https://cwiki.apache.org/confluence/display/KAFKA/KIP-94+Session+Windows,1577
Extract Method,Kafka Connect Task Restart API This covers the connector and task restart APIs as documented on KIP-52: https://cwiki.apache.org/confluence/display/KAFKA/KIP-52%3A+Connector+Control+APIs.,1578
Extract Method,Task creation time taking too long in rebalance callback Currently in Kafka Streams we create stream tasks upon getting newly assigned partitions in rebalance callback function {code} onPartitionAssigned {code} which involves initialization of the processor state stores as well (including opening the rocksDB restore the store from changelog etc which takes time).With a large number of state stores the initialization time itself could take tens of seconds which usually is larger than the consumer session timeout. As a result when the callback is completed the consumer is already treated as failed by the coordinator and rebalance again.We need to consider if we can optimize the initialization process or move it out of the callback function and while initializing the stores one-by-one use poll call to send heartbeats to avoid being kicked out by coordinator.,1579
Extract Method,Consolidate duplicate code in KGroupedTableImpl The implementations of aggregate() and reduce() in KGroupedTableImpl are essentially identical but the current implementations do not share any code.,1580
Inline Method,Make kafka producers/consumers injectable for KafkaStreams While playing with Kafka Streams I found that there's some cases that we want to take control of kafka producer/consumers instantiation inside the StreamThread.Most significant case is that we have our own impl of kafka Producer which was built to provide much reliable message delivery but there's no way to inject our own instance into StreamTask ATM.Another example is that we wanna observe the result of {{producer.send()}} that is done inside the RecordCollector. We can provide our own Callback instance but there's no way to inject that callback to RecordCollector again.Here I'd like to suggest KafkaStreams giving an interface to inject these clients. I considered various approaches to do this like passing them through constructor or make instantiation methods overridable but eventually tried to simply intorude another argument to the KafkaStreams constructor which is responsible for supplying client instances.Incomplete PR will be filled up to show changeset in my mind so please give me feedbacks. Will follow-up PR quickly if I get positive feedback.,1581
Extract Method,Add method that checks if streams are initialised Currently when streams are initialised and started with streams.start() there is no way for the caller to know if the initialisation procedure (including starting tasks) is complete or not. Hence the caller is forced to guess for how long to wait. It would be good to have a way to return the state of the streams to the caller.One option would be to follow a similar approach in Kafka Server (BrokerStates.scala).Would be good for example to keep track of whether Kafka Streams is starting/running/rebalancing,1582
Rename Method,Allow setting of default topic configs via StreamsConfig Kafka Streams currently allows you to specify a replication factor for changelog and repartition topics that it creates. It should also allow you to specify any other TopicConfig. These should be used as defaults when creating Internal topics. The defaults should be overridden by any configs provided by the StateStoreSuppliers etc.,1583
Extract Method,Close `RecordBatch.records` when append to batch fails We should close the existing `RecordBatch.records` when we create a new `RecordBatch` for the `TopicPartition`.This would mean that we would only retain temporary resources like compression stream buffers for one `RecordBatch` per partition which can have a significant impact when producers are dealing with slow brokers see KAFKA-3704 for more details.,1584
Extract Method,Add support for SASL/SCRAM mechanisms Salted Challenge Response Authentication Mechanism (SCRAM) provides secure authentication and is increasingly being adopted as an alternative to Digest-MD5 which is now obsolete. SCRAM is described in the RFC [https://tools.ietf.org/html/rfc5802]. It will be good to add support for SCRAM-SHA-256 ([https://tools.ietf.org/html/rfc7677]) as a SASL mechanism for Kafka.See [KIP-84|https://cwiki.apache.org/confluence/display/KAFKA/KIP-84%3A+Support+SASL+SCRAM+mechanisms] for details.,1585
Extract Method,Add Helper Functions Into TestUtils Per guidance from [~guozhang] from PR #1477 move helper functions from RegexSourceIntegrationTest (getProducerConfig getConsumerConfig getStreamsConfig into TestUtils and parameterize as appropriate. Also look into adding a {{waitUntil(Condition condition)}} type construct to wait for a condition to be met without relying on using Thread.sleep,1586
Extract Method,Support per-connector converters While good for default configuration and reducing the total configuration the user needs to do it's inconvenient requiring that all connectors on a cluster need to use the same converter. It's definitely a good idea to stay consistent but occasionally you may need a special converters e.g. one source of data happens to come in JSON despite you standardizing on Avro.Note that these configs are connector-level in the sense that the entire connector should use a single converter type but since converters are used by tasks the config needs to be automatically propagated to tasks.This is effectively public API change as it is adding a built-in config for connectors/tasks so this probably requires a KIP.,1587
Extract Method,Connect record types should include timestamps Timestamps were added to records in the previous release however this does not get propagated automatically to Connect because it uses custom wrappers to add fields and rename some for clarity.The addition of timestamps should be trivial but can be really useful (e.g. in sink connectors that would like to include timestamp info if available but when it is not stored in the value).This is public API so it will need a KIP despite being very uncontentious.,1588
Extract Method,ConfigDef.toRst() should create sections for each group Currently the ordering seems a bit arbitrary. There is a logical grouping that connectors are now able to specify with the 'group' field which we should use as section headers. Also it would be good to generate {{:ref:}} for each section.,1590
Rename Method,Kafka Streams resetter is slow because it joins the same group for each topic The resetter is joining the same group for each topic which takes ~10secs in my testing. This makes the reset very slow when you have a lot of topics.,1591
Rename Method,Remove caching of dirty and removed keys from StoreChangeLogger The StoreChangeLogger currently keeps a cache of dirty and removed keys and will batch the changelog records such that we don't send a record for each update. However with KIP-63 this is unnecessary as the batching and de-duping is done by the caching layer. Further the StoreChangeLogger relies on context.timestamp() which is likely to be incorrect when caching is enabled,1593
Rename Method,Refactor Connect backing stores for thread-safety In Kafka Connect there has been already significant provisioning for multi-threaded execution with respect to classes implementing backing store interfaces. A requirement for [KAFKA-3008|https://issues.apache.org/jira/browse/KAFKA-3008] is to tighten thread-safety guarantees in these implementations especially for ConfigBackingStore and StatusBackingStore and this will be the focus of the current ticket. ,1594
Extract Method,records-lag should be zero if FetchResponse is empty In Fetcher we record records-lag in terms of number of records for any partition. Currently this metric value is updated only if number of parsed records is not empty. This means that if consumer has already fully caught up and there is no new data into the topic this metric's value will be negative infinity and users can not rely on this metric to know if their consumer has caught up.We can fix this problem by assuming the lag is zero is FetchResponse is empty.,1595
Extract Method,Add Codec for ZStandard Compression ZStandard: https://github.com/facebook/zstd and http://facebook.github.io/zstd/ has been in use for a while now. v1.0 was recently released. Hadoop (https://issues.apache.org/jira/browse/HADOOP-13578) and others are adopting it. We have done some initial trials and seen good results. Zstd seems to give great results => Gzip level Compression with Lz4 level CPU.,1596
Extract Method,Create Topic Policy It would be useful to be able to validate create topics requests. More details in the KIP:https://cwiki.apache.org/confluence/display/KAFKA/KIP-108%3A+Create+Topic+Policy,1597
Extract Method,KIP-72 Allow putting a bound on memory consumed by Incoming requests this issue tracks the implementation of KIP-72 as outlined here - https://cwiki.apache.org/confluence/display/KAFKA/KIP-72%3A+Allow+putting+a+bound+on+memory+consumed+by+Incoming+requests,1598
Extract Method,"Error message from Struct.validate() should include the name of the offending field. Take a look at this repro.{code}@Testpublic void structValidate() {Schema schema = SchemaBuilder.struct().field(""one"" Schema.STRING_SCHEMA).field(""two"" Schema.STRING_SCHEMA).field(""three"" Schema.STRING_SCHEMA).build();Struct struct = new Struct(schema);struct.validate();}{code}Any one of the fields could be causing the issue. The following exception is thrown. This makes troubleshooting missing fields in connectors much more difficult.{code}org.apache.kafka.connect.errors.DataException: Invalid value: null used for required field{code}The error message should include the field or fields in the error message.",1599
Extract Method,Handle disk failure for JBOD (KIP-112) See https://cwiki.apache.org/confluence/display/KAFKA/KIP-112%3A+Handle+disk+failure+for+JBOD for motivation and design.,1600
Rename Method,Expose states of active tasks to public API https://cwiki.apache.org/confluence/display/KAFKA/KIP+130%3A+Expose+states+of+active+tasks+to+KafkaStreams+public+API,1601
Extract Method,Stream round-robin scheduler is inneficient Currently StreamThread.runloop() uses a simple round-robin scheduler where a single request is taken from each task for processing followed by poll followed by the same process over again. For example for an app that has just 2 tasks each with 3 records ready to be processed we'd have the following sequencepoll() -> process 1 request for task T1 -> process 1 request for task T2 -> poll()-> process 1 request for task T1 -> process 1 request for task T2 -> poll() -> process 1 request for task T1 -> process 1 request for task T2 -> poll() This is quite inefficient. Instead a better round robin scheduler would do:poll() -> process all 3 requests for task T1 -> process all 3 requests for task T2 -> poll(),1602
Rename Method,Only log invalid user configs and overwrite with correct one Streams does not allow to overwrite some config parameters (eg {{enable.auto.commit}}) Currently we throw an exception but this is actually not required as Streams can just ignore/overwrite the user provided value.Thus instead of throwing we should just log a WARN message and overwrite the config with the values that suits Streams. (atm it's only one parameter {{enable.auto.commit}}) but with exactly-once it's going to be more (cf. KAFKA-4923). Thus the scope of this ticket depends when it will be implemented (ie before or after KAFKA-4923).This ticket should also include JavaDoc updates that explain what parameters cannot be specified by the user.,1604
Extract Method,KIP-145 - Expose Record Headers in Kafka Connect https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+ConnectAs KIP-82 introduced Headers into the core Kafka Product it would be advantageous to expose them in the Kafka Connect Framework.Connectors that replicate data between Kafka cluster or between other messaging products and Kafka would want to replicate the headers.,1605
Rename Method,Application Reset Tool does not need to seek for internal topics As KAFKA-4456 got resolved there is no modify offsets of internal topics with the application reset tool as those offsets will be deleted anyway.,1606
Rename Method,New Short serializer deserializer serde There is no Short serializer/deserializer in the current clients component.It could be useful when using Kafka-Connect to write data to databases with SMALLINT fields (or similar) and avoiding conversions to int improving a bit the performance in terms of memory and network.,1607
Extract Method,Sticky Assignor should not cache the calculated assignment (KIP-54 follow-up) As a follow-up to KIP-54 remove the dependency of Sticky Assignor to previously calculated assignment. This dependency is not required because each consumer participating in the rebalance now notifies the group leader of their assignment prior to rebalance. So the leader can compile the previous assignment of the whole group from this information.,1608
Extract Method,Extend Consumer Group Reset Offset tool for Stream Applications KIP documentation: https://cwiki.apache.org/confluence/display/KAFKA/KIP-171+-+Extend+Consumer+Group+Reset+Offset+for+Stream+Application,1609
Extract Method,KafkaConsumer.subscribe() overload that takes just Pattern without ConsumerRebalanceListener Request: provide {{subscribe(Pattern pattern)}} overload similar to {{subscribe(Collection<String> topics)}} Today for a consumer to subscribe to topics based on a regular expression (i.e. {{Pattern}}) the only method option also requires to pass in a {{ConsumerRebalanceListener}}. This is not user-friendly to require this second argument. It seems {{new NoOpConsumerRebalanceListener()}} has to be used. Use case: multi datacenter allowing easier subscription to multiple topics prefixed with datacenter names just by using a pattern subscription.,1610
Extract Method,Add cumulative count attribute for all Kafka rate metrics Add cumulative count attribute to all Kafka rate metrics to make downstream processing simpler more accurate and more flexible.See [KIP-187|https://cwiki.apache.org/confluence/display/KAFKA/KIP-187+-+Add+cumulative+count+metric+for+all+Kafka+rate+metrics] for details.,1611
Extract Method,Add new metrics to support health checks It will be useful to have some additional metrics to support health checks.Details are in [KIP-188|https://cwiki.apache.org/confluence/display/KAFKA/KIP-188+-+Add+new+metrics+to+support+health+checks],1612
Inline Method,Refactor Streams to use LogContext We added a {{LogContext}} object which automatically adds a log prefix to every message written by loggers constructed from it (much like the Logging mixin available in the server code). We use this in the consumer to ensure that messages always contain the consumer group and client ids which is very helpful when multiple consumers are run on the same instance. Kafka Streams requires similar contextual logging by including the prefix manually in each log message. It would be better to take advantage of the new {{LogContext}}.,1613
Extract Method,Refactor Producer to use LogContext We added a {{LogContext}} object which automatically adds a log prefix to every message written by loggers constructed from it (much like the Logging mixin available in the server code). We use this in the consumer to ensure that messages always contain the consumer group and client ids which is very helpful when multiple consumers are run on the same instance. We should do something similar for the producer. We should always include the client id and if one is provided the transactional id.,1614
Extract Method,Handle SASL authentication failures as non-retriable exceptions in clients Produce and consumer changes to avoid retries on authentication failures.Details are in [KIP-152|https://cwiki.apache.org/confluence/display/KAFKA/KIP-152+-+Improve+diagnostics+for+SASL+authentication+failures],1615
Rename Method,Add AdminClient.createPartitions() It should be possible to increase the partition count using the AdminClient. See [KIP-195|https://cwiki.apache.org/confluence/display/KAFKA/KIP-195%3A+AdminClient.increasePartitions],1616
Move Method,Avoid call fetchPrevious in FlushListeners When caching is turned on for a window or session store upon {{store.put()}} the cache may gets flushed trigger the dirty flush listener calling {{maybeForward()}} which calls {{fetchPrevious()}}.Unfortunately {{fetchPrevious()}} could be a very expensive call to make and sometimes are not necessary:1. When {{KStreamWindowAggregate.process()}} already gets the previous value when aggregating so we could passing the old value through the tuple forwarder without calling fetchPrevious again.2. When we know that {{sendOldValues}} flag is turned off.,1617
Rename Method,Provide for custom error handling when Kafka Streams fails to produce This is an issue related to the following KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-210+-+Provide+for+custom+error+handling++when+Kafka+Streams+fails+to+produce,1618
Rename Method,Postpone normal processing of tasks within a thread until restoration of all tasks have completed Let's say a stream thread hosts multiple tasks A and B. At the very beginning when A and B are assigned to the thread the thread state is {{TASKS_ASSIGNED}} and the thread start restoring these two tasks during this state using the restore consumer while using normal consumer for heartbeating. If task A's restoration has completed earlier than task B then the thread will start processing A immediately even when it is still in the {{TASKS_ASSIGNED}} phase. But processing task A will slow down restoration of task B since it is single-thread. So the thread's transition to {{RUNNING}} when all of its assigned tasks have completed restoring and now can be processed will be delayed. Note that the streams instance's state will only transit to {{RUNNING}} when all of its threads have transit to {{RUNNING}} so the instance's transition will also be delayed by this scenario. We'd better to not start processing ready tasks immediately but instead focus on restoration during the {{TASKS_ASSIGNED}} state to shorten the overall time of the instance's state transition.,1619
Rename Method,report a metric of the lag between the consumer offset and the start offset of the log Currently the consumer reports a metric of the lag between the high watermark of a log and the consumer offset. It will be useful to report a similar lag metric between the consumer offset and the start offset of the log. If this latter lag gets close to 0 it's an indication that the consumer may lose data soon.,1620
Rename Method,Have State Stores Restore Before Initializing Toplogy Streams should restore state stores (if needed) before initializing the topology.,1621
Rename Method,Introduce Incremental FetchRequests to Increase Partition Scalability Introduce Incremental FetchRequests to Increase Partition Scalability. See https://cwiki.apache.org/confluence/display/KAFKA/KIP-227%3A+Introduce+Incremental+FetchRequests+to+Increase+Partition+Scalability,1622
Rename Method,Change LogSegment.delete to deleteIfExists and harden log recovery Fixes KAFKA-6194 and a delete while open issue (KAFKA-6322 and KAFKA-6075) and makes the code more robust.,1623
Extract Method,QueryableStateIntegrationTest#queryOnRebalance should accept raw text I was using QueryableStateIntegrationTest#queryOnRebalance for some performance test by adding more sentences to inputValues. I found that when the sentence contains upper case letter the test would timeout. I get around this limitation by calling {{sentence.toLowerCase(Locale.ROOT)}} before the split. Ideally we can specify the path to text file which contains the text. The test can read the text file and generate the input array.,1624
Extract Method,KIP-255: OAuth Authentication via SASL/OAUTHBEARER KIP-255: OAuth Authentication via SASL/OAUTHBEARER (https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=75968876) proposes adding the ability to authenticate to Kafka with OAuth 2 bearer tokens using the OAUTHBEARER SASL mechanism. Token retrieval and token validation are both pluggable.,1626
Extract Method,Add TimeoutException to KafkaConsumer#position() In KAFKA-4879 Kafka Consumer hangs indefinitely due to Fetcher's {{timeout}} being set to {{Long.MAX_VALUE}}. While fixing this issue it was pointed out that if a timeout was added to methods which commits offsets synchronously a stricter control on time could be achieved. https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=75974886,1627
Rename Method,Delay initiating the txn on producers until initializeTopology with EOS turned on In Streams EOS implementation the created producers for tasks will initiate a txn immediately after being created in the constructor of `StreamTask`. However the task may not process any data and hence producer may not send any records for that started txn for a long time because of the restoration process. And with default txn.session.timeout valued at 60 seconds it means that if the restoration takes more than that amount of time upon starting the producer will immediately get the error that its producer epoch is already old. To fix this we should consider instantiating the txn only after the restoration phase is done. Although this may have a caveat that if the producer is already fenced it will not be notified until then in initializeTopology. But I think this should not be a correctness issue since during the restoration process we do not make any changes to the processing state.,1628
Inline Method,Simplify state store recovery In the current code base we restore state stores in the main thread (in contrast to older code that did restore state stored in the rebalance call back). This has multiple advantages and allows us the further simplify restore code. In the original code base during a long restore phase it was possible that a instance misses a rebalance and drops out of the consumer group. To detect this case we apply a check during the restore phase that the end-offset of the changelog topic does not change. A changed offset implies a missed rebalance as another thread started to write into the changelog topic (ie the current thread does not own the task/store/changelog-topic anymore). With the new code that restores in the main-loop it's ensured that `poll()` is called regularly and thus a rebalance will be detected automatically. This make the check about an changing changelog-end-offset unnecessary. We can simplify the restore logic to just consuming until `poll()` does not return any data. For this case we fetch the end-offset to see if we did fully restore. If yes we resume processing if not we continue the restore.,1629
Extract Method,"Kafka Connect handling of bad data Kafka Connect connectors and tasks fail when they run into an unexpected situation or error but the framework should provide more general ""bad data handling"" options including (perhaps among others): # fail fast which is what we do today (assuming connector actually fails and doesn't eat errors) # retry (possibly with configs to limit) # drop data and move on # dead letter queue This needs to be addressed in a way that handles errors from: # The connector itself (e.g. connectivity issues to the other system) # Converters/serializers (bad data unexpected format etc) # SMTs # Ideally the framework as well though we obviously want to fix known bugs anyway",1630
Move Method, Add transformValues() method to KTable Add {{transformValues()}} methods to the {{KTable}} interface with the same semantics as the functions of the same name on the {{KStream}} interface.   More details in [KIP-292|https://cwiki.apache.org/confluence/display/KAFKA/KIP-292%3A+Add+transformValues%28%29+method+to+KTable].,1632
Extract Method,Consolidate ExtendedSerializer/Serializer and ExtendedDeserializer/Deserializer The Javadoc of ExtendedDeserializer states: {code} * Prefer {@link Deserializer} if access to the headers is not required. Once Kafka drops support for Java 7 the * {@code deserialize()} method introduced by this interface will be added to Deserializer with a default implementation * so that backwards compatibility is maintained. This interface may be deprecated once that happens. {code} Since we have dropped Java 7 support we should figure out how to do this. There are compatibility implications so a KIP is needed.,1633
Extract Method,Reduce NPath exceptions in Connect The [recent upgrade of Checkstyle and move to Java 8|https://github.com/apache/kafka/pull/5046/files/1a83b7d04bd3cecbb68d033211b1bcdfbe085d47#diff-6869a8c771257f000c3cefb045e9d289] has caused some existing code to not pass the NPath rule. Look at the classes to see what might need to be changed to remove the class from the suppression rule: AbstractStatus|ConnectRecord|ConnectSchema|DistributedHerder|FileStreamSourceTask|JsonConverter|KafkaConfigBackingStore,1634
Extract Method,Add getter to AbstractStream class to make internalTopologyBuilder accessible outside of package Currently the AbstractStream class defines a copy-constructor that allow to extend KStream and KTable APIs with new methods without impacting the public interface. However adding new processor or/and store to the topology is made throught the internalTopologyBuilder that is not accessible from AbstractStream subclasses defined outside of the package (package visibility).,1635
Rename Method,Remove caching wrapper stores if cache-size is configured to zero bytes Users can disable caching globally by setting the cache size to zero in their config. However this does only effectively disable the caching layer but the code is still in place. We should consider to remove the caching wrappers completely for this case. The tricky part is that we insert the caching layer at compile time ie when calling `StreamsBuilder#build()` – at this point we don't know the configuration yet. Thus we need to find a way to rewrite the topology after it is passed to `KafkaStreams` if case caching size is set to zero. KIP: [KIP-356: Add withCachingDisabled() to StoreBuilder|https://cwiki.apache.org/confluence/display/KAFKA/KIP-356%3A+Add+withCachingDisabled%28%29+to+StoreBuilder?src=jira],1637
Extract Method,Overloaded StreamsBuilder Build Method to Accept java.util.Properties Add overloaded method to {{StreamsBuilder}} accepting a {{java.utils.Properties}} instance.   KIP can be found here https://cwiki.apache.org/confluence/display/KAFKA/KIP-312%3A+Add+Overloaded+StreamsBuilder+Build+Method+to+Accept+java.util.Properties,1638
Extract Method,AdminClient should handle FindCoordinatorResponse errors Currently KafkaAdminClient.deleteConsumerGroups listConsumerGroupOffsets method implementation ignoring FindCoordinatorResponse errors. This causes admin client request timeouts incase of authorization errors.  We should handle these errors.,1639
Extract Method,Reduce number of rebalance for large consumer groups after a topic is created For a group of 200 MirrorMaker consumers with patten-based topic subscription a single topic creation caused 50 rebalances for each of these consumer over 5 minutes period. This causes the MM to significantly lag behind during this 5 minutes period and the clusters may be considerably out-of-sync during this period. Ideally we would like to trigger only 1 rebalance in the MM group after a topic is created. And conceptually it should be doable.   Here is the explanation of this repeated consumer rebalance based on the consumer rebalance logic in the latest Kafka code: 1) A topic of 10 partitions are created in the cluster and it matches the subscription pattern of the MM consumers. 2) The leader of the MM consumer group detects the new topic after metadata refresh. It triggers rebalance. 3) At time T0 the first rebalance finishes. 10 consumers are assigned 1 partition of this topic. The other 190 consumers are not assigned any partition of this topic. At this moment the newly created topic will appear in `ConsumerCoordinator.subscriptions.subscription` for those consumers who is assigned partition of this consumer or who has refreshed metadata before time T0. 4) In the common case half of the consumers has refreshed metadata before the leader of the consumer group refreshed metadata. Thus around 100 + 10 = 110 consumers has the newly created topic in `ConsumerCoordinator.subscriptions.subscription`. The other 90 consumers do not have this topic in `ConsumerCoordinator.subscriptions.subscription`. 5) For those 90 consumers if any consumer refreshes metadata it will add this topic to `ConsumerCoordinator.subscriptions.subscription` which causes `ConsumerCoordinator.rejoinNeededOrPending()` to return true and triggers another rebalance. If a few consumers refresh metadata almost at the same time they will jointly trigger one rebalance. Otherwise they each trigger a separate rebalance. 6) The default metadata.max.age.ms is 5 minutes. Thus in the worse case which is probably also the average case if number of consumers in the group is large the latest consumer will refresh its metadata 5 minutes after T0. And the rebalance will be repeated during this 5 minutes interval.      ,1640
Rename Method,KIP-328: Add Window Grace Period (and deprecate Window Retention) As described in [https://cwiki.apache.org/confluence/display/KAFKA/KIP-328%3A+Ability+to+suppress+updates+for+KTables]   This ticket only covers the grace period portion of the work.,1641
Extract Method,Migrate Streams API to Duration instead of longMs times Right now Streams API unversally represents time as ms-since-unix-epoch. There's nothing wrong per se with this but Duration is more ergonomic for an API. What we don't want is to present a heterogeneous API so we need to make sure the whole Streams API is in terms of Duration.   Implementation note: Durations potentially worsen memory pressure and gc performance so internally we will still use longMs as the representation.  KIP: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-358%3A+Migrate+Streams+API+to+Duration+instead+of+long+ms+times],1642
Rename Method,"Streams should be more fencing-sensitive during task suspension under EOS When EOS is turned on Streams did the following steps: 1. InitTxn in task creation. 2. BeginTxn in topology initialization. 3. AbortTxn in clean shutdown. 4. CommitTxn in commit() which is called in suspend() as well. Now consider this situation with two thread (Ta) and (Tb) and one task: 1. originally Ta owns the task consumer generation is 1. 2. Ta is un-responsive to send heartbeats and gets kicked out a new generation 2 is formed with Tb in it. The task is migrated to Tb while Ta does not know. 3. Ta finally calls `consumer.poll` and was aware of the rebalance it re-joins the group forming a new generation of 3. And during the rebalance the leader decides to assign the task back to Ta. 4.a) Ta calls onPartitionRevoked on the task suspending it and call commit. However if there is no data ever sent since `BeginTxn` this commit call will become a no-op. 4.b) Ta then calls onPartitionAssigned on the task resuming it and then calls BeginTxn. Then it was encountered a ProducerFencedException incorrectly. The root cause is that Ta does not trigger InitTxn to claim ""I'm the newest for this txnId and am going to fence everyone else with the same txnId"" so it was mistakenly treated as the old client than Tb. Note that this issue is not common since we need to encounter a txn that did not send any data at all to make its commitTxn call a no-op and hence not being fenced earlier on. One proposal for this issue is to close the producer and recreates a new one in `suspend` after the commitTxn call succeeded and `startNewTxn` is false so that the new producer will always `initTxn` to fence others.",1643
Extract Method,Naming Join and Grouping Repartition Topics To help make Streams compatible with topology changes we will need to give users the ability to name some operators so after adjusting the topology a rolling upgrade is possible.   This Jira is the first in this effort to allow for giving operators deterministic names. KIP-372: https://cwiki.apache.org/confluence/display/KAFKA/KIP-372%3A+Naming+Repartition+Topics+for+Joins+and+Grouping,1644
Extract Method,"Improve Streams close timeout semantics See [https://github.com/apache/kafka/pull/5682#discussion_r221473451] The current timeout semantics are a little ""magical"": * 0 means to block forever * negative numbers cause the close to complete immediately without checking the state I think this would make more sense: * reject negative numbers * make 0 just signal and return immediately (after checking the state once) * if I want to wait ""forever"" I can use {{ofYears(1)}} or {{ofMillis(Long.MAX_VALUE)}} or some other intuitively ""long enough to be forever"" value instead of a magic value.   Part of https://cwiki.apache.org/confluence/display/KAFKA/KIP-358%3A+Migrate+Streams+API+to+Duration+instead+of+long+ms+times",1645
Extract Method,Make LensClient implement java.lang.AutoClosable  This would allow users to use lens client in try-with-resources clause without worrying about closing manually.https://docs.oracle.com/javase/tutorial/essential/exceptions/tryResourceClose.html,1646
Rename Method,DriverQueryHook should support postSelect option also. as of now DriverQueryHook has a preLaunch() method which is called just before launching the query . We should also have an option to interact with hook on postSelect() which will be called once query has been accepted by lens server and appropriate driver has been selected for it.Also as of now the preLaunch() is called by individual driver. This should be moved to server and drivers should not manage this operation. ,1647
Rename Method,Convert dimension filter to fact filters for perfomace improvement on outer join queries Filters like the following can be converted to fact filters. This will help in improving query performance by pushing down filter to driving table i.e fact.e.g. where dim.name in ('x' 'y') becomeswhere fact.dimid in (select dim.id from dim where dim.name in ('x''y')),1648
Extract Method,Add option to kill the query on timeout api on timeout Right now on execute with timeout api if the query times out then query will be still running for people to pull reports.We should provide option to kill the query upon timeout for users who are not interested in result beyond timeout.,1649
Move Method,Isolate submissions for each driver We need to isolate query submitters for each driver to be different. We have seen scenarios where hive submission takes time and other interactive queries on jdbc are queued because querysubmitter was busy. Also each driver can have a pool of submitters - which should benefit interactive queries with immediate submissions especially the ones submitted on EXECUTE_WITH_TIMEOUT,1651
Extract Method,Support Asynchronous status updates from drivers 0,1652
Extract Method,Thread should have ability to wait for Events to be processed When a thread notifies event service to process any event it should have a way to wait for it to be finished.{code}LensEvent event = new LensEvent();synchronized(event) {   eventservice.notifyEvent(event);   event.wait();}{code}The EventHandler will do a notifyall() on the event it is done handling. {code}public void run() {      try {        Class<? extends LensEvent> evtClass = event.getClass();        // Call listeners directly listening for this event type        handleEvent(eventListeners.get(evtClass) event);        Class<?> superClass = evtClass.getSuperclass();        // Call listeners which listen of super types of this event type        while (LensEvent.class.isAssignableFrom(superClass)) {          if (eventListeners.containsKey(superClass)) {            handleEvent(eventListeners.get(superClass) event);          }          superClass = superClass.getSuperclass();        }      } finally {        synchronized (event) {          event.notifyAll();        }      }    }{code},1653
Extract Method,Query Resource in the lens server offers very limited information on the getAllQueries call {code}List<QueryHandle> getAllQueries(sessionid states user driver queryName fromDate toDate);{code}Currently provides very limited information (Query handle) upon this call. If consuming service where to tabulate minimal information of query history the call to getAllQueries have to be followed by a series of {code}LensQuery getStatus(sessionid queryHandle){code}.It would be very helpful to have a resource method that can provide a richer set of information as an additional method.,1655
Extract Method,Support drop partition(s) for specific update periods Facilitate dropping partitions based on update period.,1656
Rename Method,Add examples execution to ML Create a utility that can read the ml params test table etc from the conf and execute it via a script,1657
Move Method,Add distinct keyword with columns projected for dimension only queries Currently dimension only queries give duplicate rows when joining with fact for specified date range. For such queries we should be adding distinct keyword in the projected column list. ,1658
Extract Method,Provide an option to list all resources added in a session provide a command in lens-cli to list all the resources added in a particular session.,1659
Rename Method,Minor refactoring changes in Query Rewriter interface  QueryRewrite interface  needs to be moved into server-api and some minor method signature changes,1660
Extract Method,"lens-cli should not use single parameter for two values We have code like the following in many places in cli :{noformat}  @CliCommand(value = ""update storage"" help = ""update storage"")  public String updateStorage(      @CliOption(key = { """" ""storage"" } mandatory = true help = ""<storage-name> <path to storage-spec>"") String specPair) {    Iterable<String> parts = Splitter.on(' ').trimResults().omitEmptyStrings().split(specPair);{noformat}It should take separate parameters for each value than doing split on space",1662
Extract Method,"Use prepare statement for query syntax and semantic validation  Currently we are using ""select roughly"" for query validation in jdbc driver. Rough query can take more time based on the query complexity and can't guarantee to finish <50ms. On the other hand prepare query is very light weight and we can expect it to finish in defined threshold and is supported by  major DBMS.",1663
Extract Method,Timeouts on rest api calls in lens client To handle cases where server is not responding or slow in response we need to put timeouts on api calls in lens client.  Need to see if one value will suffice for all api calls or specific timeout values will be required for some api calls. ,1664
Extract Method,Driver side rewrite happening twice for execute call on JDBCDriver Seeing that driver side rewrite is happening twice on JDBCDriver. One of estimate and once again for driver.execute. It should be done only once.,1665
Extract Method,Provide a way to distinguish between fields reached by multiple paths We need a way to distinguish fields reachable from different paths.For example: If country field is reachable from two different attributes for which the values are different.,1666
Rename Method,from lens cli given a dimension find out all the dimtables currently we are able to get dimension from dimtable. but opposite is not there to do. can we add this in lens cli so that given a dimension we can figure out all the dimtables belonged to that dimension.,1667
Rename Method,Configurability in QueryExecutionServiceImpl * DriverSelector: right now it's hard coded to MinQueryCostSelector. Making it configurable via conf* Query acceptors now loaded through server conf* renamed variable acceptedQueries to queuedQueries* test cases for acceptors,1668
Rename Method,Add cli command to print query details This command prints details as per LensQuery on cli,1670
Extract Method,Add MaxCoveringFactResolver after fact partitions are resolved 0,1671
Rename Method,add jar should be able to take regex path and should be able to add multiple jars 0,1672
Extract Method,Addition of an identifier in every log line for segregating logs 0,1673
Extract Method,"Lens Cli doesnt provide an option to drop DB with cascade set to true There is no way of dropping a db from lens cli if its not empty. Please provide an option to drop db from cli with ""cascade"" set to true.",1674
Extract Method,Move Hive dependency to Apache Hive Creating wish for moving to Apache hive dependency instead of forked hive dependency. We will create follow up issues in lens and hive make them link here.,1675
Extract Method,add cli command for getting timeline of all facts or single fact or single fact+storage pair.  0,1676
Extract Method,Add start time for fact as a fact property  When new facts are added and are populated with data from some time onwards having start time property on the facts should help picking eligible facts for given range.Without the property if a light fact is added from a later time and with lightest fact first flag on the queries cannot be answered from older range. This causes queries to fail.Similarly we can add end time as well for the fact. Also there could be some facts on which data is valid from relative start time. For example : only 90 days of data is valid. Accepting relative start time will allow even if drop partitions is not uptodate query can pick proper facts.,1677
Extract Method,"Too many ""set param key"" on session restoration upon restart We are seeing too may ""set param key :"" logs on session restoration upon restrat. And each set param is involving IO with disk which in turn increases restart time.Here is a log snippet :{noformat}21 May 2015 06:05:50806 [main] INFO  org.apache.lens.server.session.HiveSessionService  - Set param key:lens.query.output.enable.compression value:true21 May 2015 06:05:50806 [main] INFO  org.apache.lens.server.session.HiveSessionService  - Request to Set param key:hive.metastore.client.socket.timeout value:60021 May 2015 06:05:50806 [main] INFO  org.apache.hive.service.cli.operation.Operation  - Putting temp output to file /tmp/lens/26064577-7a38-4279-ad73-0daa4a32a7671862119368791234528.pipeout21 May 2015 06:05:50806 [main] INFO  org.apache.lens.server.session.HiveSessionService  - Set param key:hive.metastore.client.socket.timeout value:60021 May 2015 06:05:50806 [main] INFO  org.apache.lens.server.session.HiveSessionService  - Request to Set param key:hive.metastore.connect.retries value:521 May 2015 06:05:50806 [main] INFO  org.apache.hive.service.cli.operation.Operation  - Putting temp output to file /tmp/lens/26064577-7a38-4279-ad73-0daa4a32a7671862119368791234528.pipeout21 May 2015 06:05:50806 [main] INFO  org.apache.lens.server.session.HiveSessionService  - Set param key:hive.metastore.connect.retries value:521 May 2015 06:05:50806 [main] INFO  org.apache.lens.server.session.HiveSessionService  - Request to Set param key:lens.query.enable.mail.notify value:true21 May 2015 06:05:50806 [main] INFO  org.apache.hive.service.cli.operation.Operation  - Putting temp output to file /tmp/lens/26064577-7a38-4279-ad73-0daa4a32a7671862119368791234528.pipeout21 May 2015 06:05:50807 [main] INFO  org.apache.lens.server.session.HiveSessionService  - Set param key:lens.query.enable.mail.notify value:true21 May 2015 06:05:50807 [main] INFO  org.apache.lens.server.session.HiveSessionService  - Restored session 26064577-7a38-4279-ad73-0daa4a32a76721 May 2015 06:05:50807 [main] INFO  org.apache.hadoop.hive.ql.session.SessionState  - No Tez session required at this point. hive.execution.engine=mr.21 May 2015 06:05:50808 [main] INFO  org.apache.hadoop.hive.ql.session.SessionState  - No Tez session required at this point. hive.execution.engine=mr.21 May 2015 06:05:50808 [main] INFO  org.apache.lens.server.session.HiveSessionService  - Request to Set param key:hive.metastore.failure.retries value:321 May 2015 06:05:50810 [main] INFO  org.apache.hive.service.cli.operation.Operation  - Putting temp output to file /tmp/lens/4d63f0f8-699c-4a3e-ae2d-6eaf03a5a3437220243919237070139.pipeout21 May 2015 06:05:50810 [main] INFO  org.apache.lens.server.session.HiveSessionService  - Set param key:hive.metastore.failure.retries value:321 May 2015 06:05:50810 [main] INFO  org.apache.lens.server.session.HiveSessionService  - Request to Set param key:hive.metastore.client.connect.retry.delay value:121 May 2015 06:05:50811 [main] INFO  org.apache.hive.service.cli.operation.Operation  - Putting temp output to file /tmp/lens/4d63f0f8-699c-4a3e-ae2d-6eaf03a5a3437220243919237070139.pipeout{noformat}",1678
Extract Method,Flattened columns should include reachable fields through join chains Right now flattened columns returned constructs reachable fields through references. We should change it to show reachable fields through join chains. And expanding the reachable fields through join chains should be optional in the api. ,1679
Extract Method,"Add ""time dimension not supported"" as a fact prune cause 0",1680
Rename Method,"Refactoring of testQueryCommands test case There are too many test cases running in testQueryCommands test case. It is considerably time consuming to debug a failing test case within  testQueryCommands. If these test cases are modularized and broken into separate test cases it will be much helpful in faster debugging.{code}@Test  public void testQueryCommands() throws Exception {    client = new LensClient();    client.setConnectionParam(""lens.query.enable.persistent.resultset.indriver"" ""false"");    setup(client);    LensQueryCommands qCom = new LensQueryCommands();    qCom.setClient(client);    resDir = new File(""target/results"");    assertTrue(resDir.exists() || resDir.mkdirs());    testExecuteSyncQuery(qCom);    testExecuteAsyncQuery(qCom);    testSyncResults(qCom);    testExplainQuery(qCom);    testExplainFailQuery(qCom);    testPreparedQuery(qCom);    testShowPersistentResultSet(qCom);    testPurgedFinishedResultSet(qCom);    testFailPreparedQuery(qCom);    // run all query commands with query metrics enabled.    client = new LensClient();    client.setConnectionParam(""lens.query.enable.persistent.resultset.indriver"" ""false"");    client.setConnectionParam(""lens.query.enable.metrics.per.query"" ""true"");    qCom.setClient(client);    String result = qCom.getAllPreparedQueries(""all"" """" -1 -1);    assertEquals(result ""No prepared queries"");    testExecuteSyncQuery(qCom);    testExecuteAsyncQuery(qCom);    testSyncResults(qCom);    testExplainQuery(qCom);    testExplainFailQuery(qCom);    testPreparedQuery(qCom);    testShowPersistentResultSet(qCom);    testPurgedFinishedResultSet(qCom);    testFailPreparedQuery(qCom);  }{code}",1681
Extract Method,Ability to inspect GET and POST request that are to be submitted to Lens Server We want application based handshake by including secret key as part of the headers. There are several hooks available in Lens Server to inspect the Request thus giving the opportunity to accept or reject the request however the Request coming from LensClient doesn't have hooks to be able to modify requests at a common location.This jira tracks creating hooks for javax.ws.rs.client.ClientRequestContext so that users can have a opportunity to add headers on every outgoing request from LensClient.,1683
Move Method,Allow fact start time to be specified for a storage and a update period We can specify absolute and relative start time for a fact. This improvement request is to add the ability to override the start time of a fact on a storage and a update period.,1684
Move Method,rename LogicalTableCrudCommand to ConceptualTableCrudCommand and PhysicalTableCrudCommand to LogicalTableCrudCommand in accordance with http://lens.apache.org/user/olap-cube.html#Conceptual_Tables,1685
Rename Method,"Slow response times for /metastore/nativetables API To obtain list of native tables CubeMetastoreService does the following -1. Fetches the list of tables ( one MetastoreClient call)2. Filters out the cube tables from the list. The filtering happens by looking at the table properties from the Table object. This table object is obtained with another Metastore call. So If there are 'n' tables there will be 'n' metastore calls.Here is the code snippet :  private List<String> getTablesFromDB(LensSessionHandle sessionid    String dbName boolean prependDbName)    throws MetaException UnknownDBException HiveSQLException TException LensException {    List<String> tables = getSession(sessionid).getMetaStoreClient().getAllTables(      dbName);    List<String> result = new ArrayList<String>();    if (tables != null && !tables.isEmpty()) {      Iterator<String> it = tables.iterator();      while (it.hasNext()) {        String tblName = it.next();        org.apache.hadoop.hive.metastore.api.Table tbl =          getSession(sessionid).getMetaStoreClient().getTable(dbName tblName);        if (tbl.getParameters().get(MetastoreConstants.TABLE_TYPE_KEY) == null) {          if (prependDbName) {            result.add(dbName + ""."" + tblName);          } else {            result.add(tblName);          }        }      }    }    return result;  }Instead of this we can directly fetch the list of table objects for our list of table names in a single API call using getMetaStoreClient().getTableObjectsByName() method.Currently one of our databases contain 8000+ tables which leads to very very long response times.   ",1686
Extract Method,Add relative end time for facts 0,1687
Rename Method,Expression pushdown for query optimisation in JDBC Currently Columnar rewriter skips rewrite in case of expressions used in select query. This ticket is to add the improvement in the rewriter to rewrite them and pushdown to fact subquery. ,1688
Extract Method,queries where results of two storage tables of same fact are unioned the rows should be aggregated 0,1689
Rename Method,Session close should not result in queued query failures In the current scenario if the queries are queued from lens side (because of throttling) then these queries fails on session close.,1691
Extract Method,Make `cube` keyword optional in queries 0,1693
Extract Method,Skip partition registration for very old and future dates for a fact tables Currently in fact partition registration scheme if dates flowing from source are very old(epoch start time) or any future date we don't have any check to skip them. Rather we can accept partitions only after fact start time and ignore older than that.,1695
Extract Method,Query optimization to findRoute and dequeueMessage queries - MATCHER job During the execution of MATCHER job the CorrelationKeySet used will only contain pre initialised correlation keys. https://github.com/apache/ode/blob/ode-1.3.x/bpel-runtime/src/main/java/org/apache/ode/bpel/runtime/PICK.java#L87https://github.com/apache/ode/blob/ode-1.3.x/bpel-runtime/src/main/java/org/apache/ode/bpel/runtime/EH_EVENT.java#L120Hence we can directly use the CorrelationKeySet's canonical value instead of its subsets for both findRoute and dequeueMessage method calls within matcherEvent.This avoids the use of SQL IN clause in the findRoute and dequeueMessage queries and improves the execution time of these queries.,1696
Rename Method,"ExtensionActivity and ExtensionAssignOperation: Support for parser and compiler BPEL's extension mechanisms are very powerful to ease data manipulation or debugging.Aim of this task is to bring in an implementation for correctly parsing and compiling <extensionActivity>s and <extensionAssignOperation>s. Therefore the API needs to be slightly extended with a plugin mechanism for ""extension bundles"". Bundles are related to a specific extension namespace and can consist of both ExtensionActivity and ExtensionAssignOperation implementations. Such bundles can be registered to the engine using the OdeConfiguation properties files.Parser and compiler will be extended to cope with <extensions> <extension> <extensionActivity> and <extensionAssignOperation> elements.The runtime part needs some further discussion and will be addressed in an other task.",1697
Extract Method,Deployment using a Web Service interface Allow for remote deployment using a web service. The deployment package (basically a zip containing the directory with all necessary files) should be sent as an attachment to the message.,1698
Rename Method,Axis2TestBase: centralize server management in base class The org.apache.ode.axis2.Axis2TestBase raison d'etre is to embed an ODE server for test purpose. So concrete test classes may deploy processes invoke operations etc.However as of today the server management is not at the base class level. Subclasses are responsible for starting/stopping the server. And if a test case does not stop the server the next test case to be executed will fail to start the server (since the port is already used etc).Moreover management interface is kind of fuzzy (you have to call start to start but server.stop() to shutdown).This fix aims at centralize the server management in the base class.Server is started/stopped in JUnit setUp/tearDown methods. So subclasses do not have to worry about anymore.Of course if subclasses have to perform custom setUp/tearDown steps it's their responsability to call the super methods.,1700
Extract Method,"Dynamic config for Axis2 external services What for?One might want to apply specific settings on the external endpoints used by a process. For instance: http timeout http redirections or other security settings. ODE Services are not concerned at all.This page is the place holder for material about this project.How to?It seems relevant to reuse Axis2 configuration mechanism for SOAP-bound services. No need to reinvent the wheel. However a home-made mechanism will be implemented for HTTP-bound services.An Axis2 Service Config file[1] might be dropped in the deployment unit directory for each external service involved in the process.The file name pattern will be ""{local-service-name}.axis2"".File changes (creationdeletionupdates) will be polled regularly and applied to the EPR.[1] http://ws.apache.org/axis2/1_0/axis2config.html#Service_Configuration",1701
Move Method,Fix in-memory DAOs Fix the in-memory DAO implementation principally by implementing the new message exchange methods in BpelDAOConnectionImpl and do some testing.,1702
Extract Method,XPath functions for RESTful BPEL: combineUrl composeUrl expandTemplate Provide 2 new Xpath functions as described in the ODE wiki:__combineUrl(base relative)__----------------------------------------------------Takes the relative URL and combines it with the base URL to return a new absolute URL. If the relative parameter is an absolute URL returns it instead.__composeUrl(template [name value]*)____composeUrl(template pairs)__----------------------------------------------------Expands the template URL by substituting place holders in the template for example ('/order/{id}' 'id' 5) returns '/order/5'. Substitute values are either name/value pairs passed as separate parameters or a node-set returning elements with name mapping to value. The functions applies proper encoding to the mapped values. Undefined variables are replaced by an empty string.Basically a uri is always returned.Compliant with the URI Template spec [1].__expandTemplate(template [name value]*)____expandTemplate(template pairs)__----------------------------------------------------Same behavior as composeUrl except that undefined variables are *NOT* replaced by an empty string. The corresponding expansion pattern is not replaced. The immediate coensquence is that the function may return a template.[1] http://bitworking.org/projects/URI-Templates/spec/draft-gregorio-uritemplate-03.html,1703
Rename Method,REST Connector 0,1705
Rename Method,Allow BPEL Processes To Be Provided Over JMS In general the requirements as follows:a) Allow two or more processes to provide the same service over (the same) JMS Queue endpoint.b) Allow two or more processes to provide the same port type (but different service) over (the same) JMS Queue endpoint.c) Allow two or more processes to provide the same service over (the same) JMS Topic endpoint.d) Allow two or more processes to provide the same port type (but different service) over (the same) JMS Topic endpoint.e) Allow a process to invoke a service (or process) over a JMS endpoint.We work with the following assumptions:a) Operations provided over JMS Topics must be one-way (to avoid multiple responses per request)b) Operations provided over JMS queues may be either one-way or two-way.c) As per Axis2 protocol non-durable or non-existent destination names will be qualified with either dynamicQueues or dynamicTopics.The limitations in the existing code base are:a) The name that we assign to axis services is derived from the soap:location endpoint which is assumed to follow an HTTP scheme.b) It is not possible to have two processes provide the same service as that leads to naming conflicts.c) By default the JMS transport is not enabled in Axis2.The proposed (verified) solution is:a) For testing purposes enable the JMS transport in axis2.xml. Note that by default this will be not be turned on. The configuration of JMS in Axis2 and setup of the JMS broker is left as an exercise for the user/developer.b) Derive service names from jms endpoints without making the assumptions made for HTTP endpoints. Further qualify the JMS endpoint with the bundle diagram and  process name so as to make it unique (this is necessary to avoid the naming conflict). To be precise the JMS URI template is as follows:${deploy_serverUrl}${deploy_baseSoapServicesUrl}/${deploy_bundleNcName}/${diagram_relativeURL}/${processLocalName}/${jmsDestinationName}c) Extract the jms destination name from the service name and set it as the value of the JMSConstants.DEST_PARAM of the axis service (this is required so that the JMS Connection Factory creates the right destination for that endpoint)d) Store the axis service in ODEServer against the unique name as was derived above. Use that name while destroying that service as well.e) As far as requirement (e) I believe this works out of the box.f) As far as assumption (a) I believe this constraint should be enforced by the modeler. Also the modeler should enforce assumption (c) for proper provisioning of processes over JMS.,1707
Extract Method,"Publish/Subscribe across processes By default a SOAP request is targeted at a specific BPEL process in ODE. At times though one might want to publish the request simultaneously to multiple BPEL processes especially if the invocations are one-way.This issue describes an implementation of such a feature in the BPEL runtime in a way that is agnostic of the integration layer and transport bindings.In order to facilitate message publishing processes must have a way to subscribe to messages. While there are many ways to register subscriptions we chose a implicit mechanism of subscription wherein no new deployment artifacts are required.  In our approach if two or more processes provide the same (i.e. shared) service messages targeted at the endpoint of that service will essentially fan out to each of those (subscribing) processes.In general there are two paths that need to be considered:a) Out-Of-Process invocation of the shared service: This follows the path outlined in the BpelServer.createMessageExchange() method. For shared services we create a new kind of Brokered MEX that clones and pushes the message to each of the ""subscribing"" process.b) In-Process invocation of the shared service: This follows the path outlined in the BpelProcess.invokePartner() method which bypasses the MEXs and creates the MEXDAOs directly.  Again we clone and push the message to each ""subscribing"" process.During registration services will now be associated with a list of processes that provide it which could potentially be of any size. The endpoint is physically activated with the integration layer when the first process registers on it and is physically deactivated when the last process de-registers from it. Care must be taken though to remove any older versions of processes in the server's map. Also in order to handle two-way pub-subs gracefully we take the response from one of the processes and return that to the end-consumer. Ideally the design-time tooling should take care to prevent pub-sub across any services whose operations are not one-way.",1708
Rename Method,support ws-security for external services 0,1709
Rename Method,improve rakefile to enable testing with different dbms 1. With user settings make the database properties over-ridable.2. Build testing environments where all dbms are tested nightly.,1712
Extract Method,Enable easier extensibility on ODE for custom implementation or simulation This could be done by providing an axis2 integration adapter or even by proving extensibility on some engine code.The idea is that people can implement their own hooks to trace executions or alter the standard behavior. This work does not directly do any of the fore-mentioned jobs but makes it easier.,1713
Extract Method,Instance replayer Imagine situation when client has deployed a process with a lot of active long running instances. Then he finds there's a bug in this process and a simple bugfix is needed. But with current versioning rules new version is only used when new instances are created. So there's no simple way for doing such bufixes (which are usually possible with eg. java application using database connection). It is a blocking argument for deploying ODE Bpel solution instead of a regular java application.I think the best way to deal with such situations is to add serialize/deserialize to/from xml operations for process instances in management API. Also pause/resume ODE operations would be useful. Then a bugfix procedure would look like this;-pause ode-serialize instances-deploy newer version-deserialize instances and fix manually any import errors -resume ODEIt would also be a benefit of being able to do migration from older to newer ODE and between Hibernate/JPA DAOs which I saw already in some bug reports. What do you think about it?Regards,1714
Rename Method,"Support initiate=""join"" for receives with no createInstance For now the initiate=""join"" value for correlation is only supported on receives with createInstance=""true"". Implement it in other cases as well (with the correlation comparison logic).",1715
Extract Method,Ability to override <http:address> in WSDL using server global configuration Use-case:When moving projects from development to production it's often necessary to change service URLs to reflect production systems.We should make it easy to override the <http:address> of WSDL services using the Ode global configuration to support this use-case. ,1716
Move Method,Rejecting in-out operations immediately when there's no route found A related discussion is here:http://markmail.org/thread/ethxp3y7373x72h3A goal is to implement handling in-out operations immediately - resulting in failure when there's no route registered. But in-only operation should queue messages for later dispatching (just like before). ,1717
Extract Method,"Process hydration/dehydration improvements to better control memory footprint At present the BPEL server stashes the metadata of deployed processes in an in-memory unbounded cache. Given the unbounded nature of the cache the server is bound to eventually run out of memory when presented with a sufficiently large process set. The graceful way to handle this situation would be to place bounds on the cache either in terms of the total size or number of processes that it may contain at any given point in time. While throttling the server this way may effectively reduce its throughput it is certainly the far lesser evil compared to the alternative.To that end we define the following configurable properties for the benefit of the administrator:a) ""process.hydration.throttled.maximum.size"": The value of this property specifies the maximum size of the metadata of all processes that are currently in-use and may be cached in-memory. If the process metadata was hydrated at least once by the server instance then its size is calculated once by traversing its object model. If not then we estimate its in-memory size based on the size of the (.cbp) file where its serialized bytes are persisted.b) ""process.hydration.throttled.maximum.count"": The value of this property specifies the maximum number of processes that are currently in-use and whose metadata may be cached in-memory. A process that is stored on disk but is not loaded in-memory is said to be ""unhydrated"". When the server receives a message that is targeted at an unhydrated process we must decide whether or not there is sufficient free memory in the cache for it. If not then we select the least recently used process as our victim and evict (or dehydrate) it. This check and balance is performed until there is sufficient memory in which case we may hydrate the process. On the other hand if the cache capacity is exceeded then we process the message based on its exchange pattern as described below:i) If the message is one-way (asynchronous) then we queue it back with the job scheduler so that it can be retried later. The hope is that given a sufficient amount of delay and number of retries the cache will have enough room for the targeted process.ii) If the message is two-way (synchronous) then we cannot simply re-schedule it back because the client will no doubt timeout. The logical thing to do here is to just return a SOAP fault message that forces the client to handle retries itself if it so desires.Furthermore the administrator may use the following properties to enable and control lazy-loading:c) ""process.hydration.lazy"":  The value of this property specifies whether or not a process metadata is to be lazy-loaded. If so then the server will not immediately load process metadata upon startup or deployment. In fact the process is not hydrated until the server receives a message that demands that that process be loaded in-memory. This is a global property that may be overriden on a process-by-process basis by specifying a value for its namesake in the process' deployment descriptor (deploy.xml) file.d) ""process.hydration.lazy.minimum.size"": The value of this property specifies the approximate minimum size of the process metadata for which lazy-loading should be enabled. If the server calculates the approximate size of the process metadata (based on its serialized file size) to be less than the value specified herein then it will load the process eagerly. This way you don't suffer the time delay on hydration for messages that don't deal with large-sized processes.Lastly we introduce the following property to throttle the number of instances that a process may handle:e) ""process.instance.throttled.maximum.count"": The value of this property specifies the maximum number of instances that may be simultaneously active for any given process. This is a global property that may be overriden on a process-by-process basis by specifying a value for its namesake in the process' deployment descriptor (deploy.xml) file. Note that we assume that the process is already hydrated at the time we perform this check and as such this property is not directly related to caching. However it may have an indirect effect on the cache by virtue of the fact that the it will most likely cause the process to dehydrate sooner rather than later (through dehydration policies).PS: Note that the the in-memory representation of process metadata is made up of WSDL and BPEL object models. The cache that we refer to here deals specifically with BPEL objects. For the purposes of this issue caching of WSDL objects is considered to be out of scope.",1718
Rename Method,Improve process versioning in JBI Each time you redeploy a service assembly in servicemix there's a new process version registered in ODE. Also an old entry (old cbp) is deleted which causes old instances to throw 'error reloading compiled process' error. ,1719
Rename Method,Better error reporting for WS clients Currently when a process is called and some error happens during processing the error is not communicated back to the client. Instead all the client gets is a timeout exception which can be confusing.Here is an example:2009-03-05 19:30:06873 ERROR [org.apache.ode.axis2.ODEService] Timeout orexecution error when waiting for response to MEX {MyRoleMex#632 [Client96038a45-1409-4bda-ab33-81fd29de4a48-3] calling{http://www.intalio.com/bpms/workflow/ib4p_20051115}UIFWService.completeTask(...)}java.util.concurrent.TimeoutException: Message exchangeorg.apache.ode.bpel.engine.MyRoleMessageExchangeImpl$ResponseFuture@e8b20atimed out when waiting for a response!java.util.concurrent.TimeoutException: Message exchangeorg.apache.ode.bpel.engine.MyRoleMessageExchangeImpl$ResponseFuture@e8b20atimed out when waiting for a response!       atorg.apache.ode.bpel.engine.MyRoleMessageExchangeImpl$ResponseFuture.get(MyRoleMessageExchangeImpl.java:241)       atorg.apache.ode.axis2.ODEService.onAxisMessageExchange(ODEService.java:152)       atorg.apache.ode.axis2.hooks.ODEMessageReceiver.invokeBusinessLogic(ODEMessageReceiver.java:67)       atorg.apache.ode.axis2.hooks.ODEMessageReceiver.invokeBusinessLogic(ODEMessageReceiver.java:50)       atorg.apache.axis2.receivers.AbstractMessageReceiver.receive(AbstractMessageReceiver.java:96)       at org.apache.axis2.engine.AxisEngine.receive(AxisEngine.java:145)The proposal is to report back to the client any error during processing of the request (by default). This feature could be turned off for security reasons since it may create a risk of information disclosure.,1721
Extract Method,"placeholders in endpoint properties Endpoint properties [1] now support placeholders. These placeholders can be use in other property values to avoid repeating common values. The general placeholder pattern is ${placeholder.name}Three types of placeholders shall be separated: #1  environment placeholders:  placeholders for environment variables. They follow the naming convention ala ANT:  ${env.JAVA_HOME} will retrieve the JAVA_HOME env var. #2 system placeholders: placeholders for system propertiesThey follow the naming convention: ${system.log4j.configuration} will access the system property ""log4j.configuration""System placeholders might point to environment placeholders. #3 local placeholders: placeholders defined in one endpoint property fileThese do not use any prefixes: ${mytimeout} will be replaced by the value of ""mytimeout"" placeholder.Local placeholder values might themselves used the 2 previous placeholders types (env var and sys properties).mytimeout=${env.TIMEOUT} is valid and will be replaced by the env variable TIMEOUT.Local placeholders can be defined in one file and used in another. If defined twice the last loaded value will have precedence.Here are a few examples:placeholder1=placeholder1-valuetest.placeholder1=${placeholder1}ns-alias.my-service.ode.http.socket.timeout=${system.TestSystemProperty}ns-alias.my-service.ode.mex.timeout=${env.TEST_DUMMY_ENV_VAR}See org.apache.ode.utils.HierarchicalPropertiesTest for more.[1] http://ode.apache.org/user-guide.html#UserGuide-EndpointConfiguration",1722
Rename Method,Support setting mutliple message mappers Currently the engine supports having multiple message mappers registered however there is no way for the user to configure it to do so. Add a way for the user to specify multiple mappers.,1723
Extract Method,Implement immediate transaction retries in addition to the presistent retries Currently the ode engine re-schedules a job into the ODE_JOB table when the job fails. The job will be picked up later by the job scheduler on the same node. For 2 reasons we want to pre-pend an immediate transaction retry logic before the persistent retries.1. The current way of scheduling a job involves deleting of the job and inserting/selecting of a new job for a retry. If the system is under heavy load and if the failure was due to deadlocks from database overload you do not want to put on even more load.2. The interval between retries could be pretty long. Cases like database deadlocks can be resolved by re-trying the same transaction in a relatively short time.Repeat the same transaction by configurable number of tries with a configuration interval. If all tries still fail then schedule the job for persistent retries.,1725
Rename Method,Implement process context propagation Implementation of the feature specified at http://ode.apache.org/process-contexts.html,1726
Extract Method,Implement classpath resolved xsd & wsdl imports Currently ODE JBI requires wsdl & xsds to be unpacked in Service Unit. This is not required restriction. ,1727
Rename Method,Speed up listAllProcesses listAllProcesses in ProcessAndInstanceManagementImpl.java does a lot of queries to database in order to fetch instance summary. This causes transaction timeout. on ODE load.Disabling instance summary is a workaround:diff --git a/bpel-runtime/src/main/java/org/apache/ode/bpel/engine/ProcessAndInstanceManagementImpl.java b/bpel-runtime/src/main/java/org/apache/ode/bpel/engine/ProcessAndInstanceManagementImpl.javaindex 47ada7a..0317864 100644--- a/bpel-runtime/src/main/java/org/apache/ode/bpel/engine/ProcessAndInstanceManagementImpl.java+++ b/bpel-runtime/src/main/java/org/apache/ode/bpel/engine/ProcessAndInstanceManagementImpl.java@@ -8056 +8057 @@ public class ProcessAndInstanceManagementImpl implements InstanceManagement Pro         depinfo.setDocument(pconf.getBpelDocument());         depinfo.setDeployDate(toCalendar(pconf.getDeployDate()));         depinfo.setDeployer(pconf.getDeployer());+ /*         if (custom.includeInstanceSummary()) {             TInstanceSummary isum = info.addNewInstanceSummary();             genInstanceSummaryEntry(conn isum.addNewInstances() TInstanceStatus.ACTIVE pconf);@@ -8147 +8157 @@ public class ProcessAndInstanceManagementImpl implements InstanceManagement Pro             genInstanceSummaryEntry(conn isum.addNewInstances() TInstanceStatus.SUSPENDED pconf);             genInstanceSummaryEntry(conn isum.addNewInstances() TInstanceStatus.TERMINATED pconf);             getInstanceSummaryActivityFailure(conn isum pconf);- }+ }*/          if (custom.includeDocumentLists()) {             TProcessInfo.Documents docinfo = info.addNewDocuments(); ,1729
Extract Method,Remove imageIO dependency (was: PDPage convertToImage bug creates white images from black and white pdf files.) This bug has been reported in various other tickets submitted before. I am attempting to conclusively prove that this is an issue and it needs to be attended to since all past tickets regarding this bug have been marked invalid.I have attached a video showing very basic code that will reproduce the issue. I have also attached the code that causes the issue as well as a PDF file that works (a color one) and a black and white PDF file that doesn't.The main issue is that when reading a black and white PDF file (see attached black and white pdf file) the following message is displayed and the contents of the output image is completely white.26/05/2011 3:20:14 PM org.apache.pdfbox.util.operator.pagedrawer.Invoke processWARNING: getRGBImage returned NULLWe use PDFBox in our program for reading PDF files and at least 50 percent of our customer's PDF files (from different scanners) will not read because of this issue. This is a complete show stopper and I'd be more than happy to help in any way I could to resolve it.,1734
Rename Method,"Unit tests for PDFBox features We're upgrading the pdfbox we use and to ensure there aren't any regressions while also learning pdfbox we are unit testing some of the classes especially the PDF ""primitive"" objects (COS level).",1735
Rename Method,Pattern colorspace support PDFBox doesn't support PDPattern colorspaces,1736
Rename Method,Color conversion for PDJpegs using a DeviceN colorspace PDFBOX-1116 and PDFBOX-1154 already added some color conversions for PDJpegs and PDPixelMaps except one for handling DeviceN colorspaces.,1741
Rename Method,"Speed up LZWFilter decoding I noticed that the LZW decoder performance can be improved: it'sallocating a new byte[] for every byte it visits in the stream.  Thisis actually an O(N^2) cost but N is typically fairly small.I changed LZWDictionary to use its own private growable byte[] toaccumulate each added byte.  I also changed it to not pre-enroll allinitial (0-255) codes but instead add it (lazily) on demand if thecode is used.I also randomized the TestFilters test and mixed in some""more predictable"" patterns so we get better testing of the filters.If the test fails it prints the seed used for the random numbers sowe can reproduce the failure.",1742
Inline Method,Adding Basic Job Ticket Schema Add a missing schema : Basic Job TicketSome evolution to do :Make evolution in the parser to enable arrays of structured properties Enable multi namespace description in rdf:description without forcing first description to be schema description,1743
Inline Method,Improve xmpbox code strength Fix some issues in xmpbox code to improve strength and quality : * Preserve stack Trace* System println* Loose coupling* avoid print stack trace* Integer Instanciation* Constructor calls overridable method,1744
Extract Method,Non-sequential PDF parser + PATCH Currently PDF parsing is done in sequential manner resulting in problems with stream parsing and skipping unused content. The solution is a conforming parser which first reads XREF tables and uses this information to only parse required objects and uses length information for stream parsing. A completely new implementation of such a parser is currently worked on in PDFBOX-1000. While this parser will be the long term solution a short term solution based on existing code would be desirable. A first incomplete solution was presented in PDFBOX-1104.Starting from PDFBOX-1104 I have implemented an 'as much as possible' conforming parser called 'non-sequential parser' which handles all PDF documents (even inlined with object streams etc.). The parser can be used as a drop-in-replacement for PDFParser (subclass of PDFParser). It overwrites method parse and getPage method. The only restriction is currently the need to specify a file instead of an input stream. In order to efficiently read the file and use it with the existing object parsing code I developed a RandomAccessBufferedFileInputStream which allows InputStream operations in combination with seek operations and cached read data.In order to use NonSequentialPDFParser small changes and additions on existing classes are needed. This includes changing some methods/fields from private to protected in PDFParser add parsing of stream object information from XREF streams store and get this information from XrefTrailerResolver (object ids are stored negated in order to distinguish them from offsets) and allow resetting offset in PushBackInputStream. All these changes do not change behavior of current parser. Another requirement is the long offset patch (PDFBOX-1196) which is excluded from the patch set provided here.The provided parser currently works in a forceParsing=false mode resulting in an IOException if a parsing error occurs. In most cases this shouldn't be a problem since in my use cases exceptions typically occur trying to parse unused content or streams which with this new parser are no problems anymore. In my setup I use the new parser first and if a parsing error occurs fall back to the sequential parser (a bit like Acrobat does it if XREF information is buggy):try {    // ---- try first with (mostly) standard conform parsing     doc = PDDocument.loadNonSeq( PDF_FILE raBuf );    handleDocument(doc);} catch ( IOException ioe ) {    // ---- retry with sequential parser and force parsing    doc = PDDocument.load( new FileInputStream(PDF_FILE) raBuf true );    handleDocument(doc);}For me this new parser works very well on large document collections and is a large step forward to parse all documents also accepted by common PDF tools. While its behavior is nearly 'conform' there is nevertheless a need for a clean 'real' conforming parser. For instance since the underlying object structure has no access to the parser it is necessary to first parse all objects before they can be used. This includes objects that might not be needed at all. Another normally not needed step is copying the content of a stream. Since we work on a file with random access there would be no need for it. However this parser should fill the hole until a full featured and clean conforming parser is available.,1745
Extract Method,"Adding style information to the PDF to HTML converter This patch modifies the PDF to HTML conversion in order to add style information (bold italic and size font) in the resulting file. Moreover we have deleted the ""DOCTYPE"" header because some parsers throws the following exception:[Fatal Error] loose.dtd:31:3: The declaration for the entity ""HTML.Version"" must end with '>'.org.xml.sax.SAXParseException: The declaration for the entity ""HTML.Version"" must end with '>'.",1747
Extract Method,Allow resolution to be defined when calling ImageIOUtil.writeImage I would like to call the methodprivate static boolean writeImage(BufferedImage image String imageFormat Object outputStream int resolution)but it's private.The easiest solution in my mind would be just to change this method to a public method.,1748
Extract Method,split PDFont#encode As discussed in the dev@pdfbox.apache.org (thread : Questions about toUnicode Cmap).We need to split PDFont#encode to get one method providing the string and one providing the cid.,1749
Extract Method,Support decompression of password protected pdfs The commandline utility WriteDecodedDoc tries to decrypt encrypted pdfs while decompressing them. But it doesn't work if a password is needed as one can't pass a password to the decrypt method.,1750
Extract Method,Extend commandline utilities to use the non sequential parser by choice I'd like to extend the commandline utilities at least to most important ones to use optionally the non sequential parser.,1751
Rename Method,Refactor the PdfA parser To fix the PDFBox-1274 issue the  validation of PDF/A needs a refactoring.Currently each XRef entry is checked independently. Most of the time this is enough because the required information to validate the object are present in the object.For the issue PDFBox-1274  the object validation should access to the page that uses the object.After the refactoring the valdiation unit will be the PDPage.,1752
Inline Method,Reduce the memory consumption of a RandomAccessBuffer PDFBOX-1005 introduced a faster way to handle the growth of the RandowAccessBuffer. But the buffer is expanded by just doubling it which leads to a 2^n consumption of memory.,1754
Extract Method,Create NonSequentialParser with InputStream HiCurrently the NonSequentialParser can't be initialized using an InputStream.To allow the PreflightParser to inherit from the NonSequentialParser I have create a constructor that takes an InputStream as parameter. In attachment you can find a patch that :- Creates a TmpFile with the given InputStream in the constructor method- Deletes the TmpFile at the end of the parse method.- call the parseObjectDynamically method on the Trailer entries (useful for Preflight module)What is your opinion about this proposal.RegardsEric,1755
Extract Method,Improve handling of multiline text boxes The current implementation for setting the appearance of content that is added to a multiline text box is incorrect in a number of ways:* Doesn't position the start of the text in the correct location* Incorrectly uses font size '0' instead of auto-sizing the font* Doesn't break up very long lines* If the font size is very large then the next line is started too close to the previous line.,1756
Extract Method,Add XML output option to preflight As part of a recent SPRUCE hackathon (http://wiki.opf-labs.org/display/SPR/Home) we added XML output to preflight.  It would be good if preflight was able to offer this sort of output by default.  Example outputs from our code are here: https://github.com/petecliff/pdfeh/tree/master/sample_preflight_outputs  Our XML output code is here: https://github.com/willp-bl/preflight-app-mod  You might want to implement it your own way?As an aside; we have a format corpus of test files here: https://github.com/openplanets/format-corpus  Use of the files and contributions are encouraged!Thanks,1761
Extract Method,Add PDDocument.save(File) and PDDocument.loadNonSeq(InputStream ...) This patch adds methods for saving a PDF to a File and parse from an InputStream using the non sequential parser. It also includes test cases for PDDocument save and load methods.,1763
Rename Method,Replace external glyphlist.txt with our onw implementation According to the header of glyphlist.txt Adobe encourages people to use the content of the file to create their own implementation of the glyphlist:# Permission is hereby granted free of charge to any person obtaining a# copy of this documentation file to create their own derivative works# from the content of this document to use copy publish distribute# sublicense and/or sell the derivative works and to permit others to do# the same provided that the derived work is not represented as being a# copy or version of this document.To get rid of the external dependency we should follow that advise and create our own class providing the information of the glyphlist.txt,1764
Rename Method,"Improve pdfbox tests I'd like to improve the tests for rendering.org/apache/pdfbox/util/TestPDFToImage.java is disabled in pdfbox\pom.xml . This has been disabled since 2009 ?! So I enabled it here.The subdir ""rendering"" is missing in pdfbox\target\test-output for these testsWhen a test fails because the rendered image is not identical no detailed message appears on the console. It appears only in pdfbox.log and not on the console.this is because of the settings inpdfbox\src\test\resources\logging.propertiesIf this is on purpose please change the texts in pdfbox\src\test\java\org\apache\pdfbox\util\*.java from""One or more failures see test log for details""to""One or more failures see test logfile 'pdfbox.log' for details""I wanted to attach a PDF with ccitt g4 compression and its rendering created with the 1.8.2 version but it doesn't work out seems that CIB generates files that can be rendered properly with 1.8.2. However I attach the TIFF g4 file and a JBIG2 test file from it. I don't have access to a Xerox WorkCentre (enter jbig2 in google news :-) ) so I used a free service so there's a watermark.It should be included intopdfbox\src\test\resources\input\renderingI have created the image myself and I give it into the public domain.If my suggestion is accepted it would be nice if people could create files that fail in current versions or have failed in old versions and release these files to the public domain so that they can be added to the tests.",1765
Extract Method,"Performance improvement in PDPageContentStream.drawString There is a simple way of improving the performance of drawString by replacing       string.writePDF( buffer );        appendRawCommands( new String( buffer.toByteArray() ""ISO-8859-1""));        appendRawCommands( SPACE );with       string.writePDF( buffer );        appendRawCommands( buffer.toByteArray() );        appendRawCommands( SPACE );as the appendRawCommands(String) simple does a appendRawCommands(str.getBytes( ""ISO-8859-1"" ));Therefore this optimization should spare the String creation as well as the conversion back to a byte array.",1766
Rename Method,[PATCH] Visible Signature using PDFbox In order to sign document with visible signature we have very bad solution at the moment:  passing a PDF as an InputStream that serves as a template for the appearance settings is very inconvenient. So Now Everything is well and fixed! You only set image with location zoom width height and etc and everything will be added automatically. I've just already done this and I will upload my patches too. I have wrote example too in order to see how to use it. Everything is easy!,1767
Rename Method,Refactor color spaces I'm currently working on this so I wanted to open an issue to let everyone know.Color spaces need to be refactored in 2.0.0. Tilman noticed slowness in PDFBOX-1851 due to using ICC profiles and calling ColorSpace#toRGB for every pixel. For example the file from PDFBOX-1851 went from rendering in 4 seconds to taking over 60 seconds.The solution is to use ColorConvertOp to convert an entire BufferedImage in one go taking advantage of AWT's native color management module. Color conversions done this way are almost instantaneous even for large images.The current design of color spaces within PDFBox depends upon conversions being done on a per-pixel basis so a significant refactoring is needed in order to convert images using ColorConvertOp without having to resort to per-pixel calls in cases such as a Separation color space which uses a CMYK alternate color space via a tint-transform.The color space handling code is also tightly coupled to image handling. The various classes which read images each have their own color handling code which rely on per-pixel conversions. For this reason any color space refactoring must also included a significant refactoring of image handling code. This is an opportunity to refactor all color handling so that it is encapsulated within the color space classes allowing downstream users to call toRGB(float[]) or toRGB(BufferedImage) and not need to worry about tint transforms and the like.===========Here's a summary of the changes:- PDCcitt has been removed its reading capability has moved to CCITTFaxFilter and writing capability has moved to CCITTFactory.- PDJpeg has been removed. JPEG reading is now done by new code in DCTFilter which correctly handles CMYK/YCCK color. This fixes various files where images appeared like negatives. JPEG writing is done by new code in JPEGFactory.- cleaned up JBIG2Filter- cleaned up JPXFilter in particular calling decode() caused the stream dictionary to be updated which was unsafe. I've also added a special JPXColorSpace which wraps the embedded AWT color space of a JPX BufferedImage this replaces the need for the awkward mapping of ColorSpace to PDColorSpace.- Added better error messages for missing JAI plugins (JPX JBIG2). A special exception MissingImageReaderException is now thrown.- PDXObjectForm has been renamed to PDFormXObject to match the PDF spec.- PDXObjectImage has been renamed in the same manner.- PDInlinedImage has been renamed to PDInlineImage for the same reason.- CCITTFaxDecodeFilter has been renamed to CCITTFaxFilter for consistency with the other filters.- ImageParameters has been removed it was used to represent inline image parameters which are now simply members of PDInlineImage.- added PDColor which represents a color value including patterns it is immutable for ease of use.- removed PDColorState which was a container for both a color and a color space in almost every case it was used to represent a color and so has been replaced by PDColor and occasionally PDColorSpace.- moved most of the functionality of PDXObject into its subclasses- rewrote almost all color handling code in all PDColorSpace subclasses including fixing the calculations for l*a*b DeviceN and indexed color spaces. - all color spaces now implement a toRGB(float[]) function for color conversion so external consumers of color spaces no longer have to know about internals such as tint transforms.- image color conversion is now performed in one operation using ColorConvertOp rather than pixel-by-pixel this speeds up ICC transforms by many orders of magnitude. Color spaces now expose a special method toImageRGB(Raster) for this purpose. This fixes some known performance issues with certain files.- updated Type1 Axial Radial and Gouraud shading contexts to call the new toRGB functions. This is an interim measure for better performance the color conversion should instead be done using toImageRGB after the entire gradient is drawn to the raster.- creation of AWT Paint has been moved inside color spaces hiding the details from the caller. It is no longer possible to get an AWT Color from a color space only a Paint may be obtained.- removed PDColorSpaceFactory and moved its functionality into PDColorSpace.- moved some of the new shading and tiling pattern code to PDPattern so that toPaint() is encapsulated in the color space.- new PDImage interface which is implemented by both PDInlineImage and PDImageXObject- Image XObject image reading masking  and stencilling code has been rewritten resulting in the removal of CompositeImage.- new SampledImageReader performs image reading for all formats including JPEG and CCITT. The format itself is simply a filter as is the case in the PDF spec. New image reading handles decode arrays interpolation and conversion of all image types to efficient 8bpp rasters. This replaces PDPixelMap as well as reading code from PDJpeg and PDCcitt. Handling of decod arrays fixes various issues where images were inverted especially inline images in Type 3 fonts.- removed SetNonStrokingICCBasedColor SetNonStrokingIndexed SetNonStrokingPattern SetNonStrokingSeparation SetStrokingICCBasedColor SetStrokingIndexed SetStrokingPattern SetStrokingSeparation and replaced them with SetColor.,1771
Extract Method,"Implement shading with Coons and tensor-product patch meshes Of the seven shading methods described in the PDF specification type 6 (Coons patch meshes) and type 7 (Tensor-product patch meshes) haven't been implemented. I have done type 1 4 and 5 but I don't know the math for type 6 and 7. My math days are decades away.Knowledge prerequisites: - java although you don't have to be a java ace just feel confortable- math: you should know what ""cubic BÃ©zier curves"" ""Degenerate BÃ©zier curves"" ""bilinear interpolation"" ""tensor-product"" ""affine transform matrix"" and ""Bernstein polynomials"" are or be able to learn it- maven (basic)- svn (basic)- an IDE like Netbeans or Eclipse or IntelliJ (basic)- ideally you are either a math student who likes to program or a computer science student who is specializing in graphics.A first look at PDFBOX: try the command utility here:https://pdfbox.apache.org/commandline/#pdfToImageand use your favorite PDF or the PDFs mentioned in PDFBOX-615 these have the shading types that are already implemented.Some simple source code to convert to images:String filename = ""blah.pdf"";PDDocument document = PDDocument.loadNonSeq(new File(filename) null);List<PDPage> pdPages = document.getDocumentCatalog().getAllPages();int page = 0;for (PDPage pdPage : pdPages){++page;BufferedImage bim = RenderUtil.convertToImage(pdPage BufferedImage.TYPE_BYTE_BINARY 300);ImageIO.write(bim ""png"" new File(filename+page+"".png""));}document.close();You are not starting from scratch. The implementation of type 4 and 5 shows you how to read parameters from the PDF and set the graphics. You don't have to learn the complete PDF spec only 15 pages related to the two shading types and 6 pages about shading in general. The PDF specification is here:http://www.adobe.com/devnet/pdf/pdf_reference.htmlThe tricky parts are:- decide whether a point(xy) is inside or outside a patch- decide the color of a point within the patchTo get an idea about the code look at the classes GouraudTriangle GouraudShadingContext Type4ShadingContext and Vertex herehttps://svn.apache.org/viewvc/pdfbox/trunk/pdfbox/src/main/java/org/apache/pdfbox/pdmodel/graphics/shading/or download the whole project from the repository.https://pdfbox.apache.org/downloads.html#scmIf you want to see the existing code in the debugger with a Gouraud shading try this file:http://asymptote.sourceforge.net/gallery/Gouraud.pdfTesting:I have attached several example PDFs. To see which one has which shading open them with an editor like NOTEPAD++ and search for ""/ShadingType"" (without the quotes). If your images are rendering like the example PDFs then you were successful.Optional:Review and optimize the complete shading package for speed; implement cubic spline interpolation for type 0 (sampled) functions (that one is really low-low priority see details by looking up ""cubic spline interpolation"" in the PDF spec which tells that it is disregarded in printing and I don't have a test PDF).Mentor: Tilman Hausherr (European timezone languages: german english french)",1772
Extract Method,Refactor PageDrawer operators - Clean up the code in org.apache.pdfbox.util.operator.pagedrawer.*. Formatting JavaDoc etc.- Make sure that no classes in org.apache.pdfbox.util.operator.pagedrawer.* swallow exceptions except for Invoke.- Remove setLinePath from PageDrawer which is only used for combined stroke + fill. Replace it with a new strokeAndFillPath(...) method.,1777
Extract Method,"Refactor the packages in the core pdfbox module We want to refactor the core pdfbox module packages so that there is no longer a dependency on AWT. Any packages which are moved outside the of the org.apache.pdfbox module need to be re-packaged appropriately (e.g. org.apache.pdfbox.rendering).AWT code could live in ""pdfbox-rendering"" but we need to think carefully about how to do this because e.g. some of the Filters use AWT as does FontBox.What are the use cases for modularisation currently we have:- Android- Google App EngineAndroid seems to have some support for AWT and ImageIO can somebody in the know provide more information?Google App Engine seems to blacklist ImageIO and AWT classes. Is there a strong desire to support it?Also as Fred discussed on the mailing list the ""util"" package functionality is shared across numerous parts of the code but most classes are either used only from one package or can be replaced with new Java 1.6 constructs. By the end of this refactoring the pdfbox.util package should be mostly empty containing only a handful of true utility classes.",1782
Move Method,PDFImageWriter doesn't make use of PDFStreamEngine PDFImageWriter is a subclass of PDFStreamEngine however it never uses any of its functionality the writeImage methods could be marked as static and behave in the same manner.The relationship between PDFImageWriter RenderUtil and ImageIOUtil no longer matches its historical origins and needs to be refactored.,1785
Extract Method,PDFMergerUtility support merging using non sequential parser Add support to PDFMergerUtility to merge documents using the non sequential parser (PDDocument.loadNonSeq()),1786
Rename Method,Exception Refactoring (Don't wrap Exceptions with COSVisitorException) COSVisitorException is redundant it is a simple wrapper for SignatureException CryptographyException and NoSuchAlgorithmException and should be replaced by those exceptions directly.For example we can replace:public void write(PDDocument doc) throws COSVisitorExceptionWith:public void write(PDDocument doc) throws IOException CryptographyExceptionand so on...,1787
Move Method,Support creating PDF from lossless encoded images Currently we support the insertion of TIFF and JPEG into a PDF but not PNG. We can pass a BufferedImage but this one will be JPEG compressed which is not a good thing for graphics with sharp edges. I suggest that we support PNG as well. It is possible because the Flate Filter supports both directions.My implementation (coming in a few minutes) is just an RGB based start that begs for improvement.,1791
Extract Method,PDSeparation optimization I have a 4 page black and white pdf that takes 32 seconds (8 seconds a page) to render. It uses a Separation color space and it has to run numerous functions per pixel that is causing the slow down. I have a patch where I pre calculate the black and white pixels and cache them instead of calculating them every time. This optimization gets the page rendering down to less than a second a page. I will attach my patch. I could see going forward caching all calculated colours  but floats in hash maps are tricky.,1793
Extract Method,TestFilters is non-deterministic This is a follow-up of PDFBOX-1977 which was created by John.====TestFilters uses Random().nextLong() to generate a seed for random data which means that it is non-determinate. Depending on the seed value the test may fail or succeed.====So what we need is:- a set of [deterministic tests|http://martinfowler.com/articles/nonDeterminism.html]- a set of non-deterministic testsTo see why see the discussion in PDFBOX-1977.,1794
Rename Method,Add filter parameter to PDImageXObject(document filteredStream) constructor  I am adding a third parameter to{code}public PDImageXObject(PDDocument document InputStream filteredStream){code}i.e. changing it to{code}public PDImageXObject(PDDocument document InputStream filteredStream COSBase cosFilter){code}because in the code the filter is always set afterwards. My change improves code clarity. The caller _knows_ what filter was used because he used it to prepare the filteredStream content.WDYT about also adding width height bpc and colorspace to that constructor? These four parameters are always used.This cool guy (enter his name on youtube) has arguments to use constructors parameters instead of setters:http://misko.hevery.com/2009/02/19/constructor-injection-vs-setter-injection/IMHO mixing constructor initialization and setter initialization looks confusing.,1795
Extract Method,Insert inline image in page content stream I can't find anything to insert a PDInlineImage into the page content stream. As far as I understand the spec (p. 352-355) it is rather simple i.e. just append the raw commands.,1796
Inline Method,"Improve handling and writing of header and trailer versions From [~msahyoun] in PDFBOX-1922:I'd think that instead of setting the version the current way (which is replacing the version information in the header) during the parsing the parsing should keep both version informations and getVersion within COSDocument is amended to return the correct information. What I'm suggesting is keeping both versions as this is inline with the spec and ensures that we do not override the current settings in a pdf when opening and saving out.From me:COSDocument.java has this:{code}    public void setVersion( float versionValue )    {        // update header string        if (versionValue != version)         {            headerString = headerString.replaceFirst(String.valueOf(version) String.valueOf(versionValue));        }        version = versionValue;    }{code}So it does two things set the version and set the headerString. My first Idea would be to remove the setting of the headerString because this is 1. a side effect and 2. already done elsewhere. But this was inserted in 2011 by [~lehmi] as part of PDFBOX-879 so I wonder if I will break something by removing this.from [~lehmi]:As far as I can remember it I wasn't aware of the fact that a pdf may have to versions (header + trailer). In the hindsight the change wasn't that good. I agree with Maruan Sahyoun we have to overhaul the get/setVersion methods. Both must take both possible values into account following the pdf spec.My current thought:We may need two setters. One for ""inside jobs"" (i.e. parsing) that sets each version separately. One for ""higher"" applications (e.g. merging) that sets the version in the header and in the trailer at the same time.Whatever change will be done it should not produce a regression in PDFBOX-879.",1797
Extract Method,Support for multipage TIFFs in CCITTFactory makes PDFBox capable of doing tiff2pdf I created a patch based on Sergey Ushakov's work that handles multipage TIFFs. This allows fast and efficient conversion from TIFF to PDFThe general approach is to provide a new factory method that accepts an image (page) number and then appropriate page number is located when the CCITT stream is being extracted.There's a minor inefficiency in this approach because the seek starts from the beginning for each page causing O(N^2) algorithm when extracting every page but maximum size for file appears to be 2 GB and the cost for finding a single page will still be low so I bet this will never come up in practice.There is no method that tells how many pages TIFF files have. I opted to simply return null from the factory method that accepts page number if there is no such page so users can use this as condition to break from a TIFF to PDF conversion loop.,1798
Rename Method,"Optimize clipping As already stated in a TODO comment in PageDrawer the call of Graphics2D#setClip() is time and memory consuming. The attached patch optimizes clipping by calling Graphics2D#setClip() only if the clipping path has changed. The effect depends on the document e.g. the attached one renders in 10.5s without the optimization and in 5.5 seconds in the optimized version.The clipping has to be re-applied whenever the transform in Graphics2D changes. This is not explicitly checked for the implementation rather depends on the cached value being reset manually. Currently this is only needed at one place when processing annotations (AcroForms). Also the implementation relies upon the clipping path object stored in PDGraphicsState to never change so that a comparison using == can be used. This works fine but needs a bit of awareness in future changes. To make the design more clean the clipping path could be made private to PDGraphcisState and thus really ""immutable"" from outside.",1799
Rename Method,Clean up PDFStreamEngine and PDFTextStripper PDFStreamEngine and PDFTextStripper don't really meet our coding conventions and have several unused methods and deprecated code which can safely be removed.This should clear the way to fixing some bugs in PDFStreamEngine PDFTextStripper and the various PDFont classes related to text encoding.,1800
Inline Method,Font Refactoring To fix bugs such as PDFBOX-2140 and to enable Unicode TTF embedding we need to sort out long-standing font/text encoding issues. The main issue is that encoding is done in an ad-hoc manner sometimes in the PDFont subclasses sometimes elsewhere. For example TTFGlyph2D does its own decoding and this code is copy & pasted into PDTrueTypeFont. Likewise PDFont handles CMaps and Encodings despite the fact that these two encoding methods are mutually exclusive. The end result is that the process of reading Encodings/CMaps is often following rules which are completely invalid for that font type but mostly work by luck.Phase 1- Refactor PDFont subclasses to remove setXXX methods which allow the object to be corrupted. Proper use of inheritance can remove all cases where public setXXX methods are used during font loading.- Clean up TTF loading and the loadTTF in anticipation of Unicode TTF embedding FontBox's TrueTypeFont class is externally mutable via setXXX methods used only by TTFParser: these can be made package-private.- the Encoding class and EncodingManager could do with some cleaning up prior to further refactoring.- PDSimpleFont does not do anything its functionality should be moved into its superclass PDFont.- PDFont#determineEncoding() loads CMaps when only Encodings are applicable and vice versa. Loading needs to be pushed down into the appropriate subclasses as a starting point the relevant code should at least be copied into the relevant subclasses ready for further refactoring.- TTFGlyph2D does its own decoding of char codes rather than using the font's #encode method (fair enough because #encode is broken) and there's a copy and pasted version of the same code in PDTrueTypeFont - we need to consolidate this code into PDTrueTypeFont where it belongs.Phase 2- Refactor loading of CMaps and Encodings from font dictionaries this will involve changes to PDFont and its subclasses to delegate loading to subclasses where it can be properly encapsulated- May need to alter the class hierarchy w.r.t CIDFont to facilitate this as CIDFont isn't really a PDFont - it's parent Type0 font is responsible for its CMap. We'll see.Phase 3- Refactor the decoding of character codes by PDFont and its subclasses this will involve replacing the #getCodeFromArray #encode and #encodeToCID methods.- Fix decoding of content stream character codes in PDFStreamEngine using the newly refactored PDFont and using the current font's CMap to determine the code width.,1804
Extract Method,"Improve XRef self healing mechanism PDFBOX-1769 introduced a ""self healing"" mechanism to repair corrupt XRef offsets. But that one was just a starter and there remain a lot of issues to be solved. I'm planing to solve at least some of them.All fixes and improvements are targeting the non-sequential parser and I won't port those changes to the old parser.",1806
Rename Method,"Remove usage of AWT fonts We're still using AWT fonts to render the ""standard 14"" built-in fonts which causes rendering problems and encoding issues (see  PDFBOX-2140). We're also using AWT for some fallback fonts.Removal of these AWT fonts isn't too difficult we need to load the fonts using the existing PDFFontManager mechanism which has recently been added. All missing TrueType fonts loaded from disk have been using SystemFontManager for a number of weeks now. We should ship some sensible default fonts with PDFBox such as the Liberation fonts (see PDFBOX-2169 PDFBOX-2263) in case PDFFontManager can't find anything suitable rather than falling back to the default TTF font but by default we'll probe the system for suitable fonts.",1807
Rename Method,Remove Jempbox subproject Following up PDFBOX-2107 I'm finally going to remove the Jempbox subproject.We discussed that topic several times IMHO always with the same result: discontinue Jempbox in favor of XMPBox.Those users who still prefer Jempbox might use the 1.8.x version which still should work even in combination with 2.0,1808
Extract Method,Make better use of RenderingHints PageDrawer doesn't make effective use of Java 2D's RenderingHints. The situation now is a little odd due to code having been moved around and copied and pasted. Most of the time we're not making use of higher quality renderings which are available to us.Some examples of strangeness:- drawTilingPattern sets VALUE_FRACTIONALMETRICS_ON yet this applies only to AWT fonts- drawGlyph2D sets VALUE_ANTIALIAS_ON but strokePath and fillPath set it to VALUE_ANTIALIAS_OFF- drawBufferedImage sets KEY_INTERPOLATION to VALUE_INTERPOLATION_NEAREST_NEIGHBOR which is the lowest quality image scaling method- shadingFill sets VALUE_ANTIALIAS_OFF (but that might make more sense because we're Paint-ing these ourselves OTOH if the canvas is buffered already for anti-aliasing do we save anything by disabling it?)- drawPage sets VALUE_ANTIALIAS_ON but this is always overridden by the various drawing methodsCurrently we're missing out on anti-aliasing for paths (other than glyphs) and we're getting low-quality resizing of images which makes Type 3 fonts look particularly ugly. Setting the appropriate rendering hints would improve this greatly.,1809
Rename Method,Overhaul the appearance generation for PDF forms The appearance handling for forms in 1.x is limited and does not reflect all settings possible for form fields. In addition the current code is not very modular and does not follow the box model used for form fields. Unfortunately only the basics of form handling are defined in the PDF spec. The details like padding of boxes text placement etc. have to be determined by looking at how Adobe forms are generated.Update: The file from PDFBOX-2310 has bad rendering which might be related?,1810
Extract Method,Improve high-level font APIs The PDFont and Type1Equivalent APIs could expose some higher-level details such as a consistent way to get names and Type1Equivalent instances.Some of the other font formats could also do with APIs exposing some specific useful internals such as GIDs. I'm going to add these as I find that I have a need for them during development and debugging.,1814
Extract Method,Add example code to extract embedded files in annotations Expand ExtractEmbeddedFiles.java to include embedded files in annotations (see file in PDFBOX-2993).,1823
Rename Method,Page tree handling needs rewriting The way in which PDFBox handles the Page tree needs to be rewritten preferably from scratch. Currently the document catalog returns the raw objects from the page tree wrapped in either a PDPage or PDPageNode.We need to abstract over the page tree and get rid of PDPageNode we should provide methods which can add/remove PDPage objects *only*. The existing low-level access to the page tree is not needed at the PD-level.Inheritance of page properties such as crop box resources and rotation should be reimplemented to use whatever new page tree abstraction we invent. We can finally remove the old broken methods which didn't look up the inheritance tree when retrieving these values.,1824
Inline Method,Make the non-sequential parser the default parser As proposed by Maruan on dev@ we should make the non-sequentatial parser the default parser. The different  load-methods should be simplified in that context so that all load/loadNonSeq will be replaced by a load method.,1834
Rename Method,xref stream is saved as table When saving a PDDocument PdfBox seems to always write an xref table even when the original file contains an xref stream.To reproduce load a PDF file (like the one attached) with PDDocument#load (or PDDocument#loadNonSeq same result) and then save it with PDDocument#save to another file.It seems to me that the problem is in COSWriter#doWriteXRef. When COSDocument#isXRefStream is true the xref entries should be wrapped in a stream but they're written to output one by one. I think that part should look more like its counterpart in COSWriter#doWriteXRefInc.I made some changes to doWriteXRef accordingly and it seems to work for PDFs that have never been incrementally updated but leads to corrupt files when the PDF has been incrementally updated before :(,1839
Extract Method,create TestSymmetricKeyEncryption.java similarly to org.apache.pdfbox.encryption.TestPublicKeyEncryption also test password based encryption 1) 128bit2) 256bit AES PDFBOX-1594,1840
Rename Method,Share functionality between Page Tree and Field Tree The PDFs page tree and AcroForms field tree share some common functionality e.g. resolving inheritable attributes iterating through leafs and such which could be combined into a PDTree class.,1841
Inline Method,Improve the non sequential parser to be used when signing a pdf After removing/replacing the usage of the old parser (see PDFBOX-2430) there is just one purpose left which still requires the old parser signing a pdf. We have to improve the non sequential pars so that we can finally remove the old one.,1846
Extract Method,[PATCH] Two PDFont to create PDF documents in CJK and non-ISO-8859-1 languages I made two PDFont classes for creating PDF documents in CJK and non-ISO-8859-1 languages.One is PDType0CJKFont. This is for using CJK fonts included in the Asian font package of Adobe Reader. This font doesn't require the target font at the time of creating PDF documentary. This font uses UTF-16 as a text code and supports surrogate pair characters.The other is PDType0UnicodeFont. This is for using TrueType Type0 Font which can deal with any Unicode characters like a ArialUnicodeMS. Only the characters which are used actually in the document are embedde. Realizing this you have to call the PDType0Unicode.reloadFont() method just before closing PDPageContentStream. I think this specification is ugly but I could not thought of a suitable way to remove this spec. This font uses the original glyph code of the embedded font as a text code and supports surrogate pair characters too.Example programs using these two fonts are also attached.,1851
Extract Method,"Improve PDFDebugger (This is an idea for the [Google Summer of Code 2015|https://www.google-melange.com/])Our command line utility PDFDebugger (part of the command line pdfbox-app get it [here|https://pdfbox.apache.org/downloads.html] read description [here|https://pdfbox.apache.org/commandline/] see the source code [here|https://svn.apache.org/viewvc/pdfbox/trunk/tools/src/main/java/org/apache/pdfbox/tools/PDFDebugger.java?view=markup&sortby=date]) needs some improvements:   - hex view   - view of non printable characters   - âœ“ saving streams   - binary copy & paste   - âœ“ Create a status line that shows where we are in the tree. (Like in the Windows REGEDIT)   - âœ“ Copy the current tree string into the clipboard (useful in discussions about details of a PDF)   - âœ“ (Optional not sure if easy) Jump to specific place in the tree by entering tree string   - âœ“ ability to search in streams (very useful for content streams and meta data)   - âœ“ show images that are streams   - âœ“ show PDIndexed color lookup table show the index value the base and RGB color value sets when the mouse moves   - âœ“ show PDSeparation color   - âœ“ show PDDeviceN colors   - optional idea should be developed a bit: show meaningful explanation on some attributes e.g. ""appearance stream"" when hovering over /AP   - show font encodings and characters   - âœ“ display flag bits (e.g. Annotation flags) in a way that is easy to understand. There are probably others I assume that the main work needs to be done only once   - edit attributes (should be possible to enter values as decimal hex or binary)   - edit streams while keeping or changing the compression filter   - save altered PDF    - âœ“ color mark of certain PDF operators especially Q...q and text operators (BT...ET). Ideally it should help the user understand the ""bracketing"" of these operators i.e. understand where a sequence starts and where it ends. (See ""operator summary"" in the PDF Spec) Other ""important"" operators I can think of are the matrix font and color operators. A cool advanced thing would be to show the current color or the font in a popup when hovering above such an operator.To see a product with a similar purpose that is better than PDFDebugger watch [this video|https://www.youtube.com/watch?v=g-QcU9B4qMc].I'm not asking to implement a clone of that product (I don't use it all I know is that video) but we at PDFBox really need something that makes PDF debugging easier. As an example of how the current PDFDebugger prevented me from finding a bug quickly see PDFBOX-2401 and search for ""PDFDebugger"".Prerequisites:- java programming especially the GUI components- the ability to understand existing source codeUsing external software components is possible (must have Apache License or a compatible one) but should be decided on a case-by-case basis we don't want to get too big.Development strategy: go from the easy to the difficult. The wished features are already sorted this way (mostly).Get introduced: [download the source code with svn|https://pdfbox.apache.org/downloads.html#scm] and build it with maven. Run PDFDebugger and view some PDFs to see the components of a PDF. Start with the file of PDFBOX-2401. Read up something about the structure of PDF on the web or from the [PDF Specification|https://www.adobe.com/devnet/pdf/pdf_reference.html].Mentor: Tilman Hausherr (European timezone languages: german english french). To see the GSoC2014 project I mentored go to PDFBOX-1915.",1852
Extract Method,Subset embedded TTF fonts Now that PDFBOX-922 is fixed we have working TTF embedding. However the entire font is embedded which is rather large. We already have a TTFSubsetter class in FontBox which is never used so we should make use of it.,1864
Extract Method,Remove logging from operator classes I've been trying to get better control over the logging that occurs in the operator classes but it's not easy to do so. Ideally all logging could pass through PDFStreamEngine so that subclasses can more easily filter what they want. By using exceptions instead of logging for error cases we allow more fine-grained management of exceptions. For example in my own code I wish for a missing XObject to be a terminal failure but it currently just results in a log message and I have to implement a custom DrawObject operator and copy & paste over most of the code in order to catch this exception rather than logging it.I'm therefore going to move what little logging there is in the Operator classes over to PDFStreamEngine and to throw custom exceptions (e.g. MissingResourceException) rather than writing silently to the log. The default implementation of processOperator in PDFStreamEngine will then catch these custom exceptions and simply write them to the log - keeping the current PDFBox behaviour unchanged. Only now consumers of PDFStreamEngine can override processOperator and do their own exception handling e.g. I can choose to propagate MissingResourceException exception instead of logging it.This might be useful for Preflight too as it often wants to throw errors where we would otherwise skip the offending object and just keep processing. Also it probably means that there will no longer be a need for to Preflight implement its own operators in those cases where it was done just to get stricter error handing.,1865
Rename Method,Improve PDPageContentStream API The PDPageContentStream API uses some methods with incorrect and misleading names and some unusual choices of parameters. These can be fairly easily cleaned up.,1874
Rename Method,"Allow sharing of COS objects between different documents A number of users on the mailing list have asked about how to import pages from other PDFs as forms our current solution is LayerUtility which is depends on PDFCloneUtility. Both these classes are surprisingly complex for what should be a simple task.The two main tasks which these classes perform is copying the page's COSStream and cloning every relevant COS object. However there seems to be no real need to do any of this copying and cloning - there's nothing about any of the COS objects which is specific to a given document. While a COSStream can share the same backing file as the COSDocument this isn't a problem for COSWriter even then we need only make sure that an exception is thrown if a COSStream is used after its parent COSDocument is closed.Note that there *is* one artificial dependency between COSDictionary and COSArrays and their parent COSDocument that is that calling close() on the COSDocument clears the contents of all child COSDictionary and COSArrays. However there's no need for this it seems to have come about due to some long past confusion regarding how garbage collection works in Java - we all know that it's not necessary to set objects to null or clear lists when we are done with them.I propose that we get rid of the unnecessary object and list clearing in COSDocument#close() and add some checks to COSStream to throw user-friendly exceptions when reading from a closed backing stream. This will allow us to directly share COS objects between different COSDocuments allowing simple ""x = y"" copying and making LayerUtility and PDFCloneUtility unnecessary. Instead of:{code}COSStream pageStream = (COSStream)page.getStream().getCOSObject();PDStream newStream = new PDStream(targetDoc pageStream.getUnfilteredStream() false);PDFormXObject form = new PDFormXObject(newStream);PDResources pageRes = page.getResources();PDResources formRes = new PDResources();PDFCloneUtility cloner = new PDFCloneUtility(document);cloner.cloneMerge(pageRes formRes);form.setResources(formRes);{code}We could have:{code}PDFormXObject form = new PDFormXObject(page.getStream());form.setResources(page.getResources());{code}",1882
Inline Method,Remove old parser After making the non-sequential parser the default parser (PDFBOX-2430) and enabling signing with the non-sequential parser it is time to remove the old one.,1883
Extract Method,Remove SignatureInterface dependency from COSDocument COSDocument holds a reference of the SignatureInterface for signing. This should be moved somewhere to the pdmodel package maybe into PDDocument.,1889
Extract Method,Possibility to use our own and/or overwrite PageDrawer class We use PDFBox to render PDF's. Additionally we have the posibility to add different kinds of annotation (stamp marks free text notes..) like in a wysiwyg-editor. To do this it is necessary that we paint these annotations on our own.Another reason is not to paint all parts: for example we have a pdf with an embedded picture. Behind the picture we have the OCR-text to this picture. This text is only needed for searching und should not be painted.Thus it would be useful to use our own derived PageDrawer. As I see there are some things to change.a.) remove the final from PagerDrawer-class.b.) make some global-variables (graphics xform pageSize...) protectedc.) also some methods like setRenderingHints should be protectedd.) maybe the possibility to say to the PDFRender which PageDrawer should be used.,1890
Extract Method,Make it easier to work with RadioButton Groups The current implementation of Radio Buttons in PDFBox 2.0 should be improved by- renaming getOptions() to make it clearer that this gets the potential export values (although the dictionary entry is called /Opt)- make it easier to inspect the possible values one can set for the various individual buttons,1892
Extract Method,"Overhaul font substitution The improved font substitution mechanisms in 2.0 are not quite sufficient to handle all PDFs. Specifically CJK substitution and substitution of TTF in place of CFF fonts is not possible with the current design.The CJK problems can be seen in PDFBOX-2509 and PDFBOX-2563 which does not solve the problem. Additional font API weaknesses can be found in PDFBOX-2578 and PDFBOX-2366. This meta-issue aims to address all of those sub-issues.The current problems are:- FontBox does not provide a generic font type so we have handle TrueTypeFont CFFFont and Type1Font separately. This hinders cross-format substitution.- ExternalFonts has no knowledge of the CIDSystemInfo which is necessary for CJK substitution- FontProvider contains too much public logic which should be internal to PDFBox e.g. substitution logic this makes it brittle and means we won't be able to add additional logic after 2.0 is released e.g. CJK substitution.- Too much confusion about the role of ExternalFonts particularly with regards to mapping of built-in fonts and the definition of substitute vs. fallback font.- ExternalFonts is a black box: the user cannot tell whether the font returned is an exact match or a last-resort fallback.- Confusing font substitution API users preferred having a flat file format- PDSimpleFont#getEncoding() can return null for TTFs which use built-in encodings. This has caused a lot of bugs - there must be a better way.- We still have some confusing names for example a CustomEncoding is known as a ""built-in encoding"" in the spec.- There is no fallback CFF font we resort to AdobeBlank instead which has no rendering.",1893
Rename Method,Align annotation and form public API The public API for annotation and form differs in - visibility for flag fields- method naming conventions,1895
Inline Method,Improve performance when using scratch file The current scratch file implementation uses many direct I/O calls which slows down parsing compared with in-memory scratch buffer considerably.,1896
Rename Method,"Remove COSStreamArray / SequenceRandomAccessRead This ties in with my COSStream simplification in PDFBOX-2893.COSStreamArray is a troublesome abstraction it's not a real COS object and it's the only COS object which can be generated _after_ parsing. Look at the implementation of COSStreamArray most methods throw an exception because it's _not_ a COSStream - it violates the contact of the very thing it claims to be. Even PDPageContentStream has to use instanceof to ""peer through""  the abstraction of COSStreamArray.There's no reason to have this class other than to duck-tape flaws in 1.8's APIs namely that PDPage#getStream() returns a PDStream and PDFStreamParser expects a PDStream yet both of these may be arrays of streams.We can fix this in 2.0 by getting rid of the erroneous PDPage#getStream() and by exposing the array of streams rather than attempting to hide them. Hopefully this will also fix existing errors which may be lurking throughout the codebase (see first comment below) which are associated with mistaking COSStreamArray for a COSStream. We can still provide an InputStream API which abstracts over the array of streams because there's nothing wrong with that - so users can have the same simple and convenient experience.An added benefit of doing this is that it will allow us to remove SequenceRandomAccessRead a highly complex memory-holding class.",1898
Extract Method,"Replace PDFReader with PDFDebugger As discussed on the mailing list:{quote}Here's an idea: if we switch PDFDebugger to using ""View Pages"" by default it will no longer be confusing for casual users. I've found myself using this mode most of the time anyway. We can add page up/down too of course - preferably using the actual ""Page Up"" and ""Page Down"" keys rather than the bizarre choice of the +/- keys which are currently used in PDFReader.{quote}",1899
Extract Method,PDType3Font.getWidthFromFont not supported This method is still not implemented.Does anyone working on this issue? If not I will be happy to contribute and propose implementation. In fact it's not a very complicated task : you just need to parse d0 or d1 operator from glyph's content stream (stored in CharProcs).,1908
Extract Method,Add capability to flatten AcroForm form fields There should be a capability to flatten AcroForm form fields. ,1909
Extract Method,Improve font handling (was: layout print problem) [imported from SourceForge]http://sourceforge.net/tracker/index.php?group_id=78314&atid=552832&aid=1787501Originally submitted by gjniewenhuijse on 2007-09-04 00:24.When i print the attached file some things are not printed well.- The gray box at the top- and the fonts are printed bold and thats not right.Is there any solution for now or for later? When i open and print this file with adobe reader everything is fine but with pdfbox i've got a layout problem.I used the newest pdfbox version (also tested the nightly build)[attachment on SourceForge]http://sourceforge.net/tracker/download.php?group_id=78314&atid=552832&aid=1787501&file_id=244104orarrp.pdf (application/pdf) 7871 bytespdf with print problem,1911
Extract Method,Right now PDFBOX does not permit to sign multiple files while calling an external signing service. Since to sign a PDF you forced the implementation of the SignatureInterface interface is not possible to prepare N hashes from N PDF files and then send them to a signing service that accepts multiple hashes with a single signon.For example if I use an OTP signing service.What would be nice to have is to separate the hash calculation from the signing. Instead to implement the Interface I would like to have something like this:1) calculate hash from document with the new signature dictionary bytes2) sign the hash3) insert the signature into pdfThis way I could achieve to sign for example 100 pdf files calling the service once.Right now must ask the user to sign in 100 times.Thanks in advance.Andrea. ,1912
Extract Method,Allow missing page type I came across a PDF document with missing type in the page dictionary. According to the spec that's required and PDFBox has a check and throws an IllegalState if the page is requested to the PDPageTree. Acrobat and other libs handle the same doc so I think the constraint could be relaxed a little consider valid if type is Page or missing and consider invalid if it's something else what do you think?,1915
Extract Method,Fix high memory usage during signing Hello. We have a requirement to be able to sign huge pdf files consisting entirely from paper scans. Unfortunately current implementation unnecessary buffers entire pdf contents during signing procedure. This patch suggests a fix.We'll be happy to adjust it if needed and to see it merged. ,1916
Inline Method,Reduce amount of intermediate data and objects to reduce memory footprint/complexity The CFFParser holds a lot of intermediate data and produces a lot of objects to do so. The idea is to reduce the amount of such objects and data to reduce the memory footprint and the complexity.- the class IndexData holds intermediate data creates byte array everytime when getBytes is called. I'm going to replace the class with a simple list to reduce the memory footprint and the complexity- remove unused members of private classes- create a list of strings instead of a list of byte arrays which is used to create those strings,1917
Extract Method,Add constructor with BufferedImage to PDVisibleSignDesigner In 1.8 the PDVisibleSignDesigner constructor took a JPEG stream and used that one directly to create a PDJpeg. In 2.0  the stream is read into a BufferedImage so the opportunity to create a JPEG directly is lost. But this doesn't matter as a JPEG is not the best choice for images with sharp edges (a signature scan or a text printed into an image). So I will- replace the JPEGFactory with LosslessFactoory- add constructor that takes a BufferedImage,1921
Rename Method,PDImageXObject.createFromFile should relies on header bytes PDImageXObject.createFromFile currently relies on file extension to select the correct factory.Often file extension is not set or not correct.It should be better to use the first bytes.I did something similar here if it can helps: https://github.com/sismics/docs/blob/master/docs-core/src/main/java/com/sismics/util/mime/MimeTypeUtil.java#L26,1922
Rename Method,Rename structure element setter of PDOutlineItem While playing around with the library I stumbled over a naming inconsistency between the getter and setter of a {{PDOutlineItem}} structure element.The getter is named {{getStructureElement}} whereas the setter is named {{setStructuredElement}} which is a bit confusing.,1923
Extract Method,Cache Font Bounding Boxes for Performance in Text Extraction HiI have been using pdfbox by way of Tika for a while for text extraction from PDFs.  I had a chance to fire up a profiler recently and found that getBoundingBox() in the PDXXFont.java classes are called fairly frequently -- in particular from PDFTextStreamEngine.showGlyph().  I've attached a patch that caches the BoundingBox object alongside the PDFont object inside of PDTextState.  There are a variety of other ways to accomplish the same thing -- caching inside of the various font objects themselves etc.I wrote a little test program to measure the speed difference against a few randomly selected files.  The program just uses PDFTextStripper to retrieve raw text from a PDF.Here's what I found:====plain====File: BambooCheatSheet.pdf Duration: 60037555619 rate: 81.6 files/secFile: flu.pdf Duration: 60019978409 rate: 34.46666666666667 files/secFile: megacli_user_guide.pdf Duration: 60641314800 rate: 1.1833333333333333 files/secFile: odbc-perl.pdf Duration: 60008216404 rate: 19.466666666666665 files/secFile: VerticaArchitectureWhitePaper.pdf Duration: 60084726865 rate: 7.433333333333334 files/secFile: WritingaResume.pdf Duration: 60015267784 rate: 59.4 files/sec===boundingbox caching===File: BambooCheatSheet.pdf Duration: 60005724588 rate: 106.1 files/secFile: flu.pdf Duration: 60021410660 rate: 41.916666666666664 files/secFile: megacli_user_guide.pdf Duration: 60107488363 rate: 1.7833333333333334 files/secFile: odbc-perl.pdf Duration: 60017784515 rate: 29.9 files/secFile: VerticaArchitectureWhitePaper.pdf Duration: 60012261509 rate: 9.05 files/secFile: WritingaResume.pdf Duration: 60007995996 rate: 76.5 files/secCheers,1924
Extract Method,pdf creation very slow compared to older 1.8.x versions the new 2.0.x branch is awesomely slow.benchmarks using a multipage document with few images and many text-lines indiciate a performance penalty of about 1:20 compared with the old 1.8.x branch.profiling via VisualVM indicates that the new font handling causes this performance drawback:TrueTypeFont.nameToGid() [31%]TrueTypeFont.hasGlyph() [23%]PDFont.getWidth() [16%]PDType1Font.encode() [9%]is there any workaround for this? the current setup only creates about 10 PDFs/second compared to over 200/second for the 1.8.x branch...,1926
Extract Method,PDButton.getOnValues seems to be using the wrong source for getting the allowed values We are in the process of migrating from itext to pdfbox and noticed that an exception was being thrown when trying to set a radio button with PDField.setValue().  The values returned from PDButton.getOnValues() which is used by PDButton.checkValue() called from PDButton.setValue() are only returning 01 instead of what we had been using with itext.  After investigating itext's source code and playing around with pdfbox I was able to get to what I believe are the appropriate allowed values by using the following code.{code}        if (field instanceof PDButton)        {            final COSBase item = field.getCOSObject().getItem(COSName.OPT);            if (item != null && item instanceof COSArray)            {                final COSArray optArray = (COSArray)item;                for (int i = 0 ; i < optArray.size() ; i++)                {                    // Each item in this array is an allowed value                    // optArray.getString(i);                }            }        }{code},1927
Extract Method,Optimize CID to GlyphId mapping (TTF) TTF fonts map code-points (Code IDs) to glyphs. These are mappings from int to int. Because the JDK lacks map classes for primitive types the code (e.g. in CmapSubtable) currently uses Map<IntegerInteger> for those mappings. This is inefficient in different ways:* Autoboxing/unboxing introduces a performance penalty* Boxing to Integer objects has a memory overhead* The JDK Map implementation has a big memory overhead for such simple objectsFor efficiency (execution time and memory consumption) I would propose to introduce a simple IntIntMap implementation which works with primitive integers.,1928
Extract Method,"[Patch] Improved signing of existing signature fields *Short*: The handling of signing existing signature fields must be improved (and this patch is part of that effort).*Details and background*The current implementation for visible signatures always adds new signature fields when signing documents.In that case for that signature everything has to be definied (field properties  coordinates etc.).Another quite common use case is the use of an existing signature field which should be signed. There are basically two different roles: The *document creator* who creates the document with all its texts graphics and form fields. The creator knows best where everything should be positioned and is even sometimes bound by certain regulations etc. The document creator defines his intend with the ""Usage rights"" and may add a usage right signature.Then later a *document user* e.g. a customer fills out form fields and signs those predefined signature fields. In that case the coordinates and a lot of attributes are alrady defined and there is no need (and sometimes it is even forbidden) to change the physical attributes of those signature fields. The only two things which are of interest is to set the signature dictionary and to recreate the appearance.In the current implementation however one needs to define the coordinates of an existing signature field again. But not enough since the screen coordinates in java (and in the PDFBox PDVisibleSigBuilder) and PDF coordinates have a different origin one must convert those existing PDF coordinates to screen coordinates which are later converted to PDF coordinates again. This is cumbersome error prone and totally unecessary... With the supplied patch there is no conversion of coordinates anymore.",1930
Extract Method,Read images from byte array When we create several PDF files with the same images a lot of time spend to read image's files. I added possibility to create image from byte array which user can keep in memory.,1931
Rename Method,Add COSBoolean(false) as option in PDDocumentCatalog's getOpenAction Over on Tika we've started allowing users to extract PDActions.  In our recent regression tests we found a few new exceptions while trying to get the OpenAction from PDDocumentCatalog.The easy one to fix is: {noformat}java.io.IOException: Unknown OpenAction falseat org.apache.pdfbox.pdmodel.PDDocumentCatalog.getOpenAction(PDDocumentCatalog.java:261)at org.apache.tika.parser.pdf.AbstractPDF2XHTML.startDocument(AbstractPDF2XHTML.java:460){noformat}The object is a COSBoolean with value=false.I'll open a separate issue for the other new (to us) exception.,1932
Rename Method,Improve and refactor RemoveAllText example Refactor double code and include patterns not just xobject forms when going through resources. This will be a better template for utilities that work on the content stream tokens e.g.https://stackoverflow.com/a/45259160/535646,1933
Rename Method,Add Certificate Dictionary to seed value in signature field This dictionary is important as it gives the ability to put certificate constraints on a signature field like if you want signatures that are signed by a specific issuer or authority to only be used in a field.currently tested Issuer constraint and it worked acrobat reader ignores other certificates and only allow the issuer given to sign the field. documentation is not complete waiting for the initial acceptance to complete.new class PDSeedValueCertificate is added which refers to this certificate.PDSeedValue is modified to add the new dictionary.COSName is modified to add the new pdf names that are included in the dictionary.reference for this dictionary can be found in PDF reference 1.7 section 12.7.4.5 table 235 page 457 in here http://www.adobe.com/content/dam/acom/en/devnet/pdf/PDF32000_2008.pdf or chapter 8 table 8.84 page 700 in here http://archimedespalimpsest.net/Documents/External/pdf_reference_1-7.pdfand in herehttps://www.adobe.com/devnet-docs/acrobatetk/tools/DigSig/Acrobat_DigitalSignatures_in_PDF.pdfthis is my first contribution hope everything goes well.,1934
Extract Method,Add validation data of signer to document To support Long Term Validation of a signature we need to add a Valdiation-Dictionary to the document. Inside there is most importantly an OCSP-Response of the signers. (can be multiple).As Defined in [PAdES 4|https://en.wikipedia.org/wiki/PAdES] the following elements will be added to a document: A DSS (Document Security Store) linked to the VRI(s) of the signature(s)At first I will provide an example.,1935
Extract Method,Add support for a flag disabling the rendering of PDF annotations in PDFRenderer Regardless if annotations are supposed to be printed or not on the PDF would it not be interesting to possess a flag allowing to choose if annotations should be printed on top of the document pages?Â Here is a diff of a very rough implementation of it :Â Â {code:java}diff --git a/pdfbox/src/main/java/org/apache/pdfbox/rendering/PDFRenderer.java b/pdfbox/src/main/java/org/apache/pdfbox/rendering/PDFRenderer.java--- a/pdfbox/src/main/java/org/apache/pdfbox/rendering/PDFRenderer.java+++ b/pdfbox/src/main/java/org/apache/pdfbox/rendering/PDFRenderer.java@@ -356 +359 @@ public class PDFRenderer     protected final PDDocument document;     // TODO keep rendering state such as caches here +    // parameter used to know if the rendering should include annotations +    private boolean renderAnnotations = true;+     /**      * Creates a new PDFRenderer.      * @param document the document to render@@ -2244 +22714 @@ public class PDFRenderer     {         return new PageDrawer(parameters);     }++    public void setRenderAnnotations(boolean render) +    {+        this.renderAnnotations = render;+    }++    public boolean renderAnnotations() +    {+        return renderAnnotations;+    } }diff --git a/pdfbox/src/main/java/org/apache/pdfbox/rendering/PageDrawer.java b/pdfbox/src/main/java/org/apache/pdfbox/rendering/PageDrawer.java--- a/pdfbox/src/main/java/org/apache/pdfbox/rendering/PageDrawer.java+++ b/pdfbox/src/main/java/org/apache/pdfbox/rendering/PageDrawer.java@@ -19511 +19513 @@ public class PageDrawer extends PDFGraphicsStreamEngine          processPage(getPage()); -        for (PDAnnotation annotation : getPage().getAnnotations())-        {-            showAnnotation(annotation);+        if (getRenderer().renderAnnotations()) +        { +            for (PDAnnotation annotation : getPage().getAnnotations())+            {+                showAnnotation(annotation);+            }         }-         graphics = null;     } {code}And an exemple of a use case:Â {code:java}PDDocument doc = getPDDocument(); PDFRenderer pdfRenderer = new PDFRenderer(doc);pdfRenderer.setRenderAnnotations(false);pdfRenderer.renderImage(page);Â {code}By default this would be keeping the same behavior as it used to (aka : print the annotations) but possess an opt-out feature.Â Best regardsM.Veron,1936
Rename Method,Vertical text creation I needed to output vertical Japanese text but was stymied by several limitations:* No API to load a TTF as Identity-V encoding* No support for 'vert' glyph substitution* No support for vertical metrics ('vhea' and 'vmtx' tables are parsed but not used at all)I have attached a series of patches that implement the above features. Highlights:* The GSUB glyph substitution table is parsed (limitation: type 1 lookups only; this is sufficient for many features including 'vert'/'vrt2' vertical glyph substitution)* Cmap lookup makes use of GSUB when features are enabled on a TTF* 'vhea' and 'vmtx' metrics are applied to PDCIDFont when appropriate and are embedded/subsetted correctly through the DW2/W2 CIDFont dictionary* An API has been added for loading a TTF as a vertical font setting Identity-V encoding and enabling 'vert'/'vrt2' substitutionEach patch could approximately be split out into a separate ticket if desired.Also attached is some sample code that exercises these patches and illustrates the effect of vertical glyph positioning. The sample output PDF is also attached.,1937
Extract Method,"Allow subsampled/downscaled rendering of images and rendering subimages  Suggested/contributed change to allow subsampling of images and rendering sub-regions of images.Â Â The need arises from having very large images which are highly compressed (usually JPEG or JBIG2). The current implementation decodes the entire image into memory at full resolution even if rendering is done at a much lower resolution.Â Since the change required augmenting the way Filters work (to allow partial/subsampled decoding) it also includes a partial fix forÂ PDFBOX-3340.Â Â Â This change introduces ""DecodeOptions"" which are currently only applicable for images. They include requesting only metadata (for PDImageXObject's repair method) subsampling and sub-region (similar to javax.imagio.ImageReadParam).Â Since not all filters can or do honor (use) the options the DecodeOptions class contains a flag. Filters that honor the options (subsample / decode only requested region) set it to true. If the flag is false the subsampling or cropping should be done after decoding to ensure consistency.Â PageDrawer was modified so it uses subsampling based on the ratio of the desired output to the original image.Â Â ",1938
Extract Method,Overlay class should allow user to influence transform As suggested by [~mkl] in a comment of the linked SO issue offer the user a way to do the calculation of the overlay position.,1940
Extract Method,[PATCH]: Support simple lossless compression of 16 bit RGB images The attached patch add support to write 16 bit per component images correctly. I've integrated a test for this here: [https://github.com/rototor/pdfbox-graphics2d/commit/8bf089cb74945bd4f0f15054754f51dd5b361fe9]It only supports 16-Bit TYPE_CUSTOM with DataType == USHORT images - but this is what you usually get when you read a 16 bitÂ PNGÂ file.ThisÂ would also fix [https://github.com/danfickle/openhtmltopdf/issues/173].The patch is against 2.0.9 but should apply to 3.0.0 too.There is still some room for improvements when writing lossless images as the images are currently notÂ efficiently encoded. I.e. you could use PNG encodings to get a better compression. (By adding a COSName.DECODE_PARMS with aÂ COSName.PREDICTOR == 15 andÂ encoding the images as PNG). But this is something for a later patch. It would also need another API as there is a tradeoff speed vs compression ratio.Â ,1941
Extract Method,Add quality option for compressed images to pdfbox-app Add commandline optionÂ *quality* option for compressed images to pdfbox-appex: -quality 0.75Â see [^pdfbox-tool.patch]Â ,1942
Extract Method,Refactor LosslessFactory alpha While looking into the code for PDFBOX-4184 I noticed that we try to get the alpha data in different ways despite that it is available in the main method when {{image.getRGB()}} is called. So I'm refactoring all this; as a side effect my 16 bit change in PDFBOX-4184 is no longer needed.I'll commit this in two steps; 1) changing the main method and remove the ones that are no longer being used; 2) split the main method in a gray and a color code.,1943
Extract Method,"Expose the tiff compression type to the user. Allow the user to set the compression type of Tiff files externally.In the current version the class ImageIOUtil uses TIFFUtil.setCompressionType and this sets the compression only to ""CCITT T.6"" or ""LZW"". Other choice could be allow the jpeg compression that is more efficient.It already has a TODO in code (// TODO expose this choice to the user?)Thank you.",1945
Rename Method,Improve html output Would like to improve the html output of pdf files for arabic rendering. The attached file has changes that should improve the way the -html option works. Now output files are tagged with the .html extension. We also added <DOCTYPE> information as well as a <meta> tag which writes the appropriate encoding of the file. Cleaned up a lot of code from PDFTextStripper and PDFText2HTML which wasn't being used. Added ability to set the <title> tag of the html document to be the title given in the pdf document information if it exists. Otherwise it will guess a title from the beginning first lines of the file. ,1946
Rename Method,Prepare PDFBox for release To prepare PDFBox for release we have to check on the build-process (directory structure targets etc.) and to check that everything is according to the Apache policies.,1947
Extract Method,Support ConflictsList for XRef Streams in pdf 1.5 or greater The fix for PDFBOX-183 which added a conflictList which keeps track of duplicate objects in a pdf file is insufficient for pdf files that do not include an xref table but rather have xref streams. ,1949
Extract Method,Show single pages instead of all The PDFReader always shows all pages of a document. It would be better if it shows one after the other.,1950
Extract Method,PDFBox performance issue:  PDSimpleFont PDFont performance tweaks During text extraction font size / descriptor / encoding attributes are accessed repeatedly in order to do positional calculations and byte-character conversions.The current code has several accessors for these things that redo rather slow calculations each time - even thought the font object state is not changed.The results of these calculations should be persisted in instance fields once calculated.  This greatly improves performance.I'll attach new versions of PDFont PDFontDescriptorDictionary and PDSimpleFont that have these tweaks.,1951
Extract Method,"shfill operator needs implementation I have a PDF file (for which I do not yet have release permission) that uses the ""sh"" operator equivalent to PostScript's shfill (per PDF spec 1.7 page 987).Adobe provides implementation guidance in a 78-page document at http://www.adobe.com/devnet/postscript/pdfs/TN5600.SmoothShading.pdf#17I will be trying to add this functionality this week but if anyone has hints suggestions etc. they are most certainly welcome!",1952
Rename Method,Fallback mechanism for broken CFF fonts PDFBOX-542 has not proven to be sufficiently foolproof for real-world PDF documents.PDFBox should fallback to a Type1 font with appropriate warning message (this is the default behaviour with PDFBox 0.8.0 and earlier versions) when there is a problem parsing/interpreting a CFF (aka Type1C) font.,1953
Extract Method,Ensuring non-null FontDescriptor for external TrueType fonts Class PDTrueTypeFont assumes that there is always a non-null FontDescriptor(Dictionary) available. However I've seen this assumption failing with a NullPointerException in method PDTrueTypeFont#drawString when trying to render PDF documents that make use of external TrueType fonts.I've implemented a small patch that initializes empty FontDescriptorDictionary if one is missing and tries to fill it in with information available from the external TTF resource.,1954
Extract Method,CFFFont - Management of CIDKeyed HiI'm currently using FontBox 1.0.0.I need to manage CIDKeyed Font but this feature isn't yet managed by the CFFParser.In attachment  you can find patches to parse the CIDKeyed font.I hope this contribution will help you.RegardsEric,1955
Extract Method,Adding method to manipulate the current transformation matrix Similar to the setTextMatrix and other we need a method to manipulate the current transformation matrix within PDPageContentStream,1957
Extract Method,Add the current page and the number of pages to the title You can flip through the pages of a pdf document using the PDFReader but you can't see the current page and the total number of pages of the pdf.,1958
Extract Method,Add support to draw or fill a polygon Add support to draw or fill a polygon.,1959
Extract Method,PDFTextStripper: allow access to currentPageNo variable I've extended org.apache.pdfbox.util.PDFTextStripper and I'm using it to perform a 2-pass extraction over a document. However the second pass doesnt happen because I am unable to alter the variable currentPageNo which maintains the current page number in the pdf document. It is a variable with access modifier of private and only a get method is provided.The only time currentPageNo is set to 0 is via 'writePage(PDDocument OutputStream)' which I am overriding/not calling.2 possible resolutions:- make currentPageNo protected instead of private (preferred)- add setCurrentPageNo methodThank youRyan,1961
Extract Method,"Loading TTF font files from the classpath Currently (pdfbox 1.1.0 & 1.2.0 snapshot) TTF font files can only be loaded from the file system as PDTrueTypeFont exposes two load methods:    public static PDTrueTypeFont loadTTF(PDDocument String)    public static PDTrueTypeFont loadTTF(PDDocument File)The first wraps the String in a java.io.File object and delegates to the second so all TTF reading is from the file system.It would be useful to be able to read TTF files from the classpath - or indeed from any arbitrary stream.Could we have a third method?:    public static PDTrueTypeFont loadTTF(PDDocument InputStream)This would allow TTFs to be loaded like so:PDTrueTypeFont.load(myDoc MyClass.class.getClassLoader().getResourceAsStream(""myFont.ttf""));For what it's worth here's a patch of what I did (see below).One uncertainty I had was about the use of the COSName.LENGTH1 field which only seems to be used currently in PDTrueTypeFont. So it may be reasonable or not to push the setting of this attribute into the PDStream constructor.Index: pdfbox/src/main/java/org/apache/pdfbox/pdmodel/common/PDStream.java===================================================================--- pdfbox/src/main/java/org/apache/pdfbox/pdmodel/common/PDStream.java(revision 948363)+++ pdfbox/src/main/java/org/apache/pdfbox/pdmodel/common/PDStream.java(working copy)@@ -11412 +11417 @@             {                 output = stream.createUnfilteredStream();             }+            int bytesInInputStream = 0;             byte[] buffer = new byte[ 1024 ];             int amountRead = -1;             while( (amountRead = str.read(buffer)) != -1 )             {                 output.write( buffer 0 amountRead );+                bytesInInputStream += amountRead;             }+            +            // Set the number of bytes read from the input stream+            this.stream.setInt( COSName.LENGTH1 bytesInInputStream );         }         finally         {Index: pdfbox/src/main/java/org/apache/pdfbox/pdmodel/font/PDTrueTypeFont.java===================================================================--- pdfbox/src/main/java/org/apache/pdfbox/pdmodel/font/PDTrueTypeFont.java(revision 948363)+++ pdfbox/src/main/java/org/apache/pdfbox/pdmodel/font/PDTrueTypeFont.java(working copy)@@ -1307 +1307 @@     }      /**-     * This will load a TTF to be embedding into a document.+     * This will load a TTF to be embedded into a document.      *      * @param doc The PDF document that will hold the embedded font.      * @param file A TTF file stream.@@ -13921 +13932 @@      */     public static PDTrueTypeFont loadTTF( PDDocument doc File file ) throws IOException     {+        return loadTTF( doc new FileInputStream( file ) );+    }++    /**+     * This will load a TTF to be embedded into a document.+     *+     * @param doc The PDF document that will hold the embedded font.+     * @param stream A TTF input stream.+     * @return A PDF TTF.+     * @throws IOException If there is an error loading the data.+     */+    public static PDTrueTypeFont loadTTF( PDDocument doc InputStream stream ) throws IOException+    {         PDTrueTypeFont retval = new PDTrueTypeFont();         PDFontDescriptorDictionary fd = new PDFontDescriptorDictionary();-        PDStream fontStream = new PDStream(doc new FileInputStream( file ) false );-        fontStream.getStream().setInt( COSName.LENGTH1 (int)file.length() );+        PDStream fontStream = new PDStream(doc stream false );         fontStream.addCompression();         fd.setFontFile2( fontStream );         retval.setFontDescriptor( fd );-        InputStream ttfData = new FileInputStream(file);         try         {-            loadDescriptorDictionary(retval fd ttfData);+            loadDescriptorDictionary(retval fd stream);         }         finally         {-            ttfData.close();+            stream.close();         }         //only support winansi encoding right now should really         //just use Identity-H with unicode mapping",1962
Rename Method,Implementation of function types 02 and 3 to be used in Separation and DeviceN colorspaces I'm working on an implementation of the function types 02 and 3 which are used within Separation and DeviceN colorspaces. ,1963
Extract Method,Add utility class to easily extract a range of pages from a PDF There's currently no utility to extract a range of pages (e.g. pages 3-7).  This task adds a PageExtractor class and a corresponding JUnit test class.,1965
Extract Method,Performance improvement in PDFStreamEngine and Matrix (patch included) I've been profiling PDFBox during text extraction from some large PDF documents e.g. 2000 pages mostly text 20 Mb file size.Some of these documents can take a long time to process e.g. 40s+ sometimes a lot more than that.    (I'm using a 2.5 GHz 4 Gb Mac OS X 10.5.8 Java(TM) SE Runtime Environment (build 1.6.0_22-b04-307-9M3263) with -Xms256m -Xmx1024m -XX:PermSize=256m)I've begun by profiling where the code spends its time during text extraction and I see that a lot of time is spent constructing org.apache.pdfbox.util.Matrix objects.Screenshot PDFReference_nopatch.tiff shows the most used methods in PDFBox during text extraction for a large document. When this screenshot was taken the percentages had stabilised and Matrix.<init> accounts for 40% of cpu time apparently - the largest time of any method. I was surprised.Most of these Matrix instances are being constructed within PDFStreamEngine.prcoessEncodedText(byte[])On revision 1035639 (pre-1.4.0) this method constructs one Matrix object and then a further 7 within a loop which is called for each character in the document. So that's a lot of Matrix objects.The attached patch refactors PDFStreamEngine.processEncodedText so that it now creates 5 reusable Matrix instances outside the loop and 2 within it.This was achieved by adding a new method to Matrix: Matrix.multiply(Matrix Matrix) which allows you to multiply two matrices and have the result stored in a specified Matrix object. This has the effect of reducing the number of temporary Matrix objects created during multiplication within PDFStreamEngine. This should save the garbage collector some work.I profiled PDFBox again with this patch included and Matrix.<init> now accounts for only 30% of the cpu time.Unfortunately whilst less temporary objects are being created it doesn't have an appreciable effect on the time it takes to extract text from my large documents.The profiling continues...,1967
Extract Method,PDF signing interface and improvments This is a first version of a signing interface for pdfbox. There are some design issues i could not handle without rewriting too much of the code.Here we go:- incremental update support (tested for signatures with pdf/a compatibility) not compatible with encrypted documents nor with xref-streams- cos object improvment  -- COSString with ability to force writing hexbin for given string  -- COSBase with ability to write direct into a dictionary (that means if this is set no indirect object will be wrote) (sry hard to explain what i mean actualy needed for incremental update to lower the rate of indirect objects)  -- COSBase with ability to force writing object (this hook help the COSWriter write needed objects for inkremental update)  -- COSName added new names  -- COSDocument some getter and setter for handling new signature and incremental features- SignatureException with some exceptions for handling the bunch of new possible errors-Parser improvments  -- PDFParser saves now the position of the last startxref  -- VisualSignatureParser (hook for parsing visual signature templates) (it's only for prepared visualisation that should be merged with the document)-IO improvments  -- COSFilterInput helps to find the proper content that should be hashed / signed  -- COSStandardOutputStream is tricky it helps the writer to jump to the right place in the document  -- COSWriter got some improvments for incremental update  -- COSWriterXRefEntry needed for incremental updates and writing the new Xref table- PDDocument -- got a new method addSignature with the needed implementation (do the whole signature stuff) -- cleanup- Fields and Annotations -- PDSignature represent the signature dictionary  -- PDSignatureFild / Annotation are the visible & unvisible signature representations- Signature Interface and options -- SignatureInterface the interface that shall be implemented for proper signing -- SignatureOptions some additional options for signingPatch splited into piecessry for spelling didn't include a spellchecker for english.,1968
Extract Method,Read non-conforming PDFs (attached) without throwing java.io.IOException: expected='endobj' org.apache.pdfbox.io.PushBackInputStream This happened using the following PDF (~2MB): http://biblioteca.sinbad.ua.pt/DisQSws/get.aspx?filename=2010001615.pdf&catalog=Teses&type=pdfWhen reading non-conforming PDFs like the one above the following exception is thrown and the text extraction partially fails:WARN - Parsing Error Skipping Objectjava.io.IOException: expected='endobj' firstReadAttempt='' secondReadAttempt='' org.apache.pdfbox.io.PushBackInputStream@53ab04        at org.apache.pdfbox.pdfparser.PDFParser.parseObject(PDFParser.java:607)        at org.apache.pdfbox.pdfparser.PDFParser.parse(PDFParser.java:172)        at org.apache.pdfbox.pdmodel.PDDocument.load(PDDocument.java:878)        at org.apache.pdfbox.pdmodel.PDDocument.load(PDDocument.java:843)        at org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:74)        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:137)        at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:218)        at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:84) ,1969
Extract Method,Add optional debug output to ExtractText It would be useful to have some information about the time consumption for different stages of the text extraction,1970
Extract Method,"redirect-action should be able to accept parameters The ww:url tag lets you specify an action and parameters like so:<ww:url action=""myAction""><ww:param name=""id"" value=""myParamValue""/></ww:url>The redirect-action result type in xwork.xml should be able to accept parameters as well.This is important especially when using a custom ActionMapper where the action name is not the same as the URL being used which prevents the use of the regular redirect result type.(See http://forums.opensymphony.com/thread.jspa?forumID=1&threadID=25826).",1974
Rename Method,Generate Taglib TLD from annotations/xdoclet tags Previously in WebWork 2 the taglib TLD was generated using XDoclet from XDoclet tags in the source code.  With the move to Maven 2 and Java 5 the ant tasks that performed this task have been removed and XDoclet doesn't work with Java 5 source.  Therefore either we should migrate the process to a new XDoclet 2 plugin or using some Java 5 annotation processing tool.,1975
Extract Method,Provide a hook for FilterDispatcher such that a custom Dispatcher could be used. Provide a hook just a protected method in FilterDispatcher that creates a Dispatcher instance that will be used by FilterDispatcher such that there is a way for user to plug in custom Dispatcher. Useful for testing as well.,1976
Rename Method,Clean URL (ReST-style) support via action mapping Struts should support clean urls making it possible to use them with the capability to fully customize the URL's where necessary.,1979
Rename Method,Add a Dependency Injection library to wire internals of Struts framework and plugins At the core of Struts we should have a simple dependency injection engine that wires together major Struts components and constants/settings.  This would allow us to support more powerful self-configuring plugins and simplify our internal architecture removing the need for most statics and factories.,1982
Extract Method,Add Autocompleter AJAX tag Create an Autocompleter tag that wraps Dojo's ComboBox,1983
Extract Method,Add TestNG support Strust 2 should have built-in support for TestNG as well as JUnit.  See also http://jira.opensymphony.com/browse/XW-451,1984
Rename Method,"Allow codebehind to select the starting directory point  Here's a simple patch to allow a new default ""root"" directory to be used when searching templates.  I created it because I had a wish to keep jsp and css in independent directories.The configuration property is known as  ""struts.codebehind.pathPrefix"" and requires a trailing slash It's set to default to ""/"" in both the java and in the struts-plugin.xml as an addendum per a request i've also added the default constant for ""struts.codebehind.defaultPackage"" to help the codebehind plugin remain selfcontained.",1985
Extract Method,Allow overriding of plugin properties in struts.properties Currently struts.properties is processed before all the XML files meaning you cannot override plugin properties with it.  The properties file loading should be split so that default.properties is loaded first then the XML then struts.properties.,1987
Extract Method,FileUploadInterceptor and localized messages problem See http://jira.opensymphony.com/browse/WW-1379 for details.  This is an issue in struts2 as well as webwork.  My question is: is this truely a bug or is the FileUploadInterceptor working as designed?,1988
Extract Method,separate out name creation in ExecuteAndWaitInterceptor Separate out generation of name so that ExecuteAndWaitInterceptor can be extended to change the association of background process to request.,1989
Rename Method,"Add highlight effect to ""targets"" for the ajax tags Add the following properties to bind submit and anchor:""highlightColor""""highlighDuration""if highlightColor is specified dojo.lfx.html.highlight() will be applied to the ""targets"" elements",1991
Extract Method,Url tag should accept parameters that are Iterable as it does with String[] 0,1992
Extract Method,Ability to use multiple action extensions simultaneously Currently you can't really use multiple action extensions. While requests using multiple extensions will be recognized any urls created by the framework will only use the default or first extension.  What should happen is the framework remember the original extension and use it in any future url generation for that request.,1994
Extract Method,"JasperReports plugin - exporter parameters via configuration Currently there's no way to pass the exporter parameters to JasperReports plugin from outside. It'll be good if we can specify a map containing these parameters in struts.xml and the plugin will use setParameters(java.util.Map parameters) to set these before generating the report           <action name=""newapp""  class=""GetMessagesAction""> <result name=""success"" type=""jasper"">   <param name=""location"">www/FirstReportTry.jasper</param>   <param name=""format"">HTML</param>  <param name=""parse"">false</param>  <param name=""dataSource"">dsMap</param>                                  <param name=""exportParams"">exporterParamterMap</param> </result>                 </action>",1995
Rename Method,Add filename argument to acceptFile method in FileUploadInterceptor It would be quite usefull to extend the acceptFile  method in FileUploadInterceptor with an additional argument that passes in the original filename. This allows you to filter uploads based on its extension / name and not merely on the content type. ,1996
Extract Method,"<s:text name=""key""/> OGNL-evaluates ""key"" when it is not found in resource bundle <s:text name=""key""/> outputs the resource message designated by ""key"".If no such message exists (and the body of the tag is empty) it reverts to outputting the name of the key itself as an unevaluated string.  At least that's the behaviour of the tag as described in the documentation.In fact however the name of the key is now being evaluated as an OGNL expression and if that turns out to be valid that is what's printed.This in my view is counter-intuitive and probably counter-productive.  Consider the case of a ModelDriven<Book> action exposing a Book-object to the View.Normally     <s:text name=""title""/>: ${title}would have yielded:    title: The Great Gatsbyif the resource message ""title"" was missing from the resource bundle.  That's not too bad an outcome.  However with the key being OGNL-evaluated we would get the following output:    The Great Gatsby: The Great Gatsbywhich I find a little puzzling...",1997
Rename Method,"Paramters not being set in JFreeChart Plugin <action name=""ch"" class=""org.someorg.chart.ChartAction""><result name=""success"" type=""chart""><param name=""width"">${width}</param><param name=""height"">${height}</param></result></action>In the above case I am getting an IllegalArgumentException although I have defined width and height in ChartActionpublic class ChartAction extends ActionSupport{private static final long serialVersionUID = -4845276888116145855L;private Integer width = 200;private Integer height = 400;public String execute() throws Exception{chart = new chart..return SUCCESS;}public Integer getWidth(){return width;}public void setWidth(Integer width){this.width = width;}public Integer getHeight(){return height;}public void setHeight(Integer height){this.height = height;}}",1998
Extract Method,Make the decision on whether or not a Class can be instantiated overridable by subclasses In the PackageBasedActionConfigBuilder a check is done to see if a found Action class is abstract an interface an enum or an annotation.  I would like to see this check moved to a protected method that can be overridden as there are some cases where it might be quite alright to have an interface be used for the action.  And as long as the ObjectFactory used is able to instantiate an Action for it that should really be all that matters.,1999
Extract Method,Could not create application global result. I was converting my previous webwork 2 project to struts 2. In webwork I could define global results which will be used as defaults for all action. If I use Convention plugin the global result was global for the single action. I want to define Results in supper class (BaseAction eg.) and expect all sub class will use these results and could override result with same name. I found this feature was menthioned in https://issues.apache.org/struts/browse/WW-2443 but not working in Convention plugin.,2000
Extract Method,"@Action and @Actions should be able to be applied to classes the method will be determined at runtime by Dynamic Action invocation mechanism If a method ""execute"" is defined in the class the action mapping will point to it otherwise the method will be null in the action mapping",2001
Rename Method,"Several improvements - Method to check which HTTP methods are allowed using OPTIONS.- Default implementation for POST and PUT if client sends request header ""expect: 100-continue"".- Verification of action returns to allow String HttpHeaders Result or null.- The default method ""index"" in ""RestActionSupport"" return Object instead String in order to allow for HttpHeaders or Result to be returned.- Result processing is delayed after all interceptors have finished.- New configuration parameter to show the processing time via log4j. Will show action processing time plus interceptors and result processing time (JSP XML ...).- The new flow of result processing is:    - The content is selected (model exception or errors list)    - Use HttpHeaders to apply the headers etc.    - If the result code is 304 the result is not processed.    - If the result is a HttpHeaderResult it is executed regardless of representation.    - The result is always returned according to the requested representation.    - If there are errors it will look for a result named ""default-error"" to be able to configure the error page. It could be configured in the web.xml but the request representation would not be respected.",2002
Extract Method,Enable customization of Struts2 core Attached is a patch for Dispatcher and ConfigurationManager which allows customization of various objects created by the system.  Perhaps there is a cleaner way to do this e.g. with factories?  Anyway the reason for this request is that we are extending the Struts 2 DTD which requires enhancements to the XML parser etc etc and we can't do this cleanly unless there is a way to use derived classes of StrutsXmlConfigurationProvider etc etc,2003
Extract Method,Correct support for include/exclude parameters JSONResult.setIncludeProperties() got the algorithm right but using regular expressions to match OGNL expressions is *painful*.  The attached patches add support for wildcard patterns to JSONResult and JSONInterceptor and update JSONInterceptor.setIncludeProperties() to use the correct algorithm.The patches also add include/exclude support to JSONCleaner to support filtering on the input data.  (Configuration for JSONInterceptor and JSONResult only handle output filtering.)There are also a couple of code clean-up changes to eliminate code duplication.,2005
Extract Method,Remove code duplication between DefaultActionInvocation and RestActionInvocation If DefaultActionInvocation.invokeAction() were refactored so methodResult were processed in a separate function (the way it is done in RestActionInvocation) then RestActionInvocation would not have to duplicate all the code in invokeAction().,2006
Extract Method,"placeHolder attribute for s:textfield/s:password being dynamic The placeholder field fot s:textfield and s:password should evaluate ognl expressions. Namely resource key replacements.e.g<s:textfield  theme=""simple"" placeholder=""%{getText('placeHolder.Username')}"" id=""j_username"" name=""j_username"" cssStyle=""width:180px"" />This is an issue for HTML5.I think Lukasz Lenart suggested making all attributes dynamic.  AbstractUITag line 294    public void setDynamicAttribute(String uri String localNameObject value) throws JspException {        dynamicAttributes.put(localName value);    }and replace with    public void setDynamicAttribute(String uri String localNameObject value) throws JspException {        dynamicAttributes.put(localName findString(value));    }",2007
Extract Method,ContextPath show be included in log when could not find a action or result When a invalid action is requested only 'namespace' and 'action name' value are given in the warning log. I suggest that 'ContextPath' show also be included.11:40:36570 WARN  [org.apache.struts2.dispatcher.Dispatcher:60] Could not find action or result11:40:36572 There is no Action mapped for namespace / and action name xxxx. - [unknown location]11:40:36573    at com.opensymphony.xwork2.DefaultActionProxy.prepare(DefaultActionProxy.java:189)11:40:36574    at org.apache.struts2.impl.StrutsActionProxy.prepare(StrutsActionProxy.java:61)11:40:36576    at org.apache.struts2.impl.StrutsActionProxyFactory.createActionProxy(StrutsActionProxyFactory.java:39)11:40:36576    at com.opensymphony.xwork2.DefaultActionProxyFactory.createActionProxy(DefaultActionProxyFactory.java:58)11:40:36579    at org.apache.struts2.dispatcher.Dispatcher.serviceAction(Dispatcher.java:500)11:40:36579    at org.apache.struts2.dispatcher.FilterDispatcher.doFilter(FilterDispatcher.java:434)11:40:36582    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:280)11:40:36582    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:248)11:40:36584    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:275)11:40:36585    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:161)11:40:36588    at org.jboss.as.web.security.SecurityContextAssociationValve.invoke(SecurityContextAssociationValve.java:139)11:40:36588    at org.jboss.as.web.NamingValve.invoke(NamingValve.java:57)11:40:36590    at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:154)11:40:36591    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)11:40:36594    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)11:40:36595    at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:362)11:40:36597    at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:877)11:40:36598    at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:667)11:40:36600    at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:952)11:40:36600    at java.lang.Thread.run(Thread.java:662),2010
Extract Method,"Allow results from POST and restore compatibility with struts-rest-plugin version 2.2.1 Some time ago I posted a message on the mailing list but got no reply. See http://struts.1045723.n5.nabble.com/struts-rest-plugin-2-2-3-result-from-POST-td4469274.htmlWe really would like to update to newer versions of Struts and benefit from the security fixes but we absolutely care about backward-compatibility. We are aware that it is not a ""valid"" REST practice to return results from POST operations and we wish we had not done so in the first place. Unfortunately to remain backward compatible we have no choice. Do you see a way to restore the 2.2.1 behavior to allow us to remain backward-compatible?",2011
Extract Method,Make json plugin more extendable Currently it is difficult to modify/extend the current behaviour of the json plugin.I had the following problem. We use joda-time because it is much better than standard Date. The json plugin cannot handle classes like LocalDate which is not a problem in itself. But it needs a lot of boilerplate code to output beans containing a LocalDate field using json. Currently I have to create duplicates of the service layer DTO-s annotate the fields in the UI copy with @JSONFieldBridge and copy one DTO into the other using dozer or something.This creates a lot of useless code and manual work. It would be much easier if I could extend JSONWriter and add support for my joda-time classes. No duplication of DTO-s no need to manually annotate all fields. This would require only to modify the visibility of some private methods to protected and some package-protected classes to public. This way I could create my struts2 result type which extends JSONResult and use my writer which extends JSONWriter.,2012
Extract Method,Error at setting character encoding if the parameters have been already read In my application I have an access log filter that logs all incoming requests and the parameters of the request.The struts2 filter calls request.setCharacterEncoding(encoding) which results in an error because the request encoding cannot be set after the parameters have been read. This pollutes the log with errors for every request.I cannot put the access log filter after the struts2 filter because it does not invoke chain.doFilter() for the actions.I have a custom filter before the access log filter that sets the request encoding in advance. Struts2 tries to set the encoding again later but too late in my case.The proposed patch is very simple and straightforward and would solve my problem with the polluted log. If the encoding is already set to the same value do not try to set it again. It is useless and will cause an error. In all other cases set it as now.,2013
Extract Method,Decouple token names from their respective session attribute names Currently token names are used as is to store session attributes for later token check. By namespacing session attributes security can be improved.,2014
Extract Method,Allow Package Inheritance in Plugins Allow packages in plugins to have packages defined in a different plugin as a parent.  This is currently not supported (http://struts.apache.org/2.3.4.1/docs/plugins.html).  My team is creating a plugin with some base features for our organization.  It would be nice to be able to use result types and other configuration defined in other plugins.,2015
Move Method,Merge AnnotationTools into AnnotationUtils 0,2016
Move Method,Extract static util methods from Component class into ComponentUtils class 0,2017
Extract Method,Refactor problem report generation in Dispatcher There is too much code in public void sendError(HttpServletRequest HttpServletResponse ServletContext int Exception) - it must be simplified,2018
Extract Method,"Performance - Locks in class loader JSONWriter is causing locks in class loader because it always invokes Introspector.getBeanInfo(). That in turn tries to load a class with ""BeanInfo"" suffix in Introspector.findExplicitBeanInfo(). Such class does not exists but we try to load it over and over again for every JSON result. Class loading involves a lock in the application server and this causes performance issue in heavily multithreaded applications.This could be solved by caching the BeanInfo for the classes for which I have created a patch.The issue is also mentioned here:http://stackoverflow.com/questions/12728860/java-performance-issue-tomcat-webappclassloader-locked",2020
Extract Method,Use the latest Jetty maven plugin in example apps and archetypes It would be nice to use the latest Jetty plugin{code:xml}<plugin>    <groupId>org.mortbay.jetty</groupId>    <artifactId>jetty-maven-plugin</artifactId>    <version>8.1.7.v20120910</version>    <configuration>        <stopKey>CTRL+C</stopKey>        <stopPort>8999</stopPort>        <scanIntervalSeconds>10</scanIntervalSeconds>        <scanTargets>            <scanTarget>src/main/webapp/WEB-INF/web.xml</scanTarget>        </scanTargets>    </configuration></plugin>{code},2021
Extract Method,Refactor ParametersInterceptor so it's easier to extend As a follow up to WW-3973 please find attached patch which will allow developers to easily extend the interceptor and return the old behaviour.,2022
Extract Method,"ValidationAware add callable method called from DefaultWorkflowInterceptor When a form error occurs the action is unaware of this event. There are situations whereby one should know that a form error has occurred. For example one may wish to populate the ActionStack with special handling variables or set a flag denoting special logic that the JSP should handle. In short it isn't unreasonable for one to want to know that a form error occurred and be able to respond to it with more than just a single JSP result attached to ""input.""My suggestion would be within ValidationAware add a method:void actionError()Within DefaultWorkflowInterceptor.doIntercept there is already an if condition of:if (validationAwareAction.hasErrors()) {at the end of processing that if block just before ""return resultName;"" call validationAwareAction.actionError();By the way this line:LOG.debug(""Errors on action "" + validationAwareAction + "" returning result name 'input'"");should be:LOG.debug(""Errors on action "" + validationAwareAction + "" returning result name '"" + inputResultName + ""'"");",2023
Extract Method,Support for saving locale in cookie Extend the xwork i18n interceptor to support saving a locale in a cookie.The new interceptor needs to live in the struts2 package as the xwork parts are web agnostic.,2025
Extract Method,Don't check for disallowed ognl expressions if getting from expression cache When the ognl expression cache is enabled when there is a cache hit we don't need to have the overhead of traversing the tree to see if there is a disallowed expression.  I've attached a simple patch.,2027
Extract Method,"@TypeConversion converter attribut to class Can you please change ""converter"" property type from String to class type.{code}@Conversion(conversions = { @TypeConversion(type = ConversionType.CLASS rule = ConversionRule.PROPERTY converter = ""com.xxx.MyConverter"" key = ""myKey"") }){code}---->{code}@Conversion(conversions = { @TypeConversion(type = ConversionType.CLASS rule = ConversionRule.PROPERTY converter = MyConverter.class key = ""myKey"") }){code}",2028
Move Method,Duplicated code to extract URI There are many references to exactly the same code which extracts URI from request.- DefaultActionMapper- PrepareOperationsand so on. The method should be moved to RequestUtils and be used in all that places.,2029
Extract Method,ComponentTagSupport should use TagUtils to gain access to Container instead directly calling Dispatcher.getInstance() 0,2030
Extract Method,Struts fileupload function can not get file upload process data when use org.apache.struts2.dispatcher.multipart.JakartaMultiPartRequest to upload file with struts2 we can not add org.apache.commons.fileupload.ProgressListener  to monitor the file upload process.,2031
Rename Method,Support latest stable AngularJS version in maven angularjs archetype - Upgrade maven archetype to latest stable version 1.4.2- Improve example code,2036
Extract Method,Add dedicated class to represent Http Parameters Right now {{parameters}} are represented by a {{Map}} and a lot of logic is duplicated. There is no way to check if given parameter was already evaluated.,2037
Rename Method,"ConversionErrorInterceptor to extend MethodFilterInterceptor Would it be possible to modify the {{ConversionErrorInterceptor}} to extend {{MethodFilterInterceptor}} so I can exclude the validation on certain methods? ie{code:xml}<interceptor-ref name=""conversionError"">    <param name=""excludeMethods"">executecancel*</param></interceptor-ref>{code}It seems always to be called (needs to be like the validator/workflow)I have noticed that if I there is a conversion error on a screen and I return with a redirectAction (and the action has a {{STORE}} and the destination action has a {{RETRIEVE}}) the conversion error shows on the destination action screen.Although I still get in dev mode:{noformat}Error setting expression 'bean.weight' with value ['gggg' ]{noformat}as this comes from the params interceptor (and I do not want to exclude this on my cancel) I guess we will have to live with this.",2039
Extract Method,Disallow access to HttpParameters.toMap {{HttpParameters.toMap}} can be potentially danger as allow access raw parameters' values.,2041
Rename Method,<s:text/> tag should not evaluate defaultMessage against a ValueStack by default Right now {{<s:text/>}} tag will perform evaluation of a {{defaultMessage}} against a ValueStack by default. In most cases the {{defaultMessage}} is set to value of {{name}} attribute and can be easily used wrong by a developer. Evaluation must be performed only on purpose.This change affects also {{<s:label/>}} tag and {{label}} attribute of all {{UIBean}}s,2042
Rename Method,HttpParameters should behave like a Map 0,2044
Inline Method,"AnnotationWorkflowInterceptor should supports non-public annotated methods {code:java}@Beforeprotected String prepare(){    //TODO    return null;}{code}[https://github.com/apache/struts/blob/master/core/src/main/java/com/opensymphony/xwork2/interceptor/annotations/AnnotationWorkflowInterceptor.java#L115]{code:java}List<Method> methods = new ArrayList<>(AnnotationUtils.getAnnotatedMethods(action.getClass() Before.class));{code}[https://github.com/apache/struts/blob/master/core/src/main/java/com/opensymphony/xwork2/util/AnnotationUtils.java#L123]{code:java}for (Method m : clazz.getMethods()) {code}clazz.getMethods() only return public methods so method ""prepare"" will be excluded and protected modifier is a good practice for intercept method.We should improve AnnotationUtils.getAnnotatedMethods() to return all methods. Perhaps use an ConcurrentHashMap as cache is much better.",2045
Extract Method,"Buffer/Flush behaviour in FreemarkerResult Scenario: the application use freemarker with a {{TemplateExceptionHandler.RETHROW_HANDLER}} policy but occasionally needs to produce large XML (20~200Mb) and goes out of memory.In [FreemarkerResult|http://grepcode.com/file/repo1.maven.org/maven2/org.apache.struts/struts2-core/2.5-BETA1/org/apache/struts2/views/freemarker/FreemarkerResult.java#191] there are two possible behaviours (line 191): * *Buffer-behaviour*: the whole template is processed and if everything is OK it is flushed to the output otherwise an exception is thrown and handled at higher level before any output has been sent. This is intended to be used when {{TemplateExceptionHandler.RETHROW_HANDLER}} is active* *Flush-behaviour*: template is processed and flushed according to freemarker library policies used with any other {{TemplateExceptionHandler}}Since {{TemplateExceptionHandler}} cannot be switched for a given request (it is a global configuration embedded in {{FreemarkerManager}}) there is no way to force a Flush-behaviour. (you can only force a Buffer-behaviour using {{isWriteIfCompleted}})I implemented a more flexible solution that let you force the behaviour in both ways:{code:title=FreemarkerResult.java|borderStyle=solid}    final boolean willUsebufferedWriter;    if (useBufferedWriter != null){        willUsebufferedWriter = useBufferedWriter;    }else{        willUsebufferedWriter = configuration.getTemplateExceptionHandler() == TemplateExceptionHandler.RETHROW_HANDLER;    }                    if (willUsebufferedWriter){    ...    }else{    ...    }       {code}where {{useBufferedWriter}} is a parameter that can be modified per request{code}<result type=""freemarker"">    <param name=""location"">big_feed.ftl</param>    <param name=""contentType"">text/xml</param>    <param name=""useBufferedWriter"">false</param></result>{code}",2046
Extract Method,Add proper validation if request is a multipart request 0,2054
Extract Method,Stop using DefaultLocalizedTextProvider#localeFromString static util method This method can be replaced with {{LocaleUtils#toLocale(String)}} when needed,2055
Extract Method,"expose Freemarker incompatible_improvements into FreemarkerManager and StrutsBeansWrapper The latest version of Freemarker (2.3.6) supports the use of {{default}} methods in interfaces when communicating between beans and the Freemarker template.  To enable this it's required to allow init parameters to be passed from the servlet configuration in through to the {{FreemarkerManager}} and the {{BeansWrapper}}.  The sitemesh plugin prevents the setting of these parameters because it doesn't appear to pass the parameters in through.* Release notes for 2.3.6 -- http://freemarker.org/docs/versions_2_3_26.html* Documentation of {{incompatible_improvements}} http://freemarker.org/docs/pgui_config_incompatible_improvements.html#pgui_config_incompatible_improvements_how_to_set* Documentation of DefaultObejctWrapper and configuration http://freemarker.org/docs/pgui_datamodel_objectWrapper.html#topic.defaultObjectWrapperIcI* Constructor for DefaultObjectWrapper -- http://freemarker.org/docs/api/freemarker/template/DefaultObjectWrapper.html#DefaultObjectWrapper-freemarker.template.Version- and BeansWrapper -- http://freemarker.org/docs/api/freemarker/ext/beans/BeansWrapper.htmlNote: I'm not arguing that we should just ""change these parameters but it'd be nice to be able to pass the <init-param> configuration through to Freemarker",2056
Rename Method,Fallback to ActionContext if container is null in ActionSupport RIght now an action extending {{ActionSupport}} cannot be created manually as it requires {{Container}} to be injected.,2057
Rename Method,Debug tag should not display anything when not in dev mode I noticed that the debug tag displays the content of the value stack independently of the value of devMode.I wonder if it would not be more secure to do not display anything if devMode=false.I can imagine a developer forgetting to remove such kind of debug tags before the app goes to production. Making it silent in production mode would reduce the risk to display sensitive data.,2059
Extract Method,Implement new Aware interfaces that are using withXxxx pattern instead of setters In matter of security I wonder if we should stop using setters in internal API. Like in {{SessionAware}} interface we use {{setSession()}} and each actions must implement this method. Then we have a logic to avoid mapping incoming values to {{setSession()}} to permit injecting values into Session.Instead of {{setSession()}} we can use {{withSession()}} or {{applySession()}} - the same can be applied to any *Aware interface.,2060
Extract Method,"Enable Client-side validation for visitor validations The client-side JavaScript doesn't work as advertised on:http://wiki.opensymphony.com/display/WW/Client-Side+ValidationI have the following form:<ww:form action=""'saveUser'"" validate=""true"" cssClass=""'detail'"" method=""'post'"">This generates the following onsubmit handler:onsubmit=""return(${parameters.name}_validate())"" And the following JavaScript after the form:<script type=""text/javascript"">    function ${parameters.name}_validate() {        var form = document.forms['${parameters.name}'];        var focus = ${parameters.name}_validate_actual();        if (focus != null) {            form.elements[focus].focus();            if (form.elements[focus].type == 'text' || form.elements[focus].type == 'textarea') {                form.elements[focus].select();                        }            return false;        } else {            return true;        }    }    function ${parameters.name}_validate_actual() {        var form = document.forms['${parameters.name}'];        // cannot find any applicable validators        return null;    }    </script>If I add name=""'user'"" to the <ww:form> then the JavaScript looks right but I get:    function user_validate_actual() {        var form = document.forms['user'];        // cannot find any applicable validators        return null;    }I have the following in validators.xml:   <validator name=""requiredstring""        class=""com.opensymphony.webwork.validators.JavaScriptRequiredStringValidator""/>Oddly enough XMLBuddy (in Eclipse) says that ""name"" must be declared.",2063
Rename Method,Add hlog number metric in regionserver Add hlog number metric in regionserver. We can use this metric to alert about memstore flush because of too many hlogs.,2064
Rename Method,Stronger validation of key unwrapping In EncryptionUtil#unwrapKey we use a CRC32 to validate the successful unwrapping of a data key. I chose a CRC32 to limit overhead. There is only a 1 in 2^32 chance of a random collision low enough to be extremely unlikely. However I was talking with my colleague Jerry Chen today about this. A cryptographic hash would lower the probability to essentially zero and we are only wrapping data keys once per HColumnDescriptor and once per HFile saving a few bytes here and there only really. Might as well use the SHA of the data key and in addition consider running AES in GCM mode to cover that hash as additional authenticated data.,2065
Extract Method,instead of putting expired store files thru compaction just archive them From HBASE-9648,2066
Extract Method,improve VerifyReplication to compute BADROWS more accurately VerifyReplicaiton could compare the source table with its peer table and compute BADROWS. However the current BADROWS computing method might not be accurate enough. For example if source table contains rows as {r1 r2 r3 r4} and peer table contains rows as {r1 r3 r4} BADROWS will be 3 because 'r2' in source table will make all the later row comparisons fail. Will it be better if the BADROWS is computed to 1 in this situation? Maybe we can compute the BADROWS more accurately in merge comparison?,2067
Extract Method,Region balancing does not bring newly added node within acceptable range With a 10 node cluster there were only 9 online nodes. With about 215 total regions each of the 9 had around 24 regions (average load is 24). Slop is 10% so 22 to 26 is the acceptable range.Starting up the 10th node master log showed:{code}2008-11-21 15:57:51521 INFO org.apache.hadoop.hbase.master.ServerManager: Received start message from: 72.34.249.210:600202008-11-21 15:57:53351 DEBUG org.apache.hadoop.hbase.master.RegionManager: Server 72.34.249.219:60020 is overloaded. Server load: 25 avg: 22.0 slop: 0.12008-11-21 15:57:53351 DEBUG org.apache.hadoop.hbase.master.RegionManager: Choosing to reassign 3 regions. mostLoadedRegions has 10 regions in it.2008-11-21 15:57:53351 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region streamitems^@^@^@^@^AHï¿_;12254110516322008-11-21 15:57:53351 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region streamitems^@^@^@^@^@ï¿__12254110566862008-11-21 15:57:53351 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region groups12229135809572008-11-21 15:57:53975 DEBUG org.apache.hadoop.hbase.master.RegionManager: Server 72.34.249.213:60020 is overloaded. Server load: 25 avg: 22.0 slop: 0.12008-11-21 15:57:53975 DEBUG org.apache.hadoop.hbase.master.RegionManager: Choosing to reassign 3 regions. mostLoadedRegions has 10 regions in it.2008-11-21 15:57:53976 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region upgrade12268920147842008-11-21 15:57:53976 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region streamitems^@^@^@^@^@3^Zï¿_12254110567012008-11-21 15:57:53976 DEBUG org.apache.hadoop.hbase.master.RegionManager: Going to close region streamitems^@^@^@^@^@ ^L1225411049042{code}The new regionserver received only 6 regions. This happened because when the 10th came in average load dropped to 22. This caused two servers with 25 regions (acceptable when avg was 24 but not now) to reassign 3 of their regions each to bring them back down to the average. Unfortunately all other regions remained within the 10% slop (20 to 24) so they were not overloaded and thus did not reassign off any regions. It was only chance that made even 6 of the regions get reassigned as there could have been exactly 24 on each server in which case none would have been assigned to the new node.This will behave worse on larger clusters when adding a new node has little impact on the avg load/server.,2068
Extract Method,Regionserver OOME handler should dump vital stats On OOME the regionserver should dump into the log some vital stats:- Number of regions- Number of store files- Estimated item count and size of memcache(s)- Estimated item count and size of store file indexesAssumes the reserve can be released upon OOME to allow the additional actions.,2069
Extract Method,Port 'Make flush decisions per column family' to trunk Currently the flush decision is made using the aggregate size of all column families. When large and small column families co-exist this causes many small flushes of the smaller CF. We need to make per-CF flush decisions.,2070
Rename Method,Add read log size per second metrics for replication source The current metrics of replication source contain logEditsReadRate shippedBatchesRate etc which could indicate how fast the data replicated to peer cluster to some extent. However it is not clear enough to know how many bytes replicating to peer cluster from these metrics. In production environment it may be important to know the size of replicating data per second because the services may be affected if the network become busy.,2071
Rename Method,"Auto detect data block encoding in HFileOutputFormat Currently one has to specify the data block encoding of the table explicitly using the config parameter ""hbase.mapreduce.hfileoutputformat.datablock.encoding"" when doing a bulkload load. This option is easily missed not documented and also works differently than compression block size and bloom filter type which are auto detected. The solution would be to add support to auto detect datablock encoding similar to other parameters. The current patch does the following:1. Automatically detects datablock encoding in HFileOutputFormat.2. Keeps the legacy option of manually specifying the datablock encodingaround as a method to override auto detections.3. Moves string conf parsing to the start of the program so that it failsfast during starting up instead of failing during record writes. It alsomakes the internals of the program type safe.4. Adds missing doc strings and unit tests for code serializing anddeserializing config paramerters for bloom filer type block size anddatablock encoding.",2072
Rename Method,refactor deferred-log-flush/Durability related interface/code/naming to align with changed semantic of the new write thread model By the new write thread model introduced by [HBASE-8755|https://issues.apache.org/jira/browse/HBASE-8755] some deferred-log-flush/Durability API/code/names should be change accordingly:1. no timer-triggered deferred-log-flush since flush is always done by async threads so configuration 'hbase.regionserver.optionallogflushinterval' is no longer needed2. the async writer-syncer-notifier threads will always be triggered implicitly this semantic is that it always holds that 'hbase.regionserver.optionallogflushinterval' > 0 so deferredLogSyncDisabled in HRegion.java which affects durability behavior should always be false3. what HTableDescriptor.isDeferredLogFlush really means is the write can return without waiting for the sync is done so the interface name should be changed to isAsyncLogFlush/setAsyncLogFlush to reflect their real meaning,2073
Rename Method,RegionServer graceful stop / decommissioning Right now we have a weird way of node decommissioning / graceful stop which is a graceful_stop.sh bash script and a region_mover ruby script and some draining server support which you have to manually write to a znode (really!). Also draining servers is only partially supported in LB operations (LB does take that into account for roundRobin assignment but not for normal balance) See http://hbase.apache.org/book/node.management.html and HBASE-3071I think we should support graceful stop as a first class citizen. Thinking about it it seems that the difference between regionserver stop and graceful stop is that regionserver stop will close the regions but the master will only assign them after the znode is deleted. In the new master design (or even before) if we allow RS to be able to close regions on its own (without master initiating it) then graceful stop becomes regular stop. The RS already closes the regions cleanly and will reject new region assignments so that we don't need much of the balancer or draining server trickery. This ties into the new master/AM redesign (HBASE-5487) but still deserves it's own jira. Let's use this to brainstorm on the design. ,2074
Rename Method,Improvements to the import flow Following improvements can be made to the Import logica) Make the import extensible (i.e. remove the filter from being a static member of Import and make it an instance variable of the mapper make the mappers or variables of interest protected. )b) Make sure that the Import calls filterRowKey method of the filter (Useful if we want to filter the data of an organization based on the row key or using filters like PrefixFilter which filter the data in filterRowKey method rather than the filterKeyValue method). The existing test case in TestImportExport#testWithFilter works with this assumption but is so far successful because there is only one row inserted into the table.c) Provide an option to specify the durability during the import (Specifying the Durability as SKIP_WAL would improve the performance of restore considerably.) [~lhofhansl] suggested that this should be a parameter to the import.d) Some minor refactoring to avoid building a comma separated string for the filter args.,2075
Extract Method,clean up HRegionLocation/ServerName usage  I noticed that AsyncProcess updates cache location on failures using a single HRL object that is key to the map that is intended to be by server and contains requests for multiple regions (i.e. MultiAction contains requests for regions A B C and sits in a map by HRL with HRL from server A as a key; in case of failure for e.g. request to B or entire multiaction the location from map key will be passed to updateCache... methods even though it's not for the correct region). It may cause some subtle mistakes in cache updates.I think it'd be good to clean up HRL usage around AP and other classes - if we intend to have a server name then we should use ServerName not HRL.,2076
Inline Method,Per cell TTLs Now that we have cell tags we can optionally store TTLs per cell.,2077
Extract Method,Master should support close/open/reassignment/enable/disable operations on individual regions The master should support close/open/reassignment/enable/disable operations on individual regions by way of a client API and corresponding shell command(s) and maybe also controls on the master UI.If one region out of 1000s is closed or offline for example due to transient DFS problems currently the whole table must be disabled then reenabled to trigger reassignment and hopefully successful (re)open of the offline regions. The flurry of compactions this causes can exacerbate the underlying problem and actually make cluster state worse as more DFS errors accumulate. ,2078
Extract Method,Administrative functions for table/region maintenance It would be useful to have some administrative functions available through HTable or HBaseAdmin. The two functions I'm thinking of right now are to force memcache flushes (on all regions or individually specified) and to run major compactions (on all regions or individually specified).One reason to have this is currently major compactions run once a day by default. The time that they run is related to when you brought up your cluster or created the tables. In my case this has been during peak load rather than offpeak times. We have times that we run other administrative tasks like backups cleanup etc... during off times and this would be a good time to also trigger major compactions and memcache flushes.Memcache force flushing can also be useful in the case that your cluster starts to have issues. They might be isolated to a single region or regionserver but with lots of edits sitting in memcache and potentially unappended hlogs you want to just flush things out to remove the possibility of losing anything.,2079
Rename Method,Support visibility expressions on Deletes Accumulo can specify visibility expressions for delete markers. During compaction the cells covered by the tombstone are determined in part by matching the visibility expression. This is useful for the use case of data set coalescing where entries from multiple data sets carrying different labels are combined into one common large table. Later a subset of entries can be conveniently removed using visibility expressions.Currently doing the same in HBase would only be possible with a custom coprocessor. Otherwise a Delete will affect all cells covered by the tombstone regardless of any visibility expression scoping. This is correct behavior in that no data spill is possible but certainly could be surprising and is only meant to be transitional. We decided not to support visibility expressions on Deletes to control the complexity of the initial implementation.,2081
Extract Method,Use global procedure to flush table memstore cache Currently user can trigger table flush through hbase shell or HBaseAdmin API. To flush the table cache each region server hosting the regions is contacted and flushed sequentially which is less efficient.In HBase snapshot global procedure is used to coordinate and flush the regions in a distributed way.Let's provide a distributed table flush for general use.,2082
Inline Method,Clone Snapshots on Secure Cluster Should provide option to apply Retained User Permissions Currently{code}sudo su - test_usercreate 't1' 'f1'sudo su - hbasesnapshot 't1' 'snap_one'clone_snapshot 'snap_one' 't2'{code}In this scenario the user - test_user would not have permissions for the clone table t2.We need to add improvement feature such that the permissions of the original table are recorded in snapshot metadata and an option is provided for applying them to the new table as part of the clone process.,2085
Rename Method,Remove TimeoutMontior With HBASE-8002 the TimeoutMonitor is disabled by default. Lately we haven't see much problem of region assignments during integration testing with CM. I was thinking it may be time to remove the timeout monitor now?,2087
Extract Method,Support setting custom priority per client RPC Servers have the ability to handle custom rpc priority levels but currently we are only using it to differentiate META/ROOT updates from replication and other 'priority' updates (as specified by annotation tags per RS method). However some clients need the ability to create custom handlers (e.g. PHOENIX-938) which can really only be cleanly tied together to requests by the request priority. The disconnect is in that there is no way for the client to overwrite the priority per table - the PayloadCarryingRpcController will always just set priority per ROOT/META and otherwise just use the generic priority.,2088
Extract Method,Remove duplicated code in HCM add javadoc to RegionState etc. 0,2089
Extract Method,Add RegionObserver pre hooks that operate under row lock The coprocessor hooks were placed outside of row locks. This was meant to sidestep performance issues arising from significant work done within hook invocations. However as the security code increases in sophistication we are now running into concurrency issues trying to use them as a result of that early decision. Since the initial introduction of coprocessor upcalls there has been some significant refactoring done around them and concurrency control in core has become more complex. This is potentially an issue for many coprocessor users.We should do either:\\- Move all existing RegionObserver pre* hooks to execute under row lock.- Introduce a new set of RegionObserver pre* hooks that execute under row lock named to indicate such.The second option is less likely to lead to surprises.All RegionObserver hook Javadoc should be updated with advice to the coprocessor implementor not to take their own row locks in the hook. If the current thread happens to already have a row lock and they try to take a lock on another row there is a deadlock risk.As always a drawback of adding hooks is the potential for performance impact. We should benchmark the impact and decide if the second option above is a viable choice or if the first option is required.Finally we should introduce a higher level interface for managing the registration of 'user' code for execution from the low level hooks. I filed HBASE-11125 to discuss this further.,2090
Extract Method,Potentially improve block locality during major compaction for old regions This might be a specific use case. But we have some regions which are no longer written to (due to the key). Those regions have 1 store file and they are very old they haven't been written to in a while. We still use these regions to read from so locality would be nice. I propose putting a configuration option: something likehbase.hstore.min.locality.to.skip.major.compact [between 0 and 1]such that you can decide whether or not to skip major compaction for an old region with a single store file.I'll attach a patch let me know what you guys think.,2091
Rename Method,Enable HBaseAdmin.execProcedure to return data from procedure execution HBASE-11201 enables global procedure members to return data to procedure master.HBASE-9426 lets user invoke procedure from client via HBaseAdmin.execProcedure.This JIRA is to fill the gap to enable client to get return data from master after procedure execution.,2092
Extract Method,[PE] Allow random value size Allow PE to write random value sizes. Helpful mimic'ing 'real' sizings.,2093
Extract Method,Reduce the frequency of RNG calls in SecureWALCellCodec#EncryptedKvEncoder By reducing the frequency of RNG calls in SecureWALCellCodec#EncryptedKvEncoder we can save 37% of on CPU time in that method and 3% of total on CPU time during an ingest test. WAL processing is a critical latency sensitive area.,2094
Extract Method,"Improve file size info in SnapshotInfo tool Add a ""-size-in-bytes"" flag to print the file size in byte instead of the human readable format.and add a check on the file length between the manifest and the hfile marking as ""CORRUPTED"" files with length that don't match.{noformat}Snapshot Files----------------------------------------4839b testtb/a81219be11ade1d0d2913267caeeb3fe/cf/bec5567b2cb04cd1a76c2f4106991de7 - testtb/f2622221b913c44a61a03550cb74e3a1/cf/8b02813a4f564957bd820c88fccf376a (NOT FOUND)4967b testtb/0cab854a3877697e726a73187fe21643/cf/7afb8fe1e2f141eb9b8e17d1f68cd576 (archive)12b testtb/35074c28fd4dc304930f261fa8e0ce9c/cf/fedd9b9044b74768a6631003695c2f32 (CORRUPTED)4839b testtb/bb2ac9c8efc5ac9077084268c60dd8da/cf/50a3144088b049d98007af331821abd7 4839b testtb/cda2a3ea0f5630d19018916cbe73264e/cf/da0a1008656c403cb17f37a061d04120 4905b testtb/5b6e6b804e075778185e2fb1a27bae90/cf/1177a58d798a46ec952a0bc19f902711 (archive)**************************************************************BAD SNAPSHOT: 1 hfile(s) and 0 log(s) missing.1 hfile(s) corrupted.**************************************************************{noformat}",2095
Rename Method,MetaTableAccessor shouldn't use ZooKeeeper After committing patch for HBASE-4495 there's an further improvement which can be made (discussed originally on review board to that jira).We have MetaTableAccessor and MetaTableLocator classes. First one is used to access information stored in hbase:meta table. Second one is used to deal with ZooKeeper state to find out region server hosting hbase:meta wait for it to become available and so on.MetaTableAccessor in turn should only operate on the meta table content so shouldn't need ZK. The only reason why MetaTableAccessor is using ZK - when callers request assignment information they can request location of meta table itself which we can't read from meta so in that case MetaTableAccessor relays the call to MetaTableLocator. May be the solution here is to declare that clients of MetaTableAccessor shall not use it to work with meta table itself (not it's content).,2096
Extract Method,[PE] Add 'cycling' test N times and unit tests for size/zipf/valueSize calculations Small PE diff to add a cycles argument so can have a small test run a long time against same keyset.,2098
Extract Method,Refactoring out the configuration changes for enabling VisibilityLabels in the unit tests. All the unit tests contain the code for enabling the visibility changes. Incorporating future configuration changes for Visibility Labels configuration can be made easier by refactoring them out to a single place.,2099
Extract Method,Add to RWQueueRpcExecutor the ability to split get and scan handlers RWQueueRpcExecutor has the devision between reads and writes requests but we can split also small-reads and long-reads. This can be useful to force a deprioritization of scans on the RS.,2101
Extract Method,KeyValue to Cell Convert in WALEdit APIs In almost all other main interface class/APIs we have changed KeyValue to Cell. But missing in WALEdit. This is public marked for Replication (Well it should be for CP also) These 2 APIs deal with KVsadd(KeyValue kv)ArrayList<KeyValue> getKeyValues()Suggest deprecate them and add for 0.98add(Cell kv) List<Cell> getCells()And just replace from 1.0,2103
Extract Method,"Split each tableOrRegionName admin methods into two targetted methods Purpose of this is two implement [~enis]'s suggestion to strongly type the methods that take ""tableOrRegionName"" as an argument.For instance:{code}void compact(final String tableNameOrRegionName)void compact(final byte[] tableNameOrRegionName){code}becomes{code}@Deprecatedvoid compact(final String tableNameOrRegionName)@Deprecatedvoid compact(final byte[] tableNameOrRegionName)void compact(TableName table)void compactRegion(final byte[] regionName){code}",2104
Extract Method,HFile tool should implement Tool disable blockcache by default I tried using bin/hbase hfile in a memory-constrained environment. It crashed while trying to instantiate a blockcache. Went to override the configuration and found I couldn't.Refactor HFile to remove main() put implementation entirely in HFilePrettyPrinter. Said class now extends Configured and implements Tool so configs can be overridden on cli. Also disabled blockcache by default.,2105
Rename Method,Get rid of Writables in HTableDescriptor HColumnDescriptor Currently we have protobuf for encoding this structures. Existence of Writable is misleading and need to be removed.,2106
Move Method,Add append and remove peer table-cfs cmds for replication HBASE-8751 introduces the tables/table-column families config for a replication peer. It's very flexible for practical replication in hbase clusters.But it is easy to make mistakes during add or remove a table/table-column family for a existing peer especially when the table-cfs is very long for we need to copy the current table-cfs of the peer first and then add or remove a table/table-column family to/from the table-cfs at last set back the table-cfs using the cmd: set_peer_tableCFs. So we implements two new cmds: append_peer_tableCFs and remove_peer_tableCFs to do the operation of adding and removing a table/table-column family. They are useful operation tools.,2108
Extract Method,If HBase directory exists but version file is inexistent still proceed with bootstrapping On the dev list I suggested we change the way we manage the empty HBase directory case. Stack answered:{quote}Yes. In fact its probably safe-to-do now we've left far behind thepre-history versions of hbase where there was no hbase.version file in thehbase.rootdir. If absent lets proceed and just write it rather than treatit as a non-migrated instance{quote},2109
Extract Method,RegionServers should find new master when a new master comes up. 0,2110
Extract Method,Move the core Connection creation functionality into ConnectionFactory 0,2111
Extract Method,Provide a command to list visibility labels A command to list visibility labels that are in place would be handy.This is also in line with many of the other hbase list commands.,2112
Extract Method,ZooKeeperWrapper constants cleanup A lot of the ZooKeeper constants in HConstants are only used in one place. We should remove them from there and directly put them in.,2113
Extract Method,Add new AsyncRpcClient With the changes in HBASE-12597 it is possible to add new RpcClients. This issue is about adding a new Async RpcClient which would enable HBase to do non blocking protobuf service communication.Besides delivering a new AsyncRpcClient I would also like to ask the question what it would take to replace the current RpcClient? This would enable to simplify async code in some next issues.,2115
Extract Method,Heap occupancy based client pushback If the heap occupancy of a RegionServer is beyond a configurable high water mark (suggestions: 95% 98%) then we should reject all user RPCs and only allow administrative RPCs until occupancy has dropped below a configurable low water mark (suggestions: 92%). Implement building on the HBASE-5162 changes.It might be expensive to check heap occupancy in which case we can sample it periodically with a chore and use the last known value in pushback calculations.,2116
Rename Method,Let MetaScanner recycle a given Connection It is very heavy to create a Connection on each meta scan. Connections create RpcClients RpcClients create RPC connections and all cannot be recycled.Tests with a lot of metascans are very heavy with the async client.This issue is to make anything that uses metaScans reuse the same connection.,2117
Extract Method,"Replication fails to delete all corresponding zk nodes when peer is removed When removing a peer the client side will delete peerId under peersZNode node; then alive region servers will be notified and delete corresponding hlog queues under its rsZNode of replication. However if there are failed servers whose hlog queues have not been transferred by alive servers(this likely happens if setting a big value to ""replication.sleep.before.failover"" and lots of region servers restarted) these hlog queues won't be deleted after the peer is removed. I think remove_peer should guarantee all corresponding zk nodes have been removed after it completes; otherwise if we create a new peer with the same peerId with the removed one there might be unexpected data to be replicated.",2118
Rename Method,Don't transfer all the queued hlogs of a dead server to the same alive server When a region server is down(or the cluster restart) all the hlog queues will be transferred by the same alive region server. In a shared cluster we might create several peers replicating data to different peer clusters. There might be lots of hlogs queued for these peers caused by several reasons such as some peers might be disabled or errors from peer cluster might prevent the replication or the replication sources may fail to read some hlog because of hdfs problem. Then if the server is down or restarted another alive server will take all the replication jobs of the dead server this might bring a big pressure to resources(network/disk read) of the alive server and also is not fast enough to replicate the queued hlogs. And if the alive server is down all the replication jobs including that takes from other dead servers will once again be totally transferred to another alive server this might cause a server have a large number of queued hlogs(in our shared cluster we find one server might have thousands of queued hlogs for replication). As an optional way is it reasonable that the alive server only transfer one peer's hlogs from the dead server one time? Then other alive region servers might have the opportunity to transfer the hlogs of rest peers. This may also help the queued hlogs be processed more fast. Any discussion is welcome.,2119
Extract Method,Thrift should support next(nbRow) like functionality Currently the java hbase api support calling next(number_of_rows) where as the thrift interface doesn't. We have a patch to get this working internally.,2120
Extract Method,Parallel execution for Hbck checkRegionConsistency We have a lot of regions on our cluster ~500k and noticed that hbck took quite some time in checkAndFixConsistency(). [~davelatham] patched our cluster to do this check in parallel to speed things up. I'll attach the patch.,2121
Extract Method,Upgrade Jetty to 9.2.6 The Jetty component that is used for the HBase Stargate REST endpoint is version 6.1.26 and is fairly outdated. We recently had a customer inquire about enabling cross-origin resource sharing (CORS) for the REST endpoint and found that this older version does not include the necessary filter or configuration options highlighted at: http://wiki.eclipse.org/Jetty/Feature/Cross_Origin_FilterThe Jetty project has had significant updates through versions 7 8 and 9 including a transition to be an Eclipse subproject so updating to the latest version may be non-trivial. The last update to the Jetty component in https://issues.apache.org/jira/browse/HBASE-3377 was a minor version update and did not require significant work. This update will include a package namespace update so there will likely be a larger number of required changes. ,2122
Extract Method,HBCK should print status while scanning over many regions Running simple commands like hbck -summary on a large table can take some time. We should print some information to let it be known how things are progressing.,2124
Extract Method,When a new master comes up regionservers should continue with their region assignments from the last master After HBASE-1205 we can now handle a master going down and coming up somewhere else. When this happens the new master will scan everything and reassign all the regions which is not ideal. Instead of doing that we should keep the region assignments from the last master.,2125
Extract Method,Hbase Streaming Scan Feature A scan operation iterates over all rows of a table or a subrange of the table. The synchronous nature in which the data is served at the client side hinders the speed the application traverses the data: it increases the overall processing time and may cause a great variance in the times the application waits for the next piece of data.The scanner next() method at the client side invokes an RPC to the regionserver and then stores the results in a cache. The application can specify how many rows will be transmitted per RPC; by default this is set to 100 rows. The cache can be considered as a producer-consumer queue where the hbase client pushes the data to the queue and the application consumes it. Currently this queue is synchronous i.e. blocking. More specifically when the application consumed all the data from the cache --- so the cache is empty --- the hbase client retrieves additional data from the server and re-fills the cache with new data. During this time the application is blocked.Under the assumption that the application processing time can be balanced by the time it takes to retrieve the data an asynchronous approach can reduce the time the application is waiting for data.We attach a design document.We also have a patch that is based on a private branch and some evaluation results of this code.,2128
Extract Method,Progress heartbeats for long running scanners It can be necessary to set very long timeouts for clients that issue scans over large regions when all data in the region might be filtered out depending on scan criteria. This is a usability concern because it can be hard to identify what worst case timeout to use until scans are occasionally/intermittently failing in production depending on variable scan criteria. It would be better if the client-server scan protocol can send back periodic progress heartbeats to clients as long as server scanners are alive and making progress.This is related but orthogonal to streaming scan (HBASE-13071).,2129
Extract Method,[PE] Add being able to write many columns [~jonathan.lawlor] and I need to be able to test wide rows scanning. Want to make sure nothing broke when chunking patch comes in and we want to get some coarse perf numbers.,2130
Extract Method,[PERF] Reuse the IPCUtil#buildCellBlock buffer Running some scan profiling flight recorder was mildly fingering resize of the buffer allocated in IPCUtil#buildCellBlock as a point of contention. It was half-hearted blaming it for a few hundreds of ms over a five minute sampling with a few tens of instances showing.I tried then w/ flamegraph/lightweight profiler and this reported the buffer allocations taking 22% of our total CPU. See attachment trace.svg.I enabled TRACE-level logging on org.apache.hadoop.hbase.ipc.IPCUtil and indeed every allocation was doing a resize from initial allocation of 16k -- the default up to 220k (this test returns ten randomly sized rows zipfian sized between 0 and 8k).Upping the allocation to 220k meant we now avoided the resize but the initial allocation was now blamed for 10% of allocations (see trace.2.svg attached).Lets do buffer reuse. Will save a bunch of allocation and CPU.,2131
Extract Method,Bulk Loaded HFile Replication Currently we plan to use HBase Replication feature to deal with disaster tolerance scenario.But we encounter an issue that we will use bulkload very frequentlybecause bulkload bypass write path and will not generate WAL so the data will not be replicated to backup cluster. It's inappropriate to bukload twice both on active cluster and backup cluster. So i advise do some modification to bulkload feature to enable bukload to both active cluster and backup cluster,2134
Extract Method,Allow block cache to be external Allow an external service to provide the block cache. This has the nice property of allowing failover/upgrades to happen without causing a fully cold cache.Additionally this allows read replicas to share some of the same memory.,2135
Extract Method,Revisit the security auditing semantics. More specifically the following things need a closer look. (Will include more based on feedback and/or suggestions)* Table name (say test) instead of fully qualified table name(default:test) being used.* Right now we're using the scope to be similar to arguments for operation. Would be better to decouple the arguments for operation and scope involved in checking. For e.g. say for createTable we have the following audit log{code}Access denied for user esteban; reason: Insufficient permissions; remote address: /10.20.30.1; request: createTable; context: (user=srikanth@XXX scope=default action=CREATE){code}The scope was rightly being used as default namespace but we're missing out the information like operation params for CREATE which we used to log prior to HBASE-12511.Would love to hear inputs on this!,2136
Inline Method,Replace explicit HBaseAdmin creation with connection#getAdmin() 0,2137
Extract Method,"Add a debug-warn if we fail HTD checks even if table.sanity.checks is false HBASE-10591 introduced Sanity checks for table configuration in createTable.In HMaster.sanityCheckTableDescriptor() we skip all the checks if ""hbase.table.sanity.checks"" is true.for debuggability we should log a warn when table.sanity.checks is false.there are nice checks in there like no families maxFileSize limit flush size limit compression/encryption codec available TTL block size ... that will help debug what is going on",2138
Extract Method,"Expand TestSizeFailures to include small scans From Jonathan on HBASE-13335:{quote}It may also be a good idea to extend TestSizeFailures so that it also tests to ensure that all data is seen when the scan is small (e.g. perform that same scan near the end with but configure it with Scan.setSmall(true)). Even though that wouldn't be a ""small"" scan it would test to make sure the fix behaves as expected.{quote}",2139
Extract Method,New method in HTable.java to return start and end keys for regions in a table  I am writing a custom TableInputFormat to generate splits within a region and that needs both the start and end keys for the regions.,2140
Extract Method,move up to Thrift 0.2.0 Move HBase thrift bits up to Thrift 0.2.0 when it is released.,2141
Extract Method,SyncTable - rsync for HBase tables Given HBase tables in remote clusters with similar but not identical data efficiently update a target table such that the data in question is identical to a source table. Efficiency in this context means using far less network traffic than would be required to ship all the data from one cluster to the other. Takes inspiration from rsync.Design doc: https://docs.google.com/document/d/1-2c9kJEWNrXf5V4q_wBcoIXfdchN7Pxvxv1IO6PW0-U/,2142
Rename Method,Rename *column methods in MasterObserver to *columnFamily This being an interface makes it a bit harder on implementors. It'd be easier with Java8 and default implementations.We could either# add new *columnFamily methods and deprecate the old ones or# rename the existing ones without doing a deprecation first.Implementors would need to change their code in each of those cases. But because we have the {{BaseMasterObserver}} and {{BaseMasterAndRegionObserver}} it'd make things easier for people using those classes if we go with option 1. So that's my preference.The plan would be to add these methods in 2.0.0 and remove the old ones in 3.0.0.,2143
Rename Method,Add RegionLocator methods to Thrift2 proxy. Thrift2 doesn't provide the same functionality as the java client for getting region locations. We should change that.,2144
Extract Method,ImportTsv: Add dry-run functionality and log bad rows ImportTSV job skips bad records by default (keeps a count though). -Dimporttsv.skip.bad.lines=false can be used to fail if a bad row is encountered. To be easily able to determine which rows are corrupted in an input rather than failing on one row at a time seems like a good feature to have.Moreover there should be 'dry-run' functionality in such kinds of tools which can essentially does a quick run of tool without making any changes but reporting any errors/warnings and success/failure.To identify corrupted rows simply logging them should be enough. In worst case all rows will be logged and size of logs will be same as input size which seems fine. However user might have to do some work figuring out where the logs. Is there some link we can show to the user when the tool starts which can help them with that?For the dry run we can simply use if-else to skip over writing out KVs and any other mutations if present.,2145
Inline Method,Update Thrift to use compact/framed protocol TCompactProtocol/TFramedTransport and nonblocking server option promises better efficiency and performance improvements. Consider moving HBase Thrift bits to this when full platform support is ready for TCompactProtocol.,2146
Extract Method,Optimize FuzzyRowFilter FuzzyRowFilter has some room for improvements: a lot of byte-by-byte arithmetic non-efficient algorithm of selecting next candidate row etc.,2147
Move Method,Move static helper methods from KeyValue into CellUtils Add KeyValue.parseColumn() to CellUtils (also any other public static helper),2148
Extract Method,Run MiniCluster on top of other MiniDfsCluster Similar to how we don't start a mini-zk cluster when we already have one specified this will skip starting a mini-dfs cluster if the user specifies a different one.,2149
Extract Method,Use zookeeper multi to clear znodes in ZKProcedureUtil Address the TODO in ZKProcedureUtil clearChildZNodes() and clearZNodes methods,2150
Extract Method,Move Unsafe based operations to UnsafeAccess We have this new class added. Bytes.java having unsafe based reads in it. Now HBASE-13916 will add similar unsafe based reads (on BB) in ByteBufferUtils. We can move the unsafe based operations to UnsafeAccess and otehr places just refer to here.,2151
Extract Method,Stochastic Load Balancer JMX Metrics Today’s default HBase load balancer (the Stochastic load balancer) is cost function based. The cost function weights are tunable but no visibility into those cost function results is directly provided.A driving example is a cluster we have been tuning which has skewed rack size (one rack has half the nodes of the other few racks). We are tuning the cluster for uniform response time from all region servers with the ability to tolerate a rack failure. Balancing LocalityCost RegionReplicaRack Cost and RegionCountSkew Cost is difficult without a way to attribute each cost function’s contribution to overall cost. What this jira proposes is to provide visibility via JMX into each cost function of the stochastic load balancer as well as the overall cost of the balancing plan.,2152
Extract Method,Add configuration to skip validating HFile format when bulk loading When bulk loading millions of HFile into one HTable checking HFile format is the most time-consuming phase. Maybe we could use a parallel mechanism to increase the speed but when it comes to millions of HFiles it may still cost dozens of minutes. So I think it's necessary to add an option for advanced user to bulkload without checking HFile format at all. Of course the default value of this option should be true.,2153
Extract Method,Add write sniffing in canary Currently the canary tool only sniff the read operations it's hard to finding the problem in write path. To support the write sniffing we create a system table named '_canary_' in the canary tool. And the tool will make sure that the region number is large than the number of the regionserver and the regions will be distributed onto all regionservers.Periodically the tool will put data to these regions to calculate the write availability of HBase and send alerts if needed.,2154
Extract Method,Allow setting a richer state value when toString a pv2 Debugging my procedure after a crash was loaded out of the store and its state was RUNNING. It would help if I knew in which of the states of a StateMachineProcedure it was going to start RUNNING at.Chatting w/ Matteo he suggested allowing Procedures customize the String.Here is patch that makes it so StateMachineProcedure will now print out the base state -- RUNNING FINISHED -- followed by a ':' and then the StateMachineProcedure state: e.g. SimpleStateMachineProcedure state=RUNNABLE:SERVER_CRASH_ASSIGN,2155
Inline Method,Cleanup deprecated APIs from Cell class Cleanup deprecated APIs from Cell class,2156
Extract Method,"Changing internal structure of ImmutableBytesWritable contructor. The constructor for ImmutableBytesWritable that takes byte[] newData int offset  int length is copying the byte[] as soon as it get's it. Will change that so that it just ""points"" to the byte[] being passed in.",2157
Extract Method,StoreFile.passesKeyRangeFilter need not create Cells from the Scan's start and stop Row During profiling saw that the code here in passesKeyRangeFilter in Storefile{code}KeyValue smallestScanKeyValue = scan.isReversed() ? KeyValueUtil.createFirstOnRow(scan.getStopRow()) : KeyValueUtil.createFirstOnRow(scan.getStartRow());KeyValue largestScanKeyValue = scan.isReversed() ? KeyValueUtil.createLastOnRow(scan.getStartRow()) : KeyValueUtil.createLastOnRow(scan.getStopRow());{code}This row need not be copied now considering that we have CellComparator.compareRows(Cell byte[]). We have already refactored the firstKeyKv and lastKeyKV as part of other JIRAs.,2158
Extract Method,Client API for determining if server side supports cell level security Add a client API for determining if the server side supports cell level security. Ask the master assuming as we do in many other instances that the master and regionservers all have a consistent view of site configuration.Return {{true}} if all features required for cell level security are present {{false}} otherwise or throw {{UnsupportedOperationException}} if the master does not have support for the RPC call.,2160
Extract Method,remove duplicate code getTableDescriptor in HTable As TODO in comment said {{HTable.getTableDescriptor}} is same as {{HAdmin.getTableDescriptor}}. remove the duplicate code.,2161
Extract Method,Enhance Chaos Monkey framework by adding zookeeper and datanode fault injections. One of the shortcomings of existing ChaosMonkey framework is lack of fault injections for hbase dependencies like zookeeper hdfs etc. This patch attempts to solve this problem partially by adding datanode and zk node fault injections.,2162
Extract Method,Allow load balancer to operate when there is region in transition by adding force flag This issue adds boolean parameter force to 'balancer' command so that admin can force region balancing even when there is region in transition - assuming RIT being transient.This enhancement was requested by some customer.The assumption of this change is that the operator has run hbck and has a reasonable idea why regions are stuck in transition before using the force flag.There was a recent event at the customer where a cluster ended up with a small number of regionservers hosting most of the regions on the cluster (one regionserver had 50% of the roughly 20000 regions). The balancer couldn't be run due to the small number of regions that were stuck in transition. The admin ended up killing the regionservers so that reassignment would yield a more equitable distribution of the regions.On a different cluster there was a single store file that had corrupt HDFS blocks (the SSDs on the cluster were known to lose data). However since this single region (out of 10s of 1000s of regions on this cluster) was stuck in transition the balancer couldn't run.While the state keeping in HBase isn't so good yet that the admin can kick off the balancer automatically in such scenarios knowing when it is safe to do so and when it is not having this option available for the operator to use as he / she sees fit seems prudent.,2163
Extract Method,Metrics for block cache should take region replicas into account Currently metrics for block cache are aggregates in the sense that they don't distinguish primary from secondary / tertiary replicas.This JIRA separates the block cache metrics for primary region replica from the aggregate.,2164
Rename Method,Consolidate printUsage in IntegrationTestLoadAndVerify Investigating the use of {{itlav}} is a little screwy. Subclasses are not overriding the {{printUsage()}} methods correctly so you have to pass {{--help}} to get some info and no arguments to get the rest.{noformat}[hbase@ndimiduk-112rc2-7 ~]$ hbase org.apache.hadoop.hbase.test.IntegrationTestLoadAndVerify --helpusage: bin/hbase org.apache.hadoop.hbase.test.IntegrationTestLoadAndVerify <options>Options:-h--help Show usage-m--monkey <arg> Which chaos monkey to run-monkeyProps <arg> The properties file for specifying chaos monkey properties.-ncc--noClusterCleanUp Don't clean up the cluster at the end[hbase@ndimiduk-112rc2-7 ~]$ hbase org.apache.hadoop.hbase.test.IntegrationTestLoadAndVerifyIntegrationTestLoadAndVerify [-Doptions] <load|verify|loadAndVerify>Loads a table with row dependencies and verifies the dependency chainsOptions-Dloadmapper.table=<name> Table to write/verify (default autogen)-Dloadmapper.backrefs=<n> Number of backreferences per row (default 50)-Dloadmapper.num_to_write=<n> Number of rows per mapper (default 100000 per mapper)-Dloadmapper.deleteAfter=<bool> Delete after a successful verify (default true)-Dloadmapper.numPresplits=<n> Number of presplit regions to start with (default 40)-Dloadmapper.map.tasks=<n> Number of map tasks for load (default 200)-Dverify.reduce.tasks=<n> Number of reduce tasks for verify (default 35)-Dverify.scannercaching=<n> Number hbase scanner caching rows to read (default 50){noformat},2165
Extract Method,Scan different TimeRange for each column family At present the Scan API supports only table level time range. We have specific use cases that will benefit from per column family time range. (See background discussion at https://mail-archives.apache.org/mod_mbox/hbase-user/201508.mbox/%3CCAA4mzom00ef5eoXStK0HEtxebY8mQSs61GBVGttgpASpmhQHaw@mail.gmail.com%3E)There are a couple of choices that would be good to validate. First - how to update the Scan API to support family and table level updates. One proposal would be to add Scan.setTimeRange(byte family long minTime long maxTime) then store it in a Map<byte[] TimeRange>. When executing the scan if a family has a specified TimeRange then use it otherwise fall back to using the table level TimeRange. Clients using the new API against old region servers would not get the families correctly filterd. Old clients sending scans to new region servers would work correctly.The other question is how to get StoreFileScanner.shouldUseScanner to match up the proper family and time range. It has the Scan available but doesn't currently have available which family it is a part of. One option would be to try to pass down the column family in each constructor path. Another would be to instead alter shouldUseScanner to pass down the specific TimeRange to use (similar to how it currently passes down the columns to use which also appears to be a workaround for not having the family available).,2166
Extract Method,Incremental backup and bulk loading Currently incremental backup is based on WAL files. Bulk data loading bypasses WALs for obvious reasons breaking incremental backups. The only way to continue backups after bulk loading is to create new full backup of a table. This may not be feasible for customers who do bulk loading regularly (say every day).Here is the review board (out of date):https://reviews.apache.org/r/54258/In order not to miss the hfiles which are loaded into region directories in a situation where postBulkLoadHFile() hook is not called (bulk load being interrupted) we record hfile names thru preCommitStoreFile() hook.At time of incremental backup we check the presence of such hfiles. If they are present they become part of the incremental backup image.Here is review board:https://reviews.apache.org/r/57790/Google doc for design:https://docs.google.com/document/d/1ACCLsecHDvzVSasORgqqRNrloGx4mNYIbvAU7lq5lJE,2167
Extract Method,"Refine RegionGroupingProvider Phase-2: remove provider nesting and formalize wal group name Now we are nesting DefaultWALProvider inside RegionGroupingProvider which makes the logic ambiguous since a ""provider"" itself should provide logs. Suggest to directly instantiate FSHlog in RegionGroupingProvider.W.r.t wal group name now in RegionGroupingProvider it's using sth like ""<factoryId>\-null\-<randomUUID>"" which is quite long and unnecessary. Suggest to directly use ""<providerId>.<groupName>"".For more details please refer to the initial patch.",2168
Inline Method,Remove deprecated HBaseTestingUtility#deleteTable methods HBase has had the TableName APIs since 0.96 and our test code doesn't have privacy/stability markers so we should be able to remove the deprecated methods in 2.0.0.deleteTable has some separate unit test related cleanup.,2169
Rename Method,Remove deprecated HBaseTestCase dependency from TestHFile Remove dependency on long deprecated HBaseTestCase (0.90). modified references to tfiles which haven't been part of any modern hbase.,2170
Inline Method,Exorcise deprecated Put#add(...) and replace with Put#addColumn(...) The Put API changed from #add(...) to #addColumn(...). This updates all instances of it and removes it from the Put (which was added for hbase 1.0.0),2171
Inline Method,Batching in buffered mutator is awful when adding lists of mutations. When adding a list of mutations to the buffer the limit is checked after every mutation is added.This leads to lots of tries to flush.,2172
Extract Method,"Support a ""permissive"" mode for secure clusters to allow ""simple"" auth clients When implementing HBase security for an existing cluster it can be useful to support mixed secure and insecure clients while all client configurations are migrated over to secure authentication. We currently have an option to allow secure clients to fallback to simple auth against insecure clusters. By providing an analogous setting for servers we would allow a phased rollout of security:# First security can be enabled on the cluster servers with the ""permissive"" mode enabled# Clients can be converting to using secure authentication incrementally# The server audit logs allow identification of clients still using simple auth to connect# Finally when sufficient clients have been converted to secure operation the server-side ""permissive"" mode can be removed allowing completely secure operation.Obviously with this enabled there is no effective access control but this would still be a useful tool to enable a smooth operational rollout of security. Permissive mode would of course be disabled by default. Enabling it should provide a big scary warning in the logs on startup and possibly be flagged on relevant UIs.",2173
Extract Method,Expose checkAndMutate via Thrift2 Had a user ask why checkAndMutate wasn't exposed via Thrift2.I see no good reason (since checkAndPut and checkAndDelete are already there) so let's add it.,2174
Rename Method,Don't allow multi's to over run the max result size. If a user puts a list of tons of different gets into a table we will then send them along in a multi. The server un-wraps each get in the multi. While no single get may be over the size limit the total might be.We should protect the server from this. We should batch up on the server side so each RPC is smaller.,2175
Extract Method,Don't allow Multi to retain too many blocks Scans and Multi's have limits on the total size of cells that can be returned. However if those requests are not all pointing at the same blocks then the KeyValues can keep alive a lot more data than their size.Take the following example:A multi with a list of 10000 gets to a fat row. Each column being returned in in a different block. Each column is small 32 bytes or so.So the total cell size will be 32 * 10000 = ~320kb. However if each block is 128k then total retained heap size will be almost 2gigs.,2176
Extract Method,Region normalization should be allowed when underlying namespace has quota Currently when namespace has quota HMaster#normalizeRegions() skips the tables in the namespace.However performing region merge(s) wouldn't violate quota constraint.For region split NamespaceAuditor#checkQuotaToSplitRegion() can be called to check whether quota is about to be exceeded. If not region split plan can still be executed.,2177
Extract Method,Implement inexpensive seek operations in HFile When we early-out of a row because of columns versions filters etc... we seek to the end of that row one key at a time. We should do the seek at the HFile level in cases where we would end up skipping blocks in the process. This will be very common in cases with relatively large rows and regex row filters.If calls that end up doing nothing are constant time we could also call this to seek to the next column (or even a specific column in ExplicitTracker case).,2178
Extract Method,Intra-row scanning To continue scaling numbers of columns or versions in a single row we need a mechanism to scan within a row so we can return some columns at a time. Currently an entire row must come back as one piece.,2180
Extract Method,Cleanup HTable After putting up the javadocs for trunk I noticed a few missing deprecations and also that exists() was never deprecated though a new server-side implementation exists.This issue is to ensure everything is deprecated and to add a 880 compatible exists() while deprecating the old ones.Also I'm going to do my best to keep an updated javadoc until release: http://jgray.la/hbase/javadoc-0.20.0-trunk/,2181
Extract Method,Provide an option to skip calculating block locations for SnapshotInputFormat When a MR job is reading from SnapshotInputFormat it needs to calculate the splits based on the block locations in order to get best locality. However this process may take a long time for large snapshots. In some setup the computing layer Spark Hive or Presto could run out side of HBase cluster. In these scenarios the block locality doesn't matter. Therefore it will be great to have an option to skip calculating the block locations for every job. That will super useful for the Hive/Presto/Spark connectors. ,2182
Rename Method,"HBase should manage multiple node ZooKeeper quorum I thought there was already a JIRA for this but I cannot seem to find it.We need to manage multiple node ZooKeeper quorums (required for fully distributed option) in HBase to make things easier for users.Here's relevant IRC conversation with Ryan and Andrew:{code}Jun 17 18:14:39 <dj_ryan> right now we include our client deps in hbase/libJun 17 18:14:47 <dj_ryan> so removing zookeeper would be problematicJun 17 18:14:56 <dj_ryan> but hbase does put up a private zk quorumJun 17 18:15:02 <dj_ryan> it just doesnt bother with q>1Jun 17 18:15:05 <apurtell> dj_ryan nitay: agreed so that's why i wonder about a private zk quorum managed by hbaseJun 17 18:15:12 <apurtell> q ~= 5Jun 17 18:15:22 <dj_ryan> so maybe we should ship tools to manage itJun 17 18:15:23 <apurtell> if possibleJun 17 18:15:29 <dj_ryan> i can agree with thatJun 17 18:15:39 <nitay> apurtell ok i'd be happy to bump the priority of hbase managing full cluster and work on thatJun 17 18:15:47 * iand (n=iand@205.158.58.226.ptr.us.xo.net) has joined #hbaseJun 17 18:15:48 <apurtell> nitay: that would be awesomeJun 17 18:15:57 <apurtell> then i can skip discussions with cloudera about including zk alsoJun 17 18:16:12 <apurtell> and we can use some private ports that won't conflict with a typical zk installJun 17 18:16:15 <nitay> but i also think that users should be able to point at existing clusters so as long as your rpms are compatible it should be fineJun 17 18:16:23 <nitay> apurtell isn't hadoop going to start using ZKJun 17 18:16:31 <apurtell> nitay: agree but this is the cloudera-autoconfig-rpm (and deb) caseJun 17 18:16:34 <nitay> the cloudera dude was working on using it for namenode whatnot like we do for masterJun 17 18:16:35 <dj_ryan> so there are only 2 thingsJun 17 18:16:38 <dj_ryan> - set up myidsJun 17 18:16:38 <nitay> what are they doing for thatJun 17 18:16:40 <dj_ryan> - start zkJun 17 18:16:42 <dj_ryan> - stop zkJun 17 18:16:50 <dj_ryan> we dont want to start/stop zk just when we are doing a cluster bounceJun 17 18:16:51 <nitay> ye stupid myidsJun 17 18:16:52 <dj_ryan> you start it onceJun 17 18:16:54 <dj_ryan> and be done with tiJun 17 18:16:58 * iand (n=iand@205.158.58.226.ptr.us.xo.net) has left #hbase (""Leaving."")Jun 17 18:17:13 <apurtell> dj_ryan: yes start it once. that's what i do. works fine through many hbase restarts...Jun 17 18:17:28 <nitay> so then we need a separate shell cmd or something to stop zkJun 17 18:17:35 <nitay> and start on start-hbase if not already running type thingJun 17 18:17:43 <dj_ryan> yesJun 17 18:17:58 <nitay> okJun 17 18:18:19 <apurtell> with quorum peers started on nodes in conf/regionservers up to ~5 if possibleJun 17 18:18:37 <apurtell> but what about zoo.cfg?Jun 17 18:18:51 <nitay> oh i was thinking of having separate conf/zookeepersJun 17 18:18:58 <apurtell> nitay: even betterJun 17 18:18:59 <nitay> but we can use first five RS tooJun 17 18:19:26 <nitay> apurtell yeah so really there wouldnt be a conf/zookeepers i would rip out hostnames from zoo.cfgJun 17 18:19:38 <nitay> or go the other way generate zoo.cfg from conf/zookeepersJun 17 18:19:42 <nitay> gotta do one or the otherJun 17 18:19:49 <nitay> dont want to have to edit bothJun 17 18:19:54 <apurtell> nitay: rightJun 17 18:20:21 <apurtell> well...Jun 17 18:20:29 <nitay> zoo.cfg has the right info right now cause u need things other than just hostnames i.e. client and quorum portsJun 17 18:20:31 <apurtell> we can leave out servers from our default zoo.cfgJun 17 18:20:39 <apurtell> and consider a conf/zookeepersJun 17 18:20:47 <dj_ryan> i call it conf/zoosJun 17 18:20:54 <dj_ryan> in my zookeeper configJun 17 18:20:54 <dj_ryan> dirJun 17 18:20:57 <nitay> and then have our parsing of zoo.cfg insert themJun 17 18:21:08 <nitay> cause right now its all off java Properties anywaysJun 17 18:21:12 <apurtell> and let the zk wrapper parse the files if they exist and otherwise build the list of quorum peers like it does alreadyJun 17 18:21:34 <apurtell> so someone could edit either and it would dtrtJun 17 18:21:48 <nitay> apurtell yeah makes senseJun 17 18:21:58 <nitay> we can discuss getting rid of zoo.cfg completelyJun 17 18:22:12 <nitay> put it all in XML and just create a Properties for ZK off the right propsJun 17 18:22:14 <apurtell> for my purposes i just need some files available for a post install script to lay down a static hbase cluster config based on what it discovers about the hadoop installationJun 17 18:23:56 <apurtell> then i need to hook sysvinit and use chkconfig to enable/disable services on the cluster nodes according to their roles defined by hadoop/conf/masters and hadoop/conf/regionserversJun 17 18:24:13 <apurtell> so we put the hmaster on the namenodeJun 17 18:24:17 <apurtell> and the region servers on the datanodesJun 17 18:24:35 <apurtell> hadoop/conf/slaves i meanJun 17 18:24:44 <apurtell> and pick N hosts out of slaves to host the zk quorumJun 17 18:24:50 <apurtell> make sense?Jun 17 18:25:33 <nitay> yes i think so and u'll be auto generating the hbase configs for what servers run what then?Jun 17 18:25:50 <apurtell> nitay: yesJun 17 18:25:51 <nitay> which is why a simple line by line conf/zookeepers type file is clean and easyJun 17 18:25:57 <apurtell> nitay: agreeJun 17 18:25:59 <apurtell> so i think my initial question has been answered hbase will manage a private zk ensembleJun 17 18:26:07 <apurtell> ... somehowJun 17 18:26:10 <nitay> right :)Jun 17 18:26:15 <apurtell> ok thanks{code}",2183
Extract Method,ClusterStatus should be able to return responses by scope The current ClusterStatus response returns too much information about the load per region and replication cluster wide. Sometimes that response can be quite large (10s or 100s of MBs) and methods like getServerSize() or getRegionsCount() don't really need the full response. One possibility is to provide a scope (or filter) for the ClusterStatus requests to limit the response back to the client.,2184
Rename Method,Make SnapshotManager accessible through MasterServices See this comment for background:https://issues.apache.org/jira/browse/HBASE-15411?focusedCommentId=15209640&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15209640When procedure executes on master and performs snapshot the procedure needs to access SnapshotManager.This JIRA is to add accessor to MasterServices.,2185
Extract Method,Print Procedure WAL content Let's have a printer to print the content of Procedure WAL.,2186
Inline Method,Time limit of scanning should be offered by client In RSRpcServices.scan we will set a time limit equaling to Math.min(scannerLeaseTimeoutPeriod rpcTimeout) / 2 and will response heartbeat message if we reach this limit. However two timeout settings (hbase.client.scanner.timeout.period and hbase.rpc.timeout) are read from RS's configure which may be different from client's. If client's setting is much less than server's there may still be timeout at client side.,2187
Extract Method,Add provision for adding mutations to memstore or able to write to same region in batchMutate coprocessor hooks As part of PHOENIX-1734 we need to write the index updates to same region from coprocessors but writing from batchMutate API is not allowed because of mvcc. Raised PHOENIX-2742 to discuss any alternative way to write to the same region directly or not but not having any proper solution there.Currently we have provision to write wal edits from coprocessors. We can set wal edits in MiniBatchOperationInProgress.{noformat}/*** Sets the walEdit for the operation(Mutation) at the specified position.* @param index* @param walEdit*/public void setWalEdit(int index WALEdit walEdit) {this.walEditsFromCoprocessors[getAbsoluteIndex(index)] = walEdit;}{noformat}Similarly we can allow to write mutations from coprocessors to memstore as well. Or else we should provide the batch mutation API allow write in batch mutate coprocessors.,2190
Move Method,Remove PB references from Result DoubleColumnInterpreter and any such public facing class for 2.0 This is a sub-task for HBASE-15174.,2191
Inline Method,Report metrics from JvmPauseMonitor We have {{JvmPauseMonitor}} for detecting JVM pauses; pauses are logged at WARN. Would also be good to expose this information on a dashboard via metrics system -- make it easier to get this info off the host and into a central location for the operator.,2192
Extract Method,Add override mechanism for the exempt classes when dynamically loading table coprocessor As part of Hadoop's Timeline Service v.2 (YARN-2928) we're adding a table coprocessor (YARN-4062). However we're finding that the coprocessor cannot be loaded dynamically. A relevant snippet for the exception:{noformat}java.io.IOException: Class org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunCoprocessor cannot be loadedat org.apache.hadoop.hbase.master.HMaster.sanityCheckTableDescriptor(HMaster.java:1329)at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1269)at org.apache.hadoop.hbase.master.MasterRpcServices.createTable(MasterRpcServices.java:398)at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:42436)at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2031)at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:107)at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:130)at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:107)at java.lang.Thread.run(Thread.java:745)Caused by: java.io.IOException: Class org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunCoprocessor cannot be loadedat org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.testTableCoprocessorAttrs(RegionCoprocessorHost.java:324)at org.apache.hadoop.hbase.master.HMaster.checkClassLoading(HMaster.java:1483)at org.apache.hadoop.hbase.master.HMaster.sanityCheckTableDescriptor(HMaster.java:1327)... 8 moreCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunCoprocessorat java.net.URLClassLoader$1.run(URLClassLoader.java:366)at java.net.URLClassLoader$1.run(URLClassLoader.java:355)at java.security.AccessController.doPrivileged(Native Method)at java.net.URLClassLoader.findClass(URLClassLoader.java:354)at java.lang.ClassLoader.loadClass(ClassLoader.java:425)at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)at java.lang.ClassLoader.loadClass(ClassLoader.java:358)at org.apache.hadoop.hbase.util.CoprocessorClassLoader.loadClass(CoprocessorClassLoader.java:275)at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.testTableCoprocessorAttrs(RegionCoprocessorHost.java:322)... 10 more{noformat}We tracked it down to the fact that {{CoprocessorClassLoader}} regarding all hadoop classes as exempt from loading from the coprocessor jar. Since our coprocessor sits in the coprocessor jar and yet the loading of this class is delegated to the parent which does not have this jar the classloading fails.What would be nice is the ability to exclude certain classes from the exempt classes so that they can be loaded via table coprocessor classloader. See hadoop's {{ApplicationClassLoader}} for a similar feature.Is there any other way to load this coprocessor at the table scope?,2193
Extract Method,Move memcache to ConcurrentSkipListMap from ConcurrentSkipListSet The CSLM will replace old entry with a new when you put. The CSLS will NOT replace if existent key making for a test and if present remove semantic which to be safe needs synchronizing (Replacement is a Ryan Rawson suggestion).,2195
Extract Method,CellCounter improvements Looking at the CellCounter map reduce it seems like it can be improved in a few areas:* it does not currently support setting scan batching. This is important when we're fetching all versions for columns. Actually it would be nice to support all of the scan configuration currently provided in TableInputFormat.* generating job counters containing row keys and column qualifiers is guaranteed to blow up on anything but the smallest table. This is not usable and doesn't make any sense when the same counts are in the job output. The row and qualifier specific counters should be dropped.,2197
Extract Method,An endpoint-based export tool The time for exporting table can be reduced if we use the endpoint technique to export the hdfs files by the region server rather than by hbase client.In my experiments the elapsed time of endpoint-based export can be less than half of current export tool (enable the hdfs compression)But the shortcomings is we need to alter table for deploying the endpointany comments about this? thanks,2198
Extract Method,VerifyReplication prefix filtering VerifyReplication currently lets a user verify data within a time range has been replicated to a particular peer. It can be useful to verify only data that starts with particular prefixes. (An example would be an unsalted multi-tenant Phoenix table where you wish to only verify data for particular tenants.)Add a new option to the VerifyReplication job to allow for a list of prefixes to be given.,2202
Extract Method,Cleanup TestRegionServerMetrics Had a go through TestRegionServerMetrics when looking at HBASE-15929.Patch to use - @Before and @After to setup/teardown tables using @Rule to set table name based on testname.- Refactor out copy-pasted code fragments to single function.,2203
Extract Method,"New behavior of versions considering mvcc and ts rather than ts only In HBase book we have a section in Versions called ""Current Limitations"" see http://hbase.apache.org/book.html#_current_limitations{quote}28.3. Current Limitations28.3.1. Deletes mask PutsDeletes mask puts even puts that happened after the delete was entered. See HBASE-2256. Remember that a delete writes a tombstone which only disappears after then next major compaction has run. Suppose you do a delete of everything _ T. After this you do a new put with a timestamp _ T. This put even if it happened after the delete will be masked by the delete tombstone. Performing the put will not fail but when you do a get you will notice the put did have no effect. It will start working again after the major compaction has run. These issues should not be a problem if you use always-increasing versions for new puts to a row. But they can occur even if you do not care about time: just do delete and put immediately after each other and there is some chance they happen within the same millisecond.28.3.2. Major compactions change query results…_create three cell versions at t1 t2 and t3 with a maximum-versions setting of 2. So when getting all versions only the values at t2 and t3 will be returned. But if you delete the version at t2 or t3 the one at t1 will appear again. Obviously once a major compaction has run such behavior will not be the case anymore…_ (See Garbage Collection in Bending time in HBase.){quote}These limitations result from the current implementation on multi-versions: we only consider timestamp no matter when it comes; we will not remove old version immediately if there are enough number of new versions. So we can get a stronger semantics of versions by two guarantees:1 Delete will not mask Put that comes after it.2 If a version is masked by enough number of higher versions (VERSIONS in cf's conf) it will never be seen any more.Some examples for understanding:(delete t<=3 means use Delete.addColumns to delete all versions whose ts is not greater than 3 and delete t3 means use Delete.addColumn to delete the version whose ts=3)case 1: put t2 -> put t3 -> delete t<=3 -> put t1 and we will get t1 because the put is after delete.case 2: maxversion=2 put t1 -> put t2 -> put t3 -> delete t3 and we will always get t2 no matter if there is a major compaction because t1 is masked when we put t3 so t1 will never be seen.case 3: maxversion=2 put t1 -> put t2 -> put t3 -> delete t2 -> delete t3 and we will get nothing.case 4: maxversion=3 put t1 -> put t2 -> put t3 -> delete t2 -> delete t3 and we will get t1 because it is not masked.case 5: maxversion=2 put t1 -> put t2 -> put t3 -> delete t3 -> put t1 and we can get t3+t1 because when we put t1 at second time it is the 2nd latest version and it can be read.case 6:maxversion=2 put t3->put t2->put t1 and we will get t3+t2 just like what we can get now ts is still the key of versions.Different VERSIONS may result in different results even the size of result is smaller than VERSIONS(see case 3 and 4). So Get/Scan.setMaxVersions will be handled at end after we read correct data according to CF's VERSIONS setting.The semantics is different from the current HBase and we may need more logic to support the new semantic so it is configurable and default is disabled.",2204
Extract Method,A robust way deal with early termination of HBCK When HBCK is running we want to disable Catalog Janitor Balancer and Split/Merge. Today the implementation is not robust. If HBCK is terminated earlier by Control-C the changed state would not be reset to original. HBASE-15406 was trying to solve this problem for Split/Merge switch. The implementation is complicated and it did not solve CJ and Balancer. The proposal to solve the problem is to use a znode to indicate that the HBCK is running. CJ balancer and Split/Merge switch all look for this znode before doing it operation.,2205
Rename Method,Better documentation of ReplicationPeers Some of the ReplicationPeers interface's methods are not documented and are tied to a ZooKeeper implementation of ReplicationPeers. Also some method names are a little confusing.Review board at: https://reviews.apache.org/r/48696/,2206
Extract Method,RowCounter should support multiple key ranges Currently RowCounter only allows a single key range to be used as a filter. It would be useful in some cases to be able to specify multiple key ranges (or prefixes) in the same job. (For example counting over a set of Phoenix tenant ids in an unsalted table)This could be done by enhancing the existing key range parameter to take multiple start/stop row pairs. Alternately a new --row-prefixes option could be added similar to what HBASE-15847 did for VerifyReplication.,2208
Rename Method,Add comments to ProcedureStoreTracker 0,2209
Extract Method,Reduce the number of RPCs for the large PUTs This patch is proposed to reduce the number of RPC for the large PUTs The number and data size of write thread(SingleServerRequestRunnable) is a result of three main factors_1) The flush size taken by BufferedMutatorImpl#backgroundFlushCommits2) The limit of task number3) ClientBackoffPolicyA lot of requests created with less MUTATIONs is a result of two reason: 1) many regions of target table are in different server.2) flush size in step one is summed by “all” server rather than “individual” serverThis patch removes the limit of flush size in step one and add maximum size to submit for each server in the AsyncProcess,2210
Rename Method,"Improve performance for RPC encryption with Apache Common Crypto Hbase RPC encryption is enabled by setting “hbase.rpc.protection” to ""privacy"". With the token authentication it utilized DIGEST-MD5 mechanisms for secure authentication and data protection. For DIGEST-MD5 it uses DES 3DES or RC4 to do encryption and it is very slow especially for Scan. This will become the bottleneck of the RPC throughput.Apache Commons Crypto is a cryptographic library optimized with AES-NI. It provides Java API for both cipher level and Java stream level. Developers can use it to implement high performance AES encryption/decryption with the minimum code and effort. Compare with the current implementation of org.apache.hadoop.hbase.io.crypto.aes.AES Crypto supports both JCE Cipher and OpenSSL Cipher which is better performance than JCE Cipher. User can configure the cipher type and the default is JCE Cipher.",2211
Extract Method,check REPLICATION_SCOPE's value more stringently When create table or modify table the master will check if the value of REPLICATION_SCOPE is less than 0 however the value of REPLICATION_SCOPE must be 0 or 1. Otherwise will lead to regionserver shutdown so I think should be check the values of REPLICATION_SCOPE more stringent.Beginning I don't fully understand the usage of REPLICATION_SCOPE then set REPLICATION_SCOPE to 2 by mistake.when I insert data to tablethe regionservers abort one by onefinanlythe cluster abortthe exceptions as follow:{quote}2016-08-16 12:34:45245 WARN [regionserver/host:60023.append-pool1-t1] wal.FSHLog: Append sequenceId=94 requesting roll of WALjava.lang.NullPointerExceptionat org.apache.hadoop.hbase.protobuf.generated.WALProtos$FamilyScope$Builder.setScopeType(WALProtos.java:3939)at org.apache.hadoop.hbase.wal.WALKey.getBuilder(WALKey.java:618)at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.append(ProtobufLogWriter.java:118)at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1886)at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:1750)at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:1672)at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:128)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)at java.lang.Thread.run(Thread.java:744)2016-08-16 12:34:45293 INFO [MemStoreFlusher.0] regionserver.HStore: Added hdfs://hbase-test-27/hbase1.2.2/data/default/usertable/2aa98c17845c9c6d5c8760b87b3ba09a/i/35825c94e72945c0bf7df3f0adefa1b6 entries=1161600 sequenceid=59 filesize=167.6 M2016-08-16 12:34:45296 FATAL [MemStoreFlusher.0] regionserver.HRegionServer: ABORTING region server hbase-10-166-141-99600231471262434177: Replay of WAL required. Forcing server shutdownorg.apache.hadoop.hbase.DroppedSnapshotException: region: usertable1471262560009.2aa98c17845c9c6d5c8760b87b3ba09a.at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2427)at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2105)at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2067)at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1958)at org.apache.hadoop.hbase.regionserver.HRegion.flush(HRegion.java:1884)at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:510)at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:471)at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$900(MemStoreFlusher.java:75)at org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:259)at java.lang.Thread.run(Thread.java:744)Caused by: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=94 requesting roll of WALat org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1898)at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:1750)at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:1672)at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:128)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)... 1 moreCaused by: java.lang.NullPointerExceptionat org.apache.hadoop.hbase.protobuf.generated.WALProtos$FamilyScope$Builder.setScopeType(WALProtos.java:3939)at org.apache.hadoop.hbase.wal.WALKey.getBuilder(WALKey.java:618)at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.append(ProtobufLogWriter.java:118)at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1886)... 6 more{quote},2212
Extract Method,Add option to LoadIncrementalHFiles which allows skipping unmatched column families During my work on HBASE-15449 there came the need for LoadIncrementalHFiles not to bail out on unmatched column families.The use case is like this:* table has two column families A and B* full backup is taken on this table* column family B is dropped* more data is written to the table* incremental backup is performed on the table* user performs incremental restore using the latest backupSince column family B is gone LoadIncrementalHFiles would fail with IOException for family mismatch.This issue is to add an option to LoadIncrementalHFiles so that bulk load can load hfiles for the existing column families.,2215
Inline Method,Rewrite the delegation token tests with Parameterized pattern TestDelegationTokenWithEncryption and TestGenerateDelegationToken.,2216
Extract Method,Enhance LoadIncrementalHFiles API to accept store file paths as input Currently LoadIncrementalHFiles takes the directory (output path) as input parameter.In some scenarios (incremental restore of bulk loaded hfiles) the List of paths to hfiles is known.LoadIncrementalHFiles can take the List as input parameter and proceed with loading.,2217
Inline Method,expose more load information to the client side 0,2218
Extract Method,Add option for bulk load to always copy hfile(s) instead of renaming This is related to HBASE-14417 to support incrementally restoring to multiple destinations this issue adds option which would always copy hfile(s) during bulk load.,2219
Extract Method,Define the behavior of (default) empty FilterList Current empty FilterList filters all data because the FilterList#isFamilyEssential always returns false which causes the null cell retrieved by RegionScannerImpl.storeHeap.It seems to me that empty FilterList should do nothing because the following code is common.{noformat}private static Filter makeFilter() {  FilterList filterList = new FilterList ();  for (some conditions) {    // add some filters. Or nothing to add.  }  return filterList;}{noformat}If we keep the current logic which filters all data we should add enough comments to explain it. Or add the FilterList#size() or FilterList#empty() for preventing filtering all data.Any comments? Thanks.,2221
Extract Method,Refactor the org.apache.hadoop.hbase.client.Action a) According to the previous comments the Action doesn’t apply the generic.b) Action implements the Comparable<R> but the Action#compareTo cast the object to Action.For reasons outlined above we should refactor the Action.,2222
Extract Method,Expand Mob Compaction Partition policy from daily to weekly monthly Today the mob region holds all mob files for all regions. With daily partition mob compaction policy after major mob compaction there is still one file per region daily. Given there is 365 days in one year at least 365 files per region. Since HDFS has limitation for number of files under one folder this is not going to scale if there are lots of regions. To reduce mob file number we want to introduce other partition policies such as weekly monthly to compact mob files within one week or month into one file. This jira is create to track this effort.,2223
Extract Method,"Add TaskMonitor#getTasks() variant which accepts type selection In TaskMonitorTmpl.jamon :{code}List<? extends MonitoredTask> tasks = taskMonitor.getTasks();Iterator<? extends MonitoredTask> iter = tasks.iterator();// apply requested filterwhile (iter.hasNext()) {MonitoredTask t = iter.next();if (filter.equals(""general"")) {if (t instanceof MonitoredRPCHandler)iter.remove();{code}This means when user refreshes rs-status page regardless of the type of filter we always traverse and clone MonitoredTask's.getTasks() is synchronized :{code}public synchronized List<MonitoredTask> getTasks() {{code}A variant of getTasks() can be added which takes the type of filter so that unnecessary cloning is avoided.",2224
Rename Method,Add retry to LoadIncrementalHFiles tool As using the LoadIncrementalHFiles tool with S3 as the filesystem is prone to failing due to FileNotFoundExceptions due to inconsistency simple configurable retry logic was added.,2225
Rename Method,Refactor the AsyncProcess BufferedMutatorImpl and HTable The following are reasons of refactoring.# A update-heavy application for example loader creates many BufferedMutator for batch updates. But these BufferedMutators can’t share a large threadpool because the shutdown() method will be called when closing any BufferedMutator. This patch adds a flag into BufferedMutatorParams for preventing calling the shutdown() method in BufferedMutatorImpl#close# The AsyncProcess has the powerful traffic control but the control is against the single table currently. We should allow alternate traffic control implementation for advanced user who want more control.All suggestions are welcome.,2226
Extract Method,Separate small/large file delete threads in HFileCleaner to accelerate archived hfile cleanup speed When using PCIe-SSD the flush speed will be really quick and although we have per CF flush we still have the {{hbase.regionserver.optionalcacheflushinterval}} setting and some other mechanism to avoid data kept in memory for too long to flush small hfiles. In our online environment we found the single thread cleaner kept cleaning earlier flushed small files while large files got no chance which caused disk full then many other problems.Deleting hfiles in parallel with too many threads will also increase the workload of namenode so here we propose to separate large/small hfile cleaner threads just like we do for compaction and it turned out to work well in our cluster.,2227
Extract Method,Avoid compacting already compacted  mob files with _del files Today if there is only one file in the partition and there is no _del files the file is skipped. With del file the current logic is to compact the already-compacted file with _del file. Let's say there is one mob file regionA20161101*** which was compacted. On 12/1/2016 there is _del file regionB20161201**_del mob compaction kicks in regionA20161101*** is less than the threshold and it is picked for compaction. Since there is a _del file regionA20161101**** and regionB20161201***_del are compacted into regionA20161101**_1 . After that regionB20161201**_del cannot be deleted since it is not a allFile compaction. The next mob compaction regionA20161101**_1 and regionB20161201**_del will be picked up again and be compacted into regionA20161101***_2. So in this case it will cause more unnecessary IOs.,2228
Extract Method,Add mechanism to control hbase cleaner behavior Cleaner is used to get rid of archived HFiles and old WALs in HBase.In the case of heavy workload cleaner can affect query performance by creating a lot of connections to perform costly reads/ writes against underlying filesystem.This patch allows the user to control HBase cleaner behavior by providing shell commands to enable/ disable and manually run it.Our main intention with this patch was to avoid running the expensive cleaner chore during peak times. During our experimentation we saw a lot of HFiles and WAL log related files getting created inside archive dir (didn't see ZKlock related files). Since we were replacing hdfs with S3 these delete calls will take forever to complete.,2229
Rename Method,Add observer notification before bulk loaded hfile is moved to region directory Currently the postBulkLoadHFile() hook notifies the locations of bulk loaded hfiles.However if bulk load fails after hfile is moved to region directory but before postBulkLoadHFile() hook is called there is no way for pluggable components (replication - see HBASE-17290 backup / restore) to know which hfile(s) have been moved to region directory.Even if postBulkLoadHFile() is called in finally block the write (to backup table or zookeeper) issued from postBulkLoadHFile() may fail ending up with same situation.This issue adds a preCommitStoreFile() hook which notifies path of to be committed hfile before bulk loaded hfile is moved to region directory.With preCommitStoreFile() hook write (to backup table or zookeeper) can be issued before the movement of hfile.If write fails IOException would make bulk load fail not leaving hfile in region directory.,2230
Rename Method,Near-instantaneous online schema and table state updates We should not need to take a table offline to update HCD or HTD. One option for that is putting HTDs and HCDs up into ZK with mirror on disk catalog tables to be used only for cold init scenarios as discussed on IRC. In this scheme regionservers hosting regions of a table would watch permanent nodes in ZK associated with that table for schema updates and take appropriate actions out of the watcher. In effect schema updates become another item in the ToDo list.{{/hbase/tables/<table-name>/schema}}Must be associated with a write locking scheme also handled with ZK primitives to avoid situations where one concurrent update clobbers another.,2231
Extract Method,"Avoid busy waiting in ThrottledInputStream {code:title=ThrottledInputStream.java|borderStyle=solid}// We can calculate the precise sleep time instead of busy waitingprivate void throttle() throws IOException {while (getBytesPerSec() > maxBytesPerSec) {try {Thread.sleep(SLEEP_DURATION_MS);totalSleepTime += SLEEP_DURATION_MS;} catch (InterruptedException e) {throw new IOException(""Thread aborted"" e);}}}{code}",2232
Extract Method,Correct the semantic of  permission grant Currently HBase grant operation has following semantic:{code}hbase(main):019:0> grant 'hbase_tst' 'RW' 'ycsb'0 row(s) in 0.0960 secondshbase(main):020:0> user_permission 'ycsb'User NamespaceTableFamilyQualifier:Permission hbase_tst defaultycsb: [Permission:actions=READWRITE] 1 row(s) in 0.0550 secondshbase(main):021:0> grant 'hbase_tst' 'CA' 'ycsb'0 row(s) in 0.0820 secondshbase(main):022:0> user_permission 'ycsb'User NamespaceTableFamilyQualifier:Permission hbase_tst defaultycsb: [Permission: actions=CREATEADMIN] 1 row(s) in 0.0490 seconds{code} Later permission will replace previous granted permissions which confused most of HBase administrator.It's seems more reasonable that HBase merge multiple granted permission.,2233
Extract Method,avoid copy of family when initializing the FSWALEntry We should compare the families before cloning it.{noformat}Set<byte[]> familySet = Sets.newTreeSet(Bytes.BYTES_COMPARATOR);for (Cell cell : cells) {if (!CellUtil.matchingFamily(cell WALEdit.METAFAMILY)) {// TODO: Avoid this clone?familySet.add(CellUtil.cloneFamily(cell));}}{noformat},2235
Rename Method,Put writeToWAL methods do not have proper getter/setter names Put.writeToWAL() is the getter and Put.writeToWAL(boolean) is the setter.Should be: Put.getWriteToWAL() returning boolean and Put.setWriteToWAL(boolean),2236
Rename Method,Improve PerformanceEvaluation Current PerformanceEvaluation class have two problems:- It is not updated for hadoop-0.20.0. - The approach to split maps is not strict. Need to provide correct InputSplit and InputFormat classes. Current code uses TextInputFormat and FileSplit it is not reasonable.We will fix these problems.,2237
Extract Method,Backup: further optimizations Some phases of backup and restore can be optimized:# WALPlayer support for multiple tables# Run DistCp once per all tables during backup/restoreThe eventual goal:# 2 M/R jobs per backup/restore,2238
Extract Method,PE tool random read is not totally random Recently we were using the PE tool for doing some bucket cache related performance tests. One thing that we noted was that the way the random read works is not totally random.Suppose we load 200G of data using --size param and then we use --rows=500000 to do the randomRead. The assumption was among the 200G of data it could generate randomly 500000 row keys to do the reads.But it so happens that the PE tool generates random rows only on those set of row keys which falls under the first 500000 rows. This was quite evident when we tried to use HBASE-15314 in our testing. Suppose we split the bucket cache of size 200G into 2 files each 100G the randomReads with --rows=500000 always lands in the first file and not in the 2nd file. Better to make PE purely random.,2239
Rename Method,Add generic methods for updating metrics on start and end of a procedure execution For all procedures in Procv2 framework Procedure class can have generic methods to update metrics on start and end of a procedure execution. Specific procedure can override these and implement/ update respective metrics. Default implementation needs to be provided so override and implementation is optional.,2240
Extract Method,Make large/small file clean thread number configurable in HFileCleaner Currently we have only one thread for both large and small file cleaning but when write pressure is huge we might need more cleaner threads so we need to make the thread number configurable.We observed more than 1.8PB data in archive directory online due to business access rate change and this proposal is one of the necessary changes required.Default value of the configurations would still be left to 1 to keep low pressure to NN for normal case.,2241
Move Method,Share the same EventLoopGroup for NettyRpcServer NettyRpcClient and AsyncFSWALProvider at RS side Need a find a proper way to pass a EventLoopGroup instance through a Configuration object.,2244
Inline Method,Performance issue: ClientAsyncPrefetchScanner is slower than ClientSimpleScanner Copied the test result from HBASE-17994.{code}./bin/hbase org.apache.hadoop.hbase.PerformanceEvaluation --rows=100000 --nomapred scan 1./bin/hbase org.apache.hadoop.hbase.PerformanceEvaluation --rows=100000 --nomapred --asyncPrefetch=True scan 1{code}Mean latency.|| ||Test1|| Test2 || Test3 || Test4|| Test5|||scan| 12.21 | 14.32 | 13.25 | 13.07 | 11.83 ||scan with prefetch=True | 37.36 | 37.88 | 37.56 | 37.66 | 38.28 |,2245
Extract Method,Add RowMutations support to Batch RowMutations is multiple Puts and/or Deletes atomically on a single row. Current Batch call does not support RowMutations as part of the batch.We should add this missing part. We should be able to batch RowMutations.,2246
Rename Method,Expose BucketCache values to be configured BucketCache always uses the default values for all cache configuration. However this doesn't work for all use cases. In particular users want to be able to configure the percentage of the cache that is single access multi access and in-memory access.,2247
Extract Method,Update Htrace to 4.2 HTrace is not perfectly integrated into HBase the version 3.2.0 is buggy the upgrade to 4.x is not trivial and would take time. It might not worth to keep it in this state so would be better to remove it. Of course it doesn't mean tracing would be useless just that in this form the use of HTrace 3.2 might not add any value to the project and fixing it would be far too much effort. --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Based on the decision of the community we keep htrace now and update version ,2248
Rename Method,Refactor ClusterOptions before applying to code base So far ClusterStatus.Options is not so clean that can be applied to code base.Refactoring it before next move.,2249
Rename Method,Coprocessor Design Improvements follow up of HBASE-17732 Creating new jira to track suggestions that came in review (https://reviews.apache.org/r/62141/) but are not blocker and can be done separately.Suggestions by [~apurtell]- Change {{Service Coprocessor#getService()}} to {{List<Service> Coprocessor#getServices()}}- I think we overstepped by offering [table resource management via this interface|https://github.com/apache/hbase/blob/master/hbase-client/src/main/java/org/apache/hadoop/hbase/CoprocessorEnvironment.java#L57]. There are a lot of other internal resource types which could/should be managed this way but they are all left up to the implementor. Perhaps we should remove the table ref management and leave it up to them as well.----- Checkin the finalized design doc into repo (https://docs.google.com/document/d/1mPkM1CRRvBMZL4dBQzrus8obyvNnHhR5it2yyhiFXTg/edit) (fyi: [~stack])- Added example to javadoc of Coprocessor base interface on how to implement one in the new design,2250
Rename Method,Move the transform logic of FilterList into transformCell() method to avoid extra ref to question cell  As [~anoop.hbase] and I discussed we can implement the filterKeyValue () and transformCell() methods as following to avoid saving transformedCell & referenceCell state in FilterList and we can avoid the costly cell clone. {code} ReturnCode filterKeyValue(Cell c){ ReturnCode rc = null; for(Filter filter: sub-filters){ // ... rc = mergeReturnCode(rc filter.filterKeyValue(c)); // ... } return rc; } Cell transformCell(Cell c) throws IOException { Cell transformed = c; for(Filter filter: sub-filters){ if(filter.filterKeyValue(c) is INCLUDE*) { // ----> line#1 transformed = filter.transformCell(transformed); } } return transformed; } {code} For line #1 we need to remember the return code of the sub-filter for its filterKeyValue(). because only INCLUDE* ReturnCode we need to transformCell for sub-filter. A new boolean array will be introduced in FilterList. and the cost of maintaining the boolean array will be less than the cost of maintaining the two ref of question cell. ,2252
Extract Method,Improve the stability of splitting log when do fail over The way we splitting log now is like the following figure: !https://issues.apache.org/jira/secure/attachment/12905027/split-logic-old.jpg! The problem is the OutputSink will write the recovered edits during splitting log which means it will create one WriterAndPath for each region and retain it until the end. If the cluster is small and the number of regions per rs is large it will create too many HDFS streams at the same time. Then it is prone to failure since each datanode need to handle too many streams. Thus I come up with a new way to split log. !https://issues.apache.org/jira/secure/attachment/12905028/split-logic-new.jpg! We try to cache all the recovered edits but if it exceeds the MaxHeapUsage we will pick the largest EntryBuffer and write it to a file (close the writer after finish). Then after we read all entries into memory we will start a writeAndCloseThreadPool it starts a certain number of threads to write all buffers to files. Thus it will not create HDFS streams more than *_hbase.regionserver.hlog.splitlog.writer.threads_* we set. The biggest benefit is we can control the number of streams we create during splitting log it will not exceeds *_hbase.regionserver.wal.max.splitters * hbase.regionserver.hlog.splitlog.writer.threads_* but before it is *_hbase.regionserver.wal.max.splitters * the number of region the hlog contains_*. ,2253
Extract Method,ClassLoader that loads from hdfs; useful adding filters to classpath without having to restart services 0,2254
Extract Method,"Add a ""deferred log flush"" attribute to HTD At SU we have a case where we'd like to be able to by default sync every appended edit but for some tables have the control to disable their sync. You could then rely on other tables like .META. to do syncs the optional log syncer timer down in LogSyncer and user tables that are always synced. For example set a 10ms timer on the log syncer so that in the worse case you lose 10ms of non-catalog edits.",2255
Rename Method,Make CPEnv#getConnection return a facade that throws Unsupported if CP calls #close Follows from HBASE-19301 a suggestion by [~zghaobac]. To prevent a CP accidentally closing the connection returned by CpEnv#getConnection -- which returns the hosting Servers Connection -- we should throw UnsupportedException if the CP calls #close.... Do it.,2256
Inline Method,Move to using Apache commons CollectionUtils A bunch of unused code in CollectionUtils or code that can be found in Apache Commons libraries.,2257
Extract Method,Add EXCLUDE_NAMESPACE and EXCLUDE_TABLECFS support to replication peer config This is a follow-up issue after HBASE-16868. Copied the comments in HBASE-16868. This replicate_all flag is useful to avoid misuse of replication peer config. And on our cluster we have more config: EXCLUDE_NAMESPACE and EXCLUDE_TABLECFS for replication peer. Let me tell more about our use case. We have two online serve cluster and one offline cluster for MR/Spark job. For online cluster all tables will replicate to each other. And not all tables will replicate to offline cluster because not all tables need OLAP job. We have hundreds of tables and if only one table don't need replicate to offline cluster then you will config a lot of tables in replication peer config. So we add a new config option is EXCLUDE_TABLECFS. Then you only need config one table (which don't need replicate) in EXCLUDE_TABLECFS. Then when the replicate_all flag is false you can config NAMESPACE or TABLECFS means which namespace/tables need replicate to peer cluster. When replicate_all flag is true you can config EXCLUDE_NAMESPACE or EXCLUDE_TABLECFS means which namespace/tables can't replicate to peer cluster.,2258
Extract Method,Introduce builder for ReplicationPeerConfig and make it immutable Will introduce a new ReplicationPeerConfigBuilder. And deprecated the old set* methods in ReplicationPeerConfig. Make the ReplicationPeerConfig we give out be immutable.,2260
Extract Method,Revisit the methods in ReplicationPeerConfigBuilder Add 4 methods for ReplicationPeerConfigBuilder: putConfiguration putAllConfiguration putPeerData putAllPeerData Meanwhile remove setConfiuration and serPeerData from ReplicationPeerConfigBuilder. Because previous ReplicationPeerConfig didn't support setConfiuration and serPeerData. And previous code used getConfiguration.put or putAll to add configuration. So add methods to keep consistent with old usage. ,2261
Extract Method,The Put object has no simple read methods for checking what has already been added. Once data is added to a Put object there is no simple method equivalent to add() to tell what KeyValue objects have already been added to the familyMap. It would make sense to add a getter and a boolean method or 2 for ease of use to read/search the familyMap.,2262
Rename Method,RowMutations should follow the fluent pattern Other row ops including {{Put}} {{Delete}} {{Get}} {{Scan}} do have the fluent interface. Also Changing the return type from {{Void}} to {{RowMutations}} won't break the API BC (unless someone has interest in {{Void}} object...),2263
Inline Method,Revisit the timestamp usage in MetaTableAccessor It is totally a mess and makes me confusing when reimplementing the serial replication feature. Let me do a clean up first.,2264
Extract Method,Redesign single instance pool in CleanerChore 0,2265
Inline Method,When starting HQuorumPeer try to match on more than 1 address Many new users hit the issue that the default hostname returned by DNS.getDefaultHost is not the one they configured in hbase.zookeeper.quorum and it gets confusing to debug. Instead we should just try to match to any non-local address we can find.,2267
Extract Method,"AccessControlClient API Enhancement *Background:* Currently HBase ACLs can be retrieved based on the namespace or table name only. There is no direct API available to retrieve the permissions based on the namespace table name column family and column qualifier for specific user. Client has to write application logic in multiple steps to retrieve ACLs based on table name column name and column qualifier for specific user. HBase should enhance AccessControlClient APIs to simplyfy this. *AccessControlClient API should be extended with following APIs*     # To retrieve permissions based on the namespace table name column family and column qualifier for specific user.  Permissions can be retrieved based on the following inputs       - Namespace/Table (already available)       - Namespace/Table + UserName       - Table + CF       - Table + CF + UserName       - Table + CF + CQ       - Table + CF + CQ + UserName           Scope of retrieving permission will be as follows                 - Same as existing        2. To validate whether a user is allowed to perform specified operations on a particular table will be useful to check user privilege instead of getting ACD during client                                    operation.             User validation can be performed based on following inputs                  - Table + CF + CQ + UserName + Actions             Scope of validating user privilege                    User can perform self check without any special privilege but ADMIN privilege will be required to perform check for other users.                    For example suppose there are two users ""userA"" & ""userB"" then there can be below scenarios                        - when userA want to check whether userA have privilege to perform mentioned actions                                > userA don't need ADMIN privilege as it's a self query.                        - when userA want to check whether userB have privilege to perform mentioned actions                                > userA must have ADMIN or superuser privilege as it's trying to query for other user.",2268
Extract Method,Use Configuration instead of HBaseConfiguration  HBaseConfiguration extends Configuration but does not add any functionality to it. The only function is hashCode() which really should be refactored into Hadoop Configuration. I think in all the places(especially in the client side) HBase methods and classes should accept Configuration rather than HBaseConfiguration. The creation of the configuration with the right files (hbase-site and hbase-default) should not be encapsulated in a private method but in a public static one. The issues has arisen in our nutch+hbase patch for which we include both nutch configuration and hbase configurations. Moreover people may want to include separate project-specific configuration files to their configurations without the need to be dependent on the HBaseConfiguration.,2269
Extract Method,Alternate indexed hbase implementation; speeds scans by adding indexes to regions rather secondary tables PurposeThe goal of the indexed HBase contrib is to speed up scans by indexing HBase columns. Indexed HBase (IHbase) is different from the indexed tables in transactional HBase (ITHbase): while the indexes in ITHBase are in fact hbase tables using the indexed column's values as row keys IHbase creates indexes at the region level. The differences are summarized in below.+ global orderingITHBase: yesIHBase: noComment: IHBase has an index for each region. The flip side of not having global ordering is compatibility with the good old HRegion: results are coming back in row order (and not value order as in THBase)+ Full table scan?ITHBase: noIHBase: noComment: ITHbase does a partial scan on the index table. IHbase supports specifying start/end rows to limit the number of scanned regions+ Multiple Index UsageITHBase: noIHBase: yesComment: IHBase can take advantage of multiple indexes in the same scan. IHBase IdxScan object accepts an Expression which allows intersection/ unison of several indexed column criteria+ Extra disk storageITHBase: yesIHBase: noComment: IHbase indexes are created when the region starts/flushes and do not require any extra storage+ Extra RAMITHBase: yesIHBase: yesComment: IHbase indexes are in memory and hence increase the memory overhead. THbase indexes increase the number of regions each region server has to support thus costing memory too+ Parallel scanning supportITHBase: noIHBase: yesIn ITHbase the index table needs to be consulted and then GETs are issued for each matching row. The behavior of IHBase (as perceived by the client) is no different than a regular scan and hence supports parallel scanning seamlessly. parallel GET can be implemented to speedup ITHbase scansWhy IHbase should outperform ITHBase1. More flexible: a. Supports range queries and multi-index queries b. Supports different types - not only byte arrays2. Less overhead: ITHbase pays at least two 'table roundtrips' - one for the index table and the other for the main table3. Quicker index expression evaluation: IHBase is using dedicated index data structures while ITHbase is using the regular HRegion scan facilitiesImplementation notes• Only index Storefiles.Every index scan performs a full memstore scan. Indexing the memstore will be implemented only if scanning the memstore will prove to be a performance bottleneck• Index expression evaluation is performed using bit sets.There are two types of bitsets: compressed and expanded. An index will typically store a compressed bitset while an expression evaluator will most probably use an expanded bitset+ TODOThis patch changes some some of hbase core so can instantiate other than default HRegion. Fixes bugs in filter too.Would like to add this as a contrib. package on 0.20 branch in time for 0.20.3 if possible.,2270
Rename Method,Improve snapshot manifest copy in ExportSnapshot ExportSnapshot need to copy snapshot manifest to destination cluster first then setOwner and setPermission for those paths. But it's done with one thread which lead to a long time to submit the job if your snapshot is big. I tried to make them processing in parallel which can reduce the total time of submitting dramatically.,2271
Extract Method,B&R: Delete command enhancements Make the command more useable. Currently user needs to provide list of backup ids to delete. It would be nice to have more convenient options such as: deleting all backups which are older than XXX days etc,2272
Extract Method,[Auth] Support keytab login in hbase client There're lots of questions about how to connect to kerberized hbase cluster through hbase-client api from user-mail and slack channel. {{hbase.client.keytab.file}} and {{hbase.client.keytab.principal}} are already existed in code base but they are only used in {{Canary}}. This issue is to make use of two configs to support client-side keytab based login after this issue resolved hbase-client should directly connect to kerberized cluster without changing any code as long as {{hbase.client.keytab.file}} and {{hbase.client.keytab.principal}} are specified.,2273
Extract Method,Separate region server report requests to new handlers In master rpc scheduler all rpc requests are executed in a thread pool. This task separates rs report requests to new handlers.,2274
Extract Method,Separate the config of block size when we do log splitting and write Hlog Since the block size of recovered edits and hlog are the same right now if we set a large value to block size name node may not able to assign enough space when we do log splitting. But set a large value to hlog block size can help reduce the number of region server asking for a new block. Thus I think separate the config of block size is necessary.,2277
Extract Method,HBaseTestingUtility::startMiniCluster() to use builder pattern Currently there are 13 {{startMiniCluster()}} methods to set up a mini cluster. I'm not surprised if we have a few more in future. It's good to support different combination of optional parameters. We have to pick up one of them carefully while still wondering the default values of other parameters; if we add a new option we may bring more new methods. One solution is to use builder pattern: create a class {{MiniClusterOptions}} along with a static class {{MiniClusterOptionsBuilder}} create a new method {{startMiniCluster(MiniClusterOptions)}}. In {{master}} we delete the old 13 methods while in branch-2 we deprecate the old 13 methods. Thoughts?,2278
Rename Method,Improve Snapshot Performance with Temporary Snapshot Directory when rootDir on S3 When using Apache HBase the snapshot feature can be used to make a point in time recovery. To do this HBase creates a manifest of all the files in all of the Regions so that those files can be referenced again when a user restores a snapshot. With HBase's S3 storage mode developers can store their data off-cluster on Amazon S3. However utilizing S3 as a file system is inefficient in some operations namely renames. Most Hadoop ecosystem applications use an atomic rename as a method of committing data. However with S3 a rename is a separate copy and then a delete of every file which is no longer atomic and in fact quite costly. In addition puts and deletes on S3 have latency issues that traditional filesystems do not encounter when manipulating the region snapshots to consolidate into a single manifest. When HBase on S3 users have a significant amount of regions puts deletes and renames (the final commit stage of the snapshot) become the bottleneck causing snapshots to take many minutes or even hours to complete. The purpose of this patch is to increase the overall performance of snapshots while utilizing HBase on S3 through the use of a temporary directory for the snapshots that exists on a traditional filesystem like HDFS to circumvent the bottlenecks.,2279
Extract Method,Add ability for HBase Canary to ignore a configurable number of ZooKeeper down nodes When running org.apache.hadoop.hbase.tool.Canary with args -zookeeper -treatFailureAsError the Canary will try to get a znode from each ZooKeeper server in the ensemble. If any server is unavailable or unresponsive the canary will exit with a failure code. If we use the Canary to gauge server health and alert accordingly this can be too strict. For example in a 5-node ZooKeeper cluster having one node down is safe and expected in rolling upgrades/patches. This is a request to allow the Canary to take another parameter {code:java} -permittedZookeeperFailures <N>{code} If N=1 in the 5-node ZooKeeper ensemble example then the Canary will still pass if 4 ZooKeeper nodes are reachable but fail if 3 or fewer are reachable. (This is my first Jira posting... sorry if I messed anything up.),2280
Rename Method,Save on a few log strings and some churn in wal splitter by skipping out early if no logs in dir Trivial change to splitlogmanager that saves us a log line at least per WAL dir when it goes to split. Also saves some not-needed churn in SLM.,2281
Extract Method,Refactor RegionMover 1. Move connection and admin to RegionMover's member variables. No need create connection many times. 2. use try-with-resource to reduce code 3. use ServerName instead of String 4. don't use Deprecated method 5. remove duplicate code ......,2282
Extract Method,Hooks for replication This issue is about getting all the hooks for mdc replication in core HBase.,2283
Extract Method,Add html version of default hbase-site.xml  In the hadoop getting started page - there is a html version of the default configuration (of core-site.xml / hdfs-site.xml / mapred-site.xml ) . It would be useful to have such a page for hbase-default.xml for easy reference without flipping the editors. patch creates a html file from the default hbase-default.xml usign xslt. new target - conf.gen.html - added. sample run $ ant conf.gen.htmlBuildfile: build.xmlinit:conf.gen.html:[xslt] Processing /opt/workspace/remote-ws/hbase/conf/hbase-default.xml to /opt/workspace/remote-ws/hbase/build/hbase-site.html[xslt] Loading stylesheet /opt/workspace/remote-ws/hbase/docs/conf2html.xsl[echo] HTML Format of default HBase configuration available at /opt/workspace/remote-ws/hbase/build/hbase-site.htmlBUILD SUCCESSFUL,2284
Inline Method,HRS should report to master when HMsg are available It still takes a lot of time for the client to see splits or just regions that move around with default PE it takes around 4 seconds and creating a table takes a bit more than 2 seconds. I remember having the discussion with Stack that HRS.run was not suppose to sleep if any message to send. Turns out it does sleep.,2285
Extract Method,MR to copy a table As discussed in HBASE-2197 we need a way to copy a table from one cluster to another. This requires creating the job itself and modifying TOF.,2286
Rename Method,"HFile and Memstore should maintain minimum and maximum timestamps In order to fix HBASE-1485 and HBASE-29 it would be very helpful to have HFile and Memstore track their maximum and minimum timestamps. This has the following nice properties:- for a straight Get if an entry has been already been found with timestamp X and X >= HFile.maxTimestamp the HFile doesn't need to be checked. Thus the current fast behavior of get can be maintained for those who use strictly increasing timestamps but ""correct"" behavior for those who sometimes write out-of-order.- for a scan the ""latest timestamp"" of the storage can be used to decide which cell wins even if the timestamp of the cells is equal. In essence rather than comparing timestamps instead you are able to compare tuples of (row timestamp storage.max_timestamp)- in general min_timestamp(storage A) >= max_timestamp(storage B) if storage A was flushed after storage B.",2288
Extract Method,[stargate] PerformanceEvaluation A version of PE that works with Stargate. Patch includes a number of fixes for multiuser mode and the client library also.,2289
Rename Method,Concurrent flushers in HLog sync using HDFS-895 HDFS-895 changes hflush() to be able to run concurrently from multiple threads where flushes can be concurrent with further writes to the same file.We need to rip out/amend the group commit code a bit to take advantage of this.,2291
Extract Method,Improvements to prewarm META cache on clients A couple different use cases cause storms of reads to META during startup. For example a large MR job will cause each map task to hit meta since it starts with an empty cache.A couple possible improvements have been proposed:- MR jobs could ship a copy of META for the table in the DistributedCache- Clients could prewarm cache by doing a large scan of all the meta for the table instead of random reads for each miss- Each miss could fetch ahead some number of rows in META,2292
Extract Method,Add to admin create table start and end key params and desired number of regions This would be an adornment on create table that pre-creates N regions in the new table. It came up yesterday at the hbase hackathon3.,2293
Extract Method,"Allow record filtering with selected row key values in HBase Export It is desirable to add record filtering capability to HBase Export.The following code is an example (s is the Scan):byte [] prefix = Bytes.toBytes(args[5]);if (args[5].startsWith(""^"")){s.setFilter(new RowFilter(CompareOp.EQUAL new RegexStringComparator(args[5])));}else s.setFilter(new PrefixFilter(prefix));",2294
Extract Method,During reads when passed the specified time range seek to next column When we are processing the stream of KeyValues in the ScanQueryMatcher we will check the timestamp of the current KV against the specific TimeRange. Currently we only check if it is in the range or not returning SKIP if outside the range or continuing to other checks if within the range.The check should actually return SKIP if the stamp is greater than the TimeRange and NEXT_COL if the stamp is less than the TimeRange (we know we won't take anymore columns from the current column once we are below the TimeRange).,2295
Extract Method,Cleanup arrays vs Lists of scanners Arrays and Lists are used inconsistently for working with sets of scanners throughout KeyValueHeap MemStore StoreScanner etc. We should pick one and use it consistently.,2296
Extract Method,Add the ability to easily extend some HLog actions I'd like to add the ability to extend some HLog actions like log rolling and log archiving using listeners. One could be specified on HLog creation and others could be added later on.,2297
Extract Method,"Make the hlog file names unique Currently the HLog archiving thread adds another timestamp to ensure the file names are ""unique"". This is ugly and makes it harder to track the HLogs movements from outside (like replication). Instead we could use a UUID as the file name. I was discussing with Stack the need of keeping a timestamp in the file name... I think we can't get rid of it since we need to read the HLogs in sequence when splitting.",2298
Extract Method,Harmonize the Get and Delete operations In my work on HBASE-2400 implementing deletes for the Avro server felt quite awkward. Rather than the clean API of the Get object which allows restrictions on the result set from a row to be expressed with addColumn addFamily setTimeStamp setTimeRange setMaxVersions and setFilters the Delete object hides these semantics behind various constructors to deleteColumn[s] an deleteFamily. From my naive vantage point I see no reason why it would be a bad idea to mimic the Get API exactly though I could quite possibly be missing something. Thoughts?,2299
Rename Method,speed up REST tests In the meantime before HBASE-2564 the REST tests could be a lot faster than they are currently.,2300
Extract Method,"Compaction requests should be prioritized to prevent blocking While testing the write capacity of a 4 machine hbase cluster we were getting long and frequent client pauses as we attempted to load the data. Looking into the problem we'd get a relatively large compaction queue and when a region hit the ""hbase.hstore.blockingStoreFiles"" limit it would get block the client and the compaction request would get put on the back of the queue waiting for many other less important compactions. The client is basically stuck at that point until a compaction is done. Prioritizing the compaction requests and allowing the request that is blocking other actions go first would help solve the problem.You can see the problem by looking at our log files:You'll first see an event such as a too many HLog which will put a lot of requests on the compaction queue.{noformat}2010-05-25 10:53:26570 INFO org.apache.hadoop.hbase.regionserver.HLog: Too many hlogs: logs=33 maxlogs=32; forcing flush of 22 regions(s): responseCountsRS_6eZzLtdwhGiTwHy1274232223324 responsesRS_0qhkL5rUmPCbx3K-12742130572421274513189592 responsesRS_1ANYnTegjzVIsHW-12742177419211274511001873 responsesRS_1HQ4UG5BdOlAyuE-12742167574251274726323747 responsesRS_1Y7SbqSTsZrYe7a-12743286978381274478031930 responsesRS_1ZH5TB5OdW4BVLm-12742162398941274538267659 responsesRS_3BHc4KyoM3q72Yc-12742905469871274502062319 responsesRS_3ra9BaBMAXFAvbK-12742145799581274381552543 responsesRS_6SDrGNuyyLd3oR6-12742199411551274385453586 responsesRS_8AGCEMWbI6mZuoQ-12743068574291274319602718 responsesRS_8C8T9DN47uwTG1S-12742153817651274289112817 responsesRS_8J5wmdmKmJXzK6g-12742995938611274494738952 responsesRS_8e5Sz0HeFPAdb6c-12742886414591274495868557 responsesRS_8rjcnmBXPKzI896-12743069816841274403047940 responsesRS_9FS3VedcyrF0KX2-12742459713311274754745013 responsesRS_9oZgPtxO31npv3C-12742140277691274396489756 responsesRS_a3FdO2jhqWuy37C-12742092286601274399508186 responsesRS_a3LJVxwTj29MHVa-12742{noformat}Then you see the too many log files:{noformat}2010-05-25 10:53:31364 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction requested for region responses-index--1274799047787--R_cBKrGxx0FdWjPso1274804575862/783020138 because: regionserver/192.168.0.81:60020.cacheFlusher2010-05-25 10:53:32364 WARN org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Region responses-index--1274799047787--R_cBKrGxx0FdWjPso1274804575862 has too many store files putting it back at the end of the flush queue.{noformat}Which leads to this: {noformat}2010-05-25 10:53:27061 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 60 on 60020' on region responses-index--1274799047787--R_cBKrGxx0FdWjPso1274804575862: memstore size 128.0m is >= than blocking 128.0m size2010-05-25 10:53:27061 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 84 on 60020' on region responses-index--1274799047787--R_cBKrGxx0FdWjPso1274804575862: memstore size 128.0m is >= than blocking 128.0m size2010-05-25 10:53:27065 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 1 on 60020' on region responses-index--1274799047787--R_cBKrGxx0FdWjPso1274804575862: memstore size 128.0m is >= than blocking 128.0m size{noformat}Once the compaction / split is done a flush is able to happen which unblocks the IPC allowing writes to continue. Unfortunately this process can take upwards of 15+ minutes (the specific case shown here from our logs took about 4 minutes).",2301
Extract Method,"Make HBASE-2694 replication-friendly HBASE-2694 did a good bit of reworking around ZK and removed/changed some pieces that were needed for replication to work correctly. Mainly 2 things:- listZnodes needs to offer a version that takes a Watcher else registering yourself as a listener is too much of a pain since you then filter a lot of stuff based on the path of the event/- A lot more important the new Multiton implemented in ZKW prevents from starting multiple clusters inside the same JVM since both masters would use the same ""name"".I will provide a patch for both issues.",2302
Extract Method,Utilize ROWCOL bloom filter if multiple columns within same family are requested in a Get Noticed the following snippet in StoreFile.java:Scanner:shouldSeek():{code}switch(bloomFilterType) {case ROW:key = row;break;case ROWCOL:if (columns.size() == 1) {byte[] col = columns.first();key = Bytes.add(row col);break;}//$FALL-THROUGH$default:return true;}{code}If columns.size > 1 then we currently don't take advantage of the bloom filter. We should optimize this to check bloom for each of columns and if none of the columns are present in the bloom avoid opening the file.,2303
Extract Method,hbck should have the ability to repair basic problems Right now the hbck utility can detect issues with region deployment but can't fix them.It should be able to handle basic things like closing one side of a double assignment re-adding something to META etc.,2304
Rename Method,Do some small cleanups in org.apache.hadoop.hbase.regionserver.wal Since i am touching this area its probably better to leave it in a cleaner state. Non deprecated etc,2305
Extract Method,HLog preparation and cleanup are done under the updateLock major slowdown Something I've seen quite often in our production environment:{quote}2010-08-16 16:17:27104 INFO org.apache.hadoop.hbase.regionserver.HLog: removing old hlog file /hbase/.logs/rs22600201280909840873/hlog.dat.1282000385321 whose highest sequence/edit id is 648370799502010-08-16 16:17:27286 INFO org.apache.hadoop.hbase.regionserver.HLog: removing old hlog file /hbase/.logs/rs22600201280909840873/hlog.dat.1282000392770 whose highest sequence/edit id is 648370882602010-08-16 16:17:27452 INFO org.apache.hadoop.hbase.regionserver.HLog: removing old hlog file /hbase/.logs/rs22600201280909840873/hlog.dat.1282000399300 whose highest sequence/edit id is 648370965662010-08-16 16:17:27635 INFO org.apache.hadoop.hbase.regionserver.HLog: removing old hlog file /hbase/.logs/rs22600201280909840873/hlog.dat.1282000406997 whose highest sequence/edit id is 648371048652010-08-16 16:17:27827 INFO org.apache.hadoop.hbase.regionserver.HLog: removing old hlog file /hbase/.logs/rs22600201280909840873/hlog.dat.1282000413803 whose highest sequence/edit id is 648371131532010-08-16 16:17:27993 INFO org.apache.hadoop.hbase.regionserver.HLog: removing old hlog file /hbase/.logs/rs22600201280909840873/hlog.dat.1282000421709 whose highest sequence/edit id is 648371214672010-08-16 16:17:28160 INFO org.apache.hadoop.hbase.regionserver.HLog: removing old hlog file /hbase/.logs/rs22600201280909840873/hlog.dat.1282000427333 whose highest sequence/edit id is 648371297752010-08-16 16:17:28432 INFO org.apache.hadoop.hbase.regionserver.HLog: removing old hlog file /hbase/.logs/rs22600201280909840873/hlog.dat.1282000434365 whose highest sequence/edit id is 648371380742010-08-16 16:17:28518 INFO org.apache.hadoop.hbase.regionserver.HLog: removing old hlog file /hbase/.logs/rs22600201280909840873/hlog.dat.1282000440347 whose highest sequence/edit id is 648371463762010-08-16 16:17:28612 WARN org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 39 on 60020 took 1801ms appending an edit to hlog; editcount=02010-08-16 16:17:28615 WARN org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 37 on 60020 took 1804ms appending an edit to hlog; editcount=12010-08-16 16:17:28615 WARN org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 25 on 60020 took 1805ms appending an edit to hlog; editcount=2...2010-08-16 16:17:28619 WARN org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 41 on 60020 took 1875ms appending an edit to hlog; editcount=502010-08-16 16:17:28619 WARN org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 24 on 60020 took 1876ms appending an edit to hlog; editcount=512010-08-16 16:17:28619 WARN org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 48 on 60020 took 1881ms appending an edit to hlog; editcount=54{quote}And looking at HLog.rollWriter we roll then cleanup those unused hlog files under updateLock which blocks all the appenders (as shown). We should only do the first part under that lock,2307
Move Method,"Add Thread-Local Behavior To HTable Pool It is a well-documented fact that the HBase table client (viz. HTable) is not thread-safe. Hence the recommendation has been to use a HTablePool or a ThreadLocal to manage access to tables. The downside of the latter is that it (a) requires the user to reinvent the wheel in terms of mapping table names to tables and (b) forces the user to maintain the thread-local objects. Ideally it would be nice if we could make the HTablePool handle thread-local objects as well. That way it not only becomes the ""one stop shop"" for all client-side tables but also insulates the user from the ThreadLocal object.Here we propose a way to generalize the HTablePool so that the underlying pool type is either ""reusable"" or ""thread-local"". To make this possible we introdudce the concept of a SharedMap which essentially maps a key to a collection of values the elements of which are managed by a pool. In effect that collection acts as a shared pool of resources access to which is closely controlled as dictated by the particular semantics of the pool.Furthermore to simplify the construction of HTablePools we added a couple of parameters (viz. ""hbase.client.htable.pool.type"" and ""hbase.client.hbase.pool.size"") to control the default behavior of a HTablePool.In case the size of the pool is set to a non-zero positive number that is used to cap the number of resources that a pool may contain for any given key. A size of Integer#MAX_VALUE is interpreted to mean an unbounded pool.Currently the SharedMap supports the following types of pools:* A ThreadLocalPool which represents a pool that builds on the ThreadLocal class. It essentially binds the resource to the thread from which it is accessed.* A ReusablePool which represents a pool that builds on the LinkedList class. It essentially allows resources to be checked out at which point it is (temporarily) removed from the pool. When the resource is no longer required it should be returned to the pool in order to be reused.* A RoundRobinPool which represents a pool that stores its resources in an ArrayList. It load-balances access to its resources by returning a different resource every time a given key is looked up.",2308
Extract Method,Increment multiple columns in a row at once Currently there is no way to do multiple increments to a single row in one RPC. This jira is about adding an HTable and HRegionInterface method to increment multiple columns within a single row at once.,2309
Extract Method,Make bulk assignment on cluster startup run faster Currently as of HBASE-3018 we come up with a bulk assignment plan that is sorted by server. We then spawn a thread to assign out the regions per server so we are assigning in parallel. This works but is still slow enough (It looks to be slower than the old assignment where we'd do lumps of N regions at a time). We should be able to pass a regionserver all the regions to open in one RPC. We need to figure how to keep up zk state while regionserver is processing a big lot of regions. This looks a little awkward to do since currently open handler just opens region -- there is no notion of doing a ping while waiting to run.Being able to start the cluster fast is important for those times we take it down to do major upgrade; the longer it takes to spin up the longer our 'downtime'.,2310
Inline Method,Add ability to have multiple ZK servers in a quorum in MiniZooKeeperCluster for test writing Interesting things can happen when you have a ZK quorum of multiple servers and one of them dies. Doing testing here on clusters this has turned up some bugs with HBase interaction with ZK.Would be good to add the ability to have multiple ZK servers in unit tests and be able to kill them individually.,2311
Extract Method,Allow to disable automatic shipping of dependency jars for mapreduce jobs Since HBASE-3001 TableMapReduceUtil.initTableMap/ReduceJob will automatically upload the HBase jars needed to execute a map reduce job.In my case I am building a job jar using Maven's assembly plugin this way all the necessary dependencies are in the job jar. So in such case the default behavior of HBase causes some needless upload work. It also uploads hadoop-core itself which is not necessary.Therefore I propose to add a variant of the initTableMap/ReduceJob methods with an extra boolean argument to disable the automatic adding of dependency jars.I will attach a patch with the proposed change.Note that everything works as is this is just an optimization.,2313
Extract Method,[rest] content transcoding We have a reasonable user request for support for decoding of base64 encoded values into raw/binary when servicing GET requests with an Accept header of {{application/octet-stream}}. We can introduce a table schema attribute for column families that instructs the REST gateway to perform input and/or output transcoding with base64->binary (for GET) and vice versa (for PUT or POST) as the first supported option. ,2314
Extract Method,[replication] Add the ability to enable/disable streams This jira was initially in the scope of HBASE-2201 but was pushed out since it has low value compared to the required effort (and when want to ship 0.90.0 rather soonish).We need to design a way to enable/disable replication streams in a determinate fashion.,2315
Extract Method,Review document and fix up Regions-in-Transition timeout logic In some of the testing Stack and I have been doing we've uncovered some issues with concurrent RS failure and when the Master is under heavy load. It's led to situations where we handle ZK events far after they actually occur and have uncovered some issues in our timeout logic.This jira is about reviewing the timeout semantics especially around ZK usage and ensuring that we handle things appropriately.,2317
Extract Method,[rest] Filter for gzip/deflate content encoding that wraps both input and output side After HBASE-3275 the REST gateway will return gzip or deflate encoded content to the client if the client requested it using the appropriate Accept-Encoding header. However Jetty's GzipFilter only wraps output side processing. A client can submit gzip or deflate encoded requests (i.e. Content-Encoding: gzip ; Content-Type: ...) but the data is not decoded it is simply passed through. Implement a filter that also wraps input side processing so clients can submit compressed PUT or POST bodies.,2319
Extract Method,Max Compaction Size Add ability to specify a maximum storefile size for compaction. After this limit we will not include this file in compactions. This is useful for large object stores and clusters that pre-split regions.,2320
Extract Method,Allow round-robin distribution for table created with multiple regions We can distribute the initial regions created for a new table in round-robin fashion.,2321
Extract Method,Admin API: Explicit Split Points Add the ability to explicitly split an existing region at a user-specified point. Currently you can disable automated splitting and can presplit a newly-created table at explicit boundaries but cannot explicitly bound a split of an existing region.,2322
Extract Method,Allow HBaseRpcMetrics to register custom interface methods Opened from comments on HBASE-2997. James Kennedy notes:{quote}HBaseRpcMetrics is now logging a WARN message every time it encounters an unregistered RPC method.In my case I now get huge log files filled with these warnings because the hbase-trx transactional extension of HBase uses a subclass of HRegionServer that adds new interface methods.It's easy enough to tell log4j to ignore HBaseRpcMetrics output.However it would be nice if the Server/HRegionServer HBaseRpcMetrics mechanism was more extensible so I could pass down new interfaces or grab the HBaseRpcMetrics object to add interfaces from up top...{quote}{{HBaseRpcMetrics}} already has a public method {{createMetrics(Class)}} to register method counters. We just need a way to expose the metrics class to allow the region server subclass to call it -- add a {{getMetrics()}} method to {{RpcServer}} and {{HBaseServer}}.,2323
Rename Method,Remove the KV copy of every KV in Scan; introduced by HBASE-3232 Here is offending code from inside in StoreScanner#next:{code}// kv is no longer immutable due to KeyOnlyFilter! use copy for safetyKeyValue copyKv = new KeyValue(kv.getBuffer() kv.getOffset() kv.getLength());{code}This looks wrong given philosophy up to this has been avoidance of garbage-making copies.Maybe this has been looked into before and this is the only thing to be done but why is KeyOnlyFilter not making copies rather than mutating originals?Making this critical against 0.92.,2324
Extract Method,"HFileOutputFormat to use column family's compression algorithm HFileOutputFormat currently creates HFile writer's using a compression algorithm set as configuration ""hbase.hregion.max.filesize"" with default as no compression. The code does not take into account the compression algorithm configured for the table's column family. As a result bulk uploaded tables are not compressed until a major compaction is run on them. This could be fixed by using the column family descriptors while creating HFile writers.",2326
Extract Method,HFile CLI Improvements Add some miscellaneous improvements to the HFile CLI for improved debugging of HFiles.1. Option to show only Keys in the HFile2. Option to display Block Index3. Support inspecting HFiles on a remote HDFS cluster,2327
Extract Method,"Improve the selection of regions to balance Currently LoadBalancer goes through the list of regions per RS and grabs the few first ones to balance. This is not bad but that list is often sorted naturally since the a RS that boots will open the regions in a sequential and sorted order (since it comes from .META.) which means that we're balancing regions starting in an almost sorted fashion.We discovered that because one of our internal users created a new table starting with letter ""p"" which has now grown to 100 regions in the last few hours and they are all served by 1 region server. Looking at the master's log the balancer has moved as many regions from that region server but they are all from the same table that starts with letter ""a"" (and the regions that were moved all come one after the other).The part of the code that should be modified is:{code}for (HRegionInfo hri: regions) {// Don't rebalance meta regions.if (hri.isMetaRegion()) continue; regionsToMove.add(new RegionPlan(hri serverInfo null));numTaken++;if (numTaken >= numToOffload) break;}{code}",2329
Extract Method,Improve RegionSplitter Performance When running RegionSplitter on a 100-node cluster with 900 regions (and plenty of data) the utility took around 72 hours to run. Analysis revealed two major bottlenecks:1. We are serialized on the logical split (i.e. waiting for the split request to be registered). Parallelizing this step will align configured and actual outstanding splits.2. Outstanding splits are modeled like a queue. Changing this to a list with a scanner will allow handling splits that finish out of order.,2330
Extract Method,Expose per-region request rate metrics We currently export metrics on request rates for each region server and this can help with identifying uneven load at a high level. But once you see a given server under high load you're forced to extrapolate based on your application patterns and the data it's serving what the likely culprit is. This can and should be much easier if we just exported request rate metrics per-region on each server.Dynamically updating the metrics keys based on assigned regions may pose some minor challenges but this seems a very valuable diagnostic tool to have available.,2331
Rename Method,Distinguish read and write request count in region Distinguishing read and write request counts on top of HBASE-3507 would benefit load balancer.The action for balancing read vs. write load should be different. For read load region movement should be low (to keep scanner happy). For write load region movement is allowed.Now that we have cheap(er) counters it should not be too burdensome keeping up the extra count.,2332
Extract Method,NMapInputFormat should use a different config param for number of maps Annoyingly the MR local runner drops the mapred.map.tasks parameter before running a job. Should use a different config parameter so we can specify it.,2335
Extract Method,Some improvements to Hbck to test the entire region chain in Meta and provide better error reporting The current Hbck tool will miss some inconsistencies in Meta and in other cases will detect an issue but does not provide much in the way of useful feedback. * Incorporate the full region chain tests (similar to check_meta.rb). I.e. look for overlaps holes and cycles. I believe check_meta.rb will be redundant after this change.* More unit tests and better tests that will test the actual error discovered instead of just errors true/false.* In the case of overlaps and holes output both ends of the broken chain.* Previous implementation runs check() twice. This is inefficient and more importantly reports redundant errors which could be confusing to the user.,2336
Extract Method,Clean up CompressionTest to not directly reference DistributedFileSystem Right now CompressionTest has a number of issues:- it always writes to the home directory of the user regardless of the path provided- it requires actually writing to HDFS when a local file is probably sufficient,2337
Move Method,Eliminate use of ThreadLocals for CoprocessorEnvironment bypass() and complete() In the current coprocessor framework ThreadLocal objects are used for the bypass and complete booleans in CoprocessorEnvironment. This allows the *CoprocessorHost implementations to identify when to short-circuit processing the the preXXX and postXXX hook methods.Profiling the region server however shows that these ThreadLocals can become a contention point when on a hot code path (such as prePut()). We should refactor the CoprocessorHost pre/post implementations to remove usage of the ThreadLocal variables and replace them with locally scoped variables to eliminate contention between handler threads.,2338
Extract Method,"Redefine Identity Of HBase Configuration Judging from the javadoc in {{HConnectionManager}} sharing connections across multiple clients going to the same cluster is supposedly a good thing. However the fact that there is a one-to-one mapping between a configuration and connection instance kind of works against that goal. Specifically when you create {{HTable}} instances using a given {{Configuration}} instance and a copy thereof we end up with two distinct {{HConnection}} instances under the covers. Is this really expected behavior especially given that the configuration instance gets cloned a lot?Here I'd like to play devil's advocate and propose that we ""deep-compare"" {{HBaseConfiguration}} instances so that multiple {{HBaseConfiguration}} instances that have the same properties map to the same {{HConnection}} instance. In case one is ""concerned that a single {{HConnection}} is insufficient for sharing amongst clients"" to quote the javadoc then one should be able to mark a given {{HBaseConfiguration}} instance as being ""uniquely identifiable"".Note that ""sharing connections makes clean up of {{HConnection}} instances a little awkward"" unless of course you apply the change described in HBASE-3766.",2339
Extract Method,"Add facility to track currently progressing actions/workflows A lot of troubleshooting involves answering the question ""well what is your server doing right now?"" Today that involves some combination of interpreting jstack output and/or trudging through logs. Problems with these methods are: (a) users may not have direct ssh access to regionserver machines in production environments (b) logs are very verbose so hard to separate what's still going on vs stuff that might have completed and (c) interpreting jstack requires a pretty good knowledge of the codebase plus diving into source code.I'd like to add a singleton (for now) which takes care of tracking any major actions going on in the region server and master.",2341
Rename Method,Refactor Coprocessor Compaction API After HBASE-3797 the compaction logic flow has been significantly altered. Because of this the current compaction coprocessor API is insufficient for gaining full insight into compaction requests/results. Refactor coprocessor API after HBASE-3797 is committed to be more extensible and increase visibility.,2342
Extract Method,enhance HBase RPC to support free-ing up server handler threads even if response is not ready In the current implementation the server handler thread picks up an item from the incoming callqueue processes it and then wraps the response as a Writable and sends it back to the IPC server module. This wastes thread-resources when the thread is blocked for disk IO (transaction logging read into block cache etc).It would be nice if we can make the RPC Server Handler threads pick up a call from the IPC queue hand it over to the application (e.g. HRegion) the application can queue it to be processed asynchronously and send a response back to the IPC server module saying that the response is not ready. The RPC Server Handler thread is now ready to pick up another request from the incoming callqueue. When the queued call is processed by the application it indicates to the IPC module that the response is now ready to be sent back to the client.The RPC client continues to experience the same behaviour as before. A RPC client is synchronous and blocks till the response arrives.This RPC enhancement allows us to do very powerful things with the RegionServer. In future we can make enhance the RegionServer's threading model to a message-passing model for better performance. We will not be limited by the number of threads in the RegionServer.,2343
Extract Method,Schedule all log-spliiting at startup all at once When distributed log splitting is enabled then it is better to call splitLog() for all region servers simultaneously. A large number of splitlog tasks will get scheduled - one for each log file. But a splitlog-worker (region server) executes only one task at a time and there shouldn't be a danger of DFS overload. Scheduling all the tasks at once ensures maximum parallelism.,2344
Rename Method,HLog Pretty Printer We currently have a rudimentary way to print HLog data but it is limited and currently prints key-only information. We need extend this functionality similar to how we developed HFile's pretty printer. Ideas for functionality:- filter by sequence_id- filter by row / region- option to print values in addition to key info- option to print output in JSON format (so scripts can easily parse for analysis),2345
Extract Method,HMaster.createTable could be heavily optimized Looking at the createTable method in HMaster (the one that's private) we seem to be very inefficient:- We set the enabled flag for the table for every region (should be done only once).- Every time we create a new region we create a new HLog and then close it (reuse one instead or see if it's really necessary).- We do one RPC to .META. per region (we should batch put).This should provide drastic speedups even for those creating tables with just 50 regions.,2347
Extract Method,"Users should be able to choose custom TableInputFormats without modifying TableMapReduceUtil.initTableMapperJob(). Currently org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.initTableMapperJob() forces any Hbase job to use the default TableInputFormat.class as the job's input format class.""job.setInputFormatClass(TableInputFormat.class);"" ==> This line is included in initTableMapperJob().This restriction causes users to modify initTableMapperJob() in addition to implementing their own TableInputFormat. It would be nicer if users can use custom TableInputFormats without additionally tampering with HBase source code.",2349
Extract Method,[Coprocessors] Support configuration of coprocessor at load time Users should be able to pass configuration options to coprocessors. These options should be applied at load time.- For system coprocessors just use the configuration of the container. - Extend table coprocessor load specification to allow arbitrary key-value pairs at the end (<key>=<value>{}).,2350
Extract Method,Usability improvement to HTablePool To improve the usability of the HTablePool the implementation should not rely on the user returning the connection to the pool but rather do that transparently when user closes the HTableImplementation it got.To do that a HTableImplementation proxy implementation should be returned that wraps a HTable object and holds a reference to the pool. When the client close the proxy it will actually automatically return the wrapped HTable back in pool to be reused. In this case the method HTablePool.putTable don't need to be public,2351
Extract Method,"Implement HBase version of ""show processlist"" One of the features that our DBAs use for MySQL analysis is ""show processlist"" which gives application-level stats about the RPC threads. Right now we use jstack but that is very core-developer-centric. We need to create a similar tool that DBA/Ops/AppDevs can use.",2353
Rename Method,Extend the WALActionsListener API to accomodate log archival The WALObserver interface exposes the log roll events. It would be nice to extend it to accomodate log archival events as well.,2355
Rename Method,Exposing HBase Filters to the Thrift API Currently to use any of the filters one has to explicitly add a scanner for the filter in the Thrift API making it messy and long. With this patch I am trying to add support for all the filters in a clean way. The user specifies a filter via a string. The string is parsed on the server to construct the filter. More information can be found in the attached document named Filter LanguageThis patch is trying to extend and further the progress made by the patches in the HBASE-1744 JIRA (https://issues.apache.org/jira/browse/HBASE-1744),2356
Extract Method,Add Per-Column Family Metrics Right now we have region server level statistics. However the read/write flow varies a lot based on the column family involved. We should add dynamic per column family metrics to JMX so we can track each column family individually.,2358
Extract Method,Need a flush by regionserver rather than by table option This evening needed to clean out logs on the cluster. logs are by regionserver. to let go of logs we need to have all edits emptied from memory. only flush is by table or region. We need to be able to flush the regionserver. Need to add this.,2360
Extract Method,AssignmentManager debug logs should be at INFO level for META/ROOT regions The master debug logs are quite verbose so people don't usually have them all on. But when trying to debug why a cluster's in a bad state due to META or ROOT issues it's really helpful to have this info. So for any transition info for these important regions we should always log details at INFO level.,2361
Extract Method,"Add a debugging dump servlet to the master and regionserver For debugging clusters it would be nice to have a single text-only page that can be ""curled"" and used for debugging. This servlet will include the following info:- build version- monitored task status- server list- recently aborted region servers (see HBASE-4275)- regions in transition- executor pool status (see HBASE-4281)- stack trace of all threads- configuration- last N KB of the server's logs",2363
Extract Method,Performance: Scanners and getRow return maps with duplicate data Right now whenever we get back multiple cells worth of data at a time we do so in a map of HStoreKey->byte[]. This means that there is a duplicated Text row and long timestamp at the very least between every cell. This is quite a bit wasted. It also means we have to do a lot of translation every time. We could create a new Writable that contains just one row one timestamp and a map of Text->byte[].,2364
Rename Method,Update Thrift to 0.7.0 The new version of Thrift is 0.7.0 and it has features and bug fixes that could be useful to include in the next release of HBase.,2365
Extract Method,Add a decent heuristic for region size A few of us were brainstorming this morning about what the default region size should be. There were a few general points made:- in some ways it's better to be too-large than too-small since you can always split a table further but you can't merge regions currently- with HFile v2 and multithreaded compactions there are fewer reasons to avoid very-large regions (10GB+)- for small tables you may want a small region size just so you can distribute load better across a cluster- for big tables multi-GB is probably best,2366
Extract Method,HBaseAdmin.assign() does not use force flag The HBaseAdmin.assign(){code}public void assign(final byte [] regionName final boolean force)throws MasterNotRunningException ZooKeeperConnectionException IOException {getMaster().assign(regionName force);}{code}In the HMaster we call {code}Pair<HRegionInfo ServerName> pair =MetaReader.getRegion(this.catalogTracker regionName);if (pair == null) throw new UnknownRegionException(Bytes.toString(regionName));if (cpHost != null) {if (cpHost.preAssign(pair.getFirst() force)) {return;}}assignRegion(pair.getFirst());if (cpHost != null) {cpHost.postAssign(pair.getFirst() force);}{code}The force flag is not getting used. May be we need to update the javadoc or do not provide the force flag as a parameter if we are not going to use it.,2367
Extract Method,Move block cache parameters and references into single CacheConf class From StoreFile down to HFile we currently use a boolean argument for each of the various block cache configuration parameters that exist. The number of parameters is going to continue to increase as we look at compressed cache delta encoding and more specific L1/L2 configuration. Every new config currently requires changing many constructors because it introduces a new boolean.We should move everything into a single class so that modifications are much less disruptive.,2368
Rename Method,Provide access to RpcServer instance from RegionServerServices In some cases RegionObserver coprocessors may want to directly access the running RpcServer instance on the region server. For token based authentication for example this is needed for a coprocessor to interact with the SecretManager that validates authentication tokens in the secure RPC engine. With the addition of async call handling on the server-side this becomes additionally important if coprocessors want to send back delayed responses to clients. In this case the coprocessor would need to be able to call RpcServer.getCurrentCall() to send back the response.So I propose we add access to the RpcServer in RegionServerServices:{code}/*** Returns a reference to the region server's RPC server*/public RpcServer getRpcServer();{code}We can simultaneously drop the existing RegionServerServices.getRpcMetrics() method since this could then be accessed via RpcServer.getRpcMetrics().,2369
Extract Method,The increment operation can release the rowlock before sync-ing the Hlog This allows for better throughput when there are hot rows.I have seen this change make a single row update improve from 400 increments/sec/server to 4000 increments/sec/server.,2370
Inline Method,"[hbck] Improve region map output HBASE-4375 added a region coverage visualization to hbck in details mode. When users have binary row keys the output is difficult to parse (awk/sed) or pull into programs (numeric excel) capable of handling tsv formatted data.This patch * improves output by using Bytes.toStringBinary (which escapes binary) instead of Bytes.toString when printing keys * suggests some repair actions and * collects ""problem group"" that groups regions that are overlapping.",2371
Extract Method,The put operation can release the rowlock before sync-ing the Hlog This allows for better throughput when there are hot rows. A single row update improves from 100 puts/sec/server to 5000 puts/sec/server.,2372
Inline Method,Ability to specify a custom start/end to RegionSplitter HBASE-4489 changed the default endKey on HexStringSplit from 7FFF... to FFFF... While this is correct existing users of 0.90 RegionSplitter have 7FFF as the end key in their schema and the last region will not split properly under this new code. We need to let the user specify a custom start/end key range for when situations like this arise. Optimally we should also write the start/end key in META so we could figure this out implicitly instead of requiring the user to explicitly specify it.,2374
Extract Method,Prefix Compression - Trie data block encoding The HBase data block format has room for 2 significant improvements for applications that have high block cache hit ratios. First there is no prefix compression and the current KeyValue format is somewhat metadata heavy so there can be tremendous memory bloat for many common data layouts specifically those with long keys and short values.Second there is no random access to KeyValues inside data blocks. This means that every time you double the datablock size average seek time (or average cpu consumption) goes up by a factor of 2. The standard 64KB block size is ~10x slower for random seeks than a 4KB block size but block sizes as small as 4KB cause problems elsewhere. Using block sizes of 256KB or 1MB or more may be more efficient from a disk access and block-cache perspective in many big-data applications but doing so is infeasible from a random seek perspective.The PrefixTrie block encoding format attempts to solve both of these problems. Some features:* trie format for row key encoding completely eliminates duplicate row keys and encodes similar row keys into a standard trie structure which also saves a lot of space* the column family is currently stored once at the beginning of each block. this could easily be modified to allow multiple family names per block* all qualifiers in the block are stored in their own trie format which caters nicely to wide rows. duplicate qualifers between rows are eliminated. the size of this trie determines the width of the block's qualifier fixed-width-int* the minimum timestamp is stored at the beginning of the block and deltas are calculated from that. the maximum delta determines the width of the block's timestamp fixed-width-intThe block is structured with metadata at the beginning then a section for the row trie then the column trie then the timestamp deltas and then then all the values. Most work is done in the row trie where every leaf node (corresponding to a row) contains a list of offsets/references corresponding to the cells in that row. Each cell is fixed-width to enable binary searching and is represented by [1 byte operationType X bytes qualifier offset X bytes timestamp delta offset].If all operation types are the same for a block there will be zero per-cell overhead. Same for timestamps. Same for qualifiers when i get a chance. So the compression aspect is very strong but makes a few small sacrifices on VarInt size to enable faster binary searches in trie fan-out nodes.A more compressed but slower version might build on this by also applying further (suffix etc) compression on the trie nodes at the cost of slower write speed. Even further compression could be obtained by using all VInts instead of FInts with a sacrifice on random seek speed (though not huge).One current drawback is the current write speed. While programmed with good constructs like TreeMaps ByteBuffers binary searches etc it's not programmed with the same level of optimization as the read path. Work will need to be done to optimize the data structures used for encoding and could probably show a 10x increase. It will still be slower than delta encoding but with a much higher decode speed. I have not yet created a thorough benchmark for write speed nor sequential read speed.Though the trie is reaching a point where it is internally very efficient (probably within half or a quarter of its max read speed) the way that hbase currently uses it is far from optimal. The KeyValueScanner and related classes that iterate through the trie will eventually need to be smarter and have methods to do things like skipping to the next row of results without scanning every cell in between. When that is accomplished it will also allow much faster compactions because the full row key will not have to be compared as often as it is now.Current code is on github. The trie code is in a separate project than the slightly modified hbase. There is an hbase project there as well with the DeltaEncoding patch applied and it builds on top of that.https://github.com/hotpads/hbase/tree/prefix-trie-1https://github.com/hotpads/hbase-prefix-trie/tree/hcell-scannersI'll follow up later with more implementation ideas.,2377
Rename Method,Always cache index and bloom blocks This would add a new boolean config option: hfile.block.cache.datablocksDefault would be true.Setting this to false allows HBase in a mode where only index blocks are cached which is useful for analytical scenarios where a useful working set of the data cannot be expected to fit into the (aggregate) cache.This is the equivalent of setting cacheBlocks to false on all scans (including scans on behalf of gets).I would like to get a general feeling about what folks think about this.The change itself would be simple.Update (Mikhail): we probably don't need a new conf option. Instead we will make index blocks cached by default.,2378
Rename Method,Improvements in tests Global:- when possible make the test using the default cluster configuration for the number of region (1 instead of 2 or 3). This allows a faster stop/start and is a step toward a shared cluster configuration.- 'sleep': lower or remove the sleep based synchronisation in the tests (in HBaseTestingUtility TestGlobalMemStoreSize TestAdmin TestCoprocessorEndpoint TestHFileOutputFormat TestLruBlockCache TestServerCustomProtocol TestReplicationSink)- Optimize 'put' by setting setWriteToWAL to false when the 'put' is big or in a loop. Not done for tests that rely on the WAL.Local issues:- TestTableInputFormatScan fully deletes the hadoop.tmp.dir directory on tearDown that makes it impossible to use in // with another test using this directory- TestIdLock logs too much (9000 lines per seconds). Test time lowered to 15 seconds to make it a part of the small subset- TestMemoryBoundedLogMessageBuffer useless System.out.println- io.hfile.TestReseekTo useless System.out.println- TestTableInputFormat does not shutdown the cluster- testGlobalMemStore does not shutdown the cluster- rest.client.TestRemoteAdmin: simplified does not use local admin single test instead of two.- HBaseTestingUtility#ensureSomeRegionServersAvailable starts only one server should start the number of missing server instead.- TestMergeTool should starts/stops the dfs cluster with HBaseTestingUtility,2379
Extract Method,Don't create an unnecessary LinkedList when evicting from the BlockCache When evicting from the BlockCache the code creates a LinkedList containing every single block sorted by access time. This list is created from a PriorityQueue. I don't believe it is necessary as the PriorityQueue can be used directly.,2380
Extract Method,Improve RowCounter to count rows in a specific key range. Currently RowCounter in MR package is a very simple map only job that does a full scan of a table. Enhance the utility to let the user specify a key range and count the number of rows in this range.,2382
Extract Method,"Sleeps and synchronisation improvements for tests Multiple small changes:@commiters: Removing some sleeps made visible a bug on JVMClusterUtil#HMaster#waitForServerOnline so I had to add a synchro point. You may want to review this.JVMClusterUtil#HMaster#waitForServerOnline: removed the condition was never met (test on ""!c && !!c""). Added a new synchronization point.AssignementManager#waitForAssignment: add a timeout on the wait => not stuck if the notification is received before the wait.HMaster#loop: use a notification instead of a 1s sleepHRegionServer#waitForServerOnline: new method used by JVMClusterUtil#waitForServerOnline() to replace a 1s sleep by a notificationHRegionServer#getMaster() 1s sleeps replaced by one 01s sleep and one 02s sleepHRegionServer#stop: use a notification on sleeper to lower shutdown by 05sZooKeeperNodeTracker#start: replace a recursive call by a loopZooKeeperNodeTracker#blockUntilAvailable: add a timeout on the wait => not stuck if the notification is received before the wait.HBaseTestingUtility#expireSession: use a timeout of 1s instead of 5sTestZooKeeper#testClientSessionExpired: use a timeout of 1s instead of 5s with the change on HBaseTestingUtility we are 60s fasterTestRegionRebalancing#waitForAllRegionsAssigned: use a sleep of 02s instead of 1sTestRestartCluster#testClusterRestart: send all the table creation together then check creation should be fasterTestHLog: shutdown the whole cluster instead of DFS only (more standard) JVMClusterUtil#startup: lower the sleep from 1s to 01sHConnectionManager#close: Zookeeper name in debug message from HConnectionManager after connection close was always null because it was set to null in the delete.",2383
Extract Method,"Per-CF set RPC metrics Porting per-CF set metrics for RPC times and response sizes from 0.89-fb to trunk. For each ""mutation signature"" (a set of column families involved in an RPC request) we increment several metrics allowing to monitor access patterns. We deal with guarding against an explosion of the number of metrics in HBASE-4638 (which might even be implemented as part of this JIRA).",2384
Extract Method,Support reverse Scan Reversed scan means scan the rows backward. And StartRow bigger than StopRow in a reversed scan.For example for the following rows:aaa/c1:q1/value1aaa/c1:q2/value2bbb/c1:q1/value1bbb/c1:q2/value2ccc/c1:q1/value1ccc/c1:q2/value2ddd/c1:q1/value1ddd/c1:q2/value2eee/c1:q1/value1eee/c1:q2/value2you could do a reversed scan from 'ddd' to 'bbb'(exclude) like this:Scan scan = new Scan();scan.setStartRow('ddd');scan.setStopRow('bbb');scan.setReversed(true);for(Result result:htable.getScanner(scan)){System.out.println(result);}Aslo you could do the reversed scan with shell like this:hbase> scan 'table'{REVERSED => trueSTARTROW=>'ddd' STOPROW=>'bbb'}And the output is:ddd/c1:q1/value1ddd/c1:q2/value2ccc/c1:q1/value1ccc/c1:q2/value2All the documentation I find about HBase says that if you want forward and reverse scans you should just build 2 tables and one be ascending and one descending. Is there a fundamental reason that HBase only supports forward Scan? It seems like a lot of extra space overhead and coding overhead (to keep them in sync) to support 2 tables. I am assuming this has been discussed before but I can't find the discussions anywhere about it or why it would be infeasible.,2385
Extract Method,"Replace hql w/ a hbase-friendly jirb or jython shell The hbase shell is a useful admin and debugging tool but it has a couple of downsides. To extend a fragile parser definition needs tinkering-with and new java classes must be added. The current test suite for hql is lacking coverage and the current code could do with a rewrite having evolved piecemeal. Another downside is that the presence of an HQL interpreter gives the mis-impression that hbase is like a SQL database.This 'wish' issue suggests that we jettison HQL and instead offer users a jirb or jython command line. We'd ship with some scripts and jruby/jython classes that we'd source on startup to do things like import base client classes -- so folks wouldn't have to remember all the packages stuff sat in -- and added a pretty-print for scanners and getters outputting text xhtml or binary. They would also make it easy to do HQL-things in jruby/python script.Advantages: Already-written parser with no need of extension probing deeper into hbase: i.e. better for debugging than HQL could ever be. Easy extension adding scripts/modules rather than java code. Less likely hbase could be confused for a SQL db.Downsides: Probably more verbose. Requires ruby or python knowledge (""Everyone knows some sql""). Big? (jruby lib is 24M).I was going to write security as downside but HQL suffers this at the moment too -- though it has been possible to sort the updates from the selects in the UI to prevent modification of the db from the UI something that would be hard to do in a jruby/jython parser.What do others think?",2386
Extract Method,"CellValue class for transporting cell timestamp with cell value simultaneously All of the get* methods take a timestamp parameter that means ""at least as old as X"". This is handy for getting data that fits your expectations about when it should exist. However the result you get back doesn't actually contain the real timestamp the cell was stored at. For example let's say you write the stock price for your favorite company into row ""YHOO"" at cell ""stock:price"". It takes the default timestamp of right now. Then a day passes. You want to get the most recent stock price for YHOO and also when the price was gathered. In the current system you couldn't do this at all without also doing a scan at the same time. If we added a new class called CellValue that contained the byte[] cell value as well as the long timestamp of when it was stored we could return an instance of this class wherever we used to return just the byte[]. This could be used in all the get() methods getRow getClosestAtOrBefore etc. This has the advantage of making timestamp into a first-class citizen in HBase which it hasn't been so far.Thoughts?",2388
Extract Method,Clean up some log messages code in RecoverableZooKeeper In RecoverableZooKeeper there are a number of log messages and comments which don't really read correctly and some other pieces of code that can be cleaned up. Simple cleanup - shouldn't be any actual behavioral changes.,2389
Extract Method,Allow HMsg's carry a payload: e.g. exception that happened over on the remote side. Will make it so can just look in master log and get a general sense of failure types etc. across the cluster.,2391
Rename Method,"Basic client pushback mechanism The current blocking we do when we are close to some limits (memstores over the multiplier factor too many store files global memstore memory) is bad too coarse and confusing. After hitting HBASE-5161 it really becomes obvious that we need something better.I did a little brainstorm with Stack we came up quickly with two solutions:- Send some exception to the client like OverloadedException that's thrown when some situation happens like getting past the low memory barrier. It would be thrown when the client gets a handler and does some check while putting or deleting. The client would treat this a retryable exception but ideally wouldn't check .META. for a new location. It could be fancy and have multiple levels of pushback like send the exception to 25% of the clients and then go up if the situation persists. Should be ""easy"" to implement but we'll be using a lot more IO to send the payload over and over again (but at least it wouldn't sit in the RS's memory).- Send a message alongside a successful put or delete to tell the client to slow down a little this way we don't have to do back and forth with the payload between the client and the server. It's a cleaner (I think) but more involved solution.In every case the RS should do very obvious things to notify the operators of this situation through logs web UI metrics etc.Other ideas?",2392
Extract Method,Utilize TThreadedSelectorServer and remove redundant code in ThriftServer and HRegionThriftServer TThreadedSelectorServer is good for RPC-heavy situation because IO are not limited to one CPU. Seehttps://issues.apache.org/jira/browse/Thrift-1167I am porting the related classes form thrift trunk (it is not there in thrift-0.7.0).There are lots of repeat codes in ThriftServer and HRegionThriftServer.These codes are now moved to a Runnable called ThriftServerRunner.,2395
Rename Method,"Provide basic building blocks for ""multi-row"" local transactions. In the final iteration this issue provides a generalized public mutateRowsWithLocks method on HRegion that can be used by coprocessors to implement atomic operations efficiently.Coprocessors are already region aware which makes this is a good pairing of APIs. This feature is by design not available to the client via the HTable API.It took a long time to arrive at this and I apologize for the public exposure of my (erratic in retrospect) thought processes.Was:HBase should provide basic building blocks for multi-row local transactions. Local means that we do this by co-locating the data. Global (cross region) transactions are not discussed here.After a bit of discussion two solutions have emerged:1. Keep the row-key for determining grouping and location and allow efficient intra-row scanning. A client application would then model tables as HBase-rows.2. Define a prefix-length in HTableDescriptor that defines a grouping of rows. Regions will then never be split inside a grouping prefix.#1 is true to the current storage paradigm of HBase.#2 is true to the current client side API.I will explore these two with sample patches here.--------------------Was:As discussed (at length) on the dev mailing list with the HBASE-3584 and HBASE-5203 committed supporting atomic cross row transactions within a region becomes simple.I am aware of the hesitation about the usefulness of this feature but we have to start somewhere.Let's use this jira for discussion I'll attach a patch (with tests) momentarily to make this concrete.",2396
Rename Method,Filter out the expired store file scanner during the compaction During the compaction time HBase will generate a store scanner which will scan a list of store files. And it would be more efficient to filer out the expired store file since there is no need to read any key values from these store files.This optimization has been already implemented on 89-fb and this is the building block for HBASE-5199 as well. It is supposed to be no-ops to compact the expired store files.,2397
Extract Method,Dynamic Schema Configurations Currently the ability for a core developer to add per-table & per-CF configuration settings is very heavyweight. You need to add a reserved keyword all the way up the stack & you have to support this variable long-term if you're going to expose it explicitly to the user. This has ended up with using Configuration.get() a lot because it is lightweight and you can tweak settings while you're trying to understand system behavior [since there are many config params that may never need to be tuned]. We need to add the ability to put & read arbitrary KV settings in the HBase schema. Combined with online schema change this will allow us to safely iterate on configuration settings.,2398
Extract Method,HBaseObjectWritable should be able to serialize/deserialize generic arrays HBaseObjectWritable can encode Writable[]'s but but cannot encode A[] where A extends Writable. This becomes an issue for example when adding a coprocessor method which takes A[] (see HBASE-5352).,2399
Extract Method,"[uberhbck] Add options for how to handle offline split parents.  In a recent case we attempted to repair a cluster that suffered from HBASE-4238 that had about 6-7 generations of ""leftover"" split data. The hbck repair options in an development version of HBASE-5128 treat HDFS as ground truth but didn't check SPLIT and OFFLINE flags only found in meta. The net effect was that it essentially attempted to merge many regions back into its eldest geneneration's parent's range. More safe guards to prevent ""mega-merges"" are being added on HBASE-5128.This issue would automate the handling of the ""mega-merge"" avoiding cases such as ""lingering grandparents"". The strategy here would be to add more checks against .META. and perform part of the catalog janitor's responsibilities for lingering grandparents. This would potentially include options to sideline regions deleting grandparent regions min size for sidelining and mechanisms for cleaning .META.. Note: There already exists an mechanism to reload these regions -- the bulk loaded mechanisms in LoadIncrementalHFiles can be used to re-add grandparents (automatically splitting them if necessary) to HBase.",2400
Extract Method,"Cut the link between the client and the zookeeper ensemble The link is often considered as an issue for various reasons. One of them being that there is a limit on the number of connection that ZK can manage. Stack was suggesting as well to remove the link to master from HConnection.There are choices to be made considering the existing API (that we don't want to break).The first patches I will submit on hadoop-qa should not be committed: they are here to show the progress on the direction taken.ZooKeeper is used for:- public getter to let the client do whatever he wants and close ZooKeeper when closing the connection => we have to deprecate this but keep it.- read get master address to create a master => now done with a temporary zookeeper connection- read root location => now done with a temporary zookeeper connection but questionable. Used in public function ""locateRegion"". To be reworked.- read cluster id => now done once with a temporary zookeeper connection.- check if base done is available => now done once with a zookeeper connection given as a parameter- isTableDisabled/isTableAvailable => public functions now done with a temporary zookeeper connection.- Called internally from HBaseAdmin and HTable- getCurrentNrHRS(): public function to get the number of region servers and create a pool of thread => now done with a temporary zookeeper connection-Master is used for:- getMaster public getter as for ZooKeeper => we have to deprecate this but keep it.- isMasterRunning(): public function used internally by HMerge & HBaseAdmin- getHTableDescriptor*: public functions offering access to the master. => we could make them using a temporary master connection as well.Main points are:- hbase class for ZooKeeper; ZooKeeperWatcher is really designed for a strongly coupled architecture ;-). This can be changed but requires a lot of modifications in these classes (likely adding a class in the middle of the hierarchy something like that). Anyway non connected client will always be really slower because it's a tcp connection and establishing a tcp connection is slow.- having a link between ZK and all the client seems to make sense for some Use Cases. However it won't scale if a TCP connection is required for every client- if we move the table descriptor part away from the client we need to find a new place for it.- we will have the same issue if HBaseAdmin (for both ZK & Master) may be we can put a timeout on the connection. That would make the whole system less deterministic however.",2401
Extract Method,Filter on one CF and if a match then load and return full row (WAS: Improve performance of scans with some kind of filters) When the scan is performed whole row is loaded into result list after that filter (if exists) is applied to detect that row is needed.But when scan is performed on several CFs and filter checks only data from the subset of these CFs data from CFs not checked by a filter is not needed on a filter stage. Only when we decided to include current row. And in such case we can significantly reduce amount of IO performed by a scan by loading only values actually checked by a filter.For example we have two CFs: flags and snap. Flags is quite small (bunch of megabytes) and is used to filter large entries from snap. Snap is very large (10s of GB) and it is quite costly to scan it. If we needed only rows with some flag specified we use SingleColumnValueFilter to limit result to only small subset of region. But current implementation is loading both CFs to perform scan when only small subset is needed.Attached patch adds one routine to Filter interface to allow filter to specify which CF is needed to it's operation. In HRegion we separate all scanners into two groups: needed for filter and the rest (joined). When new row is considered only needed data is loaded filter applied and only if filter accepts the row rest of data is loaded. At our data this speeds up such kind of scans 30-50 times. Also this gives us the way to better normalize the data into separate columns by optimizing the scans performed.,2402
Extract Method,Purge startUpdate usage from internal code and test cases We have batch updates now. Nothing internal should be using the deprecated startUpdate method.,2403
Extract Method,Allow Import to optionally use HFileOutputFormat importtsv support importing into a life table or to generate HFiles for bulk load.import should allow the same.Could even consider merging these tools into one (in principle the only difference is the parsing part - although that is maybe for a different jira).,2404
Extract Method,Secure Bulk Load Design doc: https://cwiki.apache.org/confluence/display/HCATALOG/HBase+Secure+Bulk+LoadShort summary:Security as it stands does not cover the bulkLoadHFiles() feature. Users calling this method will bypass ACLs. Also loading is made more cumbersome in a secure setting because of hdfs privileges. bulkLoadHFiles() moves the data from user's directory to the hbase directory which would require certain write access privileges set.Our solution is to create a coprocessor which makes use of AuthManager to verify if a user has write access to the table. If so launches a MR job as the hbase user to do the importing (ie rewrite from text to hfiles). One tricky part this job will have to do is impersonate the calling user when reading the input files. We can do this by expecting the user to pass an hdfs delegation token as part of the secureBulkLoad() coprocessor call and extend an inputformat to make use of that token. The output is written to a temporary directory accessible only by hbase and then bulkloadHFiles() is called.,2405
Extract Method,Configurable file and directory based umask Currently many all the files created by the HBase user are just written using the default file permissions granted by hdfs. However to ensure only the correct user/group views the files and directories we need to be able to apply a configurable umask to either directories or files. This ticket covers setting permissions for files written to dfs as opposed to things like pid and log files.The impetus for this was to allow the web-user to view the directory structure of hbase but not to actually see any of the actual data hbase is storing.,2406
Extract Method,Add more metrics to HBase To debug/monitor production clusters there are some more metrics I wish I had available.In particular:- Although the average FS latencies are useful a 'histogram' of recent latencies (90% of reads completed in under 100ms 99% in under 200ms etc) would be more useful- Similar histograms of latencies on common operations (GET PUT DELETE) would be useful- Counting the number of accesses to each region to detect hotspotting- Exposing the current number of HLog files,2407
Rename Method,Unify HRegion.mutateRowsWithLocks() and HRegion.processRow() mutateRowsWithLocks() does atomic mutations on multiple rows.processRow() does atomic read-modify-writes on a single row.It will be useful to generalize both and have aprocessRowsWithLocks() that does atomic read-modify-writes on multiple rows.This also helps reduce some redundancy in the codes.,2408
Extract Method,Add ability to get a table in the shell Currently all the commands that operate on a table in the shell first have to take the table as name as input. There are two main considerations:* It is annoying to have to write the table name every time when you should just be able to get a reference to a table* the current implementation is very wasteful - it creates a new HTable for each call (but reuses the connection since it uses the same configuration)We should be able to get a handle to a single HTable and then operate on that.,2409
Extract Method,Replace client ZooKeeper watchers by simple ZooKeeper reads Some code in the package needs to read data in ZK. This could be done by a simple read but is actually implemented with a watcher. This holds ZK resources.Fixing this could also be an opportunity to remove the need for the client to provide the master address and port.,2411
Extract Method,Make compaction code standalone This is part of hbase-2462. Make the compaction code standalone so can run it independent of hbase. Will make it easier to profile and try stuff out.,2412
Rename Method,hbck should disable the balancer using synchronousBalanceSwitch. hbck disable the balancer using admin.balanceSwith(bool) when it would be preferable to use the newer synchronusBalanceSwitch method found in 0.94 and trunk branches.,2414
Rename Method,hbck should handle case where .tableinfo file is missing. 0.92+ branches have a .tableinfo file which could be missing from hdfs. hbck should be able to detect and repair this properly.,2415
Extract Method,When creating a region the master initializes it and creates a memstore within the master server I didn't do a complete analysis but the attached patch saves more than 0.25s for each region creation and locally all the unit tests work.,2416
Rename Method,Parallelize load of .regioninfo files in diagnostic/repair portion of hbck. On heavily loaded hdfs's some dfs nodes may not respond quickly and backs off for 60s before attempting to read data from another datanode. Portions of the information gathered from hdfs (.regioninfo files) are loaded serially. With HBase with clusters with 100's or 1000's or 10000's regions encountering these 60s delay blocks progress and can be very painful. There is already some parallelization of portions of the hdfs information load operations and the goal here is move the reading of .regioninfos into the parallelized sections..,2417
Extract Method,Enhance hbck to sideline overlapped mega regions If there are too many regions in one overlapped group (by default more than 10) hbck currently doesn't merge them since it takes time.In this case we can sideline some regions in the group and break the overlapping to fix the inconsistency. Later on sidelined regions can be bulk loaded manually.,2418
Inline Method,Investigate IPC performance Turning off all file I/O and running the PerformanceEvaluation test of 1048576 sequential writes to HBase managed to achieve only 7285 IPCs per second.Running PerformanceEvaluation sequential write test modified to do an abort instead of a commit it was possible to do 68337 operations per second. We are obviously spending a lot of time doing IPCs. We need to investigate to find the bottleneck. Marshalling and unmarshalling? Socket setup and teardown?,2419
Rename Method,Add more to verification step in HLogPerformanceEvaluation Verify the wal has expected count of edits.,2422
Rename Method,Names in the filter interface are confusing I don't like the names of the filter methods in RowFilterInterface. They don't really tell how the methods are being used in the implementation of scanners.I'd like to change:- filter(Text) to filterRow(...)- filter(Text Text byte[]) to filterColumn(...)and the worst one is- filterNotNull(SortedMap<Text byte[]>). This should be filterRow(Text SortedMap<Text byte[]>) (so we add the row key/).It may be nice to have timestamps in the methods as well? Also the java doc could be cleaned and improved to tell how the filtering is implemented (check rows keys first then check each individual columns finally check the assembled row)Upon positive feedback and I'll create a patch.,2423
Extract Method,When a query fails because the region has moved let the regionserver return the new address to the client This is mainly useful when we do a rolling restart. This will decrease the load on the master and the network load.Note that a region is not immediately opened after a close. So:- it seems preferable to wait before retrying on the other server. An optimisation would be to have an heuristic depending on when the region was closed.- during a rolling restart the server moves the regions then stops. So we may have failures when the server is stopped and this patch won't help.The implementation in the first patch does:- on the region move there is an added parameter on the regionserver#close to say where we are sending the region- the regionserver keeps a list of what was moved. Each entry is kept 100 seconds.- the regionserver sends a specific exception when it receives a query on a moved region. This exception contains the new address.- the client analyses the exeptions and update its cache accordingly...,2424
Extract Method,Make TestAcidGuarantees usable for system testing. Currently the TestAcidGuarantees run via main() will always abort with an NPE because it digs into a non-existant HBaseTestingUtility for a flusher thread. We should tool this up so that it works properly from the command line. This would be a very useful long running test when used in conjunction with fault injections to verify row acid properties.,2425
Rename Method,[hbck] Refactor parallel WorkItem* to Futures. This would convert WorkItem* logic (with low level notifies and rough exception handling) into a more canonical Futures pattern.Currently there are two instances of this pattern (for loading hdfs dirs for contacting regionservers for assignments and soon -- for loading hdfs .regioninfo files).,2426
Extract Method,In the client code don't wait for all the requests to be executed before resubmitting a request in error. The client (in the function HConnectionManager#processBatchCallback) works in two steps:- make the requests- collect the failures and successes and prepare for retryIt means that when there is an immediate error (region moved split dead server ...) we still wait for all the initial requests to be executed before submitting again the failed request. If we have a scenario with all the requests taking 5 seconds we have a final execution time of: 5 (initial requests) + 1 (wait time) + 5 (final request) = 11s.We could improve this by analyzing immediately the results. This would lead us for the scenario mentioned above to 6 seconds. So we could have a performance improvement of nearly 50% in many cases and much more than 50% if the request execution time is different.,2427
Extract Method,Limits the amount of time an edit can live in the memstore. A colleague of mine ran into an interesting issue.He inserted some data with the WAL disabled which happened to fit in the aggregate Memstores memory.Two weeks later he a had problem with the HDFS cluster which caused the region servers to abort. He found that his data was lost. Looking at the log we found that the Memstores were not flushed at all during these two weeks.Should we have an option to flush memstores periodically. There are obvious downsides to this like many small storefiles etc.,2428
Extract Method,Adding some fuction to check if a table/region is in compaction This feature will be helpful to find out if a major compaction is going on.We can show if it is in any minor compaction too.,2429
Rename Method,Offline Snapshots in HBase 0.96 Continuation of HBASE-50 for the current trunk. Since the implementation has drastically changed opening as a new ticket.,2430
Extract Method,Improve RIT performances during assignment on large clusters The main points in this patch are:- lowering the number of copy of the RIT list- lowering the number of synchronization- synchronizing on a region rather than on everythingIt also contains:- some fixes around the RIT notification: the list was sometimes modified without a corresponding 'notify'.- some tests flakiness correction actually unrelated to this patch.,2431
Extract Method,Improvement for split-worker to speed up distributed log splitting Firstwe do the test between local-master-splitting and distributed-log-splittingEnvironment_34 hlog files 5 regionservers(after kill one only 4 rs do ths splitting work) 400 regions in one hlog filelocal-master-split:60s+distributed-log-splitting:165s+In fact in our production environment distributed-log-splitting also took 60s with 30 regionservers for 34 hlog files (regionserver may be in high load)We found split-worker split one log file took about 20s(30ms~50ms per writer.close(); 10ms per create writers )I think we could do the improvement for this:Parallelizing the create and close writers in threadsIn the patch change the logic for distributed-log-splitting same as the local-master-splitting and parallelizing the close in threads.,2432
Inline Method,Retiring regions is not used; exploit or remove There is a little dance around region close where a region first gets moved into the retiring queue. The idea IIRC was that regions in retiring could serve reads while close was going about its business. Meant that region was online that bit longer.This feature is not used any more -- regions are added to retiring but gets do not bother to look in retiring. We should either remove retiring cocept altogether or else make use of it again.,2433
Extract Method,Timeouts for row lock and scan should be separate Apparently the timeout used for row locking and for scanning is global. It would be better to have two separate timeouts.(opening the issue to make Lars George happy),2435
Extract Method,Use Visitor pattern in MetaRegion to reduce code clones in HTable and HConnectionManager HTable and HConnectionManager.TableServers both scan the meta region in the same way (but the later also retry one time if it fails). A Visitor pattern should be used in a new scanning method in MetaRegion to accept visitors that gather information such as region names for a table or the list of all tables.,2437
Extract Method,hbck should group together those sidelined regions need to be bulk loaded later Currently hbck sidelines some regions to break big overlap groups to avoid possible compaction and region split. These sidelined regions should bebulk loaded back later. Information about these regions is in the output.It will be much easier to group them together under the same sideline rootdirfor example /hbase/.hbck/to_be_loaded/. If so even we lose the outputfile we still know what regions to load back.,2438
Extract Method,Audit log messages do not include column family / qualifier information consistently The code related to this issue is in AccessController.java:permissionGranted().When creating audit logs that method will do one of the following:* grant access create audit log with table name only* deny access because of table permission create audit log with table name only* deny access because of column family / qualifier permission create audit log with specific family / qualifierSo in the case where more than one column family and/or qualifier are in the same request there will be a loss of information. Even in the case where only one column family and/or qualifier is involved information may be lost.It would be better if this behavior consistently included all the information in the request; regardless of access being granted or denied and regardless which permission caused the denial the column family and qualifier info should be part of the audit log message.,2439
Extract Method,Reading WAL files after a recovery leads to time lost in HDFS timeouts when using dead datanodes HBase writes a Write-Ahead-Log to revover from hardware failure. This log is written on hdfs.Through ZooKeeper HBase gets informed usually in 30s that it should start the recovery process. This means reading the Write-Ahead-Log to replay the edits on the other servers.In standards deployments HBase process (regionserver) are deployed on the same box as the datanodes.It means that when the box stops we've actually lost one of the edits as we lost both the regionserver and the datanode.As HDFS marks a node as dead after ~10 minutes it appears as available when we try to read the blocks to recover. As such we are delaying the recovery process by 60 seconds as the read will usually fail with a socket timeout. If the file is still opened for writing it adds an extra 20s + a risk of losing edits if we connect with ipc to the dead DN.Possible solutions are:- shorter dead datanodes detection by the NN. Requires a NN code change.- better dead datanodes management in DFSClient. Requires a DFS code change.- NN customisation to write the WAL files on another DN instead of the local one.- reordering the blocks returned by the NN on the client side to put the blocks on the same DN as the dead RS at the end of the priority queue. Requires a DFS code change or a kind of workaround.The solution retained is the last one. Compared to what was discussed on the mailing list the proposed patch will not modify HDFS source code but adds a proxy. This for two reasons:- Some HDFS functions managing block orders are static (MD5MD5CRC32FileChecksum). Implementing the hook in the DFSClient would require to implement partially the fix change the DFS interface to make this function non static or put the hook static. None of these solution is very clean. - Adding a proxy allows to put all the code in HBase simplifying dependency management.Nevertheless it would be better to have this in HDFS. But this solution allows to target the last version only and this could allow minimal interface changes such as non static methods.Moreover writing the blocks to the non local DN would be an even better solution long term.,2440
Extract Method,If mapfile index is empty run repair Our cluster lost data. Made for some interesting scenarios in hbase. One such was empty index files (Would get an EOFException when we tried to scan or open region). HBASE-646 added checking of data info and index files. Tried to run repair of indexes but was doing it wrong place and wasn't first removing the broken index so it'd fail.,2441
Extract Method,Deprecate HTablePool in favor of HConnection.getTable(...) Update:I now propose deprecating HTablePool and instead introduce a getTable method on HConnection and allow HConnection to manage the ThreadPool.Initial proposal:Here I propose a very simple TablePool.It could be called LightHTablePool (or something - if you have a better name).Internally it would maintain an HConnection and an Executor service and each invocation of getTable(...) would create a new HTable and close() would just close it.In testing I find this more light weight than HTablePool and easier to monitor in terms of resources used.It would hardly be more than a few dozen lines of code.,2442
Extract Method,Make HTable HRegion HRegionServer HStore and HColumnDescriptor subclassable 0,2444
Extract Method,HTable#coprocessorExec always scan the whole table  In current logic HTable#coprocessorExec always scans the entire META table loading it into memory and then filters the keys to return only those that fall in specified range. The version after the patch only scans the portions of meta that are in the specified key range and returns them. Put simply -- before we did a load-all-then-filter; afterwards we only-scan-what-is-needed.The former has low efficiency and greatly impacts the Regionserver carrying .META. when there are many coprocessorExec requests.,2445
Extract Method,Remove unnecessary throws IOException from Bytes.readVLong Remove the throws IOException so that caller doesn't have to catch and ignore.{code}public static long readVLong(final byte [] buffer final int offset)throws IOException{code}Also add{code}public static int readVInt(final byte [] buffer final int offset)throws IOException {return (int)readVLong(bufferoffset);}{code}and these are useful too:{code}/*** Put long as variable length encoded number at the offset in* the result byte array.* @param vint Integer to make a vint of.* @param result buffer to put vint into* @return Vint length in bytes of vint*/public static int vintToBytes(byte[] result int offset final long vint) {long i = vint;if (i >= -112 && i <= 127) {result[offset] = (byte) i;return 1;}int len = -112;if (i < 0) {i ^= -1L; // take one's complement'len = -120;}long tmp = i;while (tmp != 0) {tmp = tmp >> 8;len--;}result[offset++] = (byte) len;len = (len < -120) ? -(len + 120) : -(len + 112);for (int idx = len; idx != 0; idx--) {int shiftbits = (idx - 1) * 8;long mask = 0xFFL << shiftbits;result[offset++] = (byte)((i & mask) >> shiftbits);}return len + 1;}/*** Decode a vint from the buffer pointed at to by ptr and* increment the offset of the ptr by the length of the* vint.* @param ptr a pointer to a byte array buffer* @return the decoded vint value as an int*/public static int vintFromBytes(ImmutableBytesWritable ptr) {return (int) vlongFromBytes(ptr);}/*** Decode a vint from the buffer pointed at to by ptr and* increment the offset of the ptr by the length of the* vint.* @param ptr a pointer to a byte array buffer* @return the decoded vint value as a long*/public static long vlongFromBytes(ImmutableBytesWritable ptr) {final byte [] buffer = ptr.get();final int offset = ptr.getOffset();byte firstByte = buffer[offset];int len = WritableUtils.decodeVIntSize(firstByte);if (len == 1) {ptr.set(buffer offset+1 ptr.getLength());return firstByte;}long i = 0;for (int idx = 0; idx < len-1; idx++) {byte b = buffer[offset + 1 + idx];i = i << 8;i = i | (b & 0xFF);}ptr.set(buffer offset+len ptr.getLength());return (WritableUtils.isNegativeVInt(firstByte) ? ~i : i);}{code},2446
Rename Method,Allow the master info server to be started in a read only mode. There are some cases that a user could want a web ui to be accessible but might not want the split and compact functionality to be usable.Allowing the web ui to start in a readOnly mode would be good.,2447
Extract Method,Improve Bytes to accept byte buffers which don't allow us to directly access their backing arrays Inside HBase it seems that there is the implicit assumption that byte buffers have backed arrays and are not read-only and we can freely call ByteBuffer.array() and arrayOffset() without runtime exceptions.But some classes including Bytes are supposed to be used by users from outside of HBase and we should think the possibility that methods receive byte buffers which don't hold the assumption.,2448
Rename Method,Add Data Block Encoding and -D opts to Performance Evaluation Add the ability to specify Data Block Encoding and other configuration options.--blockEncoding=TYPE-D <property=value>Example:hbase org.apache.hadoop.hbase.PerformanceEvaluation -D mapreduce.task.timeout=60000 --blockEncoding=DIFF sequentialWrite 1,2449
Extract Method,Compression tests Add a couple of tests to verify that:* Store.createWriterInTmp() respect compression and data block encoding (HBASE-5690)* LZ4 compression support (HBASE-5838),2450
Extract Method,Add multi get to RemoteHTable REST server supports multi-get so the RemoteHTable class should as well.,2451
Extract Method,create integration test for balancing regions and killing region servers - 2 The original test is too general; need another one that would be more targeted and would test master logic in particular (e.g. not kill master). I re-discovered HBASE-6060 using it on the first run :),2452
Rename Method,Supporting for HLog appends I thank we should open a ticket to track what needs changed to support appends when the coding is done on HADOOP-1700.,2453
Extract Method,Expose master table operations for coprocessors by way of MasterServices I have something I'm working on that as a coprocessor when it initializes would like to add a column to a table should that column be missing. Exposing master table operations for coprocessors by way of MasterServices was how I solved this problem and is generally useful for all master coprocessors.,2457
Rename Method,Online Merge Support executing region merge transaction on Regionserver similar with split transactionProcess of merging two regions:a.client sends RPC (dispatch merging regions) to masterb.master moves the regions together (on the same regionserver where the more heavily loaded region resided)c.master sends RPC (merge regions) to this regionserverd.Regionserver executes the region merge transaction in the thread poole.the above bcd run asynchronouslyProcess of region merge transaction:a.Construct a new region merge transaction.b.prepare for the merge transaction the transaction will be canceled if it is unavailable e.g. two regions don't belong to same table; two regions are not adjacent in a non-compulsory merge; region is closed or has referencec.execute the transaction as the following:/*** Set region as in transition set it into MERGING state.*/SET_MERGING_IN_ZK/*** We created the temporary merge data directory.*/CREATED_MERGE_DIR/*** Closed the merging region A.*/CLOSED_REGION_A/*** The merging region A has been taken out of the server's online regions list.*/OFFLINED_REGION_A/*** Closed the merging region B.*/CLOSED_REGION_B/*** The merging region B has been taken out of the server's online regions list.*/OFFLINED_REGION_B/*** Started in on creation of the merged region.*/STARTED_MERGED_REGION_CREATION/*** Point of no return. If we got here then transaction is not recoverable* other than by crashing out the regionserver.*/PONRd.roll back if step c throws exceptionUsage:HBaseAdmin#mergeRegionsSee more details from the patch,2458
Inline Method,Add an efficient way to batch update many rows HBASE-747 introduced a simple way to batch update many rows. The goal of this issue is to have an enhanced version that will send many rows in a single RPC to each region server. To do this the client code will have to figure which rows goes to which server group them accordingly and then send them.,2459
Extract Method,Transparent table/CF encryption Introduce transparent encryption of HBase on disk data.Depends on a separate contribution of an encryption codec framework to Hadoop core and an AES-NI (native code) codec. This is work done in the context of MAPREDUCE-4491 but I'd gather there will be additional JIRAs for common and HDFS parts of it.Requirements:- Transparent encryption at the CF or table level- Protect against all data leakage from files at rest- Two-tier key architecture for consistency with best practices for this feature in the RDBMS world- Built-in key management- Flexible and non-intrusive key rotation- Mechanisms not exposed to or modifiable by users- Hardware security module integration (via Java KeyStore)- HBCK support for transparently encrypted files (+ plugin architecture for HBCK)Additional goals:- Shell support for administrative functions- Avoid performance impact for the null crypto codec case- Play nicely with other changes underway: in HFile block coding etc.We're aiming for rough parity with Oracle's transparent tablespace encryption feature described in http://www.oracle.com/technetwork/database/owp-security-advanced-security-11gr-133411.pdf as{quote}“Transparent Data Encryption uses a 2-tier key architecture for flexible and non-intrusive key rotation and least operational and performance impact: Each application table with at least one encrypted column has its own table key which is applied to all encrypted columns in that table. Equally each encrypted tablespace has its own tablespace key. Table keys are stored in the data dictionary of the database while tablespace keys are stored in the header of the tablespace and additionally the header of each underlying OS file that makes up the tablespace. Each of these keys is encrypted with the TDE master encryption key which is stored outside of the database in an external security module: either the Oracle Wallet (a PKCS#12 formatted file that is encrypted using a passphrase supplied either by the designated security administrator or DBA during setup) or a Hardware Security Module (HSM) device for higher assurance […]”{quote}Further design details forthcoming in a design document and patch as soon as we have all of the clearances in place.,2460
Rename Method,Add a costless notifications mechanism from master to regionservers & clients t would be very useful to add a mechanism to distribute some information to the clients and regionservers. Especially It would be useful to know globally (regionservers + clients apps) that some regionservers are dead. This would allow:- to lower the load on the system without clients using staled information and going on dead machines- to make the recovery faster from a client point of view. It's common to use large timeouts on the client side so the client may need a lot of time before declaring a region server dead and trying another one. If the client receives the information separatly about a region server states it can take the right decision and continue/stop to wait accordingly.We can also send more information for example instructions like 'slow down' to instruct the client to increase the retries delay and so on.Technically the master could send this information. To lower the load on the system we should:- have a multicast communication (i.e. the master does not have to connect to all servers by tcp) with once packet every 10 seconds or so.- receivers should not depend on this: if the information is available great. If not it should not break anything.- it should be optional.So at the end we would have a thread in the master sending a protobuf message about the dead servers on a multicast socket. If the socket is not configured it does not do anything. On the client side when we receive an information that a node is dead we refresh the cache about it.,2461
Extract Method,client retry timeout doesn't need to do x2 fallback when going to different server See HBASE-7520. When we go to server A get a bunch of failures then finally learn the region is on B it doesn't make sense to wait for 30 seconds before going to B.,2462
Extract Method,Don't use bulk assigner if assigning just several regions If just assign one region bulk assigner may be slower.,2464
Extract Method,HBaseTestingUtility.truncateTable() not acting like CLI I would like to discuss the behavior of the truncateTable() method of HBaseTestingUtility. It's currently only removing the data through a scan/delete pattern.However the truncate command in CLI is doing additional things: it disables the tables drop creates (with similar column descriptors) and then enables the table.I think the truncateTable() method is misleading; for example I used it to force a coprocessor to be reloaded but it did not. Of course I can disable and enable the table by myself within my unit test but perhaps it deserves to be discussed?,2466
Extract Method,Refactor OpenRegionHandler so that the cleanup happens in one place - the finally block This is based on discussion in HBASE-7698. Jimmy suggested this improvment.Look at https://issues.apache.org/jira/browse/HBASE-7698?focusedCommentId=13572736&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13572736,2467
Extract Method,Improve Hbase Thrift v1 to return results in sorted order Hbase natively stores columns sorted based on the column qualifier. A scan is guaranteed to return sorted columns. The Java API works fine but the Thrift API is broken. Hbase uses TreeMap that ensures that sort order is maintained. However Hbase thrift specification uses a simple Map to store the data. A map since it is unordered doesn't result in columns being returned in a sort order that is consistent with their storage in Hbase.,2468
Extract Method,HTU#waitTableAvailable should have a default value of 30s It's often used with 5s delay. It's not enough in my env as I parallelize heavily the tests. 30s default seems to make it.,2470
Extract Method,Use the ServerName in the Connection#getClient and Connection#getAdmin code This is again a subpart of HBASE-7590. The patch already in HBASE-7590 contains a bad fix for this (confusion with seq number). This JIRA tries to fix it correctly.The client code mostly uses hostname:port to get the connection to the region server. As a side effect if a server dies and comeback the client won't notice. This is in theory because the regions it's looking for are likely to be somewhere else.In any case if we add some dead server management in the client (HBASE-7590) we need to manage do the distinction between the server to be sure that we're not declaring dead a server that has restarted.,2471
Rename Method,make policy and compactor in default store engine separately pluggable (for things like tier-based and default policy experiments with permutations) Technically StoreEngine can be used to achieve any permutations of things but to make it more convenient to replace compaction policy/compator in standard schemes like tier-based we can add separate hooks in DefaultStoreEngine (as long as custom ones conform to its default expectations e.g. flat list of sorted files etc.),2472
Rename Method,More Table operation in TableHandler for REST interface In the current implementation only the metadata query of table allowed. It's not convinent to use the REST interface to write a third-party client library. I think the functionality of REST interface should be similar with the hbase shell. So user can create/show/update/delate/enable/disable the table.,2473
Inline Method,Remove update() and Improve ExplicitColumnTracker performance. In ColumnTracker.java the update() method is not used by anyone now. And no one will call checkColumn for different HFiles with update() in between files to re-walk through the target columns. All columns will be feed to checkColumn() in order.So within ExplicitColumnTracker the target columns can be optimized to not dynamic maintain a changing list of columns yet to match. Instead just move index through it is enough.with this optimization to save the time for avoid reconstruct a columns array upon each row the checkColumn method's performance could be improved by 10-20%.,2474
Extract Method,Provide Client API to explicitly lock and unlock rows We need to be able to perform a series of reads from and writes to a single row without any potential interference from other clients.Unfortunately this is a bit involved because normal reads currently do not acquire row locks so it requires adding additional get/getRow calls that obtain and release a row lock.In addition there will be two additional client calls lockRow/unlockRow which actually acquire and release the locks. Though each lock is associated with an HRegion this will be tracked within the HRegionServer. When a lock is acquired from the client it is handled much like a Scanner. We obtain the row lock from the HRegion store the region name and lock identifier in a synchronized Map and also obtain a lease to ensure that the lock will eventually be released even if the client dies.This also required adding a RowLockListener (implements LeaseListener) private class in HRS to handle row lock lease expiration.HRS.lockRow will return a long lockId (as openScanner does) that will be used in subsequent client calls to reuse this existing row lock. These calls will check that the lock is valid and perform the operations without any locking (wrappers around get* new versions of batchUpdate openScanner etc).This is going to really add some noise to the list of available HTable/client methods so I'm not sure if it's something people would want to commit into a normal release. Regardless this does provide some very convenient functionality that may be useful to others.We are also looking into Clint Morgan's HBASE-669 but one major downside is that there is a significant amount of overhead involved. This row locking is already built in and this will only extend the API to allow clients to work with them directly. There is little to no overhead at all. The only (obvious) performance consideration is that this should only used where necessary as rows will not be locked and unlocked as quickly with round-trip client calls. In our design we will have specific notes in our schema about which tables (or even which families or columns) must be accessed with row locks at all times and which do not.This is my first attempt at adding any additional functionality so comments criticism code reviews are encouraged.I should have a patch up tomorrow.,2475
Extract Method,TableMap should survive USE If work in a TableMap task exceeds the client lease time the task will fail with an UnknownScannerException. TableInputFormatBase should handle this by restarting with a new scanner from the last key seen.,2476
Extract Method,"Eliminate exception for ExportSnapshot against the null table snapshot (with no data in) In the test environment when snapshoting the null table (with no data) snapshot artifact can be generated successfully with snapshot meta data while there is no CF data under region dir. Then when exporting this null snapshot via ExportSnapshot ""java.io.IOException: No input paths specified in job"" occurred which affects the normal application flow. But seems snapshot for null table also makes sense just with the meta information (.snapshot/) although "".archive/"" does not show up. Later snapshot restore seems work from this ""dummy"" snapshot artifact with no CF data.Just eliminate exception for ExportSnapshot against the null table snapshot to make application work through.",2477
Extract Method,"Add truncate as HMaster method Currently truncate and truncate_preserve are only shell functions and implemented as deleteTable() + createTable().Using ACLs the user running truncate must have rights to create a table and only ""global granted"" users can create tables.Add truncate() and truncatePreserve() to HBaseAdmin/HMaster with its own ACL check.https://reviews.apache.org/r/15835/",2478
Extract Method,Provide mutability to CompoundConfiguration In discussion of HBASE-8347 it was proposed that CompoundConfiguration should support mutability.This can be done by consolidating ImmutableConfigMap's on first modification to CompoundConfiguration.,2479
Extract Method,"Support lib/*jar inside coprocessor jar Currently jar files inside a coprocessor jar should be under folder /lib/. It would be great to support jar files under lib/ too i.e no ""/"" at the front.",2480
Extract Method,deprecate TableMapReduce.addDependencyJars(Configuration class<?> ...) We expose two public static methods names {{addDependencyJars}}. One of them {{void addDependencyJars(Job}} is very helpful -- goes out of its way to detect job dependencies as well as shipping all the necessary HBase dependencies. The other is shfty and nefarious {{void addDependencyJars(Configuration Class<?>...)}} -- it only adds exactly what the user requests forcing them to resolve dependencies themselves and giving a false sense of security. We should deprecate the latter throw a big giant warning when people use that one. The handy functionality of providing help when our heuristics fail can be added via a new method signature something like {{void addDependencyJars(Job Class<?> ...}}. This method would do everything {{void addDependencyJars(Job}} does plus let the user specify arbitrary additional classes. That way HBase still can help the user but also gives them super-powers to compensate for when our heuristics fail.For reference this appears to be the reason why HBase + Pig doesn't really work out of the box. See [HBaseStorage.java|https://github.com/apache/pig/blob/trunk/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java#L730],2481
Extract Method,More options on the row query in REST interface The prior implementation only supports the exact column name(col_family:label) but we need more such as get a col family query the version.,2482
Inline Method,Consolidate multiple overloaded methods in HRegionInterface HRegionServer There are too many overloaded methods in HRegionServerInterface and consequently HRegionServer.These should be consolidated into one method per operation and the client should pass the appropriate parameters to the server.On the server side the single method should be able to handle parameters that are not supplied e.g.- long values that are not supplied should be -1- boolean values should be supplied appropriately- objects that are not supplied should be passed as nullAll these overloaded methods eventually call the same method on the server side eventually. Removing the overloads would make following control flow easier.,2483
Extract Method,HCM.isTableEnabled doesn't really tell if it is or not In current trunk if i load a table of 8M rows and then try and delete it the disable returns saying the table was successfully deleted but when I then try to drop the table it says table not disabled. I run the disable/drop cycle a few more times and still fails. Eventually if I wait long enough it succeeds. Maybe the table drop should just block if table is seen to have disabled regions in it. As is its a little disorientating the way it works. Could lead admins to distrust status messages emitted.,2484
Extract Method,"Auto-drop rollback snapshot for snapshot restore Below is an excerpt from snapshot restore javadoc:{code}* Restore the specified snapshot on the original table. (The table must be disabled)* Before restoring the table a new snapshot with the current table state is created.* In case of failure the table will be rolled back to the its original state.{code}We can improve the handling of rollbackSnapshot in two ways:1. give better name to the rollbackSnapshot (adding {code}'-for-rollback-'{code}). Currently the name is of the form:String rollbackSnapshot = snapshotName + ""-"" + EnvironmentEdgeManager.currentTimeMillis();2. drop rollbackSnapshot at the end of restoreSnapshot() if the restore is successful. We can introduce new config param named 'hbase.snapshot.restore.drop.rollback' to keep compatibility with current behavior.",2485
Extract Method,HTable.getRegionsInRange() should provide a non-cached API getRegionsInRange() calls getRegionLocation() without reloading it. It will return wrong result if the cache is outdated due to region split. If the cost of always reloading isn't significant we should consider doing that by default. Otherwise let's have an API for getRegionsInRange() that forces a reload.,2486
Extract Method,Get rid of hbase.hstore.compaction.complete setting hbase.hstore.compaction.complete is a strange setting that causes the finished compaction to not complete (files are just left in tmp) in HStore. It's used by one test.The setting with the same name is also used by CompactionTool but that usage is semi-unrelated and could probably be removed easily.,2487
Extract Method,clean up code around compaction completion in HStore The methods completeCompaction and it's caller are too long. Something I changed while doing something else there; putting in a separate easy-to-review JIRA to make the other future change smaller.,2488
Extract Method,Enhance delete_snapshot.rb to call snapshot deletion API with regex HBASE-8461 added the API to HBaseAdmin which allows user to specify regular expression for deleting snapshots.This JIRA would allow delete_snapshot.rb to utilize this functionality.,2489
Extract Method,"HLogs in ZK are not cleaned up when replication lag is minimal On a cluster with very low replication lag (as measured by ageOfLastShippedOp on source) we found HLogs accumulating and not being cleaned up as new WAL(s) are rolled.Each time we call logPositionAndCleanOldLogs() to clean older logs whenever the current WAL is not being written to any more - as suggested by currentWALBeingWrittenTo being false. However when lags are small we may hit the following block first and continue onto the next WAL without clearing the old WAL(s)...ReplicationSource::run() {if (readAllEntriesToReplicateOrNextFile(currentWALisBeingWrittenTo = false)) {// If we are here then we advance to the next WAL without any cleaning// and close existing WALcontinue;}// Ship some edits and call logPositionAndCleanOldLogs}If we hit readAllEntriesToReplicateOrNextFile(false) only once - then older logs are not cleaned out and persist in the zookeeper node since we simply call ""continue"" and skip the subsequent logPositionAndCleanOldLogs call - if its called more than once we do end up clearing the old logs.",2490
Extract Method,change exploring compaction policy to prefer smaller compactions on blocked stores Side-note from HBASE-8665 discussion. When we compact a blocked store we might want to use a different heuristic to choose between the options.,2491
Extract Method,[AccessController] Restrict HTableDescriptor enumeration Some users are concerned about having table schema exposed to every user and would like it protected similar to the rest of the admin operations for schema. This used to be hopeless because META would leak HTableDescriptors in HRegionInfo but that is no longer the case in 0.94+.Consider adding CP hooks in the master for intercepting HMasterInterface#getHTableDescriptors and HMasterInterface#getHTableDescriptors(List<String>). Add support in the AccessController for only allowing GLOBAL ADMIN to the first method. Add support in the AccessController for allowing access to the descriptors for the table names in the list of the second method only if the user has TABLE ADMIN privilege for all of the listed table names.Then fix the code in HBaseAdmin (and elsewhere) that expects to be able to enumerate all table descriptors e.g. in deleteTable. A TABLE ADMIN can delete a table but won’t have GLOBAL ADMIN privilege to enumerate the total list. So a minor fixup is needed here and in other places like this which make the same assumption.,2492
Extract Method,[replication] Change replication RPC to use cell blocks Currently the replication rpc that ships edits simply dumps the byte value of WAL edit key/value pairs into a protobuf message.Modify the replication rpc mechanism to use cell blocks so it can leverage encoding and compression.,2493
Extract Method,"Enable peer cluster to choose/change the ColumnFamilies/Tables it really want to replicate from a source cluster Consider scenarios (all cf are with replication-scope=1):1) cluster S has 3 tables table A has cfAcfB table B has cfXcfY table C has cf1cf2.2) cluster X wants to replicate table A : cfA table B : cfX and table C from cluster S.3) cluster Y wants to replicate table B : cfY table C : cf2 from cluster S.Current replication implementation can't achieve this since it'll push the data of all the replicatable column-families from cluster S to all its peers X/Y in this scenario.This improvement provides a fine-grained replication theme which enable peer cluster to choose the column-families/tables they really want from the source cluster:A). Set the table:cf-list for a peer when addPeer:hbase-shell> add_peer '3' ""zk:1100:/hbase"" ""table1; table2:cf1cf2; table3:cf2""B). View the table:cf-list config for a peer using show_peer_tableCFs:hbase-shell> show_peer_tableCFs ""1""C). Change/set the table:cf-list for a peer using set_peer_tableCFs:hbase-shell> set_peer_tableCFs '2' ""table1:cfX; table2:cf1; table3:cf1cf2""In this theme replication-scope=1 only means a column-family CAN be replicated to other clusters but only the 'table:cf-list list' determines WHICH cf/table will actually be replicated to a specific peer.To provide back-compatibility empty 'table:cf-list list' will replicate all replicatable cf/table. (this means we don't allow a peer which replicates nothing from a source cluster we think it's reasonable: if replicating nothing why bother adding a peer?)This improvement addresses the exact problem raised by the first FAQ in ""http://hbase.apache.org/replication.html"":""GLOBAL means replicate? Any provision to replicate only to cluster X and not to cluster Y? or is that for later?Yes this is for much later.""I also noticed somebody mentioned ""replication-scope"" as integer rather than a boolean is for such fine-grained replication purpose but I think extending ""replication-scope"" can't achieve the same replication granularity flexibility as providing above per-peer replication configurations.This improvement has been running smoothly in our production clusters (Xiaomi) for several months.",2494
Inline Method,A new write thread model for HLog to improve the overall HBase write throughput In current write model each write handler thread (executing put()) will individually go through a full 'append (hlog local buffer) => HLog writer append (write to hdfs) => HLog writer sync (sync hdfs)' cycle for each write which incurs heavy race condition on updateLock and flushLock.The only optimization where checking if current syncTillHere > txid in expectation for other thread help write/sync its own txid to hdfs and omitting the write/sync actually help much less than expectation.Three of my colleagues(Ye Hangjun / Wu Zesheng / Zhang Peng) at Xiaomi proposed a new write thread model for writing hdfs sequence file and the prototype implementation shows a 4X improvement for throughput (from 17000 to 70000+). I apply this new write thread model in HLog and the performance test in our test cluster shows about 3X throughput improvement (from 12150 to 31520 for 1 RS from 22000 to 70000 for 5 RS) the 1 RS write throughput (1K row-size) even beats the one of BigTable (Precolator published in 2011 says Bigtable's write throughput then is 31002). I can provide the detailed performance test results if anyone is interested.The change for new write thread model is as below:1> All put handler threads append the edits to HLog's local pending buffer; (it notifies AsyncWriter thread that there is new edits in local buffer)2> All put handler threads wait in HLog.syncer() function for underlying threads to finish the sync that contains its txid;3> An single AsyncWriter thread is responsible for retrieve all the buffered edits in HLog's local pending buffer and write to the hdfs (hlog.writer.append); (it notifies AsyncFlusher thread that there is new writes to hdfs that needs a sync)4> An single AsyncFlusher thread is responsible for issuing a sync to hdfs to persist the writes by AsyncWriter; (it notifies the AsyncNotifier thread that sync watermark increases)5> An single AsyncNotifier thread is responsible for notifying all pending put handler threads which are waiting in the HLog.syncer() function6> No LogSyncer thread any more (since there is always AsyncWriter/AsyncFlusher threads do the same job it does),2495
Extract Method,Fix a hotspot in scanners When scanning we do a lot of RPCs and this has a huge performance hit. I propose that we add a way to fetch more rows during next() and put them in cache. This should be configurable.,2496
Extract Method,"alter table operation and also related changes in REST interface I have made some changes to the alter operation on the hbase shell.Now we can add update delete the column families. Also make thechanges to the TableHandler in REST interface.changes to the hbase shell:> alter 'table' {NAME => 'cf' VERSIONS => 3}This command will try to find the column family named 'cf' at first.If has it will modifyColumn if not add the column> alter 'table' {NAME => 'cf' 'method' => 'delete'}This command will delete the column family named 'cf'.To achieve this goal i also add a method to the HBaseAdmin.javapublic TableDescriptor getTableDescriptor(byte[] tableName);changes to the TableHandler in REST interface.> curl -X PUT -T - http://localhost:60050/api/tablename<?xml version=""1.0"" encoding=""UTF-8""?><table><name>tables</name><columnfamilies><columnfamily><name>cf1</name><max-versions>2</max-versions><compression>NONE</compression><in-memory>false</in-memory><block-cache>true</block-cache></columnfamily></columnfamilies></table>It will check the column family 'cf1'. If exists modifyColumn ifnot addColumn> curl -X DELETE http://localhost:60050/api/tablename?column=cf1It will deleteColumn 'cf1'.",2497
Rename Method,protobuf message style Google's protobuf style guide (https://developers.google.com/protocol-buffers/docs/style) lays out the convention that message field names should be underscore_separated and services should be MixedCase. The .proto's in trunk do not follow this style; instead they follow Java naming conventions. The protobuf compiler will automatically change the style of names to match the language it is compiling for but it is unable to do so if the protobuf style is not used. As a result using the HBase proto files from languages with different naming conventions than Java is a little bit more painful.Since a core feature of moving to protobufs is opening the door to wire compatible implementations in other languages I think this may want to be addressed. This patch changes the naming convention in the protos. The resulting .java files that the protobuf compiler puts out are functionally the same (with the same correct Java naming style).,2498
Extract Method,Add a limit to key length check key and value length on client side. Currently there is no limit on key length and there should be. It should be a parameter in HTableDescriptor since the row key length needs to be considered in addition to the column key.It should be trivial to add since HTD can be upgraded without requiring a migration.Checking of the key length (and the value length) should be done on the client side as it will fail early rather than once the request is sent to the server.This means that a BatchUpdate needs a reference to either the HTable or to the HTD. It can be a transient reference so that the HTable (or HTD) need not be serialized/deserialized.,2499
Rename Method,Add a view/edit tool for favored node mappings for regions Add a tool that one can run offline to view the favored node mappings for regions and also fix the mappings if needed. Such a tool exists in the 0.89-fb branch. Will port it over to trunk/0.95.,2500
Extract Method,Provide interface for getting a User in the client Sometimes users will want to provide their own User class depending on the type of security they will support in their local environment. For instance running Hadoop1 vs Hadoop2 vs CDH means potentially different ways of getting the UserGroupInformation. This issue abstracts out the mechanism by which we obtain an o.a.h.hbase.security.User to a UserProvider. This UserProvider can then be extented as a Hadoop 1/2 shim as well as supporting custom authentication code.,2501
Extract Method,Add convenience methods to RowFilterSet This patch adds getOperator and addFilter to RowFilterSet. I found this useful when constructing filters from higher-level queries.,2502
Extract Method,[Optimization] Major compaction should remove deletes as well as the deleted cell Currently major compactions retains both deletes and the deleted cell. It should remove both.,2503
Extract Method,Improve performance for small scan review board:https://reviews.apache.org/r/14059/*Performance Improvement*Test shows about 1.5~3X improvement for small scan where limit<=50 under cache hit ratio=100%.See more performance test result from the picture attachment*Usage:*Scan scan = new Scan(startRowstopRow);scan.setSmall(true);ResultScanner scanner = table.getScanner(scan);Set the new 'small' attribute as true for scan object others are the sameNow one scan operation would call 3 RPC at least:openScanner();next();closeScanner();I think we could reduce the RPC call to one for small scan to get better performanceAlso using pread is better than seek+read for small scan (For this point see more on HBASE-7266)Implements such a small scan as the patch and take the performance test as following:a.Environment_patched on 0.94 versionone regionserver; one client with 50 concurrent threads;KV size:50/100;100% LRU cache hit ratio;Random start row of scanb.Results:See the picture attachment,2504
Extract Method,"REST interface:  more generic column family configure and also get Rows using offset and limit The update column family operation in REST interface will overwrite the default metadata using the default value which is unexpected. We should use the column family to get the old value and then do update if requested by the users. Also for non-default metadata value such as ""hbase.hregion.majorcompaction"" we should still be enable to create/update it using REST interface.For RowHandler the user can request with offset and limit and get the rows.",2505
Extract Method,NullOutputStream removed from Guava 15 {{com.google.common.io.NullOutputStream}} was dropped in Guava 15.0 in favor of {{com.google.common.io.ByteStreams.nullOutputStream()}} which prevents projects on this artifact from upgrading from Guava 14 to Guava 15.{noformat}ERROR 2013-09-26 17:46:12229 [hbase.master.MasterFileSystem] bootstraporg.apache.hadoop.hbase.DroppedSnapshotException: region: -ROOT-0at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1608)at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1482)at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1011)at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:959)at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:930)at org.apache.hadoop.hbase.master.MasterFileSystem.bootstrap(MasterFileSystem.java:447)at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:387)at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:134)at org.apache.hadoop.hbase.master.MasterFileSystem.<init>(MasterFileSystem.java:119)at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:536)at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:395)at java.lang.Thread.run(Thread.java:680)Caused by: java.lang.NoClassDefFoundError: com/google/common/io/NullOutputStreamat org.apache.hadoop.hbase.io.hfile.HFileWriterV2.close(HFileWriterV2.java:374)at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.close(StoreFile.java:1283)at org.apache.hadoop.hbase.regionserver.Store.internalFlushCache(Store.java:836)at org.apache.hadoop.hbase.regionserver.Store.flushCache(Store.java:747)at org.apache.hadoop.hbase.regionserver.Store$StoreFlusherImpl.flushCache(Store.java:2229)at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1583)... 11 moreCaused by: java.lang.ClassNotFoundException: com.google.common.io.NullOutputStreamat java.net.URLClassLoader$1.run(URLClassLoader.java:202)at java.security.AccessController.doPrivileged(Native Method)at java.net.URLClassLoader.findClass(URLClassLoader.java:190)at java.lang.ClassLoader.loadClass(ClassLoader.java:306)at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)at java.lang.ClassLoader.loadClass(ClassLoader.java:247)... 17 more{noformat},2506
Extract Method,DistributedHBaseCluster should not throw exceptions but do a best effort restore At the end of integration tests we are calling DistributedCluster.restoreCluster() in case CM has killed nodes so that we can leave the cluster in the same state that we have taken over. However if CM is not used in a test (for example ITLoadAndVerify) but some regions servers die or an external daemon kills the servers we will still try to restore at the end of the test which may or may not succeed (depending on configuration the region server going being unaccessible etc. )We can do two things either do a best effort restore cluster which will not fail the test if there are any errors or we can skip running restore if no disruptive actions have taken place. I am leaning towards the former one since if an RS goes down with or w/o CM due to bad disk etc. we cannot restore the cluster but we should not fail the test in this case. ,2507
Extract Method,Custom threadpool for Coprocessor obtained HTables Coprocessors currently can only use the default HTable constructor that takes a single thread-pool.This is overly constrictive for coprocessors that desire tighter control over their resources.,2508
Extract Method,hbasefsck.numthreads' property isn't passed to hbck via cmdline -D option We use generic option way to pass _'hbasefsck.numthreads'_ property to _'hbase hbck'_ but it does not accept our new setting value{code}hbase hbck -D hbasefsck.numthreads=5{code}We can still find there are threads over than 5 we already set via generic opttion{code}[2013-10-24 09:25:02561][pool-2-thread-6][DEBUG][org.apache.hadoop.security.UserGroupInformation]: PrivilegedAction as:hbase/spn-d-hdn1.sjdc@ISPN.TRENDMICRO.COM (auth:KERBEROS) from:sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) (UserGroupInformation.java:1430)[2013-10-24 09:25:02562][pool-2-thread-10][DEBUG][org.apache.hadoop.security.UserGroupInformation]: PrivilegedAction as:hbase/spn-d-hdn1.sjdc@ISPN.TRENDMICRO.COM (auth:KERBEROS) from:sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) (UserGroupInformation.java:1430)[2013-10-24 09:25:02565][pool-2-thread-13][DEBUG][org.apache.hadoop.security.UserGroupInformation]: PrivilegedAction as:hbase/spn-d-hdn1.sjdc@ISPN.TRENDMICRO.COM (auth:KERBEROS) from:sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) (UserGroupInformation.java:1430)[2013-10-24 09:25:02566][pool-2-thread-11][DEBUG][org.apache.hadoop.security.UserGroupInformation]: PrivilegedAction as:hbase/spn-d-hdn1.sjdc@ISPN.TRENDMICRO.COM (auth:KERBEROS) from:sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) (UserGroupInformation.java:1430)[2013-10-24 09:25:02567][pool-2-thread-9][DEBUG][org.apache.hadoop.security.UserGroupInformation]: PrivilegedAction as:hbase/spn-d-hdn1.sjdc@ISPN.TRENDMICRO.COM (auth:KERBEROS) from:sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) (UserGroupInformation.java:1430)[2013-10-24 09:25:02568][pool-2-thread-12][DEBUG][org.apache.hadoop.security.UserGroupInformation]: PrivilegedAction as:hbase/spn-d-hdn1.sjdc@ISPN.TRENDMICRO.COM (auth:KERBEROS) from:sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) (UserGroupInformation.java:1430)[2013-10-24 09:25:02570][pool-2-thread-7][DEBUG][org.apache.hadoop.security.UserGroupInformation]: PrivilegedAction as:hbase/spn-d-hdn1.sjdc@ISPN.TRENDMICRO.COM (auth:KERBEROS) from:sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) (UserGroupInformation.java:1430)[2013-10-24 09:25:02571][pool-2-thread-14][DEBUG][org.apache.hadoop.security.UserGroupInformation]: PrivilegedAction as:hbase/spn-d-hdn1.sjdc@ISPN.TRENDMICRO.COM (auth:KERBEROS) from:sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) (UserGroupInformation.java:1430){code},2509
Extract Method,We need a Hbase Partitioner for TableMapReduceUtil.initTableReduceJob MR Jobs When we run say 20 reducers they all get ~1/20th of the data to output to the table.The problem for us on large import jobs is the data gets sorted by key and the all Reducers pound one region at a time.we need to add onto the TableMapReduceUtil.initTableReduceJob method so it can set the partitioner and set the number of reducers = number of regions as the table map does for maps.Then the Partitioner will send all the BatchUpdates for one region to one reducer.So we get a more even spread of writers to the regions this would assure that only one reducer will send updates to one region keeping any one region from getting more overloaded the others.,2510
Extract Method,for idempotent operation dups return the result instead of throwing conflict exception After HBASE-3787 we could store mvcc in operation context and use it to convert the modification request into read on dups instead of throwing OperationConflictException.MVCC tracking will have to be aware of such MVCC numbers present. Given that scanners are usually relatively short-lived that would prevent low watermark from advancing for quite a bit more time,2511
Extract Method,Narrow getClosestRowBefore by passing column family Currently when we do getClosestRowBefore we're usually just interested in catalog table edits to the info family. As its written we'll also go trawl the region historian column family though its irrelevant and worse if we got a row out of here with no corresponding info entry we'd be hosed. Add being able to narrow the scope of the getClosestRowBefore by passing column family to dive in.,2512
Rename Method,Add support for ejb-ref resolution by matching on the ejb interfaces Some applications assume that ejb-refs can be resolved by matching on interface types.,2514
Extract Method,JSR-88 deployer should work remotely Currently the deployer just sends the File to the server so the tool and server must be on the same machine (or have the File on a network drive).It would be useful if the File or InputStream could be sent to the server allowing deployment to a remote machine.However RMI may not be the best transport for this :-),2515
Rename Method,SocketProtocolStressTest  fails SocketProtocolStressTest testConcurrentRequests() constantly fails during a maven build (cf. TIMEOUT problem GERONIMO-160) but succeeds after a 'Run | JUnit Test' from within Eclipse v. 3.0.1. I'd like to have it excluded from unitTest in the file modules/network/project.xml like DatagramProtocolTest (patch follows). BTW: Could someone with Admin rights for jira add the network module to the Component/s list. Thanks.-- Ralf,2516
Rename Method,Connection factories extracted from conceptually wrong gbean Currently connection factories/datasources (or their proxies) are obtained from the JCAManagedConnectionFactory gbean. Since there is a ConnectionFactory/Datasource gbean for the jsr-77 requirements it would make more sense to obtain the connection factory/datasource from there. This would have the additional feature of allowing one to set up several connection factories under different names that all use the same ConnectionManager and ManagedConnectionFactory. This would for instance let you set up separately named QueueConnectionFactory and TopicConnectionFactory that share the same connections: named appropriately this can let you leave out resource-refs in plans for apps that call the factories different names.,2517
Extract Method,GBeans should use jsr-77 naming conventions and these names should have mostly default components Currently the usage of object names for non-j2ee-wrapping gbeans is more or less random and confusing. We should adopt as much of jsr-77 naming as possible for our gbeans. Furthermore as little as possible of the names should be specified in the gbean xml descriptor. Here's a proposal:1. A service module that has no parent must specify domain and server name. This domain and server name will be inherited by all children recursively.2. All gbeans deployed from a service dd will have J2EEApplication=null and GeronimoModule=<configId>3. All gbeans deployed from a j2ee module or application will have J2EEApplication set from the application and GeronimoModule=<configId>4. A gbean xml descriptor will have attributes for j2eeType and name. We will invent more j2eeType names as needed and prefix them with Ger or Geronimo.I'm inclined to remove the possibility of directly specifying the entire object name. If it is really needed I'd suggest the attribute be called target-name in analogy to the usage in refs.,2518
Extract Method,"csiv2 CSS/TSS gbean references should be shorter The references to CSS beans in ejb-refs using corba now require the entire gbean name. This should be shortened to allow supplying just the parts of the gbean name not inferrable from the context (normally just the ""name"" component). This is in line with most other j2ee-related gbean references. Similarly for TSS references when they appear.",2520
Rename Method,Remaining EJB JAR JavaBeans The initial checkin of JavaBeans for the XML DDs earlier today included a partial implementation of EJB-JAR but it was pretty rough because we hadn't combined it with the J2EE objects yet. Now the EJB JAR JavaBeans are complete. I also added minimal JavaDoc to all the JavaBeans.There's a loader for the EJB JAR based on DOM that's about 1/2 complete (it handles everything under enterprise-beans but nothing under relationships or assembly-descriptor). Nevertheless it's enough to begin to work with. This involved a couple minor enhancements to the loader utility classes that Jeremy put together.This also includes the validator which uses the new beans to process META-INF/ejb-jar.xml. The actual tests so far are minimal but it should be pretty easy to extend in parallel.,2521
Extract Method,"Directory based hot deployment support Here is an initial start of directory-based hot deployment for Geronimo. I have not had the chance to update it with the things I would like to see so I'm going ahead and submitting it so a commiter can tweak it and get it into a build.Basically this is a GBean service that monitors a specified directory in the filesystem for any ""additions modifications or deletions"". Based on the event a particual module in the filesystem will either be deployed redeployed or undeployed. This service takes the assumtion that the deployment plan is packaged within the module. Also the specified directory is currently a hard-coded attribute in the deployment plan. It would be nice if the configuration started up a webapp as welland the gbean was linked to this webapp so that the directory could be changed on the fly and or multiple directories as well as other options could be specified. Perhaps this could be something configurable in the console?Also I am currently calling the org.apache.geronimo.deployment.Deployer to do the deploy. I initially wanted to reference the Deployer gbean and invoke deploy through it but was having issues with that so I used this as a workaround.The class that monitors the file sytem I found on the internet however it was only able to detected generic changes. I made modifications to the file so that these changes to the filesystem are catagorized as either additions modifiecations or deletions. See the license on the file as I am unsure weather this can be used or wether it will need to be re-written from scratch which if is the case should be trivial.Please let me know if this is a good start any suggestions and if you think this is something that can be pushed into 1.0 I will be glad to spend additional time on it to get it in. I did this more for a learning exercise so its pretty scrappy right now but I think alot could be done with it.Thanks!!Thanks.",2522
Extract Method,WorkManager implementation This patch contains an implementation of the WorkManager spi. This implementation delegates the submitted work to three distinct executors - one for each synchronization policy. Each executor is a thread pool based on the Doug Lea's PooledExecutor.The WorkManager and its WorkExecutors are MBeans and are deployed via the WorkManagement-service.xml file.During the execution of a work the WorkListener provided during a the work submission is duly notified.A test case yielding the three distinct synchronization policy is also provided. This test case uses the WorkListener notifications in order to introspect the status of submitted works.Gianny,2523
Inline Method,Provide option settings for controlling hash table size Provide session and system option settings to control the minimum and maximum hash table size for operators such as HashJoin and HashAggregate.,2524
Rename Method,Provide a reset command to reset an option to its default value Within a session currently we set configuration options and it would be very useful to have a 'reset' command to reset the value of an option to its default system value: ALTER SESSION RESET <option name> If we don't want to add a new keyword for RESET we could potentially overload the SET command and allow the user to set to the 'default' value.,2526
Extract Method,Enhance memory profiling and reduce memory consumption of key relational operators  0,2527
Extract Method,JSON projection pushdown 0,2529
Extract Method,"Create new ""all text mode"" for JSON Currently Drill fails on schema changes in JSON. Some of these are due to fundamental limitations of the Drill record representation. Others are due to Drills lack of full support for mutating schema. This can be solved temporarily by reading in all data as strings and allowing users to use some combination of case statements and UDFs to appropriately handle their own schema mutations.",2530
Extract Method,"Enable Partition Pruning for filesystem queries If you have files:/files/2012/Jan/log.csv/files/2013/Jan/log.csvAnd write the query ""select * from `/files/` where dir0 = 2012 and dir1='Jan' ""Drill will read both files and then filter out all of the second file. Drill should recognize that dir0=2012 and dir1='Jan' can be pushed down into the storage layer to prune the files read as part of the query.",2531
Extract Method,"Offload fragment profile data to DFS rather than storing in ZooKeeper PStores were really built for trivial configuration data not large query profiles. As such we should move to using the DFS for storage of query profiles when distributed mode.Release Notes:-By default the blob data (hence the fragment profiles) are stored on the local file system under {{$DRILL_LOG_DIR}} folder however this can be moved to DFS by specifying the target folder URL using {{""drill.exec.sys.store.provider.zk.blobroot""}} setting in Drill configuration.",2532
Rename Method,Avro Record Reader Record reader implementation for Avro data files.,2533
Rename Method,Update HiveRecordReader to read Hive decimal fields with scale and precision Currently HiveRecordReader reads decimal data in Hive and converts to VarChar. The reason is: Hive-0.12 doesn't enforce same precision and scale for all records. Drill may get records with different precision and scale within a column which Drill can't handle it currently. In Hive-0.13 scale and precision are enforced for all records in a column. As DRILL-1347 upgrades Hive storage plugin to work with Hive-0.13 this JIRA is created to track changes to HiveRecordReader to read Hive decimal data as decimal and not as VarChar.,2534
Move Method,Enable full engine in JDBC Layer You can get to the full exec engine now but too many things are implicitly defined. Instead we need to add explicit options.My proposal is to define a set of jdbc connection string properties:engine=ref means that the Reference interpreter's capabilities and schemas are available. engine=full (default) means that the full execution engines tools should be available. engine=both means that both types are available.For engine=full.We will have the additional property: zk=[connection string]This also requires the implementation of a smarter Schema type that provides automatic schema for all types recognized by the execution engine.,2535
Extract Method,DrillClient becomes invalid after drillbit restart DrillClient connects to drillbit; drillbit restarts; and DrillClient cannot send any query to drillbit.I haven't looked into this; should DrillClient / RpcBus be capable of handling reconnect and retry automatically or should user's code handle this?,2536
Rename Method,Merge Join Operator Implement the merge join physical operator with support for left right and inner joins.,2537
Rename Method,Implement filter pushdown for Parquet The parquet reader currently supports project pushdown for limiting the number of columns read however it does not use filter pushdown to read a subset of the requested columns. This is particularly useful with parquet files that contain statistics most importantly min and max values on pages. Evaluating predicates against these values could save some major reading and decoding time.The largest barrier to implementing this is the current design of the reader. Firstly we currently have two separate parquet readers one for reading flat files very quickly and another or reading complex data. There are enhancements we can make the the flat reader to make it support nested data in a much more efficient manner. However the speed of the flat file reader currently comes from being able to make vectorized copies out the the parquet file. This design is somewhat at odds with filter pushdown as we will only can make useful vectorized copies if the filter matches a large run of values within the file. This might not be too rare a case assuming files are often somewhat sorted on a primary field like date or a numeric key and these are often fields used to limit the query to a subset of the data. However for cases where we are filter out a few records here and there we should just make individual copies.We need to do more design work on the best way to balance performance with these use cases in mind.,2538
Rename Method,"Add peak memory allocation in a operator to OperatorStats. Currently we have ""localMemoryAllocated"" which is always set to zero as we try to fill the stats at the end of fragment execution by calling allocator.getAllocatedMemory() which at that point has already released the allocated memory. Instead if we have a stat for peak memory an allocator has seen in the lifetime of the operator execution will be useful (each operator has its own allocator). Example query on query profile: To find aggregate of peak memory of each operator across all minor fragments in a major fragment and list them in descending order of peak memory usage{code:sql}SELECTmajorFragmentIdopProfile['operatorType'] opTypesum(opProfile['peakLocalMemoryAllocated']) aggPeakMemoryAcrossAllMinorFragments FROM (SELECTmajorFragmentIdflatten(minorFragProfile['operatorProfile']) opProfileFROM (SELECTmajorFragment['majorFragmentId'] majorFragmentId flatten(majorFragment['minorFragmentProfile']) minorFragProfileFROM(SELECT flatten(fragmentProfile) as majorFragment from dfs.`/tmp/a.json`)))-- WHERE opProfile['operatorType'] = 6 -- If want to filter to particular operatorGROUP BY majorFragmentIdopProfile['operatorType']ORDER BYaggPeakMemoryAcrossAllMinorFragments DESC;{code}{code}+-----------------+------------+--------------------------------------+| majorFragmentId | opType | aggPeakMemoryAcrossAllMinorFragments |+-----------------+------------+--------------------------------------+| 1 | 4 | 115065856 || 1 | 3 | 10027008 || 0 | 3 | 1671168 || 3 | 6 | 1536000 || 2 | 6 | 901120 || 1 | 6 | 606208 || 3 | 28 | 393216 || 2 | 28 | 229376 || 3 | 10 | 122880 || 2 | 10 | 81920 || 0 | 11 | 0 || 0 | 10 | 0 || 0 | 13 | 0 || 1 | 10 | 0 || 1 | 11 | 0 |+-----------------+------------+--------------------------------------+{code}",2539
Rename Method,Make JDBC connection honor a configuration option for disabling embedded web server Current implementation of Drill's jdbc connection does not honor nor handle parameters passed from jdbc connection string(except the local mode flag). The proposal is to make the implementation recognize a bag of configuration options passed by user(specifically the one for disabling the embedded web server). A typical use case for this feature would be in unit tests. Some jdbc unit tests instantiates an individual DrillBit per test case in local mode that in turn starts an embedded web server. This prevents us from parallelizing the tests due to the fact that port assigned for the web server is currently in use by the former bit. This also impairs the test runtime as starting the web server takes from 1-2 seconds/test case.,2541
Move Method,"Enable querying partition information without reading all data When reading a series of files in nested directories Drill currently adds columns representing the directory structure that was traversed to reach the file currently being read. These columns are stored as varchar under tha names dir0 dir1 ... As these are just regular columns Drill allows arbitrary queries against this data in terms of aggregates filter sort etc. To allow optimizing reads basic partition pruning has already been added to prune in the case of an expression like dir0 = ""2015"" or a simple in list which is converted during planning to a series of ORs of equals expressions. If users want to query the directory information dynamically and not include specific directory names in the query this will prompt a full table scan and filter operation on the dir columns. This enhancement is to allow more complex queries to be run against directory metadata and only scanning the matching directories.",2542
Extract Method,Allow multithreaded copy and/or flush in PartitionSender Related to DRILL-133. As in LocalExchange we merge data from multiple receivers into LocalExchange to fan it out later to multiple Senders amount of data that needs to be sent out increases. Add ability to copy/flush data in multiple threads,2543
Extract Method,CLONE - Parquet record reader should log which file and which record caused an error in the reader I believe the title is self exploratory.If the parquet reader fails for any reason due to an offending record drill should log which file (if there are multiple files) and which line/record the error occurs at. This will improve debugging when dealing with large files/ large number of files.,2544
Rename Method,Implement Diagnostic Operator Support inserting a diagnostic physical operator into a physical plan that records the record batches output of a preceding operator. Additionally provide a tool to view this data.,2545
Rename Method,Use PojoRecordReader for all system tables The current implementation uses SystemRecordReader to populate SystemRecords which is not required. PojoRecordReader can be used to accomplish the same task.,2547
Extract Method,Move Drill to alternative cost-based planner for Join planning 0,2548
Extract Method,Add convenience methods to test builder for creating nested baseline values When building a test case we often need to create list instances to set baseline values in Java space(as opposed to using baseline queries). This issue proposes adding a static utility method TestBuilder#listOf(values) to expedite process of creating lists. The same applies for maps such like TestBuilder#mapOf(keyValueSequence),2549
Move Method,"Hive partition pruning is not happening Tested on 1.0.0 with below commit id and hive 0.13.{code}> select * from sys.version;+-------------------------------------------+--------------------------------------------------------------------+----------------------------+--------------+----------------------------+| commit_id | commit_message | commit_time | build_email | build_time |+-------------------------------------------+--------------------------------------------------------------------+----------------------------+--------------+----------------------------+| d8b19759657698581cc0d01d7038797952888123 | DRILL-3100: TestImpersonationDisabledWithMiniDFS fails on Windows | 15.05.2015 @ 01:18:03 EDT | Unknown | 15.05.2015 @ 03:07:10 EDT |+-------------------------------------------+--------------------------------------------------------------------+----------------------------+--------------+----------------------------+1 row selected (0.083 seconds){code}How to reproduce:1. Use hive to create below partition table:{code}CREATE TABLE partition_table(id INT username string)PARTITIONED BY(year STRING month STRING)ROW FORMAT DELIMITED FIELDS TERMINATED BY """";insert into table partition_table PARTITION(year='2014'month='11') select 1'u' from passwords limit 1;insert into table partition_table PARTITION(year='2014'month='12') select 2's' from passwords limit 1;insert into table partition_table PARTITION(year='2015'month='01') select 3'e' from passwords limit 1;insert into table partition_table PARTITION(year='2015'month='02') select 4'r' from passwords limit 1;insert into table partition_table PARTITION(year='2015'month='03') select 5'n' from passwords limit 1;{code}2. Hive query can do partition pruning for below 2 queries:{code}hive> explain EXTENDED select * from partition_table where year='2015' and month in ( '02''03') ;partition values:month 02year 2015partition values:month 03year 2015 explain EXTENDED select * from partition_table where year='2015' and (month >= '02' and month <= '03') ;partition values:month 02year 2015partition values:month 03year 2015{code}Hive only scans 2 partitions -- 2015/02 and 2015/03.3. Drill can not do partition pruning for below 2 queries:{code}> explain plan for select * from hive.partition_table where `year`='2015' and `month` in ('02''03');+------+------+| text | json |+------+------+| 00-00 Screen00-01 Project(id=[$0] username=[$1] year=[$2] month=[$3])00-02 SelectionVectorRemover00-03 Filter(condition=[AND(=($2 '2015') OR(=($3 '02') =($3 '03')))])00-04 Scan(groupscan=[HiveScan [table=Table(dbName:default tableName:partition_table) inputSplits=[maprfs:/user/hive/warehouse/partition_table/year=2015/month=01/000000_0:0+4 maprfs:/user/hive/warehouse/partition_table/year=2015/month=02/000000_0:0+4 maprfs:/user/hive/warehouse/partition_table/year=2015/month=03/000000_0:0+4] columns=[`*`] partitions= [Partition(values:[2015 01]) Partition(values:[2015 02]) Partition(values:[2015 03])]]])> explain plan for select * from hive.partition_table where `year`='2015' and (`month` >= '02' and `month` <= '03' );+------+------+| text | json |+------+------+| 00-00 Screen00-01 Project(id=[$0] username=[$1] year=[$2] month=[$3])00-02 SelectionVectorRemover00-03 Filter(condition=[AND(=($2 '2015') >=($3 '02') <=($3 '03'))])00-04 Scan(groupscan=[HiveScan [table=Table(dbName:default tableName:partition_table) inputSplits=[maprfs:/user/hive/warehouse/partition_table/year=2015/month=01/000000_0:0+4 maprfs:/user/hive/warehouse/partition_table/year=2015/month=02/000000_0:0+4 maprfs:/user/hive/warehouse/partition_table/year=2015/month=03/000000_0:0+4] columns=[`*`] partitions= [Partition(values:[2015 01]) Partition(values:[2015 02]) Partition(values:[2015 03])]]]){code}Drill scans 3 partitions -- 2015/01 2015/02 and 2015/03.Note: if the inlist only has 1 value Drill can do partition pruning well:{code}> explain plan for select * from hive.partition_table where `year`='2015' and `month` in ('02');+------+------+| text | json |+------+------+| 00-00 Screen00-01 Project(id=[$0] username=[$1] year=[$2] month=[$3])00-02 Scan(groupscan=[HiveScan [table=Table(dbName:default tableName:partition_table) inputSplits=[maprfs:/user/hive/warehouse/partition_table/year=2015/month=02/000000_0:0+4] columns=[`*`] partitions= [Partition(values:[2015 02])]]]){code}",2550
Rename Method,TextReader should support multibyte line delimiters lineDelimiter in the TextFormatConfig doesn't support \r\n for record delimiters.,2551
Extract Method,csv reader should allow newlines inside quotes  When reading a csv file which contains newlines within quoted strings e.g. viaselect * from dfs.`/tmp/q.csv`;Drill 1.0 says:Error: SYSTEM ERROR: com.univocity.parsers.common.TextParsingException: Error processing input: Cannot use newline character within quoted stringBut many tools produce csv files with newlines in quoted strings. Drill should be able to handle them.Workaround: the csvquote program (https://github.com/dbro/csvquote) can encode embedded commas and newlines and even decode them later if desired.,2552
Extract Method,[Umbrella] Plan reads of Hive tables as native Drill reads when a native reader for the underlying table format exists All reads against Hive are currently done through the Hive Serde interface. While this provides the most flexibility the API is not optimized for maximum performance while reading the data into Drill's native data structures. For Parquet and Text file backed tables we can plan these reads as Drill native reads. Currently reads of these file types provide untyped data. While parquet has metadata in the file we currently do not make use of the type information while planning. For text files we read all of the files as lists of varchars. In both of these cases casts will need to be injected to provide the same datatypes provided by the reads through the SerDe interface.,2553
Extract Method,"Query planning support for partition by clause in Drill's CTAS statement We are going to add ""PARTITION BY"" clause in Drill's CTAS statement. The ""PARTITION BY"" clause will specify the list of columns out of the result table's column list that will be used to partition the data. CREATE TABLE table_name [ (col_name .... ) ][PARTITION BY (col_name ...)]AS SELECT_STATEMENT;Semantics restriction for the PARTITION BY clause:- All the columns in the PARTITION BY clause have to be in the table's column list or the SELECT_STATEMENT has a * column when the base table in the SELECT_STATEMENT is schema-less. Otherwise an query validation error would be raised.- When the partition column is resolved to * column in a schema-less query this * column could not be a result of join operation. This restriction is added since for * out of join operation query planner would not know which table might produce this partition column. Example :{code}create table mytable1 partition by (r_regionkey) as select r_regionkey r_name from cp.`tpch/region.parquet`{code}{code}create table mytable2 partition by (r_regionkey) as select * from cp.`tpch/region.parquet`{code}{code}create table mytable3 partition by (r_regionkey) asselect r.r_regionkey r.r_name n.n_nationkey n.n_name from cp.`tpch/nation.parquet` n cp.`tpch/region.parquet` rwhere n.n_regionkey = r.r_regionkey{code}Invalid case 1: Partition column is not in table's column list. {code}create table mytable4 partition by (r_regionkey2) as select r_regionkey r_name from cp.`tpch/region.parquet`{code}Invalid case 2: Partition column is resolved to * out of a join operator.{code}create table mytable5 partition by (r_regionkey) asselect * from cp.`tpch/nation.parquet` n cp.`tpch/region.parquet` rwhere n.n_regionkey = r.r_regionkey{code}",2554
Rename Method,Add named metrics and named operators in OperatorProfile + Useful when reading JSON query profile.+ Rename FragmentStats#getOperatorStats to FragmentStats#newOperatorStats,2555
Extract Method,Add New HTTPD format plugin Add an HTTPD logparser based format plugin. The author has been kind enough to move the logparser project to be released under the Apache License. Can find it here:<dependency><groupId>nl.basjes.parse.httpdlog</groupId><artifactId>httpdlog-parser</artifactId><version>2.0</version></dependency>,2556
Rename Method,"Add ANSI_QUOTES option so that Drill's SQL Parser will recognize ANSI_SQL identifiers  *Added a possibility of changing characters for quoting identifiers by setting QUOTING_IDENTIFIERS system/session option:*{code}planner.parser.quoting_identifiers{code}There are three modes for quoting identifiers:1. ""BACK TICKS"" (default quoting mode):Unicode U+0060; ""GRAVE ACCENT"" {code}`{code}The character is used for setting system/session option and for quoting identifiers;2. ""DOUBLE QUOTES"" Unicode U+0022; 'QUOTATION MARK' {code}""{code}The character is used for setting system/session option and for quoting identifiers;3. ""BRACKETS"" Unicode U+005B; 'LEFT SQUARE BRACKET' {code}[{code}The character is used for setting system/session option and for quoting identifiers as left quote character. The right quote character for quoting identifiers with this mode is Unicode U+005D; 'RIGHT SQUARE BRACKET'{code}]{code}Examples of using QUOTING_IDENTIFIERS option:{code}0: jdbc:drill:zk=local> select * from sys.options where name = 'planner.parser.quoting_identifiers';+-------------------------------------+---------+---------+----------+----------+-------------+-----------+------------+| name | kind | type | status | num_val | string_val | bool_val | float_val |+-------------------------------------+---------+---------+----------+----------+-------------+-----------+------------+| planner.parser.quoting_identifiers | STRING | SYSTEM | DEFAULT | null | ` | null | null |+-------------------------------------+---------+---------+----------+----------+-------------+-----------+------------+1 row selected (0.189 seconds)0: jdbc:drill:zk=local> select `employee_id` `full_name` from cp.`employee.json` limit 1;+--------------+---------------+| employee_id | full_name |+--------------+---------------+| 1 | Sheri Nowmer |+--------------+---------------+1 row selected (0.148 seconds)0: jdbc:drill:zk=local> ALTER SESSION SET planner.parser.quoting_identifiers = '""';+-------+----------------------------------------------+| ok | summary |+-------+----------------------------------------------+| true | planner.parser.quoting_identifiers updated. |+-------+----------------------------------------------+1 row selected (0.107 seconds)0: jdbc:drill:zk=local> select ""employee_id"" ""full_name"" from cp.""employee.json"" limit 1;+--------------+---------------+| employee_id | full_name |+--------------+---------------+| 1 | Sheri Nowmer |+--------------+---------------+1 row selected (0.129 seconds)0: jdbc:drill:zk=local> ALTER SESSION SET planner.parser.quoting_identifiers = '[';+-------+----------------------------------------------+| ok | summary |+-------+----------------------------------------------+| true | planner.parser.quoting_identifiers updated. |+-------+----------------------------------------------+1 row selected (0.102 seconds)0: jdbc:drill:zk=local> select [employee_id] [full_name] from cp.[employee.json] limit 1;+--------------+---------------+| employee_id | full_name |+--------------+---------------+| 1 | Sheri Nowmer |+--------------+---------------+1 row selected (0.14 seconds)0: jdbc:drill:zk=local> ALTER SESSION SET planner.parser.quoting_identifiers = '`';+-------+----------------------------------------------+| ok | summary |+-------+----------------------------------------------+| true | planner.parser.quoting_identifiers updated. |+-------+----------------------------------------------+1 row selected (0.1 seconds)0: jdbc:drill:zk=local> select `employee_id` `full_name` from cp.`employee.json` limit 1;+--------------+---------------+| employee_id | full_name |+--------------+---------------+| 1 | Sheri Nowmer |+--------------+---------------+1 row selected (0.139 seconds){code}*Other quoting characters are not acceptable while particular one is chosen:*{code}0: jdbc:drill:zk=local> ALTER SESSION SET planner.parser.quoting_identifiers = '""';+-------+----------------------------------------------+| ok | summary |+-------+----------------------------------------------+| true | planner.parser.quoting_identifiers updated. |+-------+----------------------------------------------+1 row selected (0.561 seconds)0: jdbc:drill:zk=local> select `employee_id` `full_name` from cp.`employee.json` limit 1;Error: PARSE ERROR: Lexical error at line 1 column 8. Encountered: ""`"" (96) after : """"SQL Query select `employee_id` `full_name` from cp.`employee.json` limit 1^[Error Id: 9bfcb6b7-7d9d-46d7-8ea0-78d1d88b5c3b on vitalii-pc:31010] (state=code=0){code}*There is a possibility of setting QUOTING_IDENTIFIERS by using the ""quoting_identifiers"" property in the jdbc connection URL string.* For example:{code}jdbc:drill:zk=local;quoting_identifiers=[{code}",2558
Rename Method,"Drop table support Umbrella JIRA to track support for ""Drop table"" feature.",2559
Extract Method,Use a factory to create the root allocator Use a factory instead of the constructor for the top-level direct memory allocator so that we can replace which allocator gets instantiated.,2560
Extract Method,"Handle schema changes in ExternalSort This improvement will make use of the Union vector to handle schema changes. When a new schema appears the schema will be ""merged"" with the previous schema. The result will be a new schema that uses Union type to store the columns where this is a type conflict. All of the batches (including the batches that have already arrived) will be coerced into this new schema.A new comparison function will be included to handle the comparison of Union type. Comparison of union type will work as follows:1. All numeric types can be mutually compared and will be compared using Drill implicit cast rules.2. All other types will not be compared against other types but only among values of the same type.3. There will be an overall precedence of types with regards to ordering. This precedence is not yet defined but will be as part of the work on this issue.",2562
Rename Method,Enhance StoragePlugin interface to expose logical space rules for planning purpose Currently StoragePlugins can only expose rules that are executed in physical space. Add an interface method to StoragePlugin to expose logical space rules to planner.,2563
Rename Method,Add Experimental Kudu plugin Merge the work done here into Drill master so others can utilize the plugin: https://github.com/dremio/drill-storage-kudu,2564
Inline Method,Kerberos Authentication Drill should support Kerberos based authentication from clients. This means that both the ODBC and JDBC drivers as well as the web/REST interfaces should support inbound Kerberos. For Web this would most likely be SPNEGO while for ODBC and JDBC this will be more generic Kerberos.Since Hive and much of Hadoop supports Kerberos there is a potential for a lot of reuse of ideas if not implementation.Note that this is related to but not the same as https://issues.apache.org/jira/browse/DRILL-3584,2567
Extract Method,Drill should support inbound impersonation Today Drill supports impersonation *to* external sources. For example I can authenticate to Drill as myself and then Drill will access HDFS using impersonationIn many scenarios we also need impersonation to Drill. For example I might use some front end tool (such as Tableau) and authenticate to it as myself. That tool (server version) then needs to access Drill to perform queries and I want those queries to run as myself not as the Tableau user. While in theory the intermediate tool could store the userid & password for every user to the Drill this isn't a scalable or very secure solution.Note that HS2 today does support inbound impersonation as described here: https://issues.apache.org/jira/browse/HIVE-5155 The above is not the best approach as it is tied to the connection object which is very coarse grained and potentially expensive. It would be better if there was a call on the ODBC/JDBC driver to switch the identity on a existing connection. Most modern SQL databases (Oracle DB2) support such function.,2570
Extract Method,Allow for FileSystemPlugin subclasses to override FormatCreator FileSystemPlugin subclasses are not able to customize plugins as FormatCreator in created in FileSystemPlugin constructor and immediately used to create SchemaFactory instance.FormatCreator instantiation should be moved to a protected method so that subclass can choose to implement it differently.,2571
Rename Method,"Drill and Hive have incompatible timestamp representations in parquet git.commit.id.abbrev=83d460cI created a parquet file with a timestamp type using Drill. Now if I define a hive table on top of the parquet file and use ""timestamp"" as the column type drill fails to read the hive table through the hive storage pluginImplementation: Added int96 to timestamp converter for both parquet readers and controling it by system / session option ""store.parquet.int96_as_timestamp"".The value of the option is false by default for the proper work of the old query scripts with the ""convert_from TIMESTAMP_IMPALA"" function.When the option is true using of that function is unnesessary and can lead to the query fail.",2572
Extract Method,Add support for Views (CREATE DROP and select) 0,2573
Extract Method,Specification of Ordering (ASC DESC) on a sort plan node uses Strings for construction should also allow for use of the corresponding Calcite Enums Small change to provide a cleaner interface when constructing sort configurations in tests. The current class mixes together two tasks converting between the strings we chose to put in the plans (ASC DESC) and the Calcite enums as well as validating the allowed values. Calcite has several values that we do not currently use like STRICTLY_ASCENDING/DESCENDING and CLUSTERED. We can break these two tasks apart to allow for construction directly from the Enum but still provide validation for only the drill allowed values.,2574
Extract Method,Create Logical Plan builder for programmatic creation of a Logical Plan Will be useful when users are generating a Logical Plan within Java. First consumer would likely be the SQL parser.,2575
Move Method,Add support for more truncation units in date_trunc function Currently we support only {{YEAR MONTH DAY HOUR MINUTE SECOND}} truncate units for types {{TIME TIMESTAMP and DATE}}. Extend the functions to support {{YEAR MONTH DAY HOUR MINUTE SECOND WEEK QUARTER DECADE CENTURY MILLENNIUM}} truncate units for types {{TIME TIMESTAMP DATE INTERVAL DAY INTERVAL YEAR}}.Also get rid of the if-and-else (on truncation unit) implementation. Instead resolve to a direct function based on the truncation unit in Calcite -> Drill (DrillOptiq) expression conversion.,2576
Extract Method,Improve performance for query on INFORMATION_SCHEMA when HIVE is plugged in A query such as {code}select * from INFORMATION_SCHEMA.`TABLES` {code}is converted as calls to fetch all tables from storage plugins. When users have Hive the calls to hive metadata storage would be: 1) get_table2) get_partitionsHowever the information regarding partitions is not used in this type of queries. Beside a more efficient way is to fetch tables is to use get_multi_table call.,2577
Rename Method,Generate warning on Web UI if drillbits version mismatch is detected Display drillbit version on web UI. If any of drillbits version doesn't match with current drillbit generate warning.Screenshots - NEW_matching_drillbits.JPG NEW_mismatching_drillbits.JPG,2578
Rename Method,Expose New System Metrics + Add more metrics to the DrillMetrics registry (exposed through web UI and jconsole through JMX): pending queries running queries completed queries current memory usage (root allocator)+ Clean up and document metric registration API-+ Deprecate getMetrics() method in contextual objects; use DrillMetrics.getRegistry() directly-+ Make JMX reporting and log reporting configurable through system properties (since config file is not meant to be used in common module),2579
Extract Method,Allow casting to boolean the same literals as in Postgre Drill does not return results when we try to cast 0 and 1 to boolean inside a value constructor.Drill version : 1.7.0-SNAPSHOT commit ID : 09b26277{noformat}0: jdbc:drill:schema=dfs.tmp> values(cast(1 as boolean));Error: SYSTEM ERROR: IllegalArgumentException: Invalid value for boolean: 1Fragment 0:0[Error Id: 35dcc4bb-0c5d-466f-8fb5-cf7f0a892155 on centos-02.qa.lab:31010] (state=code=0)0: jdbc:drill:schema=dfs.tmp> values(cast(0 as boolean));Error: SYSTEM ERROR: IllegalArgumentException: Invalid value for boolean: 0Fragment 0:0[Error Id: 2dbcafe2-92c7-475e-a2aa-9745ef72c1cc on centos-02.qa.lab:31010] (state=code=0){noformat}Where as we get results on Postgres for same query.{noformat}postgres=# values(cast(1 as boolean));column1---------t(1 row)postgres=# values(cast(0 as boolean));column1---------f(1 row){noformat}Stack trace from drillbit.log{noformat}2016-05-13 07:16:16578 [28ca80bf-0af9-bc05-258b-6b5744739ed8:frag:0:0] ERROR o.a.d.e.w.fragment.FragmentExecutor - SYSTEM ERROR: IllegalArgumentException: Invalid value for boolean: 0Fragment 0:0[Error Id: 2dbcafe2-92c7-475e-a2aa-9745ef72c1cc on centos-02.qa.lab:31010]org.apache.drill.common.exceptions.UserException: SYSTEM ERROR: IllegalArgumentException: Invalid value for boolean: 0Fragment 0:0[Error Id: 2dbcafe2-92c7-475e-a2aa-9745ef72c1cc on centos-02.qa.lab:31010]at org.apache.drill.common.exceptions.UserException$Builder.build(UserException.java:543) ~[drill-common-1.7.0-SNAPSHOT.jar:1.7.0-SNAPSHOT]at org.apache.drill.exec.work.fragment.FragmentExecutor.sendFinalState(FragmentExecutor.java:318) [drill-java-exec-1.7.0-SNAPSHOT.jar:1.7.0-SNAPSHOT]at org.apache.drill.exec.work.fragment.FragmentExecutor.cleanup(FragmentExecutor.java:185) [drill-java-exec-1.7.0-SNAPSHOT.jar:1.7.0-SNAPSHOT]at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:287) [drill-java-exec-1.7.0-SNAPSHOT.jar:1.7.0-SNAPSHOT]at org.apache.drill.common.SelfCleaningRunnable.run(SelfCleaningRunnable.java:38) [drill-common-1.7.0-SNAPSHOT.jar:1.7.0-SNAPSHOT]at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]Caused by: java.lang.IllegalArgumentException: Invalid value for boolean: 0at org.apache.drill.exec.test.generated.ProjectorGen9.doSetup(ProjectorTemplate.java:95) ~[na:na]at org.apache.drill.exec.test.generated.ProjectorGen9.setup(ProjectorTemplate.java:93) ~[na:na]at org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.setupNewSchema(ProjectRecordBatch.java:444) ~[drill-java-exec-1.7.0-SNAPSHOT.jar:1.7.0-SNAPSHOT]at org.apache.drill.exec.record.AbstractSingleRecordBatch.innerNext(AbstractSingleRecordBatch.java:78) ~[drill-java-exec-1.7.0-SNAPSHOT.jar:1.7.0-SNAPSHOT]at org.apache.drill.exec.physical.impl.project.ProjectRecordBatch.innerNext(ProjectRecordBatch.java:129) ~[drill-java-exec-1.7.0-SNAPSHOT.jar:1.7.0-SNAPSHOT]at org.apache.drill.exec.record.AbstractRecordBatch.next(AbstractRecordBatch.java:162) ~[drill-java-exec-1.7.0-SNAPSHOT.jar:1.7.0-SNAPSHOT]at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:104) ~[drill-java-exec-1.7.0-SNAPSHOT.jar:1.7.0-SNAPSHOT]at org.apache.drill.exec.physical.impl.ScreenCreator$ScreenRoot.innerNext(ScreenCreator.java:81) ~[drill-java-exec-1.7.0-SNAPSHOT.jar:1.7.0-SNAPSHOT]at org.apache.drill.exec.physical.impl.BaseRootExec.next(BaseRootExec.java:94) ~[drill-java-exec-1.7.0-SNAPSHOT.jar:1.7.0-SNAPSHOT]at org.apache.drill.exec.work.fragment.FragmentExecutor$1.run(FragmentExecutor.java:257) ~[drill-java-exec-1.7.0-SNAPSHOT.jar:1.7.0-SNAPSHOT]at org.apache.drill.exec.work.fragment.FragmentExecutor$1.run(FragmentExecutor.java:251) ~[drill-java-exec-1.7.0-SNAPSHOT.jar:1.7.0-SNAPSHOT]at java.security.AccessController.doPrivileged(Native Method) ~[na:1.7.0_45]at javax.security.auth.Subject.doAs(Subject.java:415) ~[na:1.7.0_45]at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1595) ~[hadoop-common-2.7.0-mapr-1602.jar:na]at org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.java:251) [drill-java-exec-1.7.0-SNAPSHOT.jar:1.7.0-SNAPSHOT]... 4 common frames omitted{noformat}Implementation:Will be added support to allow casting the same list of strings as in Postgres [https://www.postgresql.org/docs/9.6/static/datatype-boolean.html],2580
Extract Method,FragmentExecutor should use EventProcessor and avoid blocking rpc threads Currently rpc thread can block when trying to deliver a cancel or early termination message to a blocked fragment executor.Foreman already uses an EventProcessor to avoid such scenarios. FragmentExecutor could be improved to avoid blocking rpc threads as well,2581
Rename Method,Improve metadata cache performance for queries with multiple partitions Consider queries of the following type run against Parquet data with metadata caching: {noformat}SELECT col FROM `A` WHERE dir0 = 'B`' AND dir1 IN ('1' '2' '3'){noformat}For such queries Drill will read the metadata cache file from the top level directory 'A' which is not very efficient since we are only interested in the files from some subdirectories of 'B'. DRILL-4530 improves the performance of such queries when the leaf level directory is a single partition. Here there are 3 subpartitions due to the IN list. We can build upon the DRILL-4530 enhancement by at least reading the cache file from the immediate parent level `/A/B` instead of the top level. The goal of this JIRA is to improve performance for such types of queries.,2582
Extract Method,Improve parquet reader performance Reported by a user in the field - We're generally getting read speeds of about 100-150 MB/s/node on PARQUET scan operator. This seems a little low given the number of drives on the node - 24. We're looking for options we can improve the performance of this operator as most of our queries are I/O bound. ,2583
Extract Method,"Add support for Null Equality Joins Join with an equality condition which allows null=null fails. For example if we use some of this queries:{code:sql}select ... FROM t1 t2 WHERE t1.c1 = t2.c2 OR (t1.c1 IS NULL AND t2.c2 IS NULL);select ... FROM t1 INNER JOIN t2 ON t1.c1 = t2.c2 OR (t1.c1 IS NULL AND t2.c2 IS NULL);{code}we got ""UNSUPPORTED_OPERATION ERROR"". We should add support for this option.",2586
Extract Method,Allow drillbits to advertise a configurable host address to Zookeeper There are certain situations such as running Drill in distributed Docker containers in which it is desirable to advertise a different hostname to Zookeeper than would be output by INetAddress.getLocalHost(). I propose adding a configuration variable 'drill.exec.rpc.bit.advertised.host' and passing this address to Zookeeper when the configuration variable is populated otherwise falling back to the present behavior.,2587
Extract Method,Support for Storage Plugin Optimizer Rules For example HBase storage plugin could add a rule which would transform an HBase scan followed by a qualified filter operation into an HBase scan with an HBase filter pushed directly to the scan operation.,2588
Extract Method,Temporary tables support Link to design doc - https://docs.google.com/document/d/1gSRo_w6q2WR5fPx7SsQ5IaVmJXJ6xCOJfYGyqpVOC-g/editGist - https://gist.github.com/arina-ielchiieva/50158175867a18eee964b5ba36455fbf#file-temporarytablessupport-md,2589
Extract Method,"Enhance the mock data source: better data SQL access Drill provides a mock data storage engine that generates random data. The mock engine is used in some older unit tests that need a volume of data but that are not too particular about the details of the data.The mock data source continues to have use even for modern tests. For example the work in the external storage batch requires tests with varying amounts of data but the exact form of the data is not important just the quantity. For example if we want to ensure that spilling happens at various trigger points we need to read the right amount of data for that trigger.The existing mock data source has two limitations:1. It generates only ""black/white"" (alternating) values which is awkward for use in sorting.2. The mock generator is accessible only from a physical plan but not from SQL queries.This enhancement proposes to fix both limitations:1. Generate a uniform randomly distributed set of values.2. Provide an encoding that lets a SQL query specify the data to be generated.Example SQL query:{code}SELECT id_i name_s50 FROM `mock`.employee_10K;{code}The above says to generate two fields: INTEGER (the ""_i"" suffix) and VARCHAR(50) (the ""_s50"") suffix; and to generate 10000 rows (the ""_10K"" suffix on the table.)",2590
Extract Method,Publish Operator and MajorFragment Stats in Profile page Currently we show runtimes for major fragments and minmaxavg times for setup processing and waiting for various operators.It would be worthwhile to have additional stats for the following:MajorFragment%Busy - % of the active time for all the minor fragments within each major fragment that they were busy. Operator Profile%Busy - % of the active time for all the fragments within each operator that they were busy. Records - Total number of records propagated out by that operator.,2591
Extract Method,Unit tests fail due to CTTAS temporary name space checks Drill can operate in embedded mode. In this mode no storage plugin definitions other than the defaults may be present. In particular when using the Drill test framework only those storage plugins defined in the Drill code are available.Yet Drill checks for the existence of the dfs.tmp plugin definition (as named by the {{drill.exec.default_temporary_workspace}} parameter. Because this plugin is not defined an exception occurs:{code}org.apache.drill.common.exceptions.UserException: PARSE ERROR: Unable to create or drop tables/views. Schema [dfs.tmp] is immutable.[Error Id: 792d4e5d-3f31-4f38-8bb4-d108f1a808f6 ]at org.apache.drill.common.exceptions.UserException$Builder.build(UserException.java:544)at org.apache.drill.exec.planner.sql.SchemaUtilites.resolveToMutableDrillSchema(SchemaUtilites.java:184)at org.apache.drill.exec.planner.sql.SchemaUtilites.getTemporaryWorkspace(SchemaUtilites.java:201)at org.apache.drill.exec.server.Drillbit.validateTemporaryWorkspace(Drillbit.java:264)at org.apache.drill.exec.server.Drillbit.run(Drillbit.java:135)at org.apache.drill.test.ClusterFixture.startDrillbits(ClusterFixture.java:207)...{code}Expected that either a configuration would exist that would use the default /tmp/drill location or that the check for {{drill.tmp}} would be deferred until it is actually required (such as when executing a CTTAS statement.)It seemed that the test framework must be altered to work around this problem by defining the necessary workspace. Unfortunately the Drillbit must start before we can define the workspace needed for the Drillbit to start. So this workaround is not possible.Further users of the embedded Drillbit may not know to do this configuration.,2592
Move Method,#N/A,2593
Extract Method,Limit memory usage for Hbase reader If early limit 0 optimization is set to true (alter session set `planner.enable_limit0_optimization` = true) when executing limit 0 queries Drill will return data type from available metadata if possible.When Drill can not determine data types from metadata (or if early limit 0 optimization is set to false) Drill will read first batch of data and determine schema.Hbase reader determines max batch size using magic number (4000) which can lead to OOM when row size is large. The overall vector/batch size issue will be reconsidered in future releases.This is temporary fix to avoid OOM.To limit memory usage for Hbase reader we are adding max allowed allocated memory contant which will default to 64 mb. Thus batch size will be limited to 4000 (as before if memory limit does not exceed) or to number of records that are within max allowed memory limit. If first row in batch is larger than allowed default it will be written in batch but batch will contain only this row.,2594
Extract Method,"Queue-based memory assignment for buffering operators Apache Drill already has a queueing feature based on ZK semaphores. We did a bit of testing to show that the feature does in fact work. We propose to enhance the feature with some light revisions to make work with the ""managed"" external sort and the newly-added spilling feature for the hash agg operator. The key requirement is to build on what we have for now; we may want to tackle a larger project to create a more complete solution later.Existing functionality:* Two ZK-based queues called the “small” and “large” query queues.* A threshold call it T given as a query cost to determine the queue into which a query will go.* Admit levels for the two queues: call them Qs and Ql.Basically when a query comes in:* Plan the query as usual.* Obtain the final query cost from the planner call this C.* If C<T the query goes into the small queue else it goes into the large queue.* Suppose the small queue. Ask ZK if the query can run.* ZK checks if Qs queries are already running. If so the query waits else the query runs.The proposed changes include:* Refactor the code to provide a queueing API that supports a variety of queuing mechanisms.* Provide three: the null queue (default) an in-process queue (for testing) and the ZK queues.* Modify the query profile web UI to show two new bits of information about queues:- The queue to which the query was sent.- The total planning cost.* Modify the query profile web UI to show two memory assignment numbers:- Total memory allocated to the query- Memory per sort or hash-add operatorThen add to the queue mechanism the ability to do memory assignment:* Provide a weight W: every small query gets 1 unit every large query gets W units.* Use the queue admit levels to determine total units: U = Qs + W * Ql.* Obtain total direct memory from the system. M.* Subtract a reserve percent R for overhead.* Do the math to get the memory per query for each query:* For the small queue: (M - R) / U* For the large queue: (M - R) / U * W* Use this memory amount as the “memory per query” number in the existing sort/hash-agg memory assignment (instead of the fixed 2 GB.)The result will be a nice incremental addition to what we already have and should make it a bit easier people to actually use the feature (because they can see the planning numbers and see the queues used allowing them to effectively tune the system.)The API used for the above features also allow third parties to add on a more robust admission control feature as needed perhaps tying into an existing queueing mechanism of their choice.",2596
Extract Method,"Support Impersonation without authentication for REST API Today if a user is not authenticated via REST API then there is no way to provide a user name for executing queries. It will by default be executed as ""anonymous"" user. This doesn't work when impersonation without authentication is enabled on Drill server side since anonymous user doesn't exist the query will fail. We need a way to provide a user name when impersonation is enabled on Drill side and query is executed from REST API.There are two approaches to achieve that:*1. Use form-based authentication*On Web UI user will be prompted to enter only login then session for that user will be created user will be treated as admin. Form-based authentication will cache user information so user won't need to set user name each time he / she wants to execute the query. Log in / out options will be also available. Example screenshot of login page is attached (login_page.JPG).From the programmatic perspective user would need first to authenticate and use cookie to get query result.*2. Use {{User-Name}} header in request*On Web UI on Query page additional input field will appear. User would need to enter user name before issuing the query. Example screenshot of query page is attached (query_page_with_user_name.JPG). Under the hood with user name would be added to client request as request header. On server side this header would be used to create user session principal. From the programmatic perspective user would need to add header when issuing the request.*_From the two above options second was chosen as it would ease REST API usage from the programmatic perspective plus using form-based authentication may lead to false assumption that user is authenticated which is in reality is not true._**Implementation details of the second approach:*_Note: the below implementation will take affect only if authentication is disabled and impersonation is enabled. By means of freemarker page won't include js lib and script if condition is not met._On the client side additional input field was added to the query page. When client is submitting the query request would be changed using ajax to add {{User-Name}} header which would be taken from the new input field. On the server side this header would be used to create session principal with provided user name and admin rights. If user name header was not provided (null or empty) the default anonymous principal will be used only in case when it is not POST request to /query or /query.json. If it is POST request to /query or /query.json {{User-Name}} header is required so error will be thrown. Also adding {{User Name}} input parameter is required from Web UI. User won't be able to send request until field is filled in.*Adding user name header approaches:*_Web UI_ enter user name in the User Name input field on Query page before submiiting the query (query_page_with_user_name.JPG) this step is required._sqlline_{code}./drill-localhost -n user1{code}_curl_ {code} curl -v -H ""Content-Type: application/json"" -H ""User-Name: user1"" -d '{""queryType"":""SQL"" ""query"": ""select * from sys.version""}' http://localhost:8047/query.json {code}_Java way_{code}String url = ""http://localhost:8047/query.json"";URLConnection connection = new URL(url).openConnection();connection.setDoOutput(true); // Triggers POST.connection.addRequestProperty(""User-Name"" ""user1"");connection.setRequestProperty(""Content-Type"" ""application/json"");String data = ""{\""queryType\"":\""SQL\"" \""query\"": \""select * from sys.version\""}"";try (OutputStream output = connection.getOutputStream()) {output.write(data.getBytes(StandardCharsets.UTF_8.name()));}try (InputStream response = connection.getInputStream()) {String result = IOUtils.toString(response);System.out.println(result);}{code}Note: {{Apache HttpClient}} can be used as well.",2597
Extract Method,Speed Up Unit Tests Tests can be split into categories.High-level categories:* Fast* SlowLow-level categories:* Vector* WebUI* Planner* Operator* Storage* Hive* JDBC* Kudu* Mongo* HbaseAfter the tests are categorized the Travis build can just run the fast tests.,2598
Extract Method,"Filter pushdown for parquet handles multi rowgroup file DRILL-1950 implemented the filter pushdown for parquet file but only in the case of one rowgroup per parquet file. In the case of multiple rowgroups per files it detects that the rowgroup can be pruned but then tell to the drillbit to read the whole file which leads to performance issue.Having multiple rowgroup per file helps to handle partitioned dataset and still read only the relevant subset of data without ending with more file than really needed.Let's say for instance you have a Parquet file composed of RG1 and RG2 with only one column a. Min/max in RG1 are 1-2 and min/max in RG2 are 2-3.If I do ""select a from file where a=3"" today it will read the whole file with the patch it will only read RG2.*For documentation*Support / Other section in https://drill.apache.org/docs/parquet-filter-pushdown/ should be updated.After the fix files with multiple row groups will be supported.",2599
Extract Method,Provide option to set query memory as percent of total Drill provides a parameter to set the memory per query as a static number which defaults to 2 GB. This number is a wonderful setting for the default Drillbit configuration of 8 GB heap; it allows 2-3 concurrent queries. But as Drillbit memory increases the default becomes a bit constraining. While users can change the setting they seldom do. In addition provide an option that sets memory as a percent of total memory. If the allocation is 10% say and total memory is 128 GB then each query gets ~13GB which is a big improvement. The existing option acts as a floor: the query must receive at least that much memory. *DOCUMENTATION* New option should be documented - planner.memory.percent_per_query Default - 0.05 (which is equivalent to 5 %) To disable feature set to 0. More information can be found in pull request description.,2601
Extract Method,Improve Parquet Reader Performance for Flat Data types  The Parquet Reader is a key use-case for Drill. This JIRA is an attempt to further improve the Parquet Reader performance as several users reported that Parquet parsing represents the lion share of the overall query execution. It tracks Flat Data types only as Nested DTs might involve functional and processing enhancements (e.g. a nested column can be seen as a Document; user might want to perform operations scoped at the document level that is no need to span all rows). Another JIRA will be created to handle the nested columns use-case.,2602
Rename Method,Avoid the strong check introduced by DRILL-5582 for PLAIN mechanism For PLAIN mechanism we will weaken the strong check introduced with DRILL-5582 to keep the forward compatibility between Drill 1.12 client and Drill 1.9 server. This is fine since with and without this strong check PLAIN mechanism is still vulnerable to MITM during handshake itself unlike mutual authentication protocols like Kerberos. Also for keeping forward compatibility with respect to SASL we will treat UNKNOWN_SASL_SUPPORT as valid value. For handshake message received from a client which is running on later version (let say 1.13) then Drillbit (1.12) and having a new value for SaslSupport field which is unknown to server this field will be decoded as UNKNOWN_SASL_SUPPORT. In this scenario client will be treated as one aware about SASL protocol but server doesn't know exact capabilities of client. Hence the SASL handshake will still be required from server side.,2603
Rename Method,"Implement ""CREATE TABLE IF NOT EXISTS"" Currently if a table/view with the same name exists CREATE TABLE fails with VALIDATION ERROR Having ""IF NOT EXISTS"" support for CREATE TABLE will ensure that query succeeds Also the same functionality was added for views.",2604
Move Method,"predicate pushdown support kafkaMsgOffset As part of Kafka storage plugin review below is the suggestion from Paul. {noformat} Does it make sense to provide a way to select a range of messages: a starting point or a count? Perhaps I want to run my query every five minutes scanning only those messages since the previous scan. Or I want to limit my take to say the next 1000 messages. Could we use a pseudo-column such as ""kafkaMsgOffset"" for that purpose? Maybe SELECT * FROM <some topic> WHERE kafkaMsgOffset > 12345 {noformat}",2605
Rename Method,Updating of Apache and MapR Hive libraries to 2.3.2 and 2.1.1-mapr-1710 versions respectively Currently Drill uses [Hive version 1.2.1 libraries|https://github.com/apache/drill/blob/master/pom.xml#L53] to perform queries on Hive. This version of library can be used for Hive1.x versions and Hive2.x versions too but some features of Hive2.x are broken (for example using of ORC transactional tables). To fix that it will be good to update drill-hive library version to 2.1 or newer. Tasks which should be done: - resolving dependency conflicts; - investigating backward compatibility of newer drill-hive library with older Hive versions (1.x); - updating drill-hive version for [MapR|https://github.com/apache/drill/blob/master/pom.xml#L1777] profile too. Starting from this commit: * Drill Hive client is updated to 2.3.2 and 2.1.1-mapr-1710 versions for default and MapR profiles respectively; * Drill supports of querying Hive transactional ORC bucketed tables [https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions]. Note: Updated Drill Hive client preserves backward compatibility with older 1.2.1 Hive server/metastore version.,2606
Extract Method,"Validate That Planner Does Not Assume HashJoin Preserves Ordering for FS MaprDB or Hive Explanation provided by Boaz: (As explained in the design document) The new ""automatic spill"" feature of the Hash-Join operator may cause (if spilling occurs) the rows from the left/probe side to be returned in a different order than their incoming order (due to splitting the rows into partitions). Currently the Drill planner assumes that left-order is preserved by the Hash-Join operator; therefore if not changes a query relying on that order may return wrong results (when the Hash-Join spills). A fix is needed. Here are few options (ordered from the simpler down to the most complex): # Change the order rule in the planner. Thus whenever an order is needed above (downstream) the Hash-Join the planner would add a sort operator. That would be a big execution time waste. # When the planner needs the left-order above the Hash-Join it may assess the size of the right/build side (need statistics). If the right side is small enough the planner would set an option for the runtime to avoid spilling hence preserving the left-side order. In case spilling becomes necessary the code would return an error (possibly with a message suggesting setting some special option and retrying; the special option would add a sort operator and allow the hash-join to spill). # When generating the code for the fragment above the Hash-Join (where left-order should be maintained) - at code-gen time check if the hash-join below spilled and if so add a sort operator. (Nothing like that exists in Drill now so it may be complicated).",2607
Extract Method,batch sizing for hash join limit output batch size for hash join based on memory.,2608
Extract Method,Support pushdown into System Table When querying a profile store we fetch all the records before applying a limit. {code:sql} select * from sys.profiles limit 1 {code} For a test scenario with 120K+ profiles in the store a {code:sql} select count(*) from sys.profiles {code} took 90 minutes.,2609
Extract Method,Web UI should indicate when operators have spilled in-memory data to disk Currently there is no indication of when an operator is spilling to disk which would help explain a slow running query. Suggestions are welcome but the current proposal is to simply update the Operators Overview section to show average and max spill cycles preferrably with a color code (or formatting).,2610
Extract Method,Cluster view should show more relevant information When fixing DRILL-6224 I noticed that the same information can be very useful to have in the cluster view shown on a Drillbit's homepage. The proposal is to show the following: # Heap Memory in use # Direct Memory (actively) in use - Since we're not able to get the total memory held by Netty at the moment but only what is currently allocated to running queries # Process CPU # Average (System) Load Factor Information such as the port numbers don't help much during general cluster health so it might be worth removing this information if more real-estate is needed. ,2611
Rename Method,limit batch size for hash aggregate limit batch size for hash aggregate based on memory.,2612
Rename Method,"Inconsistent method name ""field"". The following method is names as ""field"" but the method is mainly doing appending. So that rename the method as ""append"" should be better. {code:java} private void field(String label String value) { indent(); out.append(label) .append("" = "") .append(value) .append(""\n""); } {code} ",2614
Rename Method,Unordered Receiver does not report its memory usage The Drill Profile functionality doesn't show any memory usage for the Unordered Receiver operator. This is problematic when analyzing OOM conditions since we cannot account for all of a query memory usage. This Jira is to fix memory reporting for the Unordered Receiver operator.,2615
Extract Method,"Provide sqlTypeOf() and modeOf() functions Drill provides a {{typeof()}} function to return the type of a column. The returned string however has only the base data type. A Drill data type (a ""major type"") also includes a cardinality (a ""mode""). For example {{Optional Int}} or {{Required VarChar}}. This type information is useful for handling data conversions. For example if I could tell that a column value was a {{Nullable Int}} I could guess that it is one Drill invented and I could merge it by hand with the type from another file that had actual values. The two options are equivalent. Either provide a {{modeOf()}} to just return cardinality or a {{dataTypeOf()}} that returns both. (Maybe the {{modeOf()}} might be more useful.) h4. Documentation Documentation information (extracted from PR): h5. {{sqlTypeOf()}} {{sqlTypeOf()}} returns the data type (using the SQL names) whether the column is NULL or not. The SQL name is the one that can be used in a CAST statement. Thus {{sqlTypeOf( CAST(x AS <type> ))}} returns _<type>_ as the type name. If the type is {{DECIMAL}} then the type also includes precision and scale. Example: {{DECIMAL(6 3)}}. h5. {{modeOf()}} {{modeOf()}} returns the cardinality (mode) of the column as ""NOT NULL"" ""NULLABLE"" or ""ARRAY"". h5. {{drillTypeOf()}} The {{drillTypeOf()}} function that works just like {{typeOf()}} but returns the internal Drill names even if the value is NULL. h5. Example Here is an example usage that highlights our old friend ""nullable int"" for a missing column: {noformat} SELECT sqlTypeOf(a) AS a_type modeOf(a) AS a_mode FROM `json/all-null.json`; +----------+-----------+ | a_type | a_mode | +----------+-----------+ | INTEGER | NULLABLE | +----------+-----------+ {noformat} For arrays (repeated) types: {noformat} SELECT sqlTypeOf(columns) as col_type modeOf(columns) as col_mode FROM `csv/cust.csv`; +--------------------+-----------+ | col_type | col_mode | +--------------------+-----------+ | CHARACTER VARYING | ARRAY | +--------------------+-----------+ {noformat} For non-null types: {noformat} SELECT sqlTypeOf(`name`) AS name_type modeOf(`name`) AS name_mode FROM `csvh/cust.csvh`; +--------------------+------------+ | name_type | name_mode | +--------------------+------------+ | CHARACTER VARYING | NOT NULL | +--------------------+------------+ {noformat}",2617
Extract Method,Support JPPD (Join Predicate Push Down) This feature is to support the JPPD (Join Predicate Push Down). It will benefit the HashJoin Broadcast HashJoin performance by reducing the number of rows to send across the network the memory consumed. This feature is already supported by Impala which calls it RuntimeFilter ([https://www.cloudera.com/documentation/enterprise/5-9-x/topics/impala_runtime_filtering.html]). The first PR will try to push down a bloom filter of HashJoin node to Parquet’s scan node.   The propose basic procedure is described as follow: # The HashJoin build side accumulate the equal join condition rows to construct a bloom filter. Then it sends out the bloom filter to the foreman node. # The foreman node accept the bloom filters passively from all the fragments that has the HashJoin operator. It then aggregates the bloom filters to form a global bloom filter. # The foreman node broadcasts the global bloom filter to all the probe side scan nodes which maybe already have send out partial data to the hash join nodes(currently the hash join node will prefetch one batch from both sides ).       4.  The scan node accepts a global bloom filter from the foreman node. It will filter the rest rows satisfying the bloom filter.   To implement above execution flow some main new notion described as below:       1. RuntimeFilter It’s a filter container which may contain BloomFilter or MinMaxFilter.       2. RuntimeFilterReporter It wraps the logic to send hash join’s bloom filter to the foreman.The serialized bloom filter will be sent out through the data tunnel.This object will be instanced by the FragmentExecutor and passed to the FragmentContext.So the HashJoin operator can obtain it through the FragmentContext.      3. RuntimeFilterRequestHandler It is responsible to accept a SendRuntimeFilterRequest RPC to strip the actual BloomFilter from the network. It then translates this filter to the WorkerBee’s new interface registerRuntimeFilter. Another RPC type is BroadcastRuntimeFilterRequest. It will register the accepted global bloom filter to the WorkerBee by the registerRuntimeFilter method and then propagate to the FragmentContext through which the probe side scan node can fetch the aggregated bloom filter.       4.RuntimeFilterManager The foreman will instance a RuntimeFilterManager .It will indirectly get every RuntimeFilter by the WorkerBee. Once all the BloomFilters have been accepted and aggregated . It will broadcast the aggregated bloom filter to all the probe side scan nodes through the data tunnel by a BroadcastRuntimeFilterRequest RPC.      5. RuntimeFilterEnableOption   A global option will be added to decide whether to enable this new feature.   Welcome suggestion and advice from you.The related PR will be presented as soon as possible.,2618
Extract Method,Support for EMIT outcome in streaming agg Update the streaming aggregator to recognize the EMIT outcome,2619
Extract Method,Lateral excluding the columns from output container provided by projection push into rules With DRILL-6545 LateralPop will have information about list of columns to be excluded from the Lateral output container. Mostly this is used to avoid producing origin repeated column in Lateral output if it's not required from the projection list. This is needed because in absence of it Lateral has to copy the repeated column N number of times where N is the number of rows in right incoming batch for each left incoming batch row. This copy was very costly both from memory and latency perspective. Hence avoiding it is a must for Lateral-Unnest case.,2620
Rename Method,Improve RemovingRecordBatch to do transfer when all records needs to be copied SelectionVector2 contains list of indexes for the rows that RemovingRecordBatch can copy from underlying RecordBatch. SV2 is created by operator like Filter Limit etc to provide the selected rows from underlying buffer. Later then RemovingRecordBatch copies the rows based on indexes in SelectionVector2 to the output container of type NONE.  For cases when all the rows needs to be copied by RemovingRecordBatch from incoming batch it can be improved to do full transfer of ValueVectors from input to output container instead of row by row copy. For example if for an incoming batch all rows are selected by the Filter condition in FilterRecordBatch it will prepare an SV2 with all the record rowIndex. Later RemovingRecordBatch downstream of Filter can potentially do just transfer instead of row by row copy.,2622
Extract Method,"OperatingSystemMXBean class cast exception when loaded under IBM JVM Related to: https://issues.apache.org/jira/browse/DRILL-6289   [https://github.com/apache/drill/blob/1.14.0/common/src/main/java/org/apache/drill/exec/metrics/CpuGaugeSet.java#L28|https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_apache_drill_blob_1.14.0_common_src_main_java_org_apache_drill_exec_metrics_CpuGaugeSet.java-23L28&d=DwMFAg&c=cskdkSMqhcnjZxdQVpwTXg&r=-cT6otg6lpT_XkmYy7yg3A&m=f8a5MyR85-7Ns3KmymU7PI8Sk6qW8vRa9HJIa0-npNA&s=mpztPtwrTzNkgLcUORZdl5LQ6gyP5iAf3umFzgdOMeI&e=]   Exception in thread ""main"" java.lang.ExceptionInInitializerError     at java.lang.J9VMInternals.ensureError(J9VMInternals.java:141)     at java.lang.J9VMInternals.recordInitializationFailure(J9VMInternals.java:130)     at org.apache.drill.exec.metrics.DrillMetrics.getRegistry(DrillMetrics.java:111)     at org.apache.drill.exec.memory.AllocationManager.<clinit>(AllocationManager.java:64)     at org.apache.drill.exec.memory.BaseAllocator.<clinit>(BaseAllocator.java:48)     at org.apache.drill.exec.memory.RootAllocatorFactory.newRoot(RootAllocatorFactory.java:45)     at org.apache.drill.exec.memory.RootAllocatorFactory.newRoot(RootAllocatorFactory.java:40)     ... Caused by: java.lang.ClassCastException: com.ibm.lang.management.ExtendedOperatingSystem incompatible with com.sun.management.OperatingSystemMXBean     at org.apache.drill.exec.metrics.CpuGaugeSet.<init>(CpuGaugeSet.java:40)     at org.apache.drill.exec.metrics.DrillMetrics$RegistryHolder.registerSystemMetrics(DrillMetrics.java:63)     at org.apache.drill.exec.metrics.DrillMetrics$RegistryHolder.<clinit>(DrillMetrics.java:53)",2623
Extract Method,HashJoin should not build hash tables when probe side is empty. Currently when doing an Inner or a Right join we still build hashtables when the probe side is empty. A performance optimization would be to not build them.,2624
Extract Method,Push down column value predicates into HBase scan In continuation of DRILL-571 we should convert a qualified WHERE clause with columns into HBase scan.,2625
Extract Method,Drill needs to return complex types (e.g. map and array) as a JSON string Drill needs to help users understand the available columns in a given HBase table's column-family. One way to do this is to implement the DESCRIBE command for a column-family. The purpose of this is to let Drill do the column sampling. The mandatory LIMIT clause specifies the sample size so as to throttle the number of columns returned.==Instead of the above narrow proposal a general mechanism to return complex types in a JSON string so as to allow client tools (such as the ODBC driver) to operate on these complex types.Returning a map as JSON would provide transparency into HBase column-families and allow the ODBC driver to help surface the names of columns within a column-family.Returning an array as JSON would provide transparency into CSV files since that conveys the number of columns to the ODBC driver.,2627
Extract Method,Aggregate function for correlation coefficient calculation Adding new aggregate function for correlation calculation,2628
Extract Method,"Run-time code generation support when a function parameter is complex nested type. In this JIRA we will enhance the run-time code generation component in the following ways:1. Allow generate the run-time code for function whose parameter is FieldReader. A parameter FieldReader could match an argument of Repeated List Repeated Map or an element in Repeated List/map or any nested complex type.2. In addition FieldReader will also match any singular value such as int float4 varchar. With this support enabled one could implement a function with parameter FieldReader to match any simple or complex nested type.As an example of how to leverage this new code generation logic we add a new function ""convert_toJson"" which will convert any simple or complex type into a string in json format. @Param FieldReader input;@Output VarBinaryHolder out;",2629
Move Method,Backblaze B2 Cloud Storage Backblaze announced B2 which has its own object storage API:https://www.backblaze.com/b2/docs/,2630
Move Method,Add security group/firewall support to base ComputeService Right now we don't have an abstraction for dealing with security groups/firewalls across various compute APIs. As a result code that needs to deal with said security groups/firewalls has to have implementations for each cloud it supports which is...a pain to say the least. While not all clouds have support for security groups or a similar concept many do (at the very least EC2 Nova and CloudStack all do and CloudStack in fact has two different implementations depending on the network type) and this seems to be standard enough to merit a generic interface in the base ComputeService.,2632
Extract Method,Add support for subnetwork definition in google compute Google Compute added subnetwork definition in Feb 2016. Support should be added to jclouds.https://cloud.google.com/compute/docs/subnetworks#subnetworks_features,2633
Extract Method,Add temporary signed URL support for S3 S3BlobRequestSigner throws UnsupportedOperationException for the 3-arg variants of signGetBlob and signPutBlob that take an expiration timeout as input.,2636
Extract Method,Swift: Delete chunks when deleting a multipart blob jclouds automatically creates a blob for each chunk when it does a multipart upload.When the multipart blob is deleted with BlobStore.removeBlob() the chunks are left behind. It would be nice if jclouds automatically deleted them.Test case here: https://github.com/brightinteractive/jclouds-prototype/blob/1cfa515b15658b3b7f74b2714dbf5b47fb4e14f2/src/test/java/com/brightinteractive/jclouds/PutGetDeleteMultiPartTest.java (it fails at the last assert assertEquals(0 blobStore.countBlobs(container))).IMHO the automatic deletion doesn't need to be optional which means no API changes are needed - if jclouds automatically creates chunks why shouldn't it always automatically delete them?JCLOUDS-250 probably needs to be addressed before this issue otherwise jclouds might automatically delete other blobs whose names start with the multipart blob's name.,2638
Extract Method,Support >2GB payloads in a single request when running default http driver on Java 7 The default http driver uses java's HttpUrlConnection which uses an integer to determine fixed length of content. This means that it cannot be used to send >2GB content. The current workaround is to use an http driver that supports >2GB payloads in a single request.This issue will conditionally allow the default driver to support >2GB payloads in a single request. It will do so via conditionally using setFixedLengthStreamingMode(long) on Java 7 and setFixedLengthStreamingMode(int) on Java 6:https://gist.github.com/andrewgaul/6439757,2640
Rename Method,Migrate unit tests from ChefApiTest to ChefApiExpectTest Unit tests in ChefApiTest.java are using old way. It is good to migrate them to new way as we have done in ChefApiExpectTest,2642
Rename Method,Use the Omnibus installer to install the Chef client Currently when running Chef Solo or Chef to bootstrap a node the Chef client is manually installed. The installation process installs Ruby RubyGems and the Chef gems.Since gems in ruby are automatically updated and the latest version is downloaded by default this procedure may get broken when a backwards incompatible version of the chef-client gem is published. This can be workarouned by specifying the versions of Ruby and the Chef gems to install when creating a ChefContext or building a ChefSolo script but still isn't strong enough.Opscode released Omnibus to easily install the chef-client and all its dependencies. The Omnibus installer will detect the operating system of the node and install an appropriate Ruby and RubyGems distribution compatible with the desired chef-client version in a transparent and isolated way so it should be the preferred way to install Chef. This will simplify a lot the Chef bootstrap script and reduce considerably the points where failures can happen.Chef installation using the gems should still be supported for users that already have an installed Ruby or want more control on what is installed.,2644
Rename Method,Disable S3 virtual host buckets for generic S3 Not all S3-compatible providers support virtual host buckets and thuswe should disable this feature by default. Continue to enable virtualhost buckets for AWS-S3 which supports this although this featuresuffers from DNS settling issues.,2646
Rename Method,Add SecurityGroupExtension support to GCE 0,2647
Rename Method,Allow creating nodes through ComputeService with explicitly specified names Currently instance naming for nodes created through ComputeService.createNodesInGroup() etc uses a combination of the specified group name and GroupNamingConvention's unique suffix - generally that's a three character random string but for EC2 it's the id string for the instance. While this is fine for many cases where the instance name doesn't need to be referenced directly by actual humans say it's a pain for those cases. Currently you can work around this by creating instances through the per-api/provider clients/apis or through some hacks for single instance creation through ComputeService utilizing provider-specific TemplateOptions classes but there's no generalized way to get real control over the names given to instances through ComputeService. This should be possible.,2649
Rename Method,Avoid InputSupplier<InputStream>; support ByteSource ByteSource has convenience methods and avoids generics notational overhead. Guava is moving towards this:https://groups.google.com/forum/#!msg/guava-discuss/bChfnnXb9QA/xlmy2UzsmpsJNote that ByteSource implements InputSupplier<InputStream> so we should retain source compatibility.,2656
Rename Method,support Java 8 We should ensure compatibility with Java 8 in advance of its GA release in March. Presently several jclouds tests in core fail due to new collection methods and HashMap ordering differences. I have not yet tested further. We should backport any fixes to 1.7.x.,2659
Rename Method,The OpenStack KeyPairApi is missing get() org.jclouds.openstack.nova.v2_0.extensions.KeyPairApi is missing the get() method.,2661
Rename Method,Add support for arbitrary CPU and RAM in the ComputeService Some providers such as Abiquo or CloudSigma do not have the concept of Hardware Profiles and allow users to specify arbitrary CPU and RAM values.The current ComputeService abstraction assumes all providers have Hardware Profiles and forces implementations of such providers to provide a fixed (hardcoded) list just to conform the interface.It would be great to modernize the Compute workflow so Hardware Profiles are not mandatory and users can manually set their values when needed.,2664
Extract Method,Head-of-line blocking problem with DeleteAllKeysInList The current implementation of DeleteAllKeysInList suffers from the head-of-line blocking problem. It gets a PageSet of blobs from the container creates futures for deleting them and waits for the futures to complete before getting the next PageSet. This issue was originally reported by Andrew Gaul [~gaul] [1]-Shri[1] https://github.com/jclouds/legacy-jclouds/issues/1087,2665
Extract Method,Support ssh-agent authentication to access created nodes See https://github.com/jclouds/jclouds/pull/312,2666
Extract Method,Add support for Iops Encrypted and VolumeType fields for EC2 BlockDeviceMapping We don't currently have any support for the Iops Encrypted or VolumeType fields for EC2 EBS BlockDeviceMapping - they're described on http://docs.aws.amazon.com/AWSEC2/latest/APIReference/ApiReference-query-RunInstances.html. We should add them especially since VolumeType is needed to use the new General Purpose EBS SSD volumes.,2668
Rename Method,Replace hand-written domain classes with Auto-Value ones In doing maintenance and ports I've noticed that we have drift related to using guava to implement hashCode/equals on domain classes. Having an opportunity for a guava incompatibility on something like this is not high value in my opinion. Moreover we have a lot of other inconsistency in our value classes which have caused bugs and extra review time on pull requests.Auto-Value generates concrete implementations and takes out the possibility of inconsistency of field names Nullability etc. It is handled at compile time so doesn't introduce a dependency of note nor a chance of guava version conflict for our users.https://github.com/google/auto/tree/master/valueWhile it may be the case that we need custom gson adapters (ex opposed to the ConstructorAnnotation approach) or a revision to our approach I believe that this work is worthwhile.While it is the case that our Builders won't be generated I still think this is valuable. For example in many cases we shouldn't be making Builders anyway (ex. they are read-only objects never instantiated such as lists). Even if we choose to still write Builders the problem is isolated there.,2672
Rename Method,"S3 should retry on 500 InternalError jclouds retries on several errors but not on 500 InternalError with S3. The description suggests that we should retry ""We encountered an internal error. Please try again.""",2678
Move Method,"Move ListenerList implementations of interfaces into the interface itself A universal paradigm in Pivot is to have a ""listener"" interface for a class or data structure that is used to notify listeners of changes in the class/data. There is then an ""Adapter"" static class in the interface file that implements the interface with default implementations. Then there is a very separate enclosed static class that implements the ""ListenerList"" interface of that listener interface. And usually (or always) this ""listener list"" class is defined/used only in the class that needs to notify the listeners. However this class must be very parallel to not only the interface itself but also the ""Adapter"" class and yet it is in a different place. So it seems somewhat reasonable to move all these ""listener list"" classes into the interfaces themselves so all three related things are located in the same file. A preliminary POC of this concept was done with ""Query.java"" and ""QueryListener.java"" and it looks good. This doesn't seem to require changes to client code because the accessor methods only refer to ""ListenerList<....>"" and not to the listener list class itself (in order to be more general of course) but which helps us to hide the implementing class away inside the interface. I will attach the diff of the POC to hopefully make this more clear. It may seem a somewhat nebulous concept but the idea is to keep ""like things"" together for clarity.",2680
Extract Method,"Many places throw IllegalArgumentException during parameter validation but some are inconsistent Primarily the code looks like this currently: {code:java} if (param == null) throw new IllegalArgumentException(param + "" is null""); {code} But not all places have the message in the exception and not all places check the parameters as they should and not all places have the same message. So regularize this checking everywhere by making common ""core"" methods to do this null check (or other checks such as <= 0 etc.) so that the checking and messaging are common. This also simplifies the code and with JIT compiling shouldn't affect runtime speed either as this common method should get compiled and/or inlined as appropriate.",2694
Extract Method,"Move the tutorial ""Ruler"" class into the mainline code for use by others The ""Ruler"" class (and associated Skin and Listener classes) already exists in two places in the tutorials branch so it should go into the main ""wtk"" code so it can be used by others who might find it useful.",2698
Extract Method,Add a NumberRuler heading component for use with scrolling TextArea or TextPane Similiar to the Ruler component from PIVOT-1017 this would size itself and display line numbers or column counts when used as a row or column heading object for a ScrollPane around a TextArea or TextPane (when used with pure text).,2699
Extract Method,"Introduce a ""Style"" enum for compile-time checking of style names Many places use hard-coded strings for component style names and a number of which are fairly common (""font"" ""color"" ""horizontalAlignment""). It would reduce the possibility of misspelling of these style names if there were an enum for them so that the Java compiler would check spelling instead of finding out at runtime that the style name was misspelled. This would involve adding methods to Component.StyleDictionary to deal with the enum as a key.",2700
Inline Method,Optimize LabelSkin#paint(Graphics2D) This paint method is called an order of magnitude more than any other. It's actually already pretty fast but if there's one method to optimize the hell out of it's this one.,2702
Rename Method,"Create a new ""gauge"" object which can display a single value in a circular ""speedometer"" fashion For instance in a monitoring application things like CPU or disk usage can nicely be displayed as a circular gauge showing a single (the ""current"") value.  This paradigm is currently being used in such products as Apache Ambari (where it is called a ""Gauge"" widget).",2703
Extract Method,Replace disabled item indexes/paths in ListView TableView and TreeView with a disabled item filter; add disabled item filter to ListButton Using a filter to specify disabled items will be more flexible and will also allow us to preserve disabled items across model sorts.,2704
Move Method,Add a disabled date filter to Calendar and CalendarButton 0,2705
Rename Method,Change selected to highlighted in TablePane Row & Column Selected is the wrong term here. This will also cause the selectionBackgroundColor property to change to highlightBackgroundColor.,2706
Extract Method,TablePane rows and columns should collapse when vacant When a TablePane's row or column is either empty or contains only invisible components it should collapse and spacing should not be allocated for it. Note that if spanned content bleeds into a cell then that cell will not be considered empty and all rows and columns that the spanning content inhabits will be considered occupied (assuming the spanning content is visible).,2707
Rename Method,"Eliminate Component ""displayable"" property; use ""visible"" for this purpose Currently some skins treat the displayable flag a ""preferred visibility"". This may not be the correct interpretation. Arguably a non-displayable component should simply be taken out of the flow not made invisible.Also some skins currently respect the displayable flag that may not need to (e.g. WindowSkin).We should eliminate this property and use a component's visibility instead. This is the approach taken by AWT. We should also review which containers are currently trying to respect ""displayable"" and remove this code where it doesn't make sense.",2708
Rename Method,Allow Containers to obtain the keyboard focus This will allow us to resolve issue PIVOT-213 as well as address some other potential use cases that we can't currently handle (for example making TextArea a container; this will allow TextArea to act as a highly customizable form similar to how forms are supported in HTML). Additionally focusable containers are supported by other windowing toolkits including AWT so this will also eliminate a negative comparison point.,2709
Extract Method,Add orientation to Meter component 0,2710
Extract Method,Add hit detection to drawing primitives Add a new API that will allow a caller to programmatically determine which shape in a drawing the user clicks on.,2711
Rename Method,Make component editors fire events There are several use cases that call for event notifications from the editors (TableViewRowEditor TreeViewNodeEditor and ListViewItemEditor). These include preview events as well. Here's the associated API change:public interface Editor {public boolean isEditing();- public void save();+ public void saveChanges();- public void cancel();+ public void cancelEdit();}TableView {public interface RowEditor extends Editor {- public void edit(TableView tableView int rowIndex int columnIndex);+ public void editRow(TableView tableView int rowIndex int columnIndex);+ public ListenerList<RowEditorListener> getRowEditorListeners();}public interface RowEditorListener {public Vote previewEditRow(RowEditor rowEditor TableView tableView int rowIndex int columnIndex);public void editRowVetoed(RowEditor rowEditor Vote reason);public void rowEditing(RowEditor rowEditor TableView tableView int rowIndex int columnIndex);public Vote previewSaveChanges(RowEditor rowEditor TableView tableView int rowIndex int columnIndex Dictionary<String Object> changes);public void saveChangesVetoed(RowEditor rowEditor Vote reason);public void changesSaved(RowEditor rowEditor TableView tableView int rowIndex int columnIndex);public void editCancelled(RowEditor rowEditor TableView tableView int rowIndex int columnIndex);}}TreeView {public interface NodeEditor extends Editor {- public void edit(TreeView treeView Path path);+ public void editNode(TreeView treeView Path path);+ public ListenerList<NodeEditorListener> getNodeEditorListeners();}public interface NodeEditorListener {public Vote previewEditNode(NodeEditor nodeEditor TreeView treeView Path path);public void editNodeVetoed(NodeEditor nodeEditor Vote reason);public void nodeEditing(NodeEditor nodeEditor TreeView treeView Path path);public Vote previewSaveChanges(NodeEditor nodeEditor TreeView treeView Path path Object changes);public void saveChangesVetoed(NodeEditor nodeEditor Vote reason);public void changesSaved(NodeEditor nodeEditor TreeView treeView Path path);public void editCancelled(NodeEditor nodeEditor TreeView treeView Path path);}}ListView {public interface ItemEditor extends Editor {- public void edit(ListView listView int index);+ public void editItem(ListView listView int index);+ public ListenerList<ItemEditorListener> getItemEditorListeners();}public interface ItemEditorListener {public Vote previewEditItem(ItemEditor itemEditor ListView listView int index);public void editItemVetoed(ItemEditor itemEditor Vote reason);public void itemEditing(ItemEditor itemEditor ListView listView int index);public Vote previewSaveChanges(ItemEditor itemEditor ListView listView int index Object changes);public void saveChangesVetoed(ItemEditor itemEditor Vote reason);public void changesSaved(ItemEditor itemEditor ListView listView int index);public void editCancelled(ItemEditor itemEditor ListView listView int index);}},2712
Rename Method,"Add a ""variableItemHeight"" style to TerraListViewSkin Setting this value to ""true"" would ease the restriction that all items in a list view are the same height. When true the height of each item would be determined by asking the renderer for its preferred height.",2713
Move Method,"Add an ""automationID"" property to Component This ID will facilitate automated testing tools by providing a key by which such tools can obtain a reference to a component instance.",2714
Extract Method,Add a org.apache.pivot.util.Time class This class would be the time equivalent of CalendarDate.,2716
Extract Method,"Unable to easily drag multiple selected items in ListView Steps to reproduce:1) Create a list view with selectMode=""multi"" and attach a drag source to it.2) Select multiple items then click to drag the selection out of the list view.Expected behavior:You expect the multiple items to be draggedActual result:Upon mouse down the selection is reset to be a single item causing you to drag just one item.Workaround:If you hold down CTRL or SHIFT as you begin your drag you can kind of work around this issue but that is obviously a less-than-ideal user experience.Note that this could affect TableView and TreeView as well though they weren't tested as part of this ticket.",2717
Extract Method,Make Alert and Prompt properties mutable Specifically type message options and body. This will make it easier to work with these classes in WTKX.,2718
Move Method,Make data binding more robust Data binding should be more robust. It should support a configurable two-way mapping using a bind type of LOAD STORE or BOTH and should support binding to model data as well as selection state in data-driven components. The following components should be updated:ButtonCalendarCalendarButtonColorChooserColorChooserButtonLabelListButtonListViewSpinnerTableViewTextAreaTextInputTreeViewAdditionally data binding events should be moved to their own interfaces as the addition of these new bound properties will create a number of new data-binding related events.,2720
Inline Method,Remove the TablePane.Row.setVisible() method TablePane.Row.setVisible() is a little misleading in that it's just a convenience method for setting the visiblility of the components in the row (the row itself doesn't maintain an independent visibility property). Also the row will remain visible if a visible component exists in another row but spans into the row. Thus we should just remove it.,2722
Rename Method,Rename TextInput and TextArea insertText() to insert() Also remove index argument. This will be more consistent with delete() and will helps to clarify the intent of the method (which is to replace the current selection with the inserted text).,2723
Rename Method,Rename alternateRowColor style to alternateRowBackgroundColor Also add a new alternateRowColor style that will define the foreground color for alternate rows.Apply this change to both TerrraTableViewSkin and TerraListViewSkin.,2724
Move Method,"Move TerraListViewSkin ""listSize"" style to intrinsic ListButton property 0",2725
Rename Method,Window and DesktopApplicationContext should support java.awt.Window.setIconImages() to display multi-resolution icons DesktopApplicationContext currently sets the Window icon into the host frame using java.awt.Window.setIconImage(Image). However since Java6 there's a new method called setIconImages(List<Image> icons) to set multi-resolution icons e.g. you can set a 16x16 icon for the window title bar a 48x48 icon to be shown in Microsoft Windows' task switcher or a 64x64 icon to be shown in Windows Vista/7 Aero task list. (It is up to the runtime implementation to choose an appropriate icon from the list provided by setIconImages().)Without the possibility to set multi-resolution icons the OS scales the image either up or down to fit the current environment. This might lead to poor results.Suggestion: add a new method setIconImages(List<Image>) method to pivot.wtk.Window and if set use it in DesktopApplicationContext. Or maybe add setIconPictures(List<Picture>) to get around the issue of downcasting to Picture in updateFrameTitleBar().ThanksDirk.,2726
Inline Method,Eliminate ThreadUtilities? This class was created primarily for IcedTea compatibility - is this still a requirement?,2727
Extract Method,"Add color name from java.awt.Color in styles's attribute Like :<Border styles=""{color:'blue'}""><content><TextArea wtkx:id=""textArea"" styles=""{backgroundColor:'red'}""/></content></Border>",2728
Move Method,Move XML path accessor methods to XML class For parity with JSON/JSONSerializer.,2729
Extract Method,allow instance creation customization when reading WTKX files by creating an instance creation factory method inside the serializer class In order to allow instance creation customization change the serialization class by:a) Creating a protected instance creation method in the WTKXSerializer class.b) Change a private constructor to protected.c) Allow the creation of custom serialization instances when recursing into the tree by creating a protected serialization creation method.No client visible APIs are changed.,2730
Extract Method,Add getContextPath() and getLocation() to QueryServlet Without a getContextPath() method a query servlet is unable to determine its own location and won't be able to return the correct value from doPost(). getLocation() is a convenience method that returns the location of the servlet (protocol host port context path and servlet path).,2731
Rename Method,Rename Alert/Prompt getSelectedOption() to getSelectedOptionIndex() The current method name implies that the option value will be returned rather than the option index. Renaming the method will eliminate this ambiguity. A new getSelectedOption() method should be added that returns the actual option value.,2732
Extract Method,"Allow Dictionary values to be specified using elements in WTKX Currently it is possible to populate a dictionary type via attributes but not elements. For example this is supported:<HashMap abc=""123""/>as is this:<MyBeanClass><abc><MyOtherBeanClass/></abc></MyBeanClass>but this is not:<HashMap><abc><MyOtherBeanClass/></abc></HashMap>This prevents callers from populating a dictionary with anything other than primitive values. WTKXSerializer should also allow a caller to populate a dictionary with complex types as shown above.",2733
Rename Method,Bindable improvements Add arguments to Bindable#initialize() to provide the caller access to the serializer's namespace resources and location. This will allow untrusted applications to take advantage of Bindable without needing to use the BXML annotation among other things.,2734
Rename Method,Fire selection change events when selection changes indirectly Currently selection change events are fired only when an explicit call has been made that affects the selection. For example in ListView calling either setSelectedRanges() or clearSelection() will fire this event. However an operation that indirectly changes the selection state (such as adding or removing an item from the ListView's model data) does not trigger an event. This was originally done by design - selectedRangesChanged() includes the previous selection as an argument and we didn't want to have to manually re-construct that every time the selection changed as a side effect of a model change:public void selectedRangesChanged(ListView listView Sequence<Span> previousSelectedRanges);However in practice working within this model can be challenging. More than once I have registered a selection change listener expecting to receive notification of all selection changes forgetting that it is not designed that way. I'm guessing that other developers may be confused by this as well.So I am proposing that components that maintain a selection state also fire selection change events when the selection changes indirectly. In this case a null value would be passed for the previous selection. This will save the effort of re-constructing the previous selection info and will give the listener additional information about the nature of the change (i.e. null == indirect state change).This change should also be propagated to TextInput which has a similar issue with character change events. Currently TextInput fires character change events via TextInputCharacterListener and text change events via TextInputTextListener. The textChanged() event does not pass the previous text value which is inconsistent with other change events. textChanged() should be incorporated into TextInputCharacterListener and should pass the previous text value when it is changed via an explicit call to setText(); otherwise it should pass null.The updated version of TextArea should probably follow the same approach.,2735
Move Method,Make ListView selectedItem etc. notifying properties These properties don't currently fire events when they change so it is not possible to dynamically bind to them using namespace bindings.Note that they should only fire when selectMode == SINGLE.,2737
Rename Method,"Make tab pane button and accordion header content and renderer configurable Currently TabPane button content is specified via the TabPane.label and TabPane.icon attached properties. Accordion header content is set via Accordion.label and Accordion.icon. This limits the content of these buttons to an icon and/or a text string.It might be better to allow callers to specify data for these buttons directly. The attached label and icon properties could be replaced with ""tabButtonData"" and ""panelHeaderData"" (or simply ""buttonData"" and headerData"") and each container could allow a caller to specify a renderer (via get/setButtonDataRenderer() and get/setHeaderDataRenderer() respectively).",2738
Extract Method,Update BXMLSerializer extensibility Re-introduce the BXMLSerializer extensibility API (protected method hooks for subclasses) in a way that makes sense with the new BXMLSerializer API.,2739
Move Method,BindMapping for ImageView It is possible to have the BindMapping for ImageView.My exemple is when I load a Panel with a Bean which contents a Boolean I would like to display a cross image or valid image dependant on Boolean.Best regardsDuto,2740
Rename Method, Component#userData should allow multiple clients to co-exist It is very handy to be able to stash extra info on a component but when multiple libraries all want to do this it quickly gets out of hand.When multiple libraries all want to do this they can conflict e.g. if I add LibraryA and LibraryB to my pivot application and both libraries want to enhance standard Pivot behaviour by stashing stuff in the Component theywill overwrite each others UserData.A better solution would be to use a map of values with methods likevoid putUserData(Class key Object userdata)Object getUserData(Class key),2741
Extract Method,Pimping Alerts The following patch adds a title parameter to the Alert.alert() methods and shows a better default title depending on the MessageType. Also adds translation for German.,2742
Extract Method,Finishing touches on Skin Colors Finishing touches on Skin Colors some info here (and more later here in JIRA): https://issues.apache.org/jira/browse/PIVOT-579See if even the final part of this (not implemented) could be useful: https://issues.apache.org/jira/browse/PIVOT-245And there was another discussion (I have to find where in nabble) is change some color index usage to have better visual appearance (of course retrofitting existing behavior when/if possible).Note that probably after these little changes custom color json files should be a little updated.And a little thing on this:now only Tooltips doesn't use a palette color (they use an hard-coded color) ... what do you think to make them use the yellow-ish color atpalette index 19 (or one of is variants the 18 or 20) ? And use a similar (yellow-ish but a different color index if possible) color on warnings.I could adapt a little that color in all Pivot palettes to look similar to what should be ... tell me.For example someone remember the background color in Swing Tooltips (ok not the best in graphic design and colors :-) ) but with this little change we could also have this little feature where in most palettes will be yellow-ish but where needed could be different.Added some documentation in the Terra package JavaDoc file on color palette usage in the Terra Skin to help anyone wants to write their custom colors and then removed probably the best place for this is in a Tutorial.Last in ColorSchemeBuilder (and maybe even in the Kitchen Sink) add some Tooltips to see how they happen but see where they makes sense.And maybe in ColorSchemeBuilder move some elements in the Tab2 or Tab3 grouping them by similar components (some componets should be added there).,2743
Extract Method,SplashScreen control I've tried to get hold of the SplashScreen that I've defined in my manifest.mf file:>Manifest-Version: 1.0>X-COMMENT: Main-Class will be added automatically by build>SplashScreen-Image: com/macad/enbal/resources/welcome.jpgBut once the DesktopApplicationContext takes over it is impossible to get the SplashScreen (null pointer exception is thrown).Could you extend the API so that we can show the SplashScreen and close it once all the init actions have been finished just before we open the application window as the nameless gray pivot window is displayed quite a long time.,2747
Move Method,"Add a ""closeable"" property to TabPane When true the user would be able to close a tab by clicking a ""close"" icon in the tab button.",2748
Extract Method,"pivot & blocking edt Greg hello;in this thread:http://mail-archives.apache.org/mod_mbox/pivot-user/201001.mbox/%3C4B5E581D.2080604@hms.harvard.edu%3Eyour final word is:""Sorry it is not possible""but I know that you know that it is possible :-)the reason I need this is same as Martin here:http://netbeans.org/bugzilla/show_bug.cgi?id=90590namely: do some cleanup after shutdown was requested and confirmed:org.apache.pivot.wtk.Applicationpublic boolean shutdown(boolean optional) throws Exception;since you call shutdown(boolean optional) from EDT I need to block it using this approach:http://bugs.sun.com/view_bug.do?bug_id=6424157wich ""almost works"" except you have this check everywhere:Container.assertEventDispatchThread();which fails as described above:http://bugs.sun.com/view_bug.do?bug_id=6424157due to EventQueue.isDispatchThread() failing on the ""T1 vs T1*"" distinctionmy request is this:do you think you could make Container.assertEventDispatchThread();less pedantic and allow both ""current and past/next"" EDT threads to pass which are created during EventQueue push() / pop()thanks!Andrei",2749
Move Method,implement a color chooser widget Would be nice to have a color chooser.Probably easiest to copy JColorChooser from Apache Harmony and convert it from Swing to Pivot.,2750
Extract Method,Allow a Task to be executed using an ExecutorService supplied at execution time org.apache.pivot.util.concurrent.Task can be passed a an ExecutorService at construction which will be used when the Task is executed asynchronously.http://pivot.apache.org/2.0/docs/api/org/apache/pivot/util/concurrent/Task.html#Task(java.util.concurrent.ExecutorService)It would be useful to be able to override this (or the default ExecutorService if the no-arg constructor was used) when executing a Task asynchronously especially for Tasks that might be run multiple times and when the ExecutorService to use is not known when the Task is constructed.,2751
Extract Method,Display host scaling We should add the ability to scale the display host for accessibility. We could hook this into a built-in gesture (like CTRL-SHIFT-mousewheel) to allow the user to dynamically scale the display in a running Pivot app.,2755
Extract Method,Improve Performance of Graphics Improve Performance by using more Double buffering for components.Some info here:http://apache-pivot-developers.417237.n3.nabble.com/Re-Double-buffering-of-components-td3675333.htmlhttp://apache-pivot-users.399431.n3.nabble.com/Double-buffering-of-components-td3674898.htmlNote that here:http://apache-pivot-users.399431.n3.nabble.com/Performance-and-frame-resizing-patches-td3623149.htmlthere are other proposed patches from Piotr and in my opinion should be applied (maybe changing something) ... Noel (and all others) what do you think ?Something has already been committed in trunk (without specifying this issue).Thanks to Piotr for initial patches and discussion/tests on committed work.,2756
Move Method,Compilation errors with Java 7 (JDK 1.7) As seen in our Continuous Build environment on Jenkins (at Apache) but setup to use Java 7 (here: https://builds.apache.org/job/Pivot-trunk%20on%20Java%207/ ) any build of Pivot sources (from the trunk) now raise some compilation errors.Probably some has to be changes even in build.properties and build.xml to let our CI builds to force 1.7 as Java version (source an target) ... an idea could be to keep the release if specified in build.properties otherwise use those in the current Java version.Some discussion here:http://apache-pivot-developers.417237.n3.nabble.com/Compilation-errors-of-Pivot-trunk-with-Java-7-in-Apache-Jenkins-td3973664.htmlThis is an extract from the generated output:environment-info:[echo] [echo] Compile environment for pivot_build_by_jenkins-2.0.2 is:[echo] show deprecation true[echo] debug true[echo] source 1.6 target 1.6[echo] encoding UTF-8[echo] indexJars true[echo] arg -Xlint[echo] [echo] Java environment: home at '/x1/jenkins/tools/java/jdk1.7.0-32/jre' version 1.7.0[echo] core:[mkdir] Created dir: /x1/jenkins/jenkins-slave/workspace/Pivot-trunk on Java 7/pivot_trunk/core/ant-bin[javac] Compiling 133 source files to /x1/jenkins/jenkins-slave/workspace/Pivot-trunk on Java 7/pivot_trunk/core/ant-bin[javac] warning: [options] bootstrap class path not set in conjunction with -source 1.6[javac] /x1/jenkins/jenkins-slave/workspace/Pivot-trunk on Java 7/pivot_trunk/core/src/org/apache/pivot/xml/Element.java:641: error: name clash: remove(Node) in Element overrides a method whose erasure is the same as another method yet neither overrides the other[javac] public int remove(Node node) {[javac] ^[javac] first method: remove(K) in Dictionary[javac] second method: remove(T) in Sequence[javac] where KVT are type-variables:[javac] K extends Object declared in interface Dictionary[javac] V extends Object declared in interface Dictionary[javac] T extends Object declared in interface Sequence[javac] /x1/jenkins/jenkins-slave/workspace/Pivot-trunk on Java 7/pivot_trunk/core/src/org/apache/pivot/xml/Element.java:802: error: name clash: remove(String) in Element overrides a method whose erasure is the same as another method yet neither overrides the other[javac] public String remove(String attributeName) {[javac] ^[javac] first method: remove(T) in Sequence[javac] second method: remove(K) in Dictionary[javac] where TKV are type-variables:[javac] T extends Object declared in interface Sequence[javac] K extends Object declared in interface Dictionary[javac] V extends Object declared in interface Dictionary[javac] 2 errors[javac] 1 warningBUILD FAILED/x1/jenkins/jenkins-slave/workspace/Pivot-trunk on Java 7/pivot_trunk/build.xml:474: The following error occurred while executing this line:/x1/jenkins/jenkins-slave/workspace/Pivot-trunk on Java 7/pivot_trunk/build.xml:145: Compile failed; see the compiler error output for details.Total time: 3 secondsBuild step 'Invoke Ant' marked build as failureFinished: FAILURE,2757
Extract Method,Replace DesktopApplicationContext.displayException calls with ApplicationContext.handleUncaughtException There should be consistent way to handle uncaught exceptions in Pivot applications. However uncaught exceptions thrown in DesktopApplicationContext class (for example in application.startup ) are handled using private static method displayException which displays dialog and its logic cannot be overriden.May be ApplicationContext.handleUncaughtException could be made protected and calls to DesktopApplicationContext.displayException could be replaced by ApplicationContext.handleUncaughtException. And possibly Application.Adapter could implement UncaughtExceptionHandler - so current DesktopApplicationContext.displayException logic could be moved to new Application.Adapter.uncaughtExceptionThrown method.This should enable to override uncaught exception handling globally in pivot applications.Motivation:We deploy Pivot app using Java Web Start. Users have by default disabled Java Console (and they are not familiar with it). We want to display custom dialog to handle uncaught exceptions displaying full stack trace and with possiblity to report exception to help desk.I think in current implementation it is not possible to override handling of uncaught exceptions thrown during application init and other specific situations.,2759
Extract Method,Standard handling of exceptions in BXMLSerializer We require handling of all uncaught exceptions using standard mechanism (implementing UncaughtExceptionHandler interface on PivotApplication class).Similar to recently resolved https://issues.apache.org/jira/browse/PIVOT-916 it is currently not possible also in BXMLSerializer class. To resolve this I would propose to:1) Handle all exception reporting in BXMLSerializer in single protected method to enable overriding this method in BXMLSerializer descendant. I will upload patch for this.2) Change scope of static ApplicationContext.handleUncaughtException method to public. This will enable the delegation of exception handling from BXMLSerializer descendants to ApplicationContext. So hacks like this http://svn.codespot.com/a/apache-extras.org/pivot-contrib/trunk/pivot_contrib.util/src/pivot_contrib/util/serializer/InjectingSerializer.java wont be required anymore ;-).Please let me know if someone can see more elegant solution.,2760
Extract Method,Java2D performance optimizations The following are some performance numbers from a sample run on my local machine. They're only meaningful when compared against one another (they shouldn't ever be compared to someone else's numbers or against future performance runs). They show the relative run times of paint(Graphics2D) calls in our skin classes sorted by avg run time.<pre>Skin :: calls :: avg (ms) :: total (ms)pivot.wtk.skin.terra.TerraMenuPopupSkin :: 73 :: 0.000000 :: 0pivot.wtk.skin.terra.TerraPanoramaSkin :: 430 :: 0.000000 :: 0pivot.wtk.skin.terra.TerraRollupSkin :: 2562 :: 0.000000 :: 0pivot.wtk.skin.terra.TerraSplitPaneSkin :: 471 :: 0.000000 :: 0pivot.wtk.skin.terra.TerraFlowPaneSkin :: 34167 :: 0.000088 :: 3pivot.wtk.skin.terra.TerraFormSkin :: 565 :: 0.001770 :: 1pivot.wtk.skin.terra.TerraSliderSkin :: 559 :: 0.001789 :: 1pivot.wtk.skin.terra.TerraMenuBarSkin :: 176 :: 0.011364 :: 2pivot.wtk.skin.terra.TerraSplitPaneSkin$SplitterSkin :: 433 :: 0.011547 :: 5pivot.wtk.skin.terra.TerraSeparatorSkin :: 165 :: 0.012121 :: 2pivot.wtk.skin.terra.TerraCalendarSkin :: 156 :: 0.019231 :: 3pivot.wtk.skin.terra.TerraScrollPaneCornerSkin :: 291 :: 0.020619 :: 6pivot.wtk.skin.terra.TerraTabPaneSkin :: 368 :: 0.021739 :: 8pivot.wtk.skin.terra.TerraTablePaneSkin :: 863 :: 0.040556 :: 35pivot.wtk.skin.terra.TerraAccordionSkin :: 92 :: 0.043478 :: 4pivot.wtk.skin.terra.TerraSpinnerSkin$SpinButtonSkin :: 1476 :: 0.052168 :: 77pivot.wtk.skin.terra.TerraScrollBarSkin$HandleSkin :: 678 :: 0.058997 :: 40pivot.wtk.skin.terra.TerraSpinnerSkin :: 744 :: 0.060484 :: 45pivot.wtk.skin.terra.TerraCalendarSkin$DateButtonSkin :: 5818 :: 0.061705 :: 359pivot.wtk.skin.terra.TerraLabelSkin :: 36104 :: 0.066696 :: 2408pivot.wtk.skin.terra.TerraTextInputSkin :: 222 :: 0.072072 :: 16pivot.wtk.skin.ImageViewSkin :: 6488 :: 0.074445 :: 483pivot.wtk.skin.terra.TerraMeterSkin :: 804 :: 0.085821 :: 69pivot.wtk.skin.terra.TerraAlertSkin :: 124 :: 0.088710 :: 11pivot.wtk.skin.terra.TerraSliderSkin$ThumbSkin :: 555 :: 0.095495 :: 53pivot.wtk.skin.terra.TerraExpanderSkin$ShadeButtonSkin :: 256 :: 0.097656 :: 25pivot.wtk.skin.terra.TerraRollupSkin$RollupButtonSkin :: 1818 :: 0.097910 :: 178pivot.wtk.skin.terra.TerraScrollBarSkin :: 834 :: 0.100719 :: 84pivot.wtk.skin.terra.TerraScrollBarSkin$ScrollButtonSkin :: 1415 :: 0.108127 :: 153pivot.wtk.skin.terra.TerraAccordionSkin$PanelHeaderSkin :: 263 :: 0.133080 :: 35pivot.wtk.skin.terra.TerraExpanderSkin :: 266 :: 0.146617 :: 39pivot.wtk.skin.terra.TerraCheckboxSkin :: 3210 :: 0.151713 :: 487pivot.wtk.skin.terra.TerraMenuButtonSkin :: 591 :: 0.155668 :: 92pivot.wtk.skin.terra.TerraPromptSkin :: 19 :: 0.157895 :: 3pivot.wtk.skin.terra.TerraSpinnerSkin$SpinnerContentSkin :: 735 :: 0.161905 :: 119pivot.wtk.skin.terra.TerraListButtonSkin :: 134 :: 0.164179 :: 22pivot.wtk.skin.terra.TerraMenuBarItemSkin :: 664 :: 0.183735 :: 122pivot.wtk.skin.terra.TerraMenuSkin :: 69 :: 0.202899 :: 14pivot.wtk.skin.terra.TerraMenuItemSkin :: 238 :: 0.210084 :: 50pivot.wtk.skin.terra.TerraTabPaneSkin$TabButtonSkin :: 1041 :: 0.282421 :: 294pivot.wtk.skin.terra.TerraCalendarButtonSkin :: 340 :: 0.285294 :: 97pivot.wtk.skin.TextAreaSkin :: 71 :: 0.323944 :: 23pivot.wtk.skin.terra.TerraFrameSkin$FrameButtonSkin :: 122 :: 0.336066 :: 41pivot.wtk.skin.terra.TerraTableViewHeaderSkin :: 413 :: 0.341404 :: 141pivot.wtk.skin.terra.TerraBorderSkin :: 6200 :: 0.358226 :: 2221pivot.wtk.skin.terra.TerraPushButtonSkin :: 1300 :: 0.363846 :: 473pivot.wtk.skin.terra.TerraLinkButtonSkin :: 399 :: 0.385965 :: 154pivot.wtk.skin.terra.TerraScrollPaneSkin :: 1968 :: 0.462398 :: 910pivot.wtk.skin.terra.TerraRadioButtonSkin :: 565 :: 0.467257 :: 264pivot.wtk.skin.terra.TerraTreeViewSkin :: 427 :: 1.060890 :: 453pivot.wtk.skin.WindowSkin :: 802 :: 1.168329 :: 937pivot.wtk.skin.DisplaySkin :: 797 :: 1.193225 :: 951pivot.wtk.skin.terra.TerraListViewSkin :: 194 :: 1.324742 :: 257pivot.wtk.skin.terra.TerraTableViewSkin :: 465 :: 1.408602 :: 655</pre>There are a few things that jump off the page to me:1) LabelSkin is called to paint a TON! This is because it's used in virtually every renderer. Its average run time is comparatively fast but because it's so foundational it's the one paint method that should be optimized to the tilt.2) BorderSkin's paint is way costlier than you'd think it'd be relative to other skins (it's in the 80th percentile) and it's also called an awful lot. Can we find out what's taking so long and speed it up?3) DisplaySkin and WindowSkin only fill in their background color yet because they often do it on the entire clip rect (display always does window always does when it's maximized) it's surprisingly expensive! ScrollPaneSkin has the same issue but to a lesser extent. Is there any known trick to speeding up this primitive graphics operation? I doubt it but it's worth asking :),2762
Extract Method,Implement simple macro system in JSONSerializer It occurred to me since we are using JSON style sheets to style our application and it is getting quite big that a macro system (maybe similar to C/C++ with #define or something similar) would be useful especially for repeated colors and other constants (like padding values fonts etc.) This would enable using custom values consistently while avoiding inconsistencies due to typos or changes introduced one place and not others.I was thinking of a simple#define NAME valueas in C/C++ and then using $(NAME) as the substitution token. This is easily implemented in JSONSerializer.I'm open to suggestions for the syntax but I believe the feature will be very useful especially for the JSON stylesheet.,2763
Rename Method,Ability for the file manager to ingest a file in place.  There are cases where we have some products in a directory and we want to ingest and catalog them without actually moving them to a new location. We call this kind of ingestion an in place ingestion. It is very useful for the file manager to be able to ingest in place.,2764
Move Method,Rewrite Workflow Monitor webapp using Apache Wicket Similar to OODT-155 except for the Workflow Monitor web application.,2766
Inline Method,"Update CAS Curator Tutorial I've picked up a issue with the curator war deployment in the cas curator tutorial - http://oodt.apache.org/components/maven/curator/user/basic.html#section2 inIt seems that the web app is deployed to name of the xml file not the Context path attribute.I've tried on Mac Snow Leopard and Ubuntu Linux - both using some flavour of tomcat6.From Paul Ramirez - the path attribute is ignored unless the context is defined statically in the server.xml file. Otherwise the context path is determined by Tomcat by the name of the context file so ""foo.xml"" will be hosted at ""foo"". Since this is a Tomcat specific configuration we don't have a way to change that but we should update our tutorial.",2767
Rename Method,Make Resource Manager FAILURE and SUCCESS aware instead of just COMPLETE aware This patch adds a FAILURE and a SUCCESS status to the resource manager alerting users to Jobs that have arrived in either state. Currently the resource manager only understands whether a job is COMPLETE or not.,2768
Rename Method,XMLPS should be able to stream large results Currently XMLPS stores *ALL* the rows of a ResultSet in a CDEResult object. In addition this CDEResult is converted to a String for the HTTPResponse nearly doubling the memory usage. With large results heap space can easily run out despite increasing max heap space for the servlet container (e.g. -Xmx1024m). XMLPS should be able to stream/chunk its results taking into consideration the following:# ResultSets _represent_ an iterable collection of rows without actually _storing_ all the rows in memory# HTTPResponse#getOutputStream() offers a streaming response where chunked transfer-encoding and content-length headers are managed automatically by the servlet container (such as Tomcat),2769
Extract Method,"Extension to opendapps module to extract ALL variables in DDS stream This is a second patch targeted at extending the opendapps functionality for use in the CMDS JPL project. This patch contains functional modifications to four files:o OpendapProfileHandler: when harvesting metadata from multiple THREDDS catalogs avoid stopping the process if any of the catalogs results in an error rather keep harvesting all the other datasetso DatasetExtractor: parse all datasets in a THREDDS catalog even if they don't have an explicit direct <access> sub-element but rather have a ""urlPath"" attributeo DataseCrawler: - only select for metadata extraction those THREDDS datasets that have an opendap access URL disregard the others- when the THREDDS catalog has no ""authority"" attribute the dataset.getUniqueID() returns the string ""null"". In this case use the dataset.getID() method which returns the dataset ID with no authority prepended.o ProfileUtils: create an OODT <profElement> for each opendap variable found in the DDS stream wether it is explicitly configured or not to minimize manual handling of the configuration files. If the variable is configured use the configuration spec to possibly rename it and cast it as RANGED_ELEMENT_TYPE or ENUM_ELEMENT_TYPE. If the variable is not configured assume it is of RANGED_ELEMENT_TYPE.Finally all classes have additional log statements for debugging purposes.",2770
Move Method,Refactoring of metadata extraction functionality for opendapps module The main purpose of this patch is to refactor the metadata parsing functionality into an extensible framework of MetadataExtractors. The MetadataExtractor interface defines the general capability of parsing a metadata source and adding (namevalue) pairs into the CAS metadata container. The existing code for parsing THREDDS metadata catalogs has been moved from the DatasetCrawler class to a ThreddsMetadataExtractor that implements the aforementioned interface. Additionally another implementation DasMetadataExtractor has been added to parse an Opendap DAS stream and capture the NetCDF global attributes (for now). Finally an NcmlMetadataExtractor has been added as a stub implementation for future parsing of NcML documents.The patch also contains the following changes and additions:o The OODT profiles are assigned a UUID as identifier since the THREDDS dataset ID is used as the resource identifiero The THREDDS catalogs are parsed to extract the CF standard names and variable long names where found,2771
Extract Method,"Improve the richness and consistency of metadata extracted from the THREDDS catalogs The main purpose of this patch is to improve the richness and consistency of metadata extracted from the THREDDS catalogs and OpenDAP streams and to check that the required information is indeed present in the resulting OODT metadata profiles. Details on all classes affected follow:Profiler- invokes a profile-checking utility and prints out a summary of the most important metadata fields for quick review by the publisherDasMetadataExtractor- extracts variable names long names and CF standard names from the opendap DAS streamThreddsMetadataExtractor- stores additional metadata such as the hostname- parses all types of <documentation> tags including xlinks and uses the ""type"" attribute to create different metadata elements- adds additional geospatial and temporal coverage elements- stores multiple THREDDS access URLs as OODT <resLocation> attributes: the OpenDAP URL the THREDDS catalog URL and the TDS HTML landing page. All <resLocation>s are ecoded as tuple to store the multiple fields- does NOT parse tHE variable information in the THREDDS catalog since this metadata is more reliably and consistently extracted from the opendap streamProfileChecker- new utility class that checks an OODT Profile versus a list of required/optional elements.ProfileUtils- fixes bug that caused any metadata value containing a '' to be split across multiple XML elements- checks for the string ""null"" before adding a value to the metadata- allow for multiple values of the same profile element to be provided in the configuration file and includes them in the resulting OODT profileopendap.config.xml- updated example of configuration with new metadata fieldsGeneral changes- changed level of log output in several classes so that relevant information stands out more",2772
Rename Method,Changes to Curator web app to better support high frequency updates. From: https://reviews.apache.org/r/8684/The patch contain two changes to the Curator update metadata methods:o The new method used by VFASTR was converted to use the XML-RPC client instead of an embedded catalog so that only one CAS catalog access the back-end store at a timeo The old method used by other existing systems was converted to use a shared Catalog instance as opposed to create a new Catalog instance for each request. This is necessary to minimize use of resources such as database connections.Note that the second update method should also be converted to use the XML-RPC interface but only when a proper testing platform is available.,2773
Extract Method,Remove Duplicate NEWLEADER packets from the Leader to the Follower. 0,2775
Extract Method,Add zk.updateServerList(newServerList) When the set of servers changes we would like to update the server list stored by clients without restarting the clients.Moreover assuming that the number of clients per server is the same (in expectation) in the old configuration (as guaranteed by the current list shuffling for example) we would like to re-balance client connections across the new set of servers in a way that a) the number of clients per server is the same for all servers (in expectation) and b) there is no excessive/unnecessary client migration.It is simple to achieve (a) without (b) - just re-shuffle the new list of servers at every client. But this would create unnecessary migration which we'd like to avoid.We propose a simple probabilistic migration scheme that achieves (a) and (b) - each client locally decides whether and where to migrate when the list of servers changes. The attached document describes the scheme and shows an evaluation of it in Zookeeper. We also implemented re-balancing through a consistent-hashing scheme and show a comparison. We derived the probabilistic migration rules from a simple formula that we can also provide if someone's interested in the proof.,2776
Rename Method,improve ZxidRolloverTest (test seems flakey) In our jenkins job to run the ZooKeeper unit tests org.apache.zookeeper.server.ZxidRolloverTest sometimes fails.E.g.{noformat}org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /foo0at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:815)at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:843)at org.apache.zookeeper.server.ZxidRolloverTest.checkNodes(ZxidRolloverTest.java:154)at org.apache.zookeeper.server.ZxidRolloverTest.testRolloverThenRestart(ZxidRolloverTest.java:211){noformat},2777
Extract Method,"Allow server-side SASL login with JAAS configuration to be programmatically set (rather than only by reading JAAS configuration file) Currently the CnxnFactory checks for ""java.security.auth.login.config"" to decide whether or not enable SASL.* zookeeper/server/NIOServerCnxnFactory.java* zookeeper/server/NettyServerCnxnFactory.java** configure() checks for ""java.security.auth.login.config""*** If present start the new Login(""Server"" SaslServerCallbackHandler(conf))But since the SaslServerCallbackHandler does the right thing just checking if getAppConfigurationEntry() is empty we can allow SASL with JAAS configuration to be programmatically just checking weather or not a configuration entry is present instead of ""java.security.auth.login.config"".(Something quite similar was done for the SaslClient in ZOOKEEPER-1373)",2778
Extract Method,Multi-thread CommitProcessor CommitProcessor has a single thread that both pulls requests off its queues and runs all downstream processors. This is noticeably inefficient for read-intensive workloads which could be run concurrently. The trick is handling write transactions. I propose multi-threading this code according to the following two constraints- each session must see its requests responded to in order- all committed transactions must be handled in zxid order across all sessionsI believe these cover the only constraints we need to honor. In particular I believe we can relax the following:- it does not matter if the read request in one session happens before or after the write request in another sessionWith these constraints I propose the following threads- 1 primary queue servicing/work dispatching thread- 0-N assignable worker threads where a given session is always assigned to the same worker threadBy assigning sessions always to the same worker thread (using a simple sessionId mod number of worker threads) we guarantee the first constraint-- requests we push onto the thread queue are processed in order. The way we guarantee the second constraint is we only allow a single commit transaction to be in flight at a time--the queue servicing thread blocks while a commit transaction is in flight and when the transaction completes it clears the flag.On a 32 core machine running Linux 2.6.38 achieved best performance with 32 worker threads for a 56% +/- 5% improvement in throughput (this improvement was measured on top of that for ZOOKEEPER-1504 not in isolation).New classes introduced in this patch are:WorkerService (also in ZOOKEEPER-1504): ExecutorService wrapper that makes worker threads daemon threads and names then in an easily debuggable manner. Supports assignable threads (as used here) and non-assignable threads (as used by NIOServerCnxnFactory).,2779
Extract Method,Plumb ZooKeeperServer object into auth plugins I want to plumb the ZooKeeperServer object into the auth plugins so that I can store authentication data in zookeeper itself. With access to the ZooKeeperServer object I also have access to the ZKDatabase and can look up entries in the local copy of the zookeeper data.In order to implement this I make sure that a ZooKeeperServer instance is passed in to the ProviderRegistry.initialize() method. Then initialize() will try to find a constructor for the AuthenticationProvider that takes a ZooKeeperServer instance. If the constructor is found it will be used. Otherwise initialize() will look for a constructor that takes no arguments and use that instead.,2780
Extract Method,Add an async interface for multi request Currently there is no async interface for multi request in ZooKeeper java client.,2781
Extract Method,Upgrade netty version Upgrade netty version,2782
Extract Method,"Add a CLI command to recursively list a znode and children When troubleshooting applications where znodes can be multiple levels deep (eg. HBase replication) it is handy to see all child znodes recursively rather than run an ls for each node manually.So I propose adding an option to the ""ls"" command (-r) which will list all child nodes under a given znode.",2783
Extract Method,Refactor (touch|add)Session in SessionTrackerImpl.java This JIRA extends the idea of ZOOKEEPER-1978.Besides refactoring get-put operations of concurrentMap in addSession method addSession also calls touchSession which repeatedly checks if session existed. So it would be nice for refactor. Refactoring the second issue is relevant to ZOOKEEPER-1978. So I create a this JIRA to fix both.,2784
Extract Method,Backup config files. We should create a backup file for a static or dynamic configuration file before changing the file. Since the static file is changed at most twice (once when removing the ensemble definitions at which point a dynamic file doesn't exist yet and once when removing clientPort information) its probably fine to back up the static file independently from the dynamic file. To track backup history:Option 1: we could have a .bakXX extention for backup where XX is a sequence number. Option 2: have the configuration version be part of the file name for dynamic configuration files (instead of in the file like now). Such as zoo_replicated1.cfg.dynamic.1000000 then on reconfiguration simply create a new dynamic file (with new version) and update the link in the static file to point to the new dynamic one.Review place:https://reviews.apache.org/r/24208/,2785
Extract Method,Few UX improvements in ZooInspector A few simple changes would simplify using ZooInspector a lot. - Alphabetical order of nodes on a tree view- Short term caching of zookeeper nodes for faster rendering of node tree- Add/Delete node in context menu of a node- Keyboard shortcuts for add/deleting a node- Logging information that ZooInspector failed to load nodeViewers,2786
Extract Method,"Enable creation of nodes with TTLs As a user I would like to be able to create a node that is NOT tied to a session but that WILL expire automatically if action is not taken by some client within a time window.I propose this to enable clients interacting with ZK via http or other ""thin clients"" to create ephemeral-like nodes.Some ideas for the design up for discussion:The node should support all normal ZK node operations including ACLs sequential key generation etc however it should not support the ephemeral flag. The node will be created with a TTL that is updated via a refresh operation. The ZK quorum will watch this node similarly to the way that it watches for session liveness; if the node is not refreshed within the TTL it will expire.QUESTIONS:1) Should we let the refresh operation set the TTL to a different base value?2) If so should the setting of the TTL to a new base value cause a watch to fire?3) Do we want to allow these nodes to have children or prevent this similar to ephemeral nodes?",2787
Extract Method,"Improvements to FLE I'm about to attach a patch that implements the following modifications:. Currently if a server is on leader election and doesn't receive a notification for some amount of time t then it sends a new set of notifications if at least one server has delivered a message from the previous set. With this patch the amount of time a server waits for a notification before sending a new set increases exponentially;. I have separated connecting to servers and queuing new notification messages. Before they were all in the same message. The advantage is that now I can tell to an instance of QuorumCnxManager to try to connect to other servers without generating new notification messages;. I have changed the logging level of several messages on QuorumCnxManager. They were ""warn"" but they should really be either ""info"" or ""debug"". I've changed them to info.",2788
Extract Method,Remove file delete duplicate code from test code Code to delete a folder recursive is written in multiple test files. Following are the files containing the same piece of code.{code}src/java/systest/org/apache/zookeeper/test/system/QuorumPeerInstance.javasrc/java/test/org/apache/zookeeper/test/ClientBase.javasrc/java/test/org/apache/zookeeper/server/quorum/LearnerTest.javasrc/java/test/org/apache/zookeeper/server/quorum/Zab1_0Test.java{code}Remove duplicate code from these files,2789
Extract Method,"Add a Chroot request It would be nice to be able to root ZooKeeper handles at specific points in the namespace so that applications that use ZooKeeper can work in their own rooted subtree.For example if ops decides that application X can use the subtree /apps/X and application Y can use the subtree /apps/Y X can to a chroot to /apps/X and then all its path references can be rooted at /apps/X. Thus when X creates the path ""/myid"" it will actually be creating the path ""/apps/X/myid"".There are two ways we can expose this mechanism: 1) We can simply add a chroot(String path) API or 2) we can integrate into a service identifier scheme for example zk://server1:2181server2:2181/my/root. I like the second form personally.",2790
Extract Method,"cleanup the logging levels used (use the correct level) and messages generated Cleanup logging:make sure logging uses the correct level esp error and warnmake sure the messages are meaningful (esp fix ""fixmsg"" logs)",2791
Move Method,Better command line parsing in ZookeeperMain. The command line parsing in zookeepermain is very basic.We should use some kind of cli parsing (commons-cli?) or something else that is standard and improve our command line parsing. This will remove the scattered code that we have in zookeepermain and we will have much better command line parsing.,2792
Extract Method,Add additional server metrics This patch adds several new server-side metrics as well as makes it easier to add new metrics in the future. This patch also includes a handful of other minor metrics-related changes. Here's a high-level summary of the changes. # This patch extends the request latency tracked in {{ServerStats}} to track {{read}} and {{update}} latency separately. Updates are any request that must be voted on and can change data reads are all requests that can be handled locally and don't change data. # This patch adds the {{ServerMetrics}} logic and the related {{AvgMinMaxCounter}} and {{SimpleCounter}} classes. This code is designed to make it incredibly easy to add new metrics. To add a new metric you just add one line to {{ServerMetrics}} and then directly reference that new metric anywhere in the code base. The {{ServerMetrics}} logic handles creating the metric properly adding the metric to the JSON output of the {{/monitor}} admin command and properly resetting the metric when necessary. The motivation behind {{ServerMetrics}} is to make things easy enough that it encourages new metrics to be added liberally. Lack of in-depth metrics/visibility is a long-standing ZooKeeper weakness. At Facebook most of our internal changes build on {{ServerMetrics}} and we have nearly 100 internal metrics at this time – all of which we'll be upstreaming in the coming months as we publish more internal patches. # This patch adds 20 new metrics 14 which are handled by {{ServerMetrics}}. # This patch replaces some uses of {{synchronized}} in {{ServerStats}} with atomic operations. Here's a list of new metrics added in this patch: - {{uptime}}: time that a peer has been in a stable leading/following/observing state - {{leader_uptime}}: uptime for peer in leading state - {{global_sessions}}: count of global sessions - {{local_sessions}}: count of local sessions - {{quorum_size}}: configured ensemble size - {{synced_observers}}: similar to existing `synced_followers` but for observers - {{fsynctime}}: time to fsync transaction log (avg/min/max) - {{snapshottime}}: time to write a snapshot (avg/min/max) - {{dbinittime}}: time to reload database – read snapshot + apply transactions (avg/min/max) - {{readlatency}}: read request latency (avg/min/max) - {{updatelatency}}: update request latency (avg/min/max) - {{propagation_latency}}: end-to-end latency for updates from proposal on leader to committed-to-datatree on a given host (avg/min/max) - {{follower_sync_time}}: time for follower to sync with leader (avg/min/max) - {{election_time}}: time between entering and leaving election (avg/min/max) - {{looking_count}}: number of transitions into looking state - {{diff_count}}: number of diff syncs performed - {{snap_count}}: number of snap syncs performed - {{commit_count}}: number of commits performed on leader - {{connection_request_count}}: number of incoming client connection requests - {{bytes_received_count}}: similar to existing `packets_received` but tracks bytes,2793
Rename Method,"Zookeeper client supports IPv6 address and document the ""IPV6 feature"" This issue is the follow-up work of [ZOOKEEPER-3057|https://issues.apache.org/jira/browse/ZOOKEEPER-3057] 1.ZK server side supports ipv6 style like this: server.1=[2001:db8:1::242:ac11:2]:2888:3888but zk client side supports ipv6 like this:2001:db8:1::242:ac11:2:2181.we need unify them. Look at the kafka example [KAFKA-1123|https://issues.apache.org/jira/browse/KAFKA-1123]. its producer client also supports ipv6 like this: [2001:db8:1::242:ac11:2] 2.document the ""IPV6 feature"" to let user know.",2794
Rename Method,Asynchronous version of createLedger() While there are async versions for read and write there is no async version for creating a ledger. This can cause applications to have to change their whole thread design. It should be easier and more consistent to add an async version of createLedger().,2795
Extract Method,"the java shell should indicate connection status on command prompt it would be very useful if the java shell showed the current connection status as part of the command prompt.this shows itself in particular for the following use case:I attempted to connect a java shell to a remote cluster that was unavailable when I run the first command ""ls /"" onthe cluster the shell hangs. It would be nice if the shell indicated connection status in the prompt and make it moreclear that the shell is currently not connected. (it was hard to see the ""attempting to connect"" console message asit was lost in with the other messaes...)",2796
Extract Method,Startup messages should account for common error of missing leading slash in config files It would be nice if the startup noticed directories without a leading slash in the config file. That is worth a warning.Moreover if that directory exists looking from root but not looking from the current directory a very serious warning is in order.,2797
Rename Method,Server supports listening on a specified network address The issue in maililist is located:http://mail-archives.apache.org/mod_mbox/hadoop-zookeeper-user/200912.mbox/%3c4ac0d28c0912210242g58230a9ds1c55361561c70d61@mail.gmail.com%3eI have checked the server size code seems no this option provided. This feature is useful when we have more than two network interfaces one for Internet and others for intranet. We want to run ZooKeeper in our intranet and not be exposed to outside world.,2798
Extract Method,"Add monitoring four-letter word Filing a feature request based on a zookeeper-user discussion.Zookeeper should have a new four-letter word that returns key-value pairs appropriate for importing to a monitoring system (such as Ganglia which has a large installed base)This command should initially export the following:(a) Count of instances in the ensemble.(b) Count of up-to-date instances in the ensemble.But be designed such that in the future additional data can be added. For example the output could define the statistic in a comment then print a key ""space character"" value line:""""""# Total number of instances in the ensemblezk_ensemble_instances_total 5# Number of instances currently participating in the quorum.zk_ensemble_instances_active 4""""""From the mailing list:""""""Date: Mon 19 Apr 2010 12:10:44 -0700From: Patrick Hunt <phunt@apache.org>To: zookeeper-user@hadoop.apache.orgSubject: Re: Recovery issue - how to debug?On 04/19/2010 11:55 AM Travis Crawford wrote:> It would be a lot easier from the operations perspective if the leader> explicitly published some health stats:>> (a) Count of instances in the ensemble.> (b) Count of up-to-date instances in the ensemble.>> This would greatly simplify monitoring& alerting - when an instance> falls behind one could configure their monitoring system to let> someone know and take a look at the logs.That's a great idea. Please enter a JIRA for this - a new 4 letter word and JMX support. It would also be a great starter project for someone interested in becoming more familiar with the server code.Patrick""""""",2799
Extract Method,"Need a multi-update command to allow multiple znodes to be updated safely The basic idea is to have a single method called ""multi"" that will accept a list of create delete update or check objects each of which has a desired version or file state in the case of create. If all of the version and existence constraints can be satisfied then all updates will be done atomically.Two API styles have been suggested. One has a list as above and the other style has a ""Transaction"" that allows builder-like methods to build a set of updates and a commit method to finalize the transaction. This can trivially be reduced to the first kind of API so the list based API style should be considered the primitive and the builder style should be implemented as syntactic sugar.The total size of all the data in all updates and creates in a single transaction should be limited to 1MB.Implementation-wise this capability can be done using standard ZK internals. The changes include:- update to ZK clients to all the new call- additional wire level request- on the server in the code that converts transactions to idempotent form the code should be slightly extended to convert a list of operations to idempotent form.- on the client a down-rev server that rejects the multi-update should be detected gracefully and an informative exception should be thrown.To facilitate shared development I have established a github repository at https://github.com/tdunning/zookeeper and am happy to extend committer status to anyone who agrees to donate their code back to Apache. The final patch will be attached to this bug as normal.",2800
Extract Method,Remove onAttach onAttach doesn't work reliably and it's not really needed as the functionality can be moved to onBeforeRender which also gives much more flexibility to what it's possible to do in the method.Thus the onAttach method should be removed (made final actually so that we don't fail silently).,2803
Extract Method,wicket-guice should support Provider injection and TypeLiteral injection 0,2804
Extract Method,Optimize memory usage Look at the way component keeps it's state and optimize the memory usage,2805
Extract Method,"Make portlet support configurable and default disabled Currently the new portlet support is enabled automatically at runtime when WicketFilter detects the javax.portlet.PortletContext class.When it does find this class some of the Wicket settings are adjusted/overridden specifically for portlet support like setting the RenderStrategy to REDIRECT_TO_RENDER.But not always portlet support is used or needed even if deployed in a portlet supporting web container. The automatic settings changes then can result in unexpected/invalid behavior.To fix this the default behavior for portlet support will be reverted back to disabled and will need to be specifically enabled.To support transparent configuration of the portlet support e.g. if needed even without having to change the application web.xml a flexible layer of configuration settings will be provided:if a Wicket filter parameter ""detectPortletContext"" is specified:-> detect PortletContext if parameter value == ""true""else if a web.xml context parameter ""org.apache.wicket.detectPortletContext"" is specified:-> detect PortletContext if parameter value == ""true""else if a org/apache/wicket/protocol/http/portlet/WicketPortlet.properties resource can be found on the classpath:-> detect PortletContext if it specifies property/value ""org.apache.wicket.detectPortletContext=true"".Note: the WicketPortlet.properties resource is already used by WicketPortlet itself to determine the ServletContextProvider and/or PortletResourceURLFactory class (if not specified otherwise).This provides a 100% save solution while still allowing transparent enabling portlet support for Portals like Jetspeed-2 which provide a WicketPortlet.properties with appropriate values out-of-the-box.Using this solution the Wicket Examples still runs without needed changes in Jetspeed-2 but the default behavior will be reverted back to *not* automatically look for a PortletContext and thus by default not provide portlet support.",2806
Extract Method,Allow 24 hours time field in DateTimeField In most European countries the standard date format is 24-hours and not 12-hours + AM/PM. It'd be nice having the time format configurable in the component.,2807
Extract Method,open Modal Window without AjaxRequestTarget Wicket 1.2.2 included a new Modal Window component. However this component can only be used with a valid AjaxRequestTarget. It would be useful if Modal Windows could be opened programmatically at any time without an AjaxRequestTarget.,2808
Extract Method,"enable subclassing of AjaxRequestTarget In my wicket programming experience so far I always didn't feel quite comfortable with the ajax part.I had some issues in particular with these as an example:- ""Always include a common feedback panel from my template page""--> add 'target.addComponent(feedbackPanel)' just _everywhere_ (very cumbersome and not elegant at all)- add a listener using AjaxRequestTarget#addListener--> not possible without subclassing the request cycle (which is *yuk* if you ask me) to catch the short moment in between AjaxRequestTarget is instantiated and AjaxRequestTarget#onRespond() is called- automatically set focus on the first form component with errors--> add bulky code into all onSubmit() and onError() to check for errrors and call AjaxRequestTarget#setFocus- add some common function like AjaxRequestTarget#yellowFade(FormComponent)--> have some utility method and call it like this: AjaxUtil.yellowFade(target) -- not nice as functionality like this should really belong to the request targetI found that all these issues can be solved very elegantly if you could just catch the moment where AjaxRequestTarget is instantiated.I attached a very little patch (!) which solves all that issues and makes ajax just a lot more powerful inside wicket *imho*also it will not break current code but is just an enhancement you will not notice unless you need it.",2809
Move Method,Better clustering support for DiskPageStore When session from nodeA gets replicated to nodeB store the page being replicated in nodeB diskpagestore immediately rather then keeping it in nodeB session. This has two benefits* Back button support when nodeA gets down and the sessions originating from nodeA are served by nodeB* Much lower memory consumption as the pages from nodeA don't need to be in session on nodeB (because they are stored on disk),2811
Extract Method,Allow to query component markup id without creating one 0,2812
Extract Method,Improve SelectOptions: allow customization of created SelectOption objects I've created a patch that makes SelectOptions more flexible as it now allows customization ofthe created SelectOption objects (e.g. adding an AjaxEventBehavior).The patch also extends the javadoc of Select I've added an example on how to use Select SelectOptions and SelectOption,2813
Extract Method,"Image could be made ajax aware.. It could be very cool if Image could be made ajax aware(so that it would add the random noise if it's added in ajax context).. Currently if you add a new image via ajax it's not updated(because the browser doesnt know it has to reload it). Normal procedure are to use noncaching image instead but its confusing a lot of people.Pasted from mailinglist:most people want stable urls for their images i would imagine so theycan be cached by the browser.....""in case of ajax this doesnt work because the url has to change so thatbrowser needs to know to refresh it.maybe image can know if its requested within an ajax request andautomatically add random noise to the url...there maybe room forimprovement here.""......",2815
Rename Method,set session locale when constructing session object Currently we create the session objects like this:WebApplication#getSession:if (session == null){// Create session using session factorysession = getSessionFactory().newSession(request);// Set the client Locale for this sessionsession.setLocale(request.getLocale());What I propose is to change the constructor from Session/ WebSession to take in a Locale parameter as well. That would make it possible for custom session classes to fix the locale by setting it in the constructor. Now that is only possible by overriding Session#getLocale,2816
Extract Method,make AutoCompleteBehavior's configuration more flexible add AutoCompleteSettings which encapsulates all needed configuration options so we don't need hundreds of constructors just to have every combination of configuration options available.,2819
Extract Method,expose the IItemFactory in RefreshingView I have modified the RefreshingView to expose the IItemFactory to facilitate dynamic row additions using the IItemFactory impl that was used to create the original rows onPopulate. This allows code to use the protected methods of RefreshingView (newChildId() newItem() populateItem()) that they would normally have access to as well as promote code resue for this activity.,2820
Extract Method,"DatePicker javaScript should be optimized. Currently DatePicker generates 3000 bytes of javascript for each date field. Currently DatePicker generates 3000 bytes of javascript per each date field (more specifically per each date field having DatePicker).If you have a table with 2 date columns and 10 rows this means 60000 bytes excess HTTP traffic. And it shows. It slows down page rendering significantly.DatePicker javaScript should be optimized. I myself am not a javascript wizard I do not know which hooks could serve this change. But I know where the problem is: each datePicker component is given its own initializer methods instead of using parametrized method calls:// block 1initproject__employee__startdate123 = function() {Wicket.DateTime.init( {widgetId: ""project__employee__startdate123"" <---------- this should be suitably parametrizedcomponentId: ""project__employee__startdate123"" <---------- this should be suitably parametrized....}// block 2if (wicketCalendarInitFinished) {initproject__employee__startdate123(); <---------- this should be suitably parametrized} else {wicketCalendarInits.push(initproject__employee__startdate123); <---------- this should be suitably parametrized}",2821
Rename Method,StringRequestTarget is bloated and needs some care when looking at StringRequestTarget I found the following things unnecessary(1) create a Charset object where a String is sufficient for the encoding(2) Write into a stream first then read back from it and write to the response stream using an internal buffer(3) flush the output stream(4) having to specify the charset in the 'contentType' and again in the 'charset' parameter.I made up an own version of StringRequestTarget and attached a patch for it.,2822
Extract Method,"AjaxLazyLoadPanel shouldn't call getLoadingComponent(String) in constructor Use case:class MyPanel extends AjaxLazyLoadPanel {private boolean bool;public MyPanel(String id boolean bool) { super(id); <-- bool is used in this callthis.bool = bool; <-- but not assigned until this call}public getLoadingComponent(String id) {if (bool) {return componentA;} else {return componentB;}}}-----Since getLoadingComponent(String) is called as part of the super constructor then the actual value of 'bool' can never be used. Furthur if bool were an object instead of a primitive could potentially cause an NPE. Instead the loading component can be created in onBeforeRender():protected void onBeforeRender() {if (!renderedOnce) {loadingComponent = getLoadingComponent(""content"");add(loadingComponent.setRenderBodyOnly(true));renderedOnce = true;}super.onBeforeRender();}... this also requires a change to the ajax behavior:public boolean isEnabled(Component<?> component) {return get(""content"") == loadingComponent || loadingComponent == null;}",2823
Extract Method,make wicket's configuration type an enum I would suggest that (starting with wicket 1.5.x) the wicket configuration type should be converted to an enum.current:String org.apache.wicket.Application.getConfigurationType()future:ConfigurationType org.apache.wicket.Application.getConfigurationType()------------package org.apache.wicket;public enum ConfigurationType{DEVELOPMENT DEPLOYMENT}enum have a lot of benefits e.g. cover all cases in a case block or having no upper- or lower-case inconsistencies.,2824
Extract Method,"Please make RequestLogger.log(RequestData SessionData) protected Could we please make the method above protected (rather than private). This makes it very simple to do something like this:@Overrideprotected IRequestLogger newRequestLogger() {return new RequestLogger() {@Overrideprotected void log(RequestData rd SessionData sd) {// do my custom logging HERE}};}ALSO - it would be real nice if at the same time you extract that creation of the AppendingStringBuffer to a method so that the log method now looks like:protected void log(RequestData rd SessionData sd){if (log.isInfoEnabled()){log.info(createStringBuffer(rd sd true);}}protected final void createStringBuffer(RequestData rd SessionData sd boolean includeRuntimeInfo) {... all of the stuff that was taken out of log that creates the ASBif (includeRuntimeInfo){Runtime runtime = Runtime.getRuntime();long max = runtime.maxMemory() / 1000000;long total = runtime.totalMemory() / 1000000;long used = total - runtime.freeMemory() / 1000000;asb.append(""maxmem="");asb.append(max);asb.append(""Mtotal="");asb.append(total);asb.append(""Mused="");asb.append(used);asb.append(""M"");}return asb;}",2825
Extract Method,Need a way to programmaticaly configure the location of the temp directory for file uploads and the size of the chunk buffer When using FileUploadField there is no way to configure where the temporary file is created. The only way to pass it to the lower levels is to use the System.property.Also there is no way to configure the size of the memory buffer used to receive the data. It's currently hardcoded to be 4K which is too small in some applications (slow file writes). I'd like to be able to bump that up to 128K.,2826
Inline Method,"Application adds a ComponentInstantiationListener that I don't want and can't remove I am writing a unit test that requires that I instantiate a Component. When I instantiated the Component I discovered that it requires an Application attached to the current thread. No problem: I instantiated the Application and set it in the thread using Application.set.However at this point I saw the following error: ""java.lang.IllegalStateException: you can only locate or create sessions in the context of a request cycle.""The problem is that Application in its constructor adds a component instantiation listener that delegates the discovery of the authorization strategy to the session and if the session isn't present it wants to create one. I tried setting up a session but that requires a request cycle which requires a request and response etc.I see two issues here:1. Application creates a component instantiation listener that cannot be removed. The only way to remove it is to have a reference to the listener and pass it to removeComponentInstantiationListener but the listener is created anonymously inside the Application constructor.This could be solved by creating a method called something like initializeDefaultComponentInstantiationListeners that a subclass could override with a no-op.2. The listener that Application creates always creates a Session even though Session's default implementation of the method that Application calls just delegates back to Application.This issue could be resolved by the solution to #1 since applications that know that they're not going to override the authorization strategy on a per-session basis could add an authorization listener that didn't create a session.",2827
Inline Method,Back port resource caching with component provided resource cache key In 2.0 resource caching is overridable up to the component level including providing a resource cache key for the resource stream. By providing a null resource cache key you can achieve that the resource stream is never cached which is something you need when you want to use a dynamic resource stream that can change over requests/ sessions. This is a request to back port that functionality so that truly dynamic templates can be used in Wicket 1.3 as well.,2829
Extract Method,WicketTester version which does not depend on junit WicketTester has a dependency to junit (mostly in its assert* methods). It would be nice for WicketTester to be more generic so that it would be possible to use it with other unit test tools. The assert methods could for instance be methods which return Result object. Result contains a message and boolean evaluation. Example:class Result {private boolean evaluation;private String message;}void assertRenderedPage(Class expectedReneredPageClass) -> Result wasRenderedPage(Class expectedReneredPageClass)A junit specific JUnitWicketTester could extend the generic base class and provide junit style assertions as they are now implemented in WicketTester using the base class methods.void assertRenderedPage(Class expectedReneredPageClass) {Result result = wasRenderedPage(expectedReneredPageClass);Assert.true(result.evaluation() result.message());},2830
Rename Method,Remove IComponentBorder in favor of IBehavior Yeah i think we can depricate IComponentBorder in 1.4 and point to IBehaviorbefore/afterand then remove it in 1.5 and maybe remote also **public *final* Component setComponentBorder(*final* IComponentBorder border)in component or make it*public* *final* Component setComponentBorder(*final* IBehavior border)to make it more clear to people that a border can be set by using a behavior(else who knows..)- Hide quoted text -On Sat May 16 2009 at 12:21 Juergen Donnerstag <juergen.donnerstag@gmail.com> wrote:> Hi>> question: looking at that code>> // Call implementation to render component> final IComponentBorder border => getComponentBorder();> if (border != null)> {> border.renderBefore(this);> }> notifyBehaviorsComponentBeforeRender();> onRender(markupStream);> notifyBehaviorsComponentRendered();> if (border != null)> {> border.renderAfter(this);> }>> IComponentBorder could be implemented via a behavior as well. Is there> any reason why IComponentBorder needs this special treatment and why> it is not implemented as a behavior?>> -Juergen>-----------------> Yeah i think we can depricate IComponentBorder in 1.4 and point to IBehavior> before/afteri like the ComponentBorder stuff because you can set it withoutinfluencing any beahavior or component stuff.. just render some contentbefore and after a component..i use it in debugging environmentmm:)------ Wait for some and give everybody a chance to comment,2831
Extract Method,"Text on BrowserInfoPage should be customizable The text on BrowserInfoPage should be customizable. Currently users gets a flash of the text on the page and I'd rather they saw a blank page or at least something that says ""Loading...""I realise that this is duplicate of the closed issue WICKET-1591 but I have to say I think you made the wrong call on that.In order to change the text of the BrowserInfoPage I have to override the WebRequestCycle.newClientInfo() method. My problem isn't that this is tricky but implementing it requires copying a large chunk of code from the WebRequestCycle method which could potentially change in future versions of the code making my implementation prone to bugs.In think that changing the text of BrowserInfoPage is probably a relatively common requirement since it looks pretty unprofessional to have a flash of text before the first application page is displayed.",2832
Extract Method,[Patch] Adding Listener to AjaxRequestTarget Add Listener to hook some step in AjaxRequestTarget response,2833
Extract Method,TabbedPanel extract factory method for tabs-container Please consider adding a factory method for the tabs container see attached diff.,2838
Rename Method,IChainingModel implementation This is a direct implementation of IChainingModel largely taken from AbstractPropertyModel. It needs to be generic typing for 1.4.,2839
Extract Method,"AjaxLazyLoadPanel callback script rendering I have a use-case when an AjaxLazyLoadPanel needs to be loaded later then on document ""onready"" js event (triggered later by some client-side event like click on some button). The way it is implemented right now there is no way to override AjaxLazyLoadPanel & change callback handling script. It would be useful if instead of: ================================= add(new AbstractDefaultAjaxBehavior() { ... @Override public void renderHead(IHeaderResponse response) { super.renderHead(response); response.renderOnDomReadyJavascript(getCallbackScript().toString()); } ... } ================================= it would be a protected method which would do the same thing: ================================= add(new AbstractDefaultAjaxBehavior() { ... @Override public void renderHead(final IHeaderResponse response) { super.renderHead(response); handleCallbackScript(response getCallbackScript().toString()); } ... } protected void handleCallbackScript(final IHeaderResponse response final String callbackScript) { response.renderOnDomReadyJavascript(callbackScript); } =================================",2840
Extract Method,Make DatePicker on DateTimeField overrideable I'd like to use the DateTimeField component but there is no API that I cansee to override what DatePicker gets added to the component. I'd like tosubclass the DatePicker and customize the icon positioning etc. I can dothis if just dealing with the DateTextField where I add the DatePickerbehavior myself but I can't do it when using the DateTimeField.Is there a way to do this? If not could it be enhanced so we can do this?A simple protected newDatePicker() method on DateTimeField would accomplishthis. I will submit a patch.See http://old.nabble.com/DateTimeField-enhancement-td26835114.html for more.,2841
Extract Method,Improve diagnostics on serialization exceptions The JDK's default serialization exception doesn't give a whole lot of information other than the object that didn't implement serializable. We can try to improve on this by giving more info of the object tree that was being serialized,2842
Extract Method,Palette does not allow tracking the AbstractChoices components via Ajax By default the two SELECTs used by the Palette are excluded from Ajax serialisation thus there is no way of tracking mouse clicks on them via Ajax. Thus the Javascript code causing this should be moved into a protected function so that derived classes can implement a different behaviour. See attached patch.,2844
Extract Method,improve css/js contributions currently the order of css contributions ispage (1st)->container(2nd)->component(3rd)we should inverse this order and allow deepest nested components to contribute first this way an outer container's contributions will appear below allowing the container to override the css/javascript,2845
Extract Method,reduce number of SpringBeanLocator#getBeanNameOfClass calls. In our application we use @SpringBean *without* name given extensively. It causes performance problems due to the fact that each time SpringBeanLocator uses its #getBeanNameOfClass method to look up a bean name within aplicationContext. Our inhouse improvement for this is to cache bean name for once localized bean in AnnotProxyFieldValueFactory. An instance of SpringBeanLocator gest always beanName in constructor. I attached the source code. It speeded up our app about 40%.The attached file provides improvement for 1.4.3 version.,2846
Extract Method,Multiple fileuploads in one http-session It is not possible to upload files in one http-session at the same time in several tabs because the UploadInfo-object is saved in this session and set to null after the first succesful upload... Therefore you could probably save a map with UploadInfo-objects instead.Wouldn't is be useful?Besides there are some reasons i don't want to use the possibility to have multiple uploads in one form..,2847
Rename Method,Refactor / rework pageability I need to add a NavigatorLabel to a GridView now NavigatorLabel has constructors accepting DataTable DataView and PageableListView but not a GridView.Instead of creating a new constructor accepting a GridView I think it would be far better to merge IPageable and the private NavigatorLabel.PageableComponent into a single public interface and making all pageable components implement this interface.I understand this approach breaks APIs and will need one major version maybe two allowing for deprecations in between.A less breaking alternative (but not as good in my opinion) is to change the constructor accepting a DataView to make it accept an AbstractPageableView which is a common ancestor of both DataView and GridView and defines the methods needed to satisfy NavigatorLabel.PageableComponent.,2850
Extract Method,"Tag attributes values are not escaped properly during writeOutput In WICKET-741 the double quote character was escaped. But the characters: ' (single quote) and & (ampersand) are not escaped.With & not escaped if it is included in an attribute value the result is not XML compliant and XHTML validations marks it as an error.With ' not escaped if single quote is used instead of double quote as in:<tag attribute='value'/>The result will be broken just as double quote was before WICKET-741.I'm not sure if < and > characters should also be escaped. Some validators/parsers allow them but some other mark them as errors. I would also replace them.I suggest adding the lines marked below to ComponentTag.writeOutput:---// attributes without values are possible e.g.' disabled'if (value != null){response.write(""=\"""");value = Strings.replaceAll(value ""&"" ""&amp;""); // <--- addedvalue = Strings.replaceAll(value ""\"""" ""&#34;"");value = Strings.replaceAll(value ""\'"" ""&#39;""); // <----- addedvalue = Strings.replaceAll(value ""<"" ""&lt;""); // <----- addedvalue = Strings.replaceAll(value "">"" ""&gt;""); // <----- addedresponse.write(value);response.write(""\"""");}---",2851
Extract Method,Backport IHeaderContributor.renderOnLoadJavascript etc. Backport Matej's nice little onLoad stuff:http://svn.apache.org/viewvc?view=rev&revision=505792,2852
Move Method,"Copy attributes from wicket:panel to source tag When I have a page like so:{code}<div wicket:id=""content"" />{code}And two types of panels that could go in there that should have different styling I'm currently stuck using a pointless extra div:{code}<wicket:panel><div class=""style1"">Content 1</div></wicket:panel>{code}If wicket allows copying attributes on from the wicket:panel tag to the source tag of the panel; this would become a lot cleaner and neater:{code}<wicket:panel class=""style1"">Content 1</wicket:panel>{code}",2853
Extract Method,Ability to pass Locale into getString function Locale#public String getString(final String key final Component component final IModel<?> model final Locale locale final String style final String defaultValue) is deprecated.Can this functionality be readded? See mailinglist: http://apache-wicket.1842946.n4.nabble.com/get-resource-translation-with-specific-locale-tp2247162p2247162.htmlThanks Marieke Vandamme,2854
Inline Method,Remove IRequestTarget.getLock and the synchronize blocks using it Synchronization is now done in Session#getPage which is a better place and makes IRequestTarget#getLock redundant. We should remove that method and the code that depends on it.,2855
Extract Method,AbstractPropertyModel getObjectClass don't consider nested IObjectClassAwareModel targets Currently if AbstractPropertyModel has an target that implements the IObjectClassAwareModel interface the known class of that target is not used to infer the modeled property type. Requested improvement: use 'target type'+'property expression' to return the property type of the AbstractPropertyModel,2857
Extract Method,Implement page versioning in Wicket 1.5 Wicket 1.5 is missing page versioning functionality currently.The page version should be incremented after:* page initial load (#init() version 0)* component addition* component change * component model change* component state change* component removalThe point with component involved are applicable to all no-auto components.In contrast to Wicket 1.4 a page version is being created even for Ajax requests. This will allow support of browser back button for Ajax applications.,2858
Move Method,Throw an IllegalStateException for already implemented onInitialize methods As discussed on the devs mail list the oninitialize method may already be implemented by some user. The Component on the next release can throw an IllegalStateException for those situations. IMO it is better than have this method called twice. I'm sending an patch with the implementation and an test for it.On a related note the onInitialize javadoc says:* Overrides must call super#{@link #onInitialize()}. Usually this should be the first thing an* override does much like a constructor.I just put that super call on the onInitialize overriding methods at the ComponentInitializationTest.,2859
Extract Method,Feedback messages should be Serializable not String Component#error currently accepts a Serializable as message parameter. This is inconsistent with #info #fatal and #warn. What's more Serializable is a really bad idea because a message should always be a String not a random object that can be toString'ed.toString is really bad practice for generating messages for user consumption.1. It (generally) bleeds implementation details.2. It (generally) isn't localized and often impossible to localize.That also means FeedbackMessage#message should be a String and not a Serializable for the exact same reasons.,2861
Rename Method,AbstractResource should give access to the error message for http errors AbstractResource lets you set the errorCode if something goes wrong but not a custom error message. This is really annoying because the user/developer does not get any hint why something goes wrong.current:response.sendError(data.getErrorCode() null); // <------ always null no custom error messageI attached a simple patch to fix this.,2862
Extract Method,Support for starting FormComponentPanel in WicketTester Normally we use a validator and an error feeback behavior for components like textfields and ddcs. These simple components are placed on a panel and can simply be tested by WicketTester.Now we use a FormComponentPanel to provide a way for adding a validator and this error feedback behavior to a container. But it can't be tested simply with the WicketTester because it can't be started.We could create a helper panel and test it there. But it would be great if wicket could support testing this component as easy as panels and pages.,2867
Extract Method,"Improve HomePageMapper to keep ""/"" as Url after the redirect Currently requests to '/' are handled by HomePageMapper when the request comes and by BookmarkableMapper when an Url should be created (e.g. for links forms redirects etc.). So requesting '/' ends with URL in the browser address bar like : ""/wicket/bookmarkable/com.example.MyPage"" (generated by BookmarkableMapper).The final Url is a bit confusing to users.Here is suggestion for improvement:drop HomePageMapper and use MountedMapper(""/"" app.getHomePage()) instead. This mapper will be registered after SystemMapper so it will be asked before all pre-configured mappers. if the user application still wants to map something else than Application#getHomePage() at ""/"" then it can do : add(new MountedMapper(""/"" CustomPage.class).",2869
Rename Method,Use standard exception handling in AjaxRequestTarget Now the respond method of the AjaxRequestTarget is catching all the RuntimeExceptions that are throwed. I would prefer to handle this exceptions by myself. I have a requestTarget that is a wrapper of the wicket AjaxRequestTarget where we could made something in case of errors in the respond method of the AjaxRequestTarget.I'm using the AjaxRequestTarget to replace a wizard panel in case of error I would display an errors panel instance of the original panel.,2870
Extract Method,Extra client-side scripting before closing modalwindow I would like to add extra client-side scripting that is executed before the modalwindow is closed. Now you can already add server-side with overriding onCloseButtonClicked but then the window is already closing. I need this because I want to show a confirm-box to the user so that he can confirm that he wants to close the window.Thanks Marieke Vandamme,2871
Extract Method,"Servlet 3 Annotation @WebFilter is not supported Trying to run my application this way:@WebFilter(value=""/*"" urlPatterns=""/*"" initParams = {@WebInitParam(name=""applicationClassName""value=""de.logviewer.HomeApp"")})public class HomeFilter extends WicketFilter{ }results in the follwing exception:Servlet.service() for servlet default threw exception java.lang.IllegalArgumentException: Error initializing WicketFilter - you have no <filter-mapping> element with a url-pattern that uses filter: de.logviewer.HomeFilter at org.apache.wicket.protocol.http.WicketFilter.getFilterPath(WicketFilter.java:930) at org.apache.wicket.protocol.http.WicketFilter.init(WicketFilter.java:677) at org.apache.catalina.core.ApplicationFilterConfig.getFilter(ApplicationFilterConfig.java:259) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:237) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:215) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:277) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:188) at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:641) at com.sun.enterprise.web.WebPipeline.invoke(WebPipeline.java:97) at com.sun.enterprise.web.PESessionLockingStandardPipeline.invoke(PESessionLockingStandardPipeline.java:85) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:185) at org.apache.catalina.connector.CoyoteAdapter.doService(CoyoteAdapter.java:325) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:226) at com.sun.enterprise.v3.services.impl.ContainerMapper.service(ContainerMapper.java:165) at com.sun.grizzly.http.ProcessorTask.invokeAdapter(ProcessorTask.java:791) at com.sun.grizzly.http.ProcessorTask.doProcess(ProcessorTask.java:693) at com.sun.grizzly.http.ProcessorTask.process(ProcessorTask.java:954) at com.sun.grizzly.http.DefaultProtocolFilter.execute(DefaultProtocolFilter.java:170) at com.sun.grizzly.DefaultProtocolChain.executeProtocolFilter(DefaultProtocolChain.java:135) at com.sun.grizzly.DefaultProtocolChain.execute(DefaultProtocolChain.java:102) at com.sun.grizzly.DefaultProtocolChain.execute(DefaultProtocolChain.java:88) at com.sun.grizzly.http.HttpProtocolChain.execute(HttpProtocolChain.java:76) at com.sun.grizzly.ProtocolChainContextTask.doCall(ProtocolChainContextTask.java:53) at com.sun.grizzly.SelectionKeyContextTask.call(SelectionKeyContextTask.java:57) at com.sun.grizzly.ContextTask.run(ContextTask.java:69) at com.sun.grizzly.util.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:330) at com.sun.grizzly.util.AbstractThreadPool$Worker.run(AbstractThreadPool.java:309) at java.lang.Thread.run(Thread.java:662)",2872
Extract Method,don't use @see upperClass when javadoc inheritance is sufficient I see this all the time:/*** @see org.apache.wicket.Application#getApplicationKey()*/@Overridepublic final String getApplicationKey(){return getName();}The javadoc links to the parent javadoc using @see. This is not required since javadoc inheritance is enabled by default. Unless you want to modify the javadoc from the parent class it's sufficient to just don't declare javadoc at all. less work and better result!@Overridepublic final String getApplicationKey(){return getName();}will automatically inherit the javadoc from the method it overrides.Quite often the @see link is broken after refactoring.So the @see generates a lot of unnessecary work (fix links after refactors) and makes javadoc less usable.Shouldn't we just abandon that style of documentation if the parent javadoc is fine for the child???,2873
Extract Method,Add restartResponseAtSignInPage() Method to AuthenticatedWebApplication I have a situation where I need to be able to get the sign in page class outside the context of the AuthenticatedWebApplication class itself (or a subclass). Basically I've got a Spring Security framework that's going to register a custom exception mapper (that wraps the default one). If it sees an AuthenticationCredentialsNotFoundException it needs to throw a RestartResponseAtInterceptPage(getSignInPage()). Can we add a method that does that? Patch coming...,2874
Extract Method,Download link does not set Content-Type and Content-Length headers When using a wicket download link response headers Content-Type and Content-Length headers are not set.,2875
Extract Method,"Log a warn or throw an exception when an AjaxFormComponentUpdatingBehavior is added to an choice component I was thinking about warn on console:String.format(""AjaxFormComponentUpdatingBehavior is not suposed to be add in the form component %s. AjaxFormChoiceComponentUpdatingBehavior is meant for choices/groups that are not one component in the html but many"" getComponent().getPageRelativePath())at AjaxFormComponentUpdatingBehavior#bind when we detect that the component is an instance of: RadioChoice CheckBoxMultipleChoice RadioGroup CheckGroupJust don't know if it is better throw an exception instead.",2877
Extract Method,Propose removing 'final' modifier to AbstractSingleSelectChoice.convertValue() While creating a subclass of DropDownChoice I wanted to write a specialized reverse mapping function that would allow the option value to be used as the primary key for direct lookup of the object. Unfortunately AbstractSingleSelectChoice.convertValue() is has a 'final' modifier. (I'm guessing this is an inside joke because it does a linear search... ;-))Any chance of removing the final modifier?,2878
Extract Method,@SpringBean support of non-singleton beans Two fixes:In wicket-spring: SpringBeanLocator got a new property called singletonBean which is true if the bean is a singleton(the property is set in the constructor)In wicket-spring-annot:AnnotProxyFieldValueFactory is modified that if the bean is a non-singleton then bypass the cache,2880
Extract Method,How to set TABPanels'titel to EscapeModel(false) To let a TABPanel's title label display whatever we want (e.g. to setEscapeModelString(false)) we need a callback method like the one for the linke the one forprotected WebMarkupContainer newLink(MarkupContainer parent String linkId final int index){return new Link(parent linkId){private static final long serialVersionUID = 1L;@Overridepublic void onClick(){setSelectedTab(index);}};}Then we can overwrite the e.g. newLinkLabel method in an annonymous class.,2881
Inline Method,"fix documentation errors and review models Review the newly backported models for bugs in documentation. Also consider these remarks from this http://www.nabble.com/Model-quirks-tf3489227.html email:- PropertyModel#propertyType(Component)Is never called has a @see to non existent methodAbstractPropertyModel#propertyType(Component)- BoundCompoundPropertyModel#propertyType(Component)Ditto makes BoundCompoundPropertyModel.Binding#type worthless- CompoundPropertyModelSince we don't have a ICompoundModel any longer shouldn't this class bebetter named InheritablePropertyModel?- AttachedCompoundPropertyModelThis inner class implements IInheritableModel which is superfluousbecause in Component#initModel() it is tested on IWrapModel before everbeing able to be used as an IInheritableModel.- IWrapModel and IWrapModel#getNestedModel()The name of this class and method are misguiding:Wrapping models inside other models is quite common in Wicket (seePropertyModel() javadoc).But this interface is mainly used as a marker for cases where modelsare inherited ""from components higher in the hierarchy"" - seeComponent#initModel() and MarkupContainer#setModel().Why not call it IInheritedModel and #getInheritableModel() then?I know that the same interface is used forIAssingmentAwareModel#wrapOnAssignment(Component) but this method couldequally well just return an IModel or be a #setComponent(Component).",2882
Rename Method,Allow IResourceStream.length() to return -1 When IResourceStream.length() returns -1 Wicket's ResourceStreamRequestTargetResponse will not send a Content-Length header. When an IResourceStream doesn't know in advance the number of bytes that allows to send a response without buffering the whole resouse.,2883
Rename Method,"IHeaderResponse.renderOnUnLoadJavascript(String javascript); Where there is a renderOnLoadJavascript there ought to be a renderOnUnLoadJavascript too.This is all just copy past so it's a little smelly.Beyond that I wouldn't mind to be able to specify which element an event should be added.Index: wicket/src/main/java/org/apache/wicket/markup/html/internal/HeaderResponse.java===================================================================--- wicket/src/main/java/org/apache/wicket/markup/html/internal/HeaderResponse.java (Revision 529942)+++ wicket/src/main/java/org/apache/wicket/markup/html/internal/HeaderResponse.java (Arbeitskopie)@@ -1974 +19719 @@}}+ /**+ * @see org.apache.wicket.markup.html.IHeaderResponse#renderOnUnLoadJavascript(java.lang.String)+ */+ public void renderOnUnLoadJavascript(String javascript)+ {+ List token = Arrays.asList(new Object[] { ""javascript-event"" ""unload"" javascript });+ if (wasRendered(token) == false)+ {+ renderJavascriptReference(WicketEventReference.INSTANCE);+ JavascriptUtils.writeJavascript(getResponse()+ ""Wicket.Event.add(window \""unload\"" function() { "" + javascript + "";});"");+ markRendered(token);+ }+ }+}Index: wicket/src/main/java/org/apache/wicket/markup/html/IHeaderResponse.java===================================================================--- wicket/src/main/java/org/apache/wicket/markup/html/IHeaderResponse.java (Revision 529942)+++ wicket/src/main/java/org/apache/wicket/markup/html/IHeaderResponse.java (Arbeitskopie)@@ -1744 +17411 @@* @param javascript*/public void renderOnLoadJavascript(String javascript);++ /**+ * Renders javascript that is executed after the page is unloaded.+ * + * @param javascript+ */+ public void renderOnUnLoadJavascript(String javascript);}",2884
Extract Method,"Prevent setTimeout for AjaxSelfUpdatingTimerBehavior from firing after its contributing component has been replaced Currently the setTimeout for an AjaxTimerBehavior will fire after the contributing component has been replaced or removed through an ajax request. This will result in an exception server side and a full page refresh client side.One possible solution is to override getCallbackScript() to enclose the ajax callback in another function that will only execute the ajax request if the markup for the contributing component is still in the dom tree:protected CharSequence getCallbackScript(boolean recordPageVersion){String mId = getComponent().getMarkupId();StringBuilder sb = new StringBuilder(""exec_func(function() { "");sb.append(""var el = wicketGet('"" + mId + ""'); "");sb.append(""if(null != el) {"");sb.append(super.getCallbackScript(recordPageVersion));sb.append(""}"");sb.append(""})"");return sb.toString ();}Other options are to have the request cycle swallow the ""component not found"" exception for timer behaviors (since they will only fire 1 extra time)ora more complicated scheme would involve wicket actively managing javascript timer tokens client side so that they can be removed with clearTimeout when their contributing component is removed through an ajax request. The javascript to do this would be trivial (a simple container object mapping dom id to timer token) but the wicket lifecycle would need to support an onRemove callback for components to perform cleanup.",2887
Rename Method,TextField should determine the object type from the model if the model supports it Currently it is mandatory to specify the type of model object in textfield's constructor for the convesion to work properly. With certain models (PropertyModel CompoundPropertyModel) wicket should determine the target property type automatically.,2888
Extract Method,Variation of JavaScriptHeaderContributor: ExternalJavaScriptHeaderContributor Where JavaScriptHeaderContributor contributes a reference to a javascript file relative to the context ExternalJavaScriptHeaderContributor allows references to any javascript files just as long as the reference is recognized by the 'src' attribute of the <script> element.This is not really an extension rather a 'down'tesnsion since ExternalJavaScriptHeaderContributor equals (JavaScriptHeaderContributor - the lines of code that prepend the getContextPath to the given reference).mf,2889
Inline Method,Configuration of app mode isn't customisable Everything else in wicket is programatically configurable apart from DEPLOYMENT or DEVELOPMENT app modes. I can't set System properties due to security constraints and I don't want to have to set things up in web.xml because it's tedious and I already have which deployment ID I'm using configured somewhere else using Spring.I talked to ivaynberg about this on ##wicket and we reckon Application should have an abstract getConfigurationMode():String function which is overridden in WebApplication and PortletApplication appropriately to replicate current functionality. The default implementations should /not/ be final so you can override them to pull the config from wherever you like.If anyone has any objections to changing things to work like this please shout now otherwise I will supply a patch for 1.x and trunk shortly.,2891
Extract Method,Tree components cleanup Remove old tree from wicket-extensions.Move tree and tree-table from core to extension.Commit new tree components to core.,2893
Extract Method,Do not throw an error when image not found while testing While testing I came across the following error:junit.framework.AssertionFailedError: expected:<WebTestPage> but was:<DummyHomePage>The real error in this is: gfx/offline.gif does not exist. If i change it to a file that exists everything goes fine.If I test it in my Tomcat I get a proper 404:Unable to find package resource [path = default/gfx/offline.gif style = null locale = en]see also: http://www.nabble.com/jUnit-testing-and-not-existing-images-tf3907536.html,2895
Rename Method,make getConvertedInput final again and remove final from convert which should be renamed to convertInput See http://www.nabble.com/Re%3A-Use-getConverterInput-rather-than-updateModel-in-FormComponentPanel-p11399356.html,2899
Extract Method,Improve PageStore * create AbstractFileStore which contains the (de)serialization logic * create SimpleSynchronousFilePageStore to demonstrate developing custom pageStores* create DiskPageStore that uses one file per pagemap to improve performance under hight load* move page versions infromation from sessionstore to pagestore (used when getting page with -1 specified as (ajax) version number* make it possible for certain pagestores to reuse serialized page data when serializing the pagemap (improve clustering efficiency),2900
Extract Method,date converters should try to use any components they are coupled to get the locale Date converters should try to use any components they are coupled to get the locale. Without this the dates example doesn't use the appropriate date patterns.,2901
Extract Method,Wrap Guice-Injector with proxying for Objects Since the GuiceComponentInjector already has proxying support build in I'd like to use this to inject other Objects besides from Components.A function likeguiceComponentInjector.inject( Object xy )would be great!,2902
Rename Method,Backport of Header contribution filtering to 1.x The attached patch backports to branch 1.x the modifications made with rev.461786 to the wicket trunk (IHeaderResponse and related classes)Hope this help!,2903
Rename Method,Update ImageButton to handle ResourceReferenc To be consistent with Image and its support for ResourceReferences ImageButton was modified to exhibit the same behaviors.New constructors and methods have been added.Existing methods have been modified to behave similiar as Image but maintaining the Button constructs.,2904
Move Method,Improve multiple DISTINCT aggregation. Currently tajo provides three stage for optimizing distinct query aggregation. But it just supports one column for distinct aggregation as follows:{code:title=Query1|borderStyle=solid}select a.flag count(distinct a.id) as cnt sum(distinct a.id) as totalfrom table1group by a.flag{code}If you write two more columns for distinct aggregation you can't apply optimized distinct aggregation as follows:{code:title=Query2|borderStyle=solid}select a.flag count(distinct a.id) as cnt sum(distinct a.id) as total count(distinct a.name) as cnt2 count(distinct a.code) as cnt3from table1group by a.flag{code}In this case you may see low performance for your query. Thus we need to improve multiple DISTINCT aggregation. Correctly we should support three stage for multiple DISTINCT aggregation.,2905
Move Method,Remove hadoop native dependency of pullserver Currently tajo does not support multiple hadoop version on same binary.Therefore Many user should need to build the binary.So we should remove the code dependency of NativeIO,2906
Extract Method,Separate SQL Statements from Catalog Stores When developing the additional catalog stores for another database systems it is needed to add additional sql statements such as triggers and indexes. These sql statements could increase the code size of catalog store and when database schema has changed it may affect the catalog store source codes. I feel that it is needed to separate the sql statements from the java source codes.,2907
Rename Method,Concurrent execution of independent execution blocks Currently Tajo can execute ExecutionBlocks one by one even though there remain enough resources to execute two or more ExecutionBlocks.We can improve the query processing performance by executing two or more independent ExecutionBlocks if possible.,2908
Extract Method,Improve the comparison of timestamp and date types See the discussion at https://groups.google.com/d/msg/tajo-user-kr/5u8KMPtLdaY/0PNt-OAlwDEJ.The comparison of timestamp and date may be widely used in many applications. However the type conversion is required for that. We need to improve the comparison of two types.,2909
Extract Method,Refactor and Improve Datum A datum constructor called too many times. and there are so many overhead of converting between a primitive and string* Change to lazy datum types* Improve memory efficiency* Improve text number processing,2910
Extract Method,Change default client and table time zone behavior By default TajoClient uses GMT as client time zone unless session variable {{TIMEZONE}} is specified. Also by default Table uses GMT as table time zone unless table property {{timezone}} is specified.This patch changes these default behavior as follows:* TajoClient will use {{TimeZone.getDefault()}} by default.* Table implicitly uses tajo.timezone by default.* In other words this default time zone does not affect the table property in catalog.I also added the documentation about time zone.http://people.apache.org/~hyunsik/timezone/time_zone.html,2912
Extract Method,Cleanup TajoAsyncDispatcher and interrupt stop events See the titile.{{TajoAsyncDispatcher}} is an implementation copied from yarn {{AsyncDispatcher}} for log message handling. but too many create the thread interrupted message.We should improve the stop event.{noformat}2014-12-17 10:08:35327 WARN: org.apache.tajo.master.TajoAsyncDispatcher (stop(115)) - Interrupted Exception while stopping2014-12-17 10:08:35328 WARN: org.apache.tajo.master.TajoAsyncDispatcher (stop(115)) - Interrupted Exception while stopping2014-12-17 10:08:36896 WARN: org.apache.tajo.master.TajoAsyncDispatcher (stop(115)) - Interrupted Exception while stopping2014-12-17 10:08:36898 WARN: org.apache.tajo.master.TajoAsyncDispatcher (stop(115)) - Interrupted Exception while stopping2014-12-17 10:08:37745 FATAL: org.apache.tajo.master.TajoAsyncDispatcher (dispatch(143)) - Error in dispatcher thread:QUERY_JOB_HEARTBEATorg.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.InterruptedExceptionat org.apache.tajo.master.TajoAsyncDispatcher$GenericEventHandler.handle(TajoAsyncDispatcher.java:204)at org.apache.tajo.master.querymaster.QueryInProgress.heartbeat(QueryInProgress.java:290)at org.apache.tajo.master.querymaster.QueryInProgress.access$000(QueryInProgress.java:53)at org.apache.tajo.master.querymaster.QueryInProgress$QueryInProgressEventHandler.handle(QueryInProgress.java:196)at org.apache.tajo.master.querymaster.QueryInProgress$QueryInProgressEventHandler.handle(QueryInProgress.java:192)at org.apache.tajo.master.TajoAsyncDispatcher.dispatch(TajoAsyncDispatcher.java:137)at org.apache.tajo.master.TajoAsyncDispatcher$1.run(TajoAsyncDispatcher.java:79)at java.lang.Thread.run(Thread.java:701)Caused by: java.lang.InterruptedExceptionat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1222)at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:269)at org.apache.tajo.master.TajoAsyncDispatcher$GenericEventHandler.handle(TajoAsyncDispatcher.java:199)... 7 more2014-12-17 10:08:37746 WARN: org.apache.tajo.master.TajoAsyncDispatcher (stop(115)) - Interrupted Exception while stopping{noformat},2913
Extract Method,Remove hadoop-common dependency from tajo-rpc tajo-rpc depends on hadoop-common due to only one integer configurable parameter. As a result it naturally depends on lots of third-party libraries.The main objective of this issue removes a hadoop-common dependency from tajo-rpc. It will maker TAJO-1160 easier.,2914
Rename Method,Cleanup the relationship of QueryInProgress and QueryJobManager Each QueryInProgress instance maintains an individual event handler involving each thread. It complicates the relationship between QueryJobManager and QueryInprogress. The main objective of this issue is to remove each event handler from QueryInProgress and distinguishes their roles of both classes.,2915
Rename Method,Change the default output file format. Currently the default output file is CSV. Due to its nature CSV has mainly three problems:* Its line or field delimiter can be duplicated to some character included in the result data.* Plan text file is likely to be larger than other file formats.* Its read and write performance is slow.We need to change the default output file format into other file formats. We also need to investigate which file format is the best for it.,2917
Rename Method,Bump up hadoop to 2.2.0 Hadoop 2.2.0 has been released. We need to bump up hadoop to 2.2.0. This hadoop version uses protobuf-2.5.0. So Tajo also needs to bump up protobuf-2.5.0.,2918
Rename Method,Support multi-bytes delimiter for CSV file Supports multi-character / non-ascii delimiter for CSV file.,2919
Extract Method,Remove locking on RMContext Seemed not necessary.,2920
Rename Method,In predicate support See the title. In predicate is a basic part of SQL standards. We need to support this.,2921
Extract Method,"Support reconnect on tsql In development we need restart tajo-master frequently. But then we should restart all tsql clients also which is a little annoying.With the new option ""--reconnect"" for tsql we can make our life easier a little bit.",2922
Extract Method,Minor performance improvement of MemSortExec Currently tajo uses Collections.sort() with TupleCompartor which can incur some overheads on comparing.,2923
Extract Method,Make IntermediateEntryProto more compact List<Pair<Long Pair<Integer Integer>>> can be just a long[],2924
Rename Method,Implement hash anti-join operator Hash anti-join operator is used in 'NOT IN' clause. We need it.,2926
Extract Method,"Support ""explain global"" to get physical plan Inconvenient to see that in log file.",2927
Extract Method,Comparing two date or two timestamp need not normalizing date and timestamp types are converted to normalized form (TimeMeta) for comparing even between datum with same type.,2929
Move Method,Clean up CatalogStore Currently CatalogStore is designed to read schema xml file on initial mode. But MySQLStore and MariaDBStore read SQL statement files. Thus we need to clean up these files.,2931
Extract Method,INSERT INTO with wrong target columns causes NPE. h3. How to reproduce{code}CREATE TABLE T1 (col1 int col2 int);INSERT INTO T1 (col1 col3) select l_orderkey l_partkey from default.lineitem;{code}When the target column name is wrong (i.e. col3 in INSERT statement) the query will cause NPE.,2932
Extract Method,Removing rest api to create table POST /databases/{database-name}/tables interface Currently POST /databases/{database-name}/tables interface is too hard to use. Because it just serialize TableDesc Class. so I think just using query interface is better than create table interfaceand for them changed result of query interfaceremoving location header and add response body has query and ResultCodeand url.(if query is finished directlry uri isn't),2933
Extract Method,Tajo Java Client version 2 I propose Tajo Java Client version 2. Its motivations are as follows:* After TAJO-1625 the error propagation has been changed significantly. Java Client should make use the new error propagation system. Java Client should throw the proper exceptions.* Many Java API exposes internal data structure and protocol buffer data structure. It requires users to understand its internal behavior and architecture. We should hide them from users.,2935
Extract Method,Precompute the hash value of various kinds of ids QueryId ExecutionBlockId TaskId and TaskAttemptId are used for key of HashMap. Statically maintaining hash value will improve the performance.,2936
Rename Method,Add examples for TajoClient v2 This patch adds the example code for TajoClient v2.,2937
Rename Method,Move Tajo into Java 8 As we discussed in the mailing list (http://search-hadoop.com/m/ZGIO9XqiTPKruL7/java+8&subj=+DISCUSSION+Migration+to+Java+8) this patch moves Tajo into Java 8. It changes the language level in maven and fix some unit test failure.,2938
Inline Method,tajo-cluster-tests is not available when it is used as an external maven module. tajo-cluster-tests was separated in order to help other maven modules to use cluster tests. But it is actually unavailable because its test resources (table ddl and data sets) can be only accessed through absolute paths.This patch makes the test resources accessable from other maven modules.,2939
Extract Method,Creating too many TableMetaProto objects might lead a potential memory leak In the current TableMetaImpl a pb object is created for every getProto() calls.Since TableMeta.getProto() is called for creating a Fragment the current implementation shall cause an out of memory exception when too many pb objects are created for a large data.,2941
Move Method,Well support for self-describing data formats *Problem*Tajo already has a support for self-describing data formats like JSON Parquet or ORC. While they are capable of providing schema information by themselves users must define schema to query on them with the current implementation. To solve this inconvenience we have to improve our query planner to support self-describing data formats well. *Solution*First we need to allow omitting schema definition for the create table statement. When a query is submitted for a self-describing table the columns which don't exist in that table will be filled with Nulls.,2942
Inline Method,Using TUtil.newHash(Set/Map) should be replaced by Java's diamond operator See the title. We introduced Java 7. We don't need TUtil.newHash(Set|Map) utility methods anymore. We need to eliminate them.,2943
Rename Method,LogicalNode should have an identifier to distinguish each logical node instance. In some parts of LogicalPlan an instance object id is used as a key in some map data. If we have identifiers to distinguish each logical node instance it would be better than the current implementation.,2944
Extract Method,Implement HBaseTablespace::getTableVolume() method Table volume has an important role in query planning. Tajo's query optimizer makes many decisions based on table volume because it is currently the only available statistics. However HBaseTablespace doesn't support getTableVolume() method so our plan is not good when the query involves any HBase tables.,2945
Extract Method,Implement Enforcer that forces physical planner to choose specified algorithms Tajo worker generates a physical plan from a logical plan received from QueryMaster. However some plans need to force a physical planner to choose specified algorithms. Traditionally Enforcer plays a role to provide physical properties of a query plan and enforce physical algorithms. We also need this feature.,2946
Extract Method,Improve memory usage of ExternalSortExec ExternalSortExec keeps tuple list for sort. but it causes too many GC.We should change to off-heap tuple instead of VTuple,2947
Rename Method,Rename the name 'option' to 'property' in TableMeta. This is a trivial patch to rename 'option' to 'property' in TableMeta. This is necessary to make naming more consistent.,2948
Extract Method,INSERT INTO ... SELECT We should support 'INSERT INTO ... SELECT' statement.,2949
Rename Method,By default Optimizer should use the table volume in TableStat. Currently the optimizer by default gets table volumes through storage manager and employ them for join optimization. But in some cases it causes performance degradation because aggregating all file volumes is not cheap in large partitioned tables on S3 or HDFS.So this patch improves TableStatUpdateRewriter to use table volumes of TableStat by default and it also adds a session variable 'USE_TABLE_VOLUME' to allow the optimizer to use the table volume through storage handler.,2950
Extract Method,Add AsyncTaskServer to TajoMaster TajoMaster performs various tasks for query lifetime and the tasks are likely to be easily delayed. They sometimes may be the main cause of unnecessary blocking problems. My patch adds AsyncTaskServer to run delayed tasks in an asynchronous way.,2951
Extract Method,QueryMaster and TajoWorker should support the exception propagation Error propagation system was refactored by TAJO-1625. But worker and query master is missing in TAJO-1625. This issue improve error propagation system in TajoWorker and QueryMaster,2952
Rename Method,Upgrading ORC reader version Currently Tajo uses presto-orc-0.86 but it was old version even when it was integrated because Presto was using JDK 1.8 at that time.Now that Tajo is based on JDK 1.8 it can be upgraded to a recent version 0.132.It becomes more robust and some minor features are added.Additionally hive compatibility is improved so upgrading is necessary to support Hive-catalog.,2953
Extract Method,"Error or progress update should use stderr instead of stdout Error or progress update message should be separated from stderr for those who want to use ""tsql -c"" command.",2954
Extract Method,Use Type instead of DataType for EvalNode See TAJO-2042 and TAJO-2043. This issue will change the return type of EvalNode::getValueType() to be org.apache.tajo.type.Type and its related code. Also I'll some converter to order to keep existing APIs.,2955
Rename Method,Implement an example HTTP tablespace See discussion at http://mail-archives.apache.org/mod_mbox/tajo-dev/201605.mbox/browser.This tikcet is to implement a simple example http tablespace.,2956
Rename Method,Add 'ALTER TABLE UNSET PROPERTY' statement to Tajo DDL For version 0.11.x now there is only set statement for table property. When the user makes a typo with DDL statement this mistake cannot be removed. We also need the way to remove already set properties.,2957
Extract Method,Support PostgreSQL CatalogStore See the title. PostgreSQL is also an widely used open source DBMS. Like TAJO-179 we need to support postgresql catalog store.,2958
Rename Method,Implement LogicalPlanVerifier to check if a logical plan is valid The current Tajo does not have any verification system to check whether a logical plan is valid or not. LogicalPlanVerifier will verify the followings of a logical plan:* operand types checking** some operators have type restrictions. For example plus(\+)'s operands must be numerical values.* Table and column existence check. ** Examples are as follows:*** a create table statement must check if the table already exists*** columns included in select list must exist in corresponding tables.,2959
Rename Method,Maintaining connectivity to Tajo master regardless of the restart of the Tajo master Currently when you restart the Tajo master you should restart all the workers and clients also.When client or worker has problem with connection to Tajo master due to the master restart it needs to close the previous connection and try to reconnect to the master,2963
Rename Method,drop table command should not remove data files in default h3. ProblemIn the current implementation Tajo removes a data directory when a user issues 'drop table' command. But it is very dangerous in many cases. For example some users may lost large data sets.h3. SolutionIn default 'DROP TABLE' should not remove a data directory. - and we need to add a config property 'tajo.command.drop_table.data_removal' for those who want to change the behavior of 'drop table'.- In addition Tajo should provide 'DROP TABLE table_name PURGE' for removing all data.,2965
Move Method,Implement chr(int) function chr function returns one character from the ascii code argument.{code}text chr(int){code},2967
Rename Method,Add Database support to Tajo Currently all tables reside in single namespace (default). Tajo should support multiple namespaces (i.e. databases) so that users can create tables in independent namespace.,2971
Extract Method,Improve TajoClient to directly get query results in the first request Currently TajoClient cannot deal with simple queries (e.g. select * from table limit 1 or select 1) which are executed in TajoMaster without distributed execution. The final results are always stored in HDFS and TajoClient gets the result via scanner with TableDesc obtained from GetQueryResultResponse. For simple queries directly executed at TajoMaster TajoClient needs to directly get some binary serialized rows results from GetQueryStatusResponse or GetQueryResultResponse instead of reading materialized tables.This feature would be also useful for low latency queries EXPLAIN clauses and expr-only statements without FROM clause.,2972
Extract Method,"Implement find_in_set function find_in_set(strstr_array) - Returns the first occurrence of str in str_array where str_array is a comma-delimited string.Returns null if either argument is null. Returns 0 if the first argument has any commas.Example:""select find_in_set('cr''crtccrcdef') as col1 ""-> result: 3",2973
Rename Method,Add missing visitor methods of AlgebraVisitor and BaseAlgebraVisitor This patch primarily adds all missing operator types' visitor methods of AlgebraVisitor and implements concrete methods in BaseAlgebraVisitor. Currently BaseAlgebraVisitor may cause incorrect PlanningException because BaseAlgebraVisitor does not handle all operator types. This patch eliminates this potential bug.In addition this patch contains two refactors in order to eliminate duplicate names:* Rename tajo.algebra.DataType to DataTypeExpr* Rename tajo.algebra.Target to TargetExpr,2974
Extract Method,Improve integration with Hive Hi guys~ I wish to discuss about HCatalogStore. HiveMetaStore types consists of three types: embedded local remote. This type will set up at hive configuration file which named 'hive-site.xml' and HiveMetaStoreClient can use MetaStore by configuration file. So if Tajo have only to add hive configuration file to classpath Tajo can use HiveMetaStore with HiveMetaStoreClient. But current HCatalogStore set up configurations about HiveMetaStore as follows:- hive.metastore.uris- hive.metastore.kerberos.principal- hive.metastore.local- hive.metastore.sasl.enabledAnd Tajo doesn't need 'tajo.catalog.uri' property because 'hive-site.xml' already includes metastore host and port. So it looks like unnecessary settings. What do you think about this suggestion? :),2975
Rename Method,Improve intermediate file Currently some intermediate file is text format Tajo should change to binary format.* Support configurable storage type(RAW CSV),2976
Rename Method,Visit methods of LogicalPlanVisitor should take a query block as parameter A logical plan is composed of multiple query blocks. Each logical node must belong to one query block. A query block instance provides lots of information. So it is essential information in many rewrite rules and optimizer implementations. However so far individual rewrite rule or optimizer implementation have dealt with query block directly. It may be error-prone and cause duplicated codes. This patch refactors each visitor method of LogicalPlanVisitor to take a query block as a parameter. I'm expecting that this change will provide more convenience for rewrite rules and optimization development.,2977
Extract Method,Extract ColumnPartitonUtils class for ColumnPartition rewrite In ColumnPartitionedTableStoreExec.java and SeqScanExec.javaThey have similar rewriteColumnPartitionedTableSchema function.so we extract util class and should use this.,2979
Extract Method,Add a test development kit for unit tests based on executions of queries Relational algebra has numerous combinations so SQL queries also have numerous cases that we can't figure out. One nice way of various ways to make and keep Tajo stable is to add lots of unit tests that cover many cases.In many unit tests we use front-end tests which execute SQL queries and verify their results. So far we have implemented Java code to verify them. I think that its productivity is bad. It finally make us lazy to add various cases.This patch adds QueryCaseTestBase class to help developers to add unit tests with little effort. QueryTestCaseBase provides useful methods to easily execute queries and verify their results.It uses four resource directories:* src/test/resources/dataset - contains a set of data files. It contains sub directories each of which corresponds each test class. All data files in each sub directory can be used in the corresponding test class.* src/test/resources/queries - This is the query directory. It contains sub directories each of which corresponds each test class. All query files in each sub directory can be used in the corresponding test class.* src/test/resources/results - This is the result directory. It contains sub directories each of which corresponds each test class. All result files in each sub directory can be used in the corresponding test class.For example if you create a test class named TestJoinQuery you should create a pair of query and result set directories as follows:{noformat}src-||- resources|- dataset| |- TestJoinQuery| |- table1.tbl| |- table2.tbl||- queries| |- TestJoinQuery| |- TestInnerJoin.sql| |- table1_ddl.sql| |- table2_ddl.sql||- results|- TestJoinQuery|- TestInnerJoin.result{noformat}QueryTestCaseBase basically provides the following methods:* executeQuery() - executes a corresponding query and returns an ResultSet instance* executeQuery(String fileName) - executes a given query file included in the corresponding query file in the current class's query directory* assertResultSet() - check if the query result is equivalent to the expected result included in the corresponding result file in the current class's result directory.* cleanQuery() - clean up all resources* executeDDL() - execute a DDL query like create or drop table.In order to make use of the above methods query files and results file must be as follows:* Each query file must be located on the subdirectory whose structure must be src/resources/queries/$\{ClassName\} where $\{ClassName\} indicates an actual test class's simple name.* Each result file must be located on the subdirectory whose structure must be src/resources/results/$\{ClassName\} where $\{ClassName\} indicates an actual test class's simple name.Especially executeQuery() and assertResultSet(ResultSet) methods automatically finds a query file to be executed and a result to be compared which are corresponding to the running class and method. For them query and result files must additionally comply the followings:* Each result file must have the file extension '.result'* Each query file must have the file extension '.sql'.,2980
Rename Method,Rename killQuery of QMClientProtocol to closeQuery As we discussed in TAJO-305 the name 'killQuery' is wrong and does not represent its purpose. This patch renames killQuery to closeQuery. This is a trivial change.,2981
Rename Method,Add getParentCount() getParents() getParent() functions to DirectedGraph See the title.In the current implementation DirectedGraph provides only getParent() because it assumes that there is only one parent. However multiple parents should be supported in DirectedGraph.,2983
Extract Method,Extend TajoClient to run a query with a plan context serialized as the JSON form In some OLAP applications which has a separate query engine such as Tajo they accept SQL-like languages and parse them to generate query plans. Mondrian is a representative example of these applications. If TajoClient accepts query plans of the JSON form user queries can be efficiently executed without the duplicated parse phase.,2984
Extract Method,Rearrange reserved and non-reserved keywords Keywords of Tajo are classified as reserved and non-reservedand reserved keywords cannot be used as table name or column name.Because reserved keywords of Tajo are different from other DBs like PostgreSQL and MySQL migrating tables from those DBs to Tajo induces table creation error sometimes.So we need to rearrange reserved and non-researved keywords.Following shows keywords which are allowed in PostgreSQL but not allowed in Tajo.{noformat}mydb=# \dList of relationsSchema | Name | Type | Owner --------+-----------+-------+--------public | filter | table | ktparkpublic | first | table | ktparkpublic | format | table | ktparkpublic | grouping | table | ktparkpublic | hash | table | ktparkpublic | index | table | ktparkpublic | insert | table | ktparkpublic | last | table | ktparkpublic | location | table | ktparkpublic | max | table | ktparkpublic | min | table | ktparkpublic | national | table | ktparkpublic | nullif | table | ktparkpublic | overwrite | table | ktparkpublic | precision | table | ktparkpublic | range | table | ktparkpublic | regexp | table | ktparkpublic | rlike | table | ktparkpublic | set | table | ktparkpublic | unknown | table | ktparkpublic | var_pop | table | ktparkpublic | var_samp | table | ktparkpublic | varying | table | ktparkpublic | zone | table | ktparkpublic | bigint | table | ktparkpublic | bit | table | ktparkpublic | blob | table | ktparkpublic | bool | table | ktparkpublic | boolean | table | ktparkpublic | bytea | table | ktparkpublic | char | table | ktparkpublic | date | table | ktparkpublic | decimal | table | ktparkpublic | double | table | ktparkpublic | float | table | ktparkpublic | float4 | table | ktparkpublic | float8 | table | ktparkpublic | inet4 | table | ktparkpublic | int | table | ktparkpublic | int1 | table | ktparkpublic | int2 | table | ktparkpublic | int4 | table | ktparkpublic | int8 | table | ktparkpublic | integer | table | ktparkpublic | nchar | table | ktparkpublic | numeric | table | ktparkpublic | nvarchar | table | ktparkpublic | real | table | ktparkpublic | smallint | table | ktparkpublic | text | table | ktparkpublic | time | table | ktparkpublic | timestamp | table | ktparkpublic | timestamptz | table | ktparkpublic | timetz | table | ktparkpublic | tinyint | table | ktparkpublic | varbinary | table | ktparkpublic | varbit | table | ktparkpublic | varchar | table | ktpark{noformat},2985
Rename Method,Rewrite the projection part of logical planning The projection part of LogicalPlanner was designed long time ago. It has evolved to support many SQL expressions. However due to its rough design it is hard to be improved for further SQL expressions and it causes many bugs.The current logical planner has the following problems:* other expressions except for column can be used in group-by clause.** TAJO-422* other expressions except for column can not be used in order-by clause.** TAJO-444* An expression including some aggregation function must be evaluated in group-by executor.** As a result some aggregation operator like HashAggregateExec has to keep all intermediate results of a complex expression in a hash table.** It also causes frequent GC and large memory consumption.The too high code complexity also causes many bugs like* TAJO-434 - java.lang.NullPointerException for invalid column name* TAJO-428 - CASE WHEN IS NULL condition is a problem using LEFT OUTER JOIN* TAJO-463 - ProjectionPushDownRule incorrectly rewrite the output schema of StoreTableNode* TAJO-443 - Order by query gives NullPointerException at at org.apache.tajo.catalog.Schema.getColumnId(Schema.java:142)The major reason of this problem is as follows:* TargetListManager keeps only the final target list.** SELECT col1 sum(col2) as col2 ... <- the final target list* TargetListManager deals with each expression described in a target list or other clauses like group-by clause as a singleton expression.The main objective of this issue is to rewrite the projection part of logical planning in order to those problems.For 2 weeks I've rewritten this part. I'll submit the patch soon.,2986
Rename Method,InsertNode and CreateTableNode should play their roles Currently CreateTableNode and InsertNode are just intermediate representations. They are rewritten to StoreTableNode. But StoreTableNode does not contain some necessary fields such as output table target table target columns overwrite flag and create table flag. So far these fields are kept in QueryContext.This implementation causes unnecessary and complex rewrite of DistributedQueryHookManager. As a result it is hard to maintain and manage CREATE/INSERT plans.The main objective of this issue is to improve LogicalPlanner to use CreateTableNode and InsertNode throughout the planning phase and eliminate complex rewrite in DistributedQueryHookManager.,2987
Rename Method,Add a method to the TajoClient to get finished query lists The current TajoClient only provides a method for retrieving lists of running queries. However users may want to see the queries which are already finished failed or even not started. (some queries are not started immediately after TAJO-540.),2988
Rename Method,Improve distributed merge sort In Tajo sort operator is similar to merge sort and it works in a distributed manner. The first sort phase sorts each fragment in local machine the intermediate data are shuffled in range partition and then the second sort phase in each node sorts the range-partitioned data.However the second sort phase reads all shuffled data via one scanner. It misses the opportunity to exploit already-sorted data. This patch improves the second sort phase to merge directly multiple already-sorted intermediate data sets. It significantly reduces the response time of sort queries.I carried out some simple benchmark with the following query on TPC-H 100GB data sets:{code:sql}select l_orderkey from lineitem order by l_orderkey;{code}The lineitem table occupies 75GB. The query response time are dramatically reduced from 480 to 260 secs. This patch exploits the design of TAJO-36. So this patch requires TAJO-36.,2989
Extract Method,HashJoin or HashAggregation is too slow if there is many unique keys HashJoin or HashAggregation is too slow if there is many unique keys.Java's native Map is inefficient to handle many items. In case more than 1 million items in HashMap Adding 10000 items takes more than 7 ~ 10 seconds. This should be improved.,2991
Extract Method,Refactor GlobalEngine to handle DDL statements In the current implementation the codes to handle DDL statements like CREATE TABLE and DROP TABLE are distributed across ClientServiceHandler and GlobalEngine. This patch will move the codes to GlobalEngine and clean up them.,2992
Extract Method,Improve file splitting for large number of splits In currently The storageManager invoke the getFileBlockStorageLocations() per input path it occurred too many rpc to the associated datanodes* reducing remote call to datanode,2993
Rename Method,Supports expressions in 'IN predicate' Tajo does not support expression IN statement.{noformat}tpch100> select * from nation where n_nationkey in (1 1+1 1+2);ERROR: extraneous input '+' expecting {'' ')'}LINE 1:47 select * from nation where n_nationkey in (1 1+1 1+2){noformat},2995
Extract Method,Support executing LINUX shell command and HDFS command. See the title. {noformat}\! [COMMAND]: executes command in TAJO shell\dfs [COMMAND]: executes a dfs command in TAJO shell default>\! ls -altotal 208drwxr-xr-x 27 babokim staff 918 4 2 17:34 .drwxr-xr-x 95 babokim staff 3230 4 2 17:28 ..drwxr-xr-x 13 babokim staff 442 4 2 17:41 .git-rw-r--r-- 1 babokim staff 144 4 2 17:29 .gitignoredrwxr-xr-x 12 babokim staff 408 4 2 17:41 .idea-rw-r--r-- 1 babokim staff 2117 4 2 17:29 BUILDING.txt-rw-r--r-- 1 babokim staff 34170 4 2 17:29 CHANGES.txt-rw-r--r-- 1 babokim staff 17172 4 2 17:29 LICENSE.txt-rw-r--r-- 1 babokim staff 396 4 2 17:29 NOTICE.txt-rw-r--r-- 1 babokim staff 2095 4 2 17:29 READMEdrwxr-xr-x 4 babokim staff 136 4 2 17:29 dev-supportdefault>\dfs -ls /tajo/warehouseFound 93 itemsdrwxr-xr-x - tajo supergroup 0 2014-03-25 17:45 /tajo/warehouse/defaultdrwxr-xr-x - tajo supergroup 0 2014-03-04 10:06 /tajo/warehouse/lineitem_100_gzipdrwxr-xr-x - tajo supergroup 0 2014-03-22 14:01 /tajo/warehouse/lineitem_100_pdrwxr-xr-x - tajo supergroup 0 2014-03-24 14:22 /tajo/warehouse/lineitem_100_rcdrwxr-xr-x - tajo supergroup 0 2014-03-19 02:07 /tajo/warehouse/lineitem_tmp{noformat} ,2996
Rename Method,"APIs in TajoClient and JDBC should be case sensitive. Identifiers that APIs in TajoClient and Tajo JDBC driver take are normalized in client API side. So identifiers composed of upper and lower mixed characters should be used with double quote (""). This convention is different from existing JDBC driver's convention. It makes ugly code when users use TajoClient.This patch changes their APIs to be case sensitive. In addition this patch adds some missing JDBC APIs and fixes wrong behaviors of {{getTables()}} and {{getColumns()}} in JDBC APIs.",2997
Move Method,Implements function COALESCE See title.The next description is from the postgresql document(http://www.postgresql.org/docs/9.1/static/functions-conditional.html). {noformat}COALESCE(value [ ...]){noformat}The COALESCE function returns the first of its arguments that is not null. Null is returned only if all arguments are null. It is often used to substitute a default value for null values when data is retrieved for display for example:{code:sql}SELECT COALESCE(description short_description '(none)') ...{code}Like a CASE expression COALESCE only evaluates the arguments that are needed to determine the result; that is arguments to the right of the first non-null argument are not evaluated. This SQL-standard function provides capabilities similar to NVL and IFNULL which are used in some other database systems.,2998
Extract Method,A minor improvements for HCatalogStore A minor code improvements for HCatalogStore,3000
Rename Method,Multiple distinct should be supported. Currently the following query is not supported.{code}default> select id count(distinct age) count(distinct name) from table2 group by id;ERROR: different DISTINCT columns are not supported yet: age name{code},3001
Extract Method,add simple fifo scheduler support Currently we don't support query scheduling.Hyunsik and Min Zhou started to implement the tajo scheduler on TAJO-540.It should be a big changes and there are many challenge.This issue will be a temporary solution before TAJO-540,3003
Extract Method,NULL handling in JDBC. Currently JDBC doesn't handle NULL value only wasnull() is provided. JDBC handles NULL data like the following rule.- Text type: return java null.- Int Float type: return 0- Date/Time type: return null. - Boolean: return false - getObject(): return java null,3004
Extract Method,JDBC should be support getTime getDate and getTimestamp. Currently Tajo JDBC not support getTime getTimestamp. And after TAJO-825 getDate() return wrong result.,3005
Extract Method,Supporting MariaDB-based Store which is compatible with MySQL. There exists an open-source database MariaDB (http://mariadb.org) which can be replacement for MySQL; Since the interface of MariaDB is perfectly compatible to that of MySQL it is easy to construct CatalogStore to use MariaDB in Tajo.Therefore I hereby suggest MariaDBStore by duplicating MySQLStore.,3006
Extract Method,Refactoring FilterPushDown for OUTER JOIN Currently Tajo doesn't support a filter OUTER JOIN's ON clause.or has some bugs. There is some rules for this in the following urls.- http://www.ibm.com/developerworks/data/library/techarticle/purcell/0112purcell.html- https://cwiki.apache.org/confluence/display/Hive/OuterJoinBehaviorBriefly summarized as follows.- Join Predicate on Preserved Row Table: Used for join condition(not filter)- Join Predicate on Null Supplying Table: Can push down to the table scan- Where Predicate on Preserved Row Table: Can push down to the table scan- Where Predicate on Null Supplying Table: Used for filter with join result data. This filter condition is attached to SELECTION Node.,3007
Extract Method,Supports INSERT INTO with UNION Currently INSERT INTO with UNION occurs the following exception.{noformat}2014-05-28 01:40:42105 ERROR: org.apache.tajo.master.GlobalEngine (executeQuery(152)) - Stack Trace:java.lang.RuntimeException: Wrong child node type: UNION for insertat org.apache.tajo.engine.planner.LogicalPlanner.buildProjectedInsert(LogicalPlanner.java:1269)at org.apache.tajo.engine.planner.LogicalPlanner.buildInsertIntoTablePlan(LogicalPlanner.java:1234)at org.apache.tajo.engine.planner.LogicalPlanner.visitInsert(LogicalPlanner.java:1141)at org.apache.tajo.engine.planner.LogicalPlanner.visitInsert(LogicalPlanner.java:59)at org.apache.tajo.engine.planner.BaseAlgebraVisitor.visit(BaseAlgebraVisitor.java:123)at org.apache.tajo.engine.planner.LogicalPlanner.createPlan(LogicalPlanner.java:122)at org.apache.tajo.engine.planner.LogicalPlanner.createPlan(LogicalPlanner.java:109)at org.apache.tajo.master.GlobalEngine.createLogicalPlan(GlobalEngine.java:475)at org.apache.tajo.master.GlobalEngine.executeQuery(GlobalEngine.java:147)at org.apache.tajo.master.TajoMasterClientService$TajoMasterClientProtocolServiceHandler.submitQuery(TajoMasterClientService.java:261)at org.apache.tajo.ipc.TajoMasterClientProtocol$TajoMasterClientProtocolService$2.callBlockingMethod(TajoMasterClientProtocol.java:495){noformat},3008
Rename Method,Implements TRUNCATE table. Tajo should support TRUNCATE table feature.{noformat}TRUNCATE [TABLE] <table name1> [ <table name2> ...]{noformat},3009
Rename Method,ConstEval should not be included in target list of projectable nodes In some applications aliased constant values can be used in WHERE GROUP BY HAVING ORDER BY clauses. For those cases current planner evaluates constant value in target list of projectable nodes.{code}SELECT '1994' end as year ... FROM lineitem WHERE group by year;{code}This approach works well so far but there are rooms for significant improvement.The main problem is that constant target requires many workaround code in NamedExprManager and TargetListManager. As a result it makes code complexity higher. The second problem is that many constant values evaluated in each row consume unnecessary I/O and network bandwidth for storing and transmitting.The solution seems to be simple. In logical planning phase we should rewrite column references which actually indicates constant values.,3010
Move Method,Some left outer join cases are not optimized as the broadcast join. The next query has three small tables and expected broadcast join but not. {code:sql}select count(*) from large1 left outer join large2 on large1_id = large2_idleft outer join small1 on large1_id = small1_idleft outer join small2 on large1_id = small2_idleft outer join small3 on large1_id = small3_id{code}The next is the upper query's plan.{noformat}|-eb_1404411535695_0000_000011|-eb_1404411535695_0000_000010|-eb_1404411535695_0000_000009 (join)|-eb_1404411535695_0000_000008 (small)|-eb_1404411535695_0000_000007 (join)|-eb_1404411535695_0000_000006 (small)|-eb_1404411535695_0000_000005 (join)|-eb_1404411535695_0000_000004 (small)|-eb_1404411535695_0000_000003 (join)|-eb_1404411535695_0000_000002 (large)|-eb_1404411535695_0000_000001 (large){noformat}Optimized plan should be the next.{noformat}|-eb_1404411906426_0000_000005|-eb_1404411906426_0000_000004|-eb_1404411906426_0000_000003 (broadcast small1 small2 small3)|-eb_1404411906426_0000_000002 (large)|-eb_1404411906426_0000_000001 (large){noformat},3011
Extract Method,Simple query (non-forwarded query) should be supported against partition tables. There are two types of queries according to whether a query is executed across cluster nodes or not.We call a query which is executed across cluster nodes *forwarded query* meaning that TajoMaster forwards the query to a query master. In contrast we call a query without distributed execution *simple query* or *non-forwarded query* which executed in only client side. The following query is an example of simple query.{code}select * from table limit 10;{code}Currently simple query is only supported against a normal table. We also should support it against partitioned tables.,3012
Extract Method,Cleanup of child blocks after parent execution block is complete Child execution block working directory is not deleted until the query is complete. there are too many files in directory. we should delete the child execution block,3014
Extract Method,"When Autobuilding a class the constructor to be used should be identified in the trace output The operation trace is useful for tracking down IoC injection problems but in this case it's slightly sketchy (I've elided some class names):* Autobuilding instance of ....SecurityFilter* Determining injection value of parameter #4 (....PageTitleExtractor)* Resolving object of type ....PageTitleExtractor using MasterObjectProviderI think there should be a line between #1 and #2 saying ""Using constructor ...PageTitleExtract(....) and listing the parameter types",3016
Extract Method,"When Tapestry is loading templates or other files on case-insensitive OSs (Windows) it should trigger an error if the file name case is incorrect (which will result in a runtime failure on case-sensitive OSs such as Linux) In other words on Windows you might find file ""myComponent.tml"" when it should be named ""MyComponent.tml"" (to match the name of the class MyComponent). This is irritating to find in testing or production and Tapestry should be able to add a check that the case of the file name does not match the expected case.",3017
Extract Method,"Live class reloading for service implementations It should be possible to create a class loader for (each) service implementation that can reload the service when its underlying class changes.Once could imagine this as a special proxy; possibly this would require a particular service scope (""reloadable"").Periodically a check could occur to let the proxies see if the underlying service implementation class file has changed and if so create a new class loader to load that specific class much as tapestry-core does with component.This would involve moving some number of services from tapestry-core to tapestry-ioc and there are implications related to some public services.I think it would be too much to support reloadable modules just service implementations. Therefore services that are constructed via a builder method would not be reloadable.",3018
Extract Method,Validator Macros: Combine multiple common validators into a single term In some cases a particular type of field may have a series of independent validations that are used consistently as a group. It would be nice if there was a way to specify a single value and bring in a group of validators.,3020
Rename Method,Rewrite live reload integration tests to use new SeleniumTestCase instead of deprecated AbstractIntegrationTestSuite 0,3022
Rename Method,Add support for startup() methods in modules as an easy way to add startup logic There's a mechanism for adding startup logic by contributing to the RegistryStartup service configuration.It would be nice if there was an optimized way to accomplish the same thing i.e. starutp() methods (with injected parameters) as an easier way to accomplish the same thing.,3023
Extract Method,"Change Tapestry client-side JavaScript to make the tapx/Confirm component easier to implement The tapx/Confirm component needs to ""hook into"" links and submit components in order to hook up the confirmation. As currently implemented this requires too much internal knowledge of the tapestry.js internals.The approach I've been taking is to replace a simple ""click"" event handler into two parts: a ""click"" event handler that cancels the event and fires a ""tapestry:action"" event and a handler for the ""tapestry:action"" event. Using this tapx/Confirm can override the default ""click"" event handler but get back to the original logic after confirmation by firing the ""tapestry:action' event.",3024
Rename Method,The application global message catalog should be injectable into services It would be nice if services could see the global message catalog.,3025
Extract Method,New annotation: @HeartbeatDeferred to mark component methods that should execute at the end of the current Heartbeat So basically instead of creating a Runnable injecting the Heartbeat environmental and passing the runnable to the defer() method you just slap this annotation on a method.,3027
Rename Method,"Define new namespace p: for block parameters Towards greater conciseness:How about making these equivalent:<t:beaneditform t:id=""new"" object=""newPosting"" submitlabel=""message:post"" include=""titlecontent""><t:parameter name=""content""><t:label for=""content""/><br/><t:richtextarea t:id=""content"" rows=""10"" cols=""80"" value=""newPosting.content""/></t:parameter></t:beaneditform>-and-<t:beaneditform t:id=""new"" object=""newPosting"" submitlabel=""message:post"" include=""titlecontent""><p:content><t:label for=""content""/><br/><t:richtextarea t:id=""content"" rows=""10"" cols=""80"" value=""newPosting.content""/></p:content>Obviuosly p: would need to be mapped to a Tapestry namespace.",3028
Rename Method,When in development mode Tapestry should pretty-print JSON content Reading giant blobs of JSON can be a challenge to read given how deeply Tapestry tends to nest everything.,3030
Extract Method,Make better use of the OperationTracker to identify what's going on during a request (and especially during page construction) 0,3032
Rename Method,Eliminate page pooling using shared page instances that separate their structure from the mutable state This has been suggested before but the recent changes to class transformation API makes it much more reasonable to accomplish.The goal here is to identify all transient or mutable state in the page and store it via the PerThreadManager. This will be invisible to user code; the pages will appear to be individual instances with internal state ... but in fact it will be a single instance (per locale) with all internal mutable state stored elsewhere.Because this changes the semantics of some aspects of the component class transformation pipeline there will be a compatibility mode that will allow pages to be pooled as with 5.1 while any third party libraries that contribute workers update.Why do all this? For large applications with very complex pages this will be a big win as Tapestry has been shown to strain the limits of available JVM heap (surprising to me but apparently true) once you have a few dozen (or hundred) page instances (of each page type) floating around.,3039
Rename Method,BeanBlockContribution should be split into two sub-classes: EditBlockContribution and DisplayBlockContribution 0,3046
Rename Method,Define a special CSS class to prevent a client-side form from submitting normally (for Ajax use cases) Just occured to me during class; right now there's some logic about getting the FormManager for the form and I'm hoping to minimize that kind of thing now before locking down the Javascript APIs in 5.3.,3047
Extract Method,"Additional method for Link: addParameterValue(StringObject) that uses ContextPathEncoder to encode object value to a string This would make adding parameters to a link more consistent with setting the values in the page activation context ... you give the ""raw"" values and Tapestry figures out how to encode it into the URL.",3048
Extract Method,Improve page testing facilities Currently the PageTester is returning a Document instance. This makes it very difficult to assert response-related functionality like sending a redirect or setting a HTTP header. Implement new methods for PageTester returning TestableResponse instance. Also implement various methods for TestableResponse which throw nyi exception.,3050
Extract Method,Provide convenient methods for Element and Document to find elements 0,3051
Extract Method,Component report should accept multiple root packages It is possible to have multiple root packages inside a JAR file but it is only possible to generate the component reference for a single root package. The component report should accept multiple root packages. <configuration><rootPackages><rootPackage>foo.bar.baz</rootPackage><rootPackage>org.example</rootPackage></rootPackages></configuration>,3052
Rename Method,"When contributing to a service configuration values should be coerced to the correct type rather than rejected if not the correct type This will make it much easier to migrate the behavior of services as long as contributions of the old type can be coerced to contributions of the new type.It may also make it easier to contribute symbol constants as it will be possible to pass true not ""true"" or literal numbers.",3053
Extract Method,Add Zone parameter to Select component Add AJAX ability to selection in a Select component to allow the classic chaining of Select components.Eg. for filtering car advertisements: 3 Select components - Brand Make Model. Choosing a Brand causes the Make to be enabled showing the possible makes. Similarly choosing a Make causes the Model to be enabled showing the possible models.,3054
Extract Method,Provide support for JSR-330 It should be possible to use JSR-330 annotations for the injection points.,3055
Rename Method,@Autobuild annotation for parameters implicitly invokes ObjectLocator.autobuild() Rather than injecting the ObjectLocator to invoke autobuild() on it it would be nice if we could inject the RESULT of ObjectLocator.autobuild().,3056
Extract Method,Should be possible to perform an Ajax request without linking the component (link or form) to a Zone To specify that you want to perform an XHR request (EventLink ActionLink Form etc) you need to supply the zone parameter. The existence of this parameter is a flag that tells the component to use XHR. In some cases this may be useful (although I'm yet to find one). This strikes me as bad design since there is not necessarily a known One-to-One relationship between the event and the zone(s) updated.If you return a Zone or MultiZoneUpdate from your event handler the actual zone you supply the requesting component is irrelevant.Since all of my XHR event handlers return a MultiZoneUpdate I ended up creating a dummy zone on every page and component and supplying that to every zone parameter. The dummy zone was always hidden and never actually updated. This hack made it easier to code and maintain my pages because without it i would need to search for an arbitrary zone on each page when creating callbacks.The solution would be to add a 'xhr' parameter to the components (EventLink Form etc) and for the zone parameter to be an optional if xhr is true.You could even hard set xhr=true if zone!=null for backwards compatibility.This would require Tapestry to lose the dependency on zones to create contextual XHR requests which I think is the limiting design decision plaguing this area of tapestry.,3061
Extract Method,Coercion from List to SelectModel should use the SelectModelFactory service It should use SelectModelFactory. Labels are extracted from objects using strategy pattern. Users can contribute providers for their own objects.,3062
Rename Method,Allow for multiple application root packages by contributing additional LibraryMappings with empty virtual folder So the first package is defined in web.xml but addition ones can be defined as ComponentClassResolver contributions.,3063
Extract Method,Stack assets should return 404 in stead of exception if they don't exist Currently when trying to access a stack asset I get the following exception:java.lang.RuntimeException: Invalid path for a stack asset request.This should just be a 404.,3070
Extract Method,"Optimize document scans used by Tapestry.FieldEventManager to not locate the label or icon until actually needed While creating the Field Event Manager we are initializing not only the basic features but also extra information for being use later like label and icon. For getting the icon we need to search for the element in the DOM (using a $) and for the label it is searching the DOM for a specific label which is a very expensive operation in ie7.If we move the initialization of these elements until they are really needed we are saving some client side timing.### Eclipse Workspace Patch 1.0#P tapestry-coreIndex: src/main/resources/org/apache/tapestry5/tapestry.js===================================================================--- src/main/resources/org/apache/tapestry5/tapestry.js (revision 1131061)+++ src/main/resources/org/apache/tapestry5/tapestry.js (working copy)@@ -166713 +16676 @@initialize : function(field) {this.field = $(field);- var id = this.field.id;-- var selector = ""label[for='"" + id + ""']"";-- this.label = this.field.up(""form"").down(selector);- this.icon = $(id + '_icon');-this.translator = Prototype.K;var fem = $(this.field.form).getFormEventManager();@@ -16987 +169124 @@this.validateInput.bindAsEventListener(this));}}+ + getLabel : function() {+ if (!this.label) {+ var id = this.field.id;+ var selector = ""label[for='"" + id + ""']"";+ this.label = this.field.form.down(selector);+ }+ return this.label;+ }+ getIcon : function() {+ if (!this.icon) {+ var id = this.field.id;+ this.icon = $(id + '_icon');+ }+ return this.icon;+ }+/*** Removes validation decorations if present. Hides the ErrorPopup if it* exists.@@ -170611 +171611 @@removeDecorations : function() {this.field.removeClassName(""t-error"");- if (this.label)- this.label.removeClassName(""t-error"");+ if (this.getLabel())+ this.getLabel().removeClassName(""t-error"");- if (this.icon)- this.icon.hide();+ if (this.getIcon())+ this.getIcon().hide();if (this.errorPopup)this.errorPopup.hide();@@ -173012 +174012 @@this.field.addClassName(""t-error"");- if (this.label)- this.label.addClassName(""t-error"");+ if (this.getLabel())+ this.getLabel().addClassName(""t-error"");- if (this.icon) {- if (!this.icon.visible())- new Effect.Appear(this.icon);+ if (this.getIcon()) {+ if (!this.getIcon().visible())+ new Effect.Appear(this.getIcon());}if (this.errorPopup == undefined)",3088
Extract Method,"FormFragment should allow more fine grained control over when to be considered ""invisible"" The 5.2 line of Tapestry introduced the ""alwaysSubmit"" parameter to form fragment. This is nice because it allows the fragment to be submitted even if hidden. However it doesn't cover all use cases. Consider a situation like:<form><div id=""tab1"">...<t:formfragment ...><t:textfield validate=""required"".../></t:formfragment></div><div id=""tab2"">...<t:formfragment ...><t:textfield validate=""required"".../></t:formfragment></div><t:submit/></form>User reveals tab 1 then reveals the form fragment on tab1 and makes changes. Now user reveals tab2. Note that the fragment on tab1 is still revealed in the context of tab1 but the entire tab1 is hidden. There is currently no way to make it so that ""submit"" will submit the information from the formfragment in both tabs and behave correctly in all situations. I will enumerate. Some definitions for clarity:fragmentX is the fragment on tabX. fragmentX visibility refers to the state of the actual fragment rather than the state of the containing tab. So if fragment1 is visible it means it's visible when tab1 is active... and I am considering it visible when tab2 is active even though the entire tab1 is invisible.1) If ""alwaysSubmit"" is false and fragment1 is invisible you will get the correct behavior regardless of tab1/tab2 visibility2) If ""alwaysSubmit"" is false and fragment1 is visible you will get the correct behavior iff tab1 is active. If tab2 is active fragment1's fields will not be submitted.3) If ""alwaysSubmit"" is true and fragment1 is invisible you will get incorrect behavior (well technically it's ""correct"": the information will be submitted as per alwaysSubmit but this is a case where you don't actually /want/ the information submitted if the fragment isn't visible)4) If ""alwaysSubmit"" is true and fragment is visible you will get correct behavior.You can conditionally ""alwaysSubmit"": alwaysSubmit on the same condition for visibility as the ""visible"" trigger. The problem here comes in the following scenario:User opens a page with fragment1 initially visible but no data yet in the required field. User marks fragment1 as invisible. User submits the form. The submission will fail because ""alwaysSubmit"" was true at the time the form rendered.The culprit behind this is Tapestry's ""isDeepVisible"" method. It searches for visibility up to the point where it finds a form element. But in the case above the form element contains the tab divs so the fragment is determined to be invisible and the data not submitted for the inactive tab even if the user clicked on the trigger to make the fragment visible while the tab was active.This is something of an edge case but I think it can be handled cleanly by introducing a new parameter to formfragment such as ""visiblebound"" (but better named!). The idea is to allow developers to specify an element or selector expression that bounds the search for visibility. The default would be the containing form element which would preserve the current behavior.",3090
Rename Method,Excessive warnings Tracking issue for cleaning up compiler warnings.,3091
Rename Method,"Add a @QueryParameter annotation for parameters to event handler method It would be nice is some cases to have Tapestry map query parameters to event handler method parameters rather than path info. This is typically about the Ajax case where it is more reliable (and easier) to take a URL and add query parameters to it than it is to add extra path info.public void onActionFromAjaxWidget(@QueryParameter(""action"") String widgetAction @QueryParameter(""count"") int count) { .... }",3092
Inline Method,Enable OperationTracker to produce debug trace of all operations The OperationTracker is great at identifying what leads up to an error but it only keeps the nested operations leading up to a particular failure; it would be nice if it could emit debug logging for every operation including timing for the operation as a way to identify what's going on leading up to an error but not tracked in the specific OperationTracker trace.,3095
Rename Method,"Components which use PrimaryKeyEncoder should be changed to use ValueEncoder and PrimaryKeyEncoder should be deprecated While working on an application I noticed that my objects were being serialized ""weird"" into a form by the loop component. I realized that I hadn't provided the primary key encoder and once I did things worked as expected. That got me to thinking that it would be nice if the Loop component and other components that rely on PrimaryKeyEncoders could check to see if there is an encoder available for the value-type if none is explicitly bound by the user. That way module-authors could provide PrimaryKeyEncoders that makes things work ""like magic"". For example tapestry-hibernate could contribute PrimaryKeyEncoders for each entity type so that the objects are automatically and properly encoded into forms.",3096
Extract Method,"Tapestry could create non-singleton services more efficiently For services that may be created again on each request Tapestry does a lot of extra work to analyze the class constructor and fields. It should be able to roll up all this information into a repeatable ""plan"" that can simply be re-executed on each subsequent instance creation.",3098
Extract Method,"Tapestry-beanvalidator isn't validating nested DTO objects / Doesn't mark the invalid fields in the UI Tapestry-beanvalidator isn't validating nested objects correctly the problem is that component parameters i.e. the TextField#value is only bound to the attribute itself but doesn't contain the full objectpath so using a DTO like:class TestDTO {@NotNullprivate String firstName;@Validprivate EmbeddedObject embeddedObject;...}class EmbeddedObject {@NotNullprivate String lastName;...}Using the TestDTO on a page in the following way:class MyPage{...@Propertyprivate TestDTO testDTO;@Component(parameters = {""validate=testDTO""})private Form form;@Component(parameters = {""value=testDTO.firstName""})private TextField firstName;@Component(parameters = {""value=testDTO.embeddedObject.lastName""})private TextField embeddedField;...}Submitting the form validates all the attributes correctly (also the embedded object validation error is listed in the t:errors component) but when it comes to BeanFieldValidator only lastName is on the Environment Stack in the BeanValidationContext. As TestDTO doesn't contain the property lastName and there's no objectpath available it doesn't traverse down the objects and can't assign the correct validation error.The outcome is that even the property is validated correctly there's no red-frame (css error class) on the appropriate field in the UI.For reference: http://tapestry.1045711.n5.nabble.com/tapestry-beanvalidation-td4921787.html",3101
Rename Method,Component fields should not need to be private merely non-public Currently Plastic assets early that all instance fields are private. Instead it should check fields as transformations are applied to them and ensure that they are merely non-public. Access to the fields from other classes (including inner classes) must be routed through access methods.Inner classes will now need a limited set of transformations to handle the cases where a protected or package private field is directly accessed in which case the appropriate accessor method will be used instead.It seems possible that two transformed classes that each access the other's non-public fields might cause an endless loop; if so this should be identified and reported.,3102
Rename Method,KaptchaField should have a parameter to allow it to operate as a visible text field rather than a password field Most captcha implementations are based on input type=text instead of input type=password. It would be nice to allow for both.,3103
Extract Method,Merge functionality of Tynamo.org's tapestry-exceptionpage module with the built-in ExceptionHandler As discussed on the dev list: http://markmail.org/search/?q=Bringing+Tynamo%27s+tapestry-exceptionpage+into+the+core#query:Bringing%20Tynamo%27s%20tapestry-exceptionpage%20into%20the%20core+page:1+mid:wocbkhyqqe7a2tm7+state:results,3104
Rename Method,Improve error reporting when a javascript asset is intended to be included on page which has no <html> element I know - silly of me to have a page with no <body></body> element. I spent an entire day trying to figure out why the javascript wasn't getting included.It was my error but I can see other people making the same mistake...a little error-reporting would be nice.,3105
Rename Method,Add a SubmitMode (for Submit and LinkSubmit components) for unconditionally submitting the form As described in TAP5-1856 there's a functionality gap that prevents a button from submitting a form but still allowing server-side processing to occur (such as having the Submit component fire its selected event).,3106
Extract Method,Rendering components in Alerts We've just upgraded to 5.3.2 and checked out some of the new components.The Alerts seems pretty useful but looks like it's only possible to pass on strings.I'm thinking a pretty common use-case would be to render some components/markup in it - like links.I experimented a little by manually rendering a block and pass that on to alertManager. Got some inspiration from this thread about rendering blocks - http://tapestry.1045711.n5.nabble.com/Howto-render-a-block-and-put-it-into-a-JSON-reply-td5486823.html--------RenderCommand renderCommand = (RenderCommand)alertBlock;MarkupWriter markupWriter = new MarkupWriterImpl();RenderQueueImpl renderQueue = new RenderQueueImpl(log);renderQueue.push(renderCommand);renderQueue.run(markupWriter);alertManager.info(markupWriter.toString());--------That seems to work but it's a bit clumsy and don't know if it's the recommended approach for rendering blocks. Does a convenience method exist for rendering blocks/components?Not sure if it's possible but how about if one could pass blocks to alerts directly. That could be pretty flexible as well...######REPLY FROM KALLE######Certainly the recommended approach is to use the provided renderqueue rather than create your own - but obviously the currentimplementation doesn't always allow to do so. Completely agree withyou that rendering links and in general rendering blocks would bevery useful for alerts. I don't see any major issue why it couldn't besupported. Please open an issue for it.,3107
Rename Method,Typo in interface LocalizationSetter.setNonPeristentLocaleFromLocaleName() The method is missing an 's' in Peristent.,3109
Extract Method,"Copy annotations from service implementation to proxy (was JPA annotations expose implementation details in service interfaces) The commit after and persistence context annotations are required on the service interface definition thereby exposing internal implementation details (see below example from docs). Details of implementation should be hidden at the interface level both these annotations break the rule.Perhaps this code could appear in the Impl classes or be provided in configuration somehow?public interface UserDAO {@CommitAfter@PersistenceContext(unitName = ""DemoUnit"")void add(User user);",3110
Rename Method,"Support multiple @PageActivationContext The @PageActivationContext annotation (and PageActivationContextWorker) could be improved to accept an ""index"" parameter. This way I could have multiple @PageActivationContext properties.eg{code}public class MyPage {@PageActivationContext(index=0)private Category category;@PageActivationContext(index=1)private Item item;...}{code}I'd expect tapestry to generate the following URL's:- /mypage (category and item is null)- /mypage/category1 (item is null)- /mypage/$N/item1 (category is null)- /mypage/category1/item1 ",3113
Extract Method,It should be possible to control whether a given JavaScript library is minimized or not The current closure compiler version fails to minimize angular.js (https://github.com/angular/angular.js/issues/1304). A WroRuntimeException is thrown from GoogleClosureCompressorProcessor which is not handled so the asset is not sent to the client at all.When an asset cannot be minimized for whatever reason Tapestry should just send the original asset.,3114
Extract Method,Add support for distributed documentation Please add support for a distributed documentation system. The basic requirements are:1. Access to a list of Pages/Compoents/Mixins. (ComponentClassResolver supports pages)2. Access to a Map of all Configurations. The map would have the configuration class as the Key and contain an object such a list or map that contains the configuration.3. Access to a list of configured services.From this it should be possible to build documentation of a running system. ThanksBarry,3116
Extract Method,Make tapestry5 java8 compatible As it stands Tapestry does not work on java8. This seems to be caused by ASM. Updating the enclosed asm to 5.0_Bet seems to do the trick.,3117
Rename Method,When you have multiple forms on the same page that share (some of) the same properties it is not possible to differentiate validation constraints and messages in the message catalog For example if you have both a login form and a registration form that both collect a userId property from the user then:userid-required=You must provide the same user id you supplied when registerring.Will be applied to both the login form (correct) and the registration form (inaccurate and very confusing).Extra differentiation is needed:login-userid-required=You must provide ...,3118
Rename Method,The Hibernate ENTITY session persistent strategy should store transient entities as-is The ENTITY session persistence strategy does the right thing for persistent entities; however when first creating an entity it is often useful to store the transient entity in the session. In this case the entity itself should be stored not the entity type and entity PK.,3119
Rename Method,Unify injection; allow @Inject annotation on fields of service implementations This has come up again and again especially in recent training. Although I'm very strongly in favor of constructor injection (and the use of final fields to store dependencies) being allowed to inject into a field even if it requires the use of reflection would be a real boon.,3120
Extract Method,Decrease number of operations with HashMap #2 During profiling of tapestry framework I found that HashMap is actively used in following code:{code:title=NamedSet.java}public void eachValue(Worker<T> worker){F.flow(getValues()).each(worker);}{code}Here HashSet (which internally uses HashMap) is created inside getValues() only to iterate over it.I changed code to use ArrayList instead of HashSet.With following patch time per request decreased on 3.5 ms (7.4% of overall time). Measurements were done with apache benchmark on a real application after warm up phase.,3121
Rename Method,"Prevent interaction with page until fully loaded With the emphasis on JavaScript there is an issue where a user on a slow connection interacts with the page before the page has fully loaded its JavaScript and run initializations. This can lead to client-side JavaScript exceptions or server-side failures (when ordinary requests are sent to URLs that expect an Ajax/XHR request).The right solution is for Tapestry to provide a ""pageloading mask"" a div that masks the entire page from user input (and provides a loading image) until page initializations complete.The implementaton of this uses a <script> tag with document.write to introduce the mask element at the top of the page (so that non-JavaScript-enabled clients will be able to interact with the page to some degree).CSS animations are used to fade in the mask after a delay. The presentation of the mask can be modified via CSS overrides. By default it is black with 50% opacity.",3122
Extract Method,"Provide access to annotations of service implementation class In some situations it would be useful to have direct access to annotations of service implementation class. This would allow us during registry startup detect services with some specific class or method level annotations and take related actions. For instance imagine tapestry-quartz integration based on simple declarativemechanism where it would be possible to use something like this:public class MyServiceImpl implements MyService {@Scheduled(cronExpression=""0/5 * * * * ?"")public void myMethod() {...}}and framework would be able during registry startup automatically detect all service methods annotated by @Scheduled annotation and register them in the scheduler.I see two possible solutions:1. Modify ServiceDef to hold information about service implementation class.2. Service proxy could inherit all annotations from service implementationclass then we would be able to check annotations directly on service proxy.But maybe there is another more elegant solution.For more details see thread:http://thread.gmane.org/gmane.comp.java.tapestry.user/67116/focus=67116",3124
Extract Method,The Cookies service interface could be simplified using a builder pattern There's half a dozen different variations that could all be simplified using the builder pattern where you create a CookieBuilder() chain a sequence of method calls on it then invoke a commit() method to actually create and add the Cookie to the request.,3125
Extract Method,Improve component reports by providing links to javadocs of tapestry classes This can inspect the text content added in (for instance) org.apache.tapestry.mojo.ComponentReport#addChildand include links to the api docs.,3126
Move Method,tapestry-hibernate should be split into two parts: tapestry-hibernate-core and tapestry-hibernate with tapestry-hibernate-core being usable outside of a Tapestry web application People often want to use the facilities of tapestry-hibernate in a batch/non-web application. It's currently a bit difficult but could easily be simplified by splitting tapestry-hibernate in two.,3127
Rename Method,Allow component libraries to contribute extra resources to the global application catalog Components in libraries have the same desire to share common message strings that components in an application do.My earlier thoughts were to define a way to have a lib.properties for a component library.However my current thinking is to extend ComponentMessagesSource (the source for the application message catalog as well) so that component libraries can contribute Resources for message catalogs into the global message catalog (typically before the app.properties file so that an application has the ability to override specific component library messages).,3128
Extract Method,Annotation for services to indicate that the service should not be decorated It is often nice to apply decorators to wide ranges of services using regular expression or glob matching. However frequently this results in problems when the services being decorated are dependent on services that are affected by the decoration. For example if you decorate many of the services related to MasterObjectProvider you will likely get a cycle dependency exception.It would be nice if there was an annotation that indicates that the service should be ignored for decoration much as the limited number of services provided by TapestryIOCModule are.,3129
Rename Method,Tapestry should verify that all public methods of a module class are meaningful to Tapestry (build decorate contribute or bind) other methods should cause an exception to be thrown as a likely typo in the name 0,3130
Inline Method,Assets should no longer attempt to generate relative URIs With the changes related to localization it is unlikely that assets will ever have a relative URL that is shorter than the absolute URL since both context and classpath assets are stored below the /assets folder (and have additional qualifying folders for version numbers etc.).Using non-relative URIs will also help with some JavaScript issues (as there is no way to currently no public way to ask for the non-relative URI for an asset; this can trip up some dynamically generated JavaScript).,3131
Rename Method,Tracking issue for changes required by com.formos.tapestry:tapestry-template The off-Apache project tapestry-template (which allows Tapestry pages to be used as templates for generating static files or mail content) requires a few tweaks to tapestry-core.The following site will be live soon:http://tapestry.formos.com/nightly/tapestry-template,3132
Rename Method,"Change If and Unless to render thier template element if provided (i.e. when using t:type) as well as informal parameters I use a lot of dummy texts in my template to see what the page will actually look like when rendered. To filter out these fragments I created a ""Dummy"" component whose setupRender method always returns ""false"".I don't know if this is efficient but it would be nice to have something like this already in T5's component library.More efficient would be a special marker XML attribute that the template parser recognizes. Maybe something like a t:test attribute which is works like <t:if test=""..."">...</t:if>. If the contained expression evaluations to false or null the marked element is filtered.<ul><li t:test=""show.item"">conditional item</li><li t:test=""literal:false"">dummy item</li></ul>",3133
Rename Method,ObjectLocator.getService(Class) should be expanded to pass a varargs of marker annotation types I have @SomeMarker annotation:@Marker({@SomeMarker})void SomeService buildSomeService() {...}How get SomeService by his marker (in ObjectLocator I not found like method).method like this: <T> T getService(<? extends Annotation> marker Class<T> interface),3135
Rename Method,In the exception report page JVM system property org.apache.catalina.jsp_classpath should be displayed as a list like other .path value 0,3136
Rename Method,Provide support for URL rewriting Tapestry should provide some way configured via Tapestry-IoC to provide URL rewriting.,3137
Move Method,"Tapestry should have support for IE conditional stylesheets From the mailing list:hiIn my web application are different css for IE6 and IE7.<link rel=""stylesheet"" media=""screen projection""href='${asset:context:css/main.css}' /><!--[if IE 7]><link rel=""stylesheet"" media=""screen projection""href='${asset:context:css/ie7.css}'"" /><![endif]--><!--[if lte IE 6]><link rel=""stylesheet"" media=""screen projection""href='${asset:context:css/ie6.css}'"" /><![endif]-->But IF IE7 and If IE6 are ordinary html comments thats only browserunderstands.But Tapestry does not.Becouse of tapestry wants ${asset:context:css/main.css} to include css filei have no chance to include ie7.css or ie6.css.Seems like something on RenderSupport could support this much as we support media types.",3138
Move Method,Add a service responsible for encoding client data (as gzipp'ed base 64) and decoding that data Several other issues rely on being able to override such a service to control exactly how such data is represented on the client (i.e. as a pointer to data kept on the server or with additional checksum data to prevent tampering).,3139
Extract Method,Add simple PageRenderLinkSource service to allow services to create Links to pages public interface PageRenderLinkSource{Link createPageRenderLink(String pageName);Link createPageRenderLinkWithContext(String pageName Object... context);Link createPageRenderLink(Class pageClass);Link createPageRenderLinkWithContet(Class pageClass Object... context);},3140
Rename Method,"Allow page classes to have a ""Page"" suffix that is not included in the URL I have an application with a lot of read-only pages. For example I have a page that shows a company and I would like a URI such as: /company/1234However if I name the page class ""Company"" then I get a naming clash with the domain object ""Company"". What I would like to do is call the Tapestry 5 class ""CompanyPage"" - after all that is what the class represents and it's certainly how the team refers to that thing internally and with our business (i.e. ""Have you seen the new company page?"").So please could the ComponentClassResolverImpl remove the suffix ""Page"" (if it exists) from the class name when it constructs the logical page name?",3141
Extract Method,Allow blackbird to be disabled in production mode blackbird's use of F2 to show the console is interfering with our application which uses F-keys as hotkeys to access various parts of the application.It should be possible to either completely disable blackbird in production mode (avoiding unnecessary .css and .js downloads) or at least disable the console hotkey.,3142
Extract Method,"Add annotation @Contribute to allow service contributor methods to be arbitrary named Tapestry used to require this naming convention for configuring services:public static Foo buildFoo(...) {...}public static void contrubuteFoo(...) {...}Then it allowed the first convention to be simplified as:public static Foo build(...) {...}It would be nice for the ""contribute..."" methods to allow also simpler naming and use the type of the ""configuration"" parameter to determine the configured service which will also have the same type of parameter.For example:in Tapestry 5.0.5 TapestryModule.java:public ServletApplicationInitializer build(... List<ServletApplicationInitializerFilter> configuration ... )in my AppModule.java Tapestry 5.0.5 requires this naming:public void contributeServletApplicationInitializer(OrderedConfiguration<ServletApplicationInitializerFilter> configuration)Perhaps it could be simplified as:public void contribute(OrderedConfiguration<ServletApplicationInitializerFilter> configuration)If it will not be simplified it would be nice to make the documentation about Tapestry IoC Configurations more clear thatthe naming of the contribute methods is important not the type of configuration parameter.",3143
Rename Method,"JavaScript libraries should be automatically packed/minimalized Tapestry should catch downloads of JavaScript libraries and should ""pack"" the JavaScript ... remove comments and unecessary whitespace. I believe Dojo has a library to do this it may even shorten variable names (!).A smart implementation of this would manage to cache the compressed JS and notice when the uncompressed version changed.",3144
Rename Method,"It is too much work to hide all T5 pages inside a virtual folder for use in mixed-implementation deployments In a mixed-implementation deployment (mixing Tapestry 5 with Tapestry 4 or some other framework) it would be nice if the T5 apps could be ""hidden"" in a virtual /t5 folder.This is doable but awkward and ugly today.Ideally this would be a matter of changing the web.xml mapping to:<url-filter>/t5/*</url-filter>and making some form of configuration change i.e.configuration.add(ConfigurationConstants.TAPESTRY_APP_FOLDER ""t5"");This would affect link generation prefixing urls with ""t5/"" (including the virtual /assets folder which would be /t5/assets). Since the /t5 portion is part of the URL mapping it would not be part of the request pathInfo so existing dispatch code would not need to change. Of course Websphere has some bugs in this area that might cause some grief.",3145
Extract Method,"The Form event ""validateForm"" is awkwardly named and should be replaced with the simpler name ""validate"" You end up with methods like onValidateFormFromLogin() which is just asking for trouble.onFormValidateFromLogin() is a bit better.Obviously we keep the old name as well ... have to fire two events at the validate form stage.What would be a better name? Something that doesn't have ""form"" is it; perhaps ""finalValidation"" or just ""validate"". I remember this was a problem with calling it ""validate"" before because this conflicted with the validate event form form control element components ... but is that really a problem?",3146
Extract Method,Provide ApplicationStatePersistenceStrategy for Hibernate entities Persisting a hibernate entity in HttpSession is not good idea because the entity becomes detached from the Hibernate session.It would be nice to have a ApplicationStatePersistenceStrategy which stores only the primary key of the entity into the session.This strategy should make use of the class PersistedEntity like EntityPersistentFieldStrategy does.,3147
Extract Method,Change proxy generation to use volatile fields rather than synchronized blocks As currently coded the service proxies used for Tapestry services use a synchronized block to a) check to see if the Registry has shut down and b) obtain (if needed) the realized service implementation (wrapped by interceptors etc.).It seems that with some juggling these could largely be replaced with AtomicBoolean and AtomicReferences.,3148
Extract Method,Move away from Javassist A long-term multi-release strategy to replace ClassFactory/ClassGen and ClassTransformation methods with equivalents that are not tied to Javassist.Over a couple of releases the methods could be introduced (still implemented on top of Javassist) and the Javassist-centric methods deprecated then eventually disabled (NotImplementedException) or even removed.Rationale: Javassist is unprofessionally and fitfully maintained; many users have problems under Java6 due to Javassist.,3149
Extract Method,"Context assets should be versioned and provided with a far future expires header just like classpath assets This is as-per YSlow recommendations assets in the context (i.e. the context: asset prefix) should be treated like classpath: assets; the URL sent to the browser should reflect a version number (defined by the application) and the files should be given a far future expiration date.So the URL might be ""/app-assets/1.2.3/images/logo.gif"" for a file referenced as ""context:images/logo.gif"". The application would define the version number. It would be the developer's responsibility to advance the version number whenever the context files change (i.e. on each new deployment of the application).",3174
Extract Method,"Optimize page construction for repeated construction of the same page Construction of pages is probably the largest expense for any request as it involves considerable work to identify what components to instantiate what bindings to create and what template tokens to be converted into what ComponentPageElements.It should be possible to devise a ""page template"" that is a list of commands for constructing a page. The current PageLoaderProcessor would generate that list of commands. Creating a page instance would be a matter of executing the commands. This would decrease the amount of time needed to generate the 2nd (and later) instances of a page and would increase the likelyhood that common page elements for literal text could be re-used across page instances.Making page instance creation less expensive would allow Tapestry to more aggressively cull unused page instances (i.e. shorten the active window) while not sacrificing the ability to handle a request surge.",3175
Extract Method,Add @Optional annotation to mark contribution methods that can be ignored if the indicated service does not exist Currently the registry wil not start if a module makes contributions to a service which does not exist (e.g. because the module with the service is not loaded). This makes problems with applications that allow installations with a subset of all modules started. A module should be able to contribute to an extension point only if it is available. In case the service does not exist the contribution should be ignored.,3176
Extract Method,ObjectLocator.autobuild would be more useful with an override that allowed a message about the object to be described The message would be used with an OperatonTracker.,3177
Extract Method,FormFragment component should include a parameter to control whether non-visible content is included in the form submission The current behavior (that non-visible content does not submit with the form) is correct for most but not all applications. It would be nice if this was selectable more readily ... a new parameter on FormFragment that allows the remove-if-not-visible logic to be disabled in particular cases.,3178
Extract Method,The TypeCoercer should be able to coerce String to Enum types even without a specific contribution In the current implementation to allow coersion form string literal to Enum we need to add contribution to the TypeCoercer. This is working. But this coersion can be performed by default using just Enum.valueOf (Class String) so there are not required to register new coersion for every used enum type. In this case whe are lost the case insensibility of enum coersions but if this is meaningfull for everyone we can still use contributed coersions and if no one is found fallback do the default. Enum.valueOf,3179
Rename Method,Reorganize ComponentClassTransformWorkers to start moving away from Javassist Begin moving code into forms that can be implemented without Javassist by creating new methods on ClassTransformation.,3180
Extract Method,"Allow absolute filename with FileResourceLoader There is no way of providing an absolute path (e.g. c:\temp\wibble.tmp) to the FileResourceLoader because it always attempts to use the 2-argument File constructor (even if the path component is empty).The following fix resolves this problem:In FileResourceLoader.findTemplate replace:File file = new File( path template );withFile file = null;if("""".equals(path))file = new File( template );elsefile = new File ( path template );Note this does not introduce any security risks as the FileResourceLoader must be configured to search the empty ("""") path.",3181
Extract Method,ResourceFactory not extensible The class org.apache.velocity.runtime.resource.ResourceFactory providesno mechanism for allowing sub-classes of Template or ContentResourcesto be returned.Since ResourceManagerImpl makes a call to ResourceFactory.getResource()the only way to override the behavior is to subclass ResourceManagerImpland cut-paste-then-modify the code for loadResource().At the very least moving the ResourceFactory.getResource() call intoa separate protected method in ResourceManagerImpl would greatly simplifysub-classing Templates and/or ContentResources.,3182
Extract Method,can't load macros in file loaded with #parse I think this is a big bug. I've used velocity in my projects erveything is okbut this. i want to know it's this problem will be resolved in next version? ifit's not i have to abandon velocity.the bug is :from doc:This is important to remember if you try to #parse() a template containinginline #macro() directives. Because the #parse() happens at runtime and theparser decides if a VM-looking element in the template is a VM at parsetime#parse()-ing a set of VM declarations won't work as expected. To get aroundthis simply use the velocimacro.library facility to have Velocity load yourVMs at startup.,3183
Extract Method,"Improved Syntax for Maps and Collections I would like to see some syntatic sugar for Maps and Collections and perhaps other Objects too. (I have read that there will be a syntax for map literals in 1.5 that's a first step.)I want to have something like in groovy: http://groovy.codehaus.org/CollectionsScroll down to ""Slicing with the subscript operator"".PS: Please do never implement this terrible confusing groovy map bean syntax where ""map.foo"" ist equivalent to ""map.get(""foo"")"".",3184
Extract Method,Improve Resource existence detection Depending on the ResourceLoader used testing the existence of a resource is somewhat expensive as the resource has to be opened to test for its existence.I'm proposing the following changes:1.Add a new method to org.apache.velocity.runtime.resource.loader.ResourceLoader:public boolean resourceExists(String source) {InputStream is = null;try {is = resourceLoader.getResourceStream(resourceName);if (is != null) {return true;}} catch (ResourceNotFoundException e) {} finally {if (is != null) {try {is.close();} catch (Exception e) {}}}return false;}This method keeps compatibility with all current ResourceLoaders and can be overriden by subclasses.2. org.apache.velocity.runtime.resource.ResourceManagerImplModify the String getLoaderNameForResource(String resourceName) method to usethe new ResourceLoader.resourceExists(String) method.,3186
Rename Method,Velocity 1.4 does not support Iterable collections. In the new for loop in Java 5 e.g.for(Object obj: iterable)the collection iterated only needs to be of type IterableHowever In Velocity a foreach loop must be either a Collection or a Map but cannot be just an Iterable.Suggestion support Iterable containers.,3187
Rename Method,"add documentation to explain precedence for resolving property The Velocity user guide is not clear about the precedence for resolving the property of a variable. As in the UberspectImpl.java code the precedence to resolve a property should be something like in this order:getbar()getBar()get(""bar"")isBar()This information will be very useful to the user. I suggest that this is added to the user guide under the ""Properties"" section.",3188
Extract Method,Some user feedback / wishes from the O'Reilly blog Look into the wishlist at http://www.oreillynet.com/onjava/blog/2007/03/velocity_15.html,3189
Extract Method,Add ablity to add directives programatically Some directives may have complex initialization and setup requirements. So much so that it's simpler to configure them in the calling code and not thru some reflection mechanism. The attached patch adds methods to support this.,3190
Extract Method,Display consistent file and line number format for errors. The provided patch changes the display of file errors so that it is of one consistent format. The format implemented is the following:$filename[line $lnum column $cnum]If the filename is not available then filename is substituted with <unknown file>A consistent format help tools to detect and process file references in velocity logs and exceptions.,3191
Extract Method,Fix correct template name reporting enhance error logging information Fix template name reporting for #include and #parse if an exception occurs. Error reporting in Velocity tends to use context.getTemplateName() which is intended for scoping information and does not always provide the template name containing the node or directive that generates an error. This adds a templateName field to the Directive object and assigns it on creation if a template name is available.Also added template and location info when logging errors thrown from #parse. This compliments the pseudo-stack trace that is already logged to error for macros. So now a complete trace is logged to error of the macro and template layers with template name and location. Yea!,3192
Rename Method,"Parsing errors on content inside #literal() #end block I have some velocity templates that include quit some javascript. Inside the javascript a javascrip template engine is used which also uses ${varname}Escaping each occurance would make the code rather unreable so to prevent velocity from parsing the javascript code I put a #literal() around it.However velocity still PARSES the contents of this block which of course results in parsing exceptions.My feeling with ""literal"" is that it is completely UNINTERPRETED content?This SHOULD work:#literal()var myId = 'someID';$('#test).append($.template('<div id=""${myId}""></div>').apply({myId: myId}));#end",3193
Extract Method,Blockmacro support (allows any AST as macro body argument) Inspired by VELOCITY-583 (BlockMacro support) I implemented the same functionality in a slightly different way.The new syntax is:#@yourMacroName($arg1 $arg2) any valid velocity AST here #endso basically the syntax is exactly the same as for normal macros except you put that @ prefix to the macro name. That tells Velocity that there's a macro AST body that should be passed to the actual macro.And in the macro you can refer to the passed body 0-N times. Like:#macro(yourMacroName $foo $bar)$bodyContent#end,3194
Rename Method,Minor performance tweaks based on Findbugs findings Mainly change two inner classes to static inner classes and a few other slight modifications. See the patch.,3195
Inline Method,Macro pass by value option Add a configuration property which would change macro parameter passing from pass by name to pass by value. I think in many instances this option will provide behavior that is more intuitive to the user. There are two important exceptions to this references created by #define and references created by BlockMacro's $bodyContent. This allows the user to still specify pass by name semantics when desired for example:#define($x) #if($foo)$foo.bar#{else}-None-#end #end#go($x)In the above example '#if($foo)$foo.bar#{else}-None-#end' will be passed by name. In all other cases pass by value rules apply for example:#go($foo.bar [1 2 3] #if($bar)Yes#{else}No#end)In the above example all parameters will be evaluated to a value first then passed to #go. This has potential performance improvements also.,3196
Rename Method,! preceding a reference in strict mode ignores a null exception Change strict mode (runtime.references.strict = true) behavior such that when Velocity attempts to render a reference that evaluates to null then throw an exception. If the reference is preceded by '$!' as in $!foo then simply render nothing and ignore the null value as it does for non strict mode.,3197
Rename Method,"Add macro default parameters Add the ability to specify default parameters to macros for example:#macro(foo $x $y=""legit"")$x$y#endcalling this with #foo(2) would give:2legitAny number of default parameters can be specified but no non-default parameters can not follow default parameters. Assignment of calling values begins from left to right and all left over default arguments are assigned their default values.",3198
Extract Method,"VTL Simplicity - ""Control"" objects In the discussion for VELOCITY-680 Claude suggested the addition of what i'm calling ""control"" objects as a solution. These would have the same name as the block directive or macro to which they belong. At a minimum these would provide get(key) set(key value) and stop() methods to control the reference scoping and execution of the block to which they belong. Directives could extend the basic control object to provide additional functions such as index() and hasNext() for #foreach. Here's some examples:#foreach( $user in $users )$user#if( $foreach.hasNext ) #end#if( $foreach.count > 10 ) $foreach.stop() #end#end#macro( foo $bar )blah blah #if( $bar == 'bar' ) $foo.stop() #end#set( $foo.woogie = 'woogie' )$foo.woogie#end#foreach( $item in $list )#set( $outer = $foreach )#foreach( $attr in $item.attributes )#if ( $attr == $null ) $outer.stop()#end#end#end------foo.vm---------blah blah $template.stop() blah------------------------#define( $foo )blah blah $define.stop() blah#endThis could allow us to greatly simplify all sorts of things. We could remove the #break #stop and #return directives. We would no longer need to have ""local"" contexts for foreach loops or macros; instead users could set and get local variables directly in the provided namespace. All else would be global. This may even cut down our internal code complexity a fair bit. It'll certainly obviate the need for several configuration properties and internal contexts. Everything becomes much more explicit obvious and robust. I also don't think it looks ugly. :)We would of course have to make sure that the StopExceptions thrown by stop() aren't wrapped into MethodInvocationExceptions. We'd have to make the directives clean up their control when done rendering and if they're nested in a directive of the same type then they should save and restore the reference to the parent control. We'd also have to figure out a good default name to give the control objects for the top-level control object and whether it would be different than the name of the control object used during a #parse call. $template? $parse? $velocity? If we wanted to use $template--which i think works well for both top-level and #parse--then we'd probably have to make it configurable since that's likely to conflict. And if we make that configurable i suppose we may as well make it configurable for the others too.I'm struggling to think of any real downside to this. Most of the replaced features (implicit macro localscope #stop #break $velocityHasNext) are either not default behavior or are new features. I'd wager that most people would only have to change $velocityCount to $foreach.count. Even that's no big deal since this would be for a major version change.  The worst i can think of is the fact that for a couple of these controls it would mean a few more keystrokes. Considering all the gains in extensibility explicitness and simplification (for us and users) i think it's worth a few keystrokes.What do you guys think?",3200
Rename Method,"Implement default values in formal references We should take profit of the formal reference syntax to implement default values: {{  ${foo|""bar""}}} It could even be repeated or nested... {{  ${foo|$bar|""baz""}}} {{  ${foo|${bar|""baz}}}}  ",3206
Rename Method,Remove download-hadoop profile requirement and cache downloads 0,3207
Rename Method,Normalize the user:group mapping for end to end tests 0,3208
Extract Method,Setup automated patch testing 0,3209
Inline Method,Fix Sentry Precommit tests 0,3210
Extract Method,Generate audit trail for Sentry DBStore service actions The Sentry DB store should generate audit logs for authorization metadata change requests.,3212
Extract Method,Add Sentry service APIs to query roles and privileges 0,3213
Inline Method,Add schematool for creating Sentry store schema from the SQL scripts We need a tool similar to Hive schemaTool for creating the Sentry store schema.,3214
Extract Method,Create tool to dump and load of entire Sentry service We are storing entire content of the Sentry service in a database. It would be useful to have a sentry tool that would be able to dump entire content in sentry specific format (that will be independent on the used database backend) and it's counterpart that would be able to read this format and load the data into another instance of the Sentry Service.Such tool can be very helpful for:* Backups* Migration from one backend store to another* Debugging content of the underlying database,3216
Extract Method,Do the user: group lookup in the Sentry db policy server 0,3219
Extract Method,SHOW GRANT ROLE xxx ON [SERVER DATABASE TABLE URI] xxx Handling the filtering of privileges based on DB objects at Service level will also reduce the amount of privilege data transferred on the wire,3220
Extract Method,Support SHOW CURRENT ROLES 0,3222
Extract Method,Create a diagnostics tool for configuration validation Create a tool for offline troubleshooting -Validate the configurationList permissions for a given userOffline query validation,3224
Extract Method,Support auth admin delegation via SQL construct 'with grant option' Currently the authorization admins are created statically by setting a config property. Sentry should support admin delegation via 'with grant option' statement.,3225
Rename Method,"Add more granular privileges to the DBModel Specifically it would be good to split ""All"" privilege into ""Create"" ""Drop"" and ""Alter""",3226
Extract Method,Support Sentry service API to retrieve applicable privileges for a given authorizable object The current implementation of list_sentry_privileges_for_provider is specifically to facilitate Sentry auth engine. It's not intended to be a general purpose metadata query.We should add a new API that returns the list of privileges (TSentryPrivilege) for the give authorizable object and all it's applicable privileges.,3227
Extract Method,"Enable sentry end to end tests to run on a real cluster Adding a DFS implementation ClusterDFS which along with UnmanagedHiveServer can be used to run tests on real secure cluster. - Adding a maven profile which sets the required classpath and runs only the tests which are supported to run on real secure cluster with an UnmanagedHiveServer.-- Tests which will be run with ""cluster-haoop"" profile:--- TestCrossDbOps--- TestEndToEnd--- TestMetadataObjectRetrieval--- TestMetadataPermissions--- TestMovingToProduction--- TestPerDatabasePolicyFile--- TestPrivilegeAtTransform--- TestPrivilegesAtDatabaseScope--- TestPrivilegesAtTableScope--- TestSandboxOps--- TestExportImportPrivileges--- TestUriPermissions--- TestRuntimeMetadataRetrieval-- Tests which will not be run:--- TestUserManagement (had non static user:group mapping)--- TestServerConfiguration (uses a managed hive server)--- TestSentryOnFailureHookLoading (uses a managed hive server)--- TestPrivilegesAtFunctionScope (has not been added as it results in a failure due to SENTRY-23)--- TestPerDBConfiguration (has not been added as it uses a special java property)Running the tests:Prerequisites:- Need to setup a secure cluster with sentry enabled- Need to have the static user:group mapping setup- Need to set HIVE_CONF_DIR and HADOOP_CONF_DIRmvn clean package -Pcluster-hadoop -Dsentry.e2etest.policyOwnerKeytab=</path/to/keytab> -Dsentry.e2etest.policyOwnerPrincipal=<principal> -Dhive.server2.thrift.bind.host=<hs2Host> -Dhive.server2.authentication.kerberos.principal=<hivePrincipal>",3228
Extract Method,Synchronization of HDFS permissions with Sentry permissions An HDFS file or directory that is associated with an Authorizable Object managed by Sentry (Such as a HiveMetaStore table partition a Solr/Search collection/document or an HBase Table etc.) should have the permissions that reflect those that were granted/revoked via Sentry. This logic should be enforced by a Sentry Authorization Plugin which would be an implementation of the HDFS AuthorizationProvider as described in [HDFS-6826|https://issues.apache.org/jira/browse/HDFS-6826]This is an umbrella JIRA,3229
Extract Method,"Ban additional configs in getConfigVal() Comments from [~bcwalrus] in email:{quote}Looking at SentryPolicyStoreProcessor.java the forbidden configs are hardcoded in that check.* You want to also forbid `sentry.store.jdbc.password' as well.* It'll get unwieldy as people add new sensitive configs. But I don't have any good idea to keep this up-to-date.* Does this call require admin privilege? Why would normal users need to know the server config?{quote}my reply{quote}Looking at SentryPolicyStoreProcessor.java the forbidden configs are hardcoded in that check.* You want to also forbid `sentry.store.jdbc.password' as well.Good point - it might just be easier to forbid "".*\.jdbc\..*"" entirely - I can't see how a client would want/need that. Also should forbid "".*password.*"" just to be sure. * It'll get unwieldy as people add new sensitive configs. But I don't have any good idea to keep this up-to-date.A config option? :-) But that itself would have to be kept up to date. * Does this call require admin privilege? Why would normal users need to know the server config?The use case for this is that impala would like to be able to cache certain config items like admin.group. It's an internal call for clients. Since the client is trusted (the other calls simply pass the name of the requester) the lack of a requesting user isn't a big deal.{quote}This issue is to additionally ban "".jdbc."" and ""password"".",3230
Extract Method,"Improve grant role to groups Now if we execute ""grant role role1 to group group1 group group2 group group3"" we will execute 3 thrift call. Currently we have thrift API to support grant/revoke one role to/from a set of groups so we can improve the client call.",3231
Rename Method,Implement grant user to role Currently sentry only support grant group to role there should be a reasonable feature to grant user to role for Hive the following command should be supported in sentry:GRANT role_name TO USER user,3232
Extract Method,Add column-level privileges for Hive/Impala Currently the finest grain of privilege is at the table/view level. This leads to the unwieldy scenario where a different view has to be created for each combination of columns that need to be restricted. With column level privileges this would not be required.In the policy file column privileges might potentially look like:server=server1->db=default->table=employees->column=salary->action=select,3233
Rename Method,Sentry client should support cache based kerberos ticket for secure zookeeper connection The Sentry service client create a Jaas context for keytab based login for connecting to secure zookeeper. It should also support ticket cache based login for clients that don't have keytab or not performing keytab based login.,3234
Inline Method,"Apply PMD plugin to Sentry source As discussed on the mailing list this item is to apply the PMD (Project Mess Detector) plugin to the project. The patch is quite large although the changes are almost all trivial. I will post it on the review board as well to seek some clarification on a few points.PMD is run by default. You can use the ""-Pnochecks"" profile to avoid running it if you want.",3235
Extract Method,"Import Transform option using supertype instead of a specific type Users can provide a tranforms option while import to replace @cl1 in hive_table to @cl2 using the following JSON.{code}{""options"": {""transforms"": ""{ \""hive_table\"": { \""qualifiedName\"": [ \""replace:@cl1:@cl2\"" ] } }""}}{code}It would be easy to specify a super type like 'Asset' to transform all types in the export items which inherit from the super type to have ""@cl1"" replaced with ""@cl2"" like{code}{""options"": {""transforms"": ""{ \""Asset\"": { \""qualifiedName\"": [ \""replace:@cl1:@cl2\"" ] } }""}}{code}",3237
Extract Method,Data Migration: Moving Data from Earlier Versions of Atlas to Most Recent *Background* The most recent version of Atlas uses _JanusGraph_ as database. Earlier versions used to use _Titan v0.5.4_. The formats used for storing data have changed. The current version of Atlas also implement features within Atlas entities. This makes the storages structures differ. A data migration approach thus becomes necessary to address the format incompatibilities. *Approach* Earlier version of Atlas could use _Export_ process to extract data out of the _Titan_ database. This ZIP file would then be moved to a cluster with newer version of Atlas installed. An _Import_ process on the new cluster would update the new cluster with data migrated to the new format. It should be possible for _Data Migration_ to be initiated from Ambari so that it becomes part of the Ambari's upgrade process. It should also be possible to see the status of the migration as it progresses. During the process no hook messages should be processed or any notifications sent out.,3238
Extract Method,Lineage information to include relationship guid Currently Atlas only includes to and from guid for each lineage relation. This change adds relationship guid as well,3239
Extract Method,Atlas Glossary support 0,3240
Inline Method,Data Migration: Import: Add support for BigDecimal BigInteger Add ability to migration-import to import data containing _BigDecimal BigInteger_ data types.  ,3241
Extract Method,[Metrics] Process typenames in batch of 25 The Gremlin query compilation fails when the number of typenames in within clause of the following query exceed 255 {noformat} g.V().has('__typeName' within(...)).has('__state' 'ACTIVE').groupCount().by('__typeName').toList() {noformat} The updated implementation breaks down the calls in chunks of 25 (configurable) typeNames and executes multiple queries with smaller batches.,3242
Extract Method,Enhance GraphTransactionInterceptor to ignore inner commit/rollback Some transactions need to be committed in one single shot right now any layer that orchestrates the data persistence using different stores (e.g. AtlasEntityStore and AtlasRelationshipStore) will see two different commits. With introduction of relationships and Glossary it's become a necessity that the entity and relationships are persisted in one single transaction or else the object gets created partially and leads to an inconsistent state. This change restricts the commit/rollback from the outermost method invocation.,3243
Rename Method,Change primitive map type storage in vertex Currently primitive map type in atlas is stored in vertex as: * typeName.mapAttr         = [key1 key2 key3] * typeName.mapAttr.key1 = value1 * typeName.mapAttr.key2 = value2 * typeName.mapAttr.key3 = value3 Since JanusGraph supports Map datatype we can store map value within a single vertex property. Also we need to create edge label and property key for array and map primitive types.,3244
Extract Method,Enhance AtlasClient to use UGI's authentication method to initialize URL connection handler Currently the atlas client checks if kerberos authentication is enabled using a property - *atlas.authentication.method.kerberos=true* The client needs to be updated to inspect the UGI's authentication method (SIMPLE or KERBEROS) and initialize the right URL Connection Handler if the above property is not set.    ,3245
Extract Method,Atlas Cluster Entity Background   In cases where Atlas data is synchronized to other Atlas clusters it is necessary to store some information about the nature of operation.   REST APIs to access this entity should be available as these can be used to fetch data about the cluster entity.   Use cases: * Scenario: Data generated by Export operation on one cluster (say cl1) can be used to Import into another cluster (say cl2). This operation will result in data such at AtlasImportResult that can be used later for performing subsequent operations or for audits. The AtlasCluster entity will be a good home for this data. REST APIs on AtlasCluster can be used for retrieveing this data.   Approach guidance: * Create AtlasCluster entity. * Create ClusterService that will use the OGM framework within Atlas to aid with save and load operations,3246
Extract Method,Atlas Client Support for Export & Import APIs *Background*  Support for Export and Import APIs within _AtlasClient_ needs improvement. The methods available are not sufficient to carry out the said operation. *Guidance for Implementation* * The _AtlasClient_ should support _OCTET_STREAM_ media type. * Usage of _MultipartWriter_ needs improvement. This should be in line with recommended approach.  * _Integration Test_ that uses this API should be added.,3247
Extract Method,Export & Import Process: Add Support to Use Detailed Audits *Background* The implementation for ATLAS-2798 allowed for detailed audits to be captured. The implementation for ATLAS-2797 allowed for representation of Cluster entity. This Jira captures the integration of the 2 new entities within the Export and Import process. *Approach Guidance* * Additional attribute in _AtlasExportRequest_ and _AtlasImportRequest_ say _updateMetaInfo_ will indicate that audits need to be captured. * Within Export process the value of _updateMetaInfo_ will indicate the cluster for which the export operation was destined for.  * Within Import process the value of _updateMetaInfo_ will indicate the cluster from which the exported ZIP originated. * Searching on _AtlasCluster_ entity will indicate the operations performed on that cluster. Detailed logs can be searched using approach.  * Ideally web user interface should be updated to display audits in the properties view of an _AtlasCluster_.,3248
Extract Method,Slow UI load and REST improvement for entities with ownedRef The approach is to check if query param- {color:#212121}*minExtInfo*:{color}*true* and the entity has schemaOptions then the server will send the attributes accordingly in referredEntities.,3249
Rename Method,"Provide option whether to delete propagated classification on entity delete during add classification By default when an entity is deleted - its associated classifications which have been propagated to downstream entities will be retained. This JIRA provides a boolean option when adding a new classification to an entity - *""Remove Propagations on Entity Delete""* When this flag is set to : *TRUE* - Propagated classifications are removed during entity delete *FALSE* - Propagated classifications are retained during entity delete",3250
Extract Method,Import Process: New Transform Framework: AddClassification Action *Background* The new transformation framework introduced earlier does not have provision for applying the transform that adds classification to an entity. *Approach Guidance* * New condition that is entity-level. * New action that applies to entity. * Mechanism to detect and create new classification if necessary. * Apply the new action to entity that results in addition of new classification for that entity.,3251
Extract Method,Import Process: Handle Import of Empty Zip Elegantly *Background* Existing implementation of _ZipSource_ always assumes that the zip that is being passed has valid data. It starts serializing the data and that is where it encounters serialization error. This case can be handled in a more elegant way. *Approach Guidance* * In _ZipSource_ ctor detect case for empty file throw error message accordingly. * Handle the case where requested _ZipEntry_ is not found.,3252
Rename Method,"Decouple MiniAccumuloCluster from integration test base class (apologies if I already had a ticket for this somewhere I couldn't find it)Our integration tests are very nice and automated at the moment because we can use MiniAccumuloCluster to ""provision"" an Accumulo instance (or used a shared instance) and run a test against it. For the most part this works well and provides an accurate test harness.Thus to run integration tests you need a sufficiently beefy machine since the same host will be running all of Accumulo as well as performing any client work. When resources are available to use it would be nice to leverage them -- whether these are yarn mesos a vanila installation etc.In addition to the additional computational power from using extra hardware it also encourages us to use the public API as much as possible instead of relying on ""hidden"" impl methods in MiniAccumuloCluster.I propose making changes to the IT test base (AbstractMacIT SimpleMacIT ConfigurableMacIT) to add an extra step between them an test classes to allow ""injection"" of a more generic Accumulo ""cluster"" that is not associated with MAC.",3253
Rename Method,"More IT stabilizations Bunch of tests still have the timeout parameter on the Test annotation which makes the scaling factor useless. Also we can reuse the scaling factor when tests sit to wait for ""something"" to happen.",3255
Extract Method,Add ability to create a table with user specified initial properties This change would allow for table properties to be set before the default tablet is created. Instead of just adding a new create method a NewTableConfiguration class could be created and passed and the other create methods deprecated.,3256
Extract Method,"Configuration objects created with CredentialProvider load defaults unnecessarily Jstack'ed a tserver and caught it in the middle of doing a bunch of URLClassloader and Jar stuff when creating a Hadoop Configuration. Obviously this struck me as odd (given that I know those aren't ""fast"" methods).Looking at it some more when I created the Configuration objects I didn't pass in {{false}} so the defaults were getting loaded. Regardless of how expensive it actually is it's unnecessary.",3257
Extract Method,"Random port for ZK in MiniAccumulo might not be unique To start ZooKeeper in a way that won't interfere with other processes on the node we have to choose a ""random"" port on the node and then configure Accumulo with that port (as we don't have a means to get the random port that ZooKeeper itself would bind to).The problem is that there is a delay between our choosing of a random port and ZooKeeper binding to that port which introduces a race condition.",3258
Extract Method,Add introspection of long running assignments At least once I've seen a tablet assignment hang for some inexplicable reason. We could track the active compaction and report on assignments that are taking an excessive amount of time.{noformat}Assignment for 54;7;6 has been running for at least 13445ms.java.util.zip.Deflater.deflateBytes(Native Method)java.util.zip.Deflater.deflate(Deflater.java:430)java.util.zip.Deflater.deflate(Deflater.java:352)org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater.compress(BuiltInZlibDeflater.java:54)org.apache.hadoop.io.compress.CompressorStream.compress(CompressorStream.java:81)org.apache.hadoop.io.compress.CompressorStream.write(CompressorStream.java:76)org.apache.accumulo.core.file.rfile.bcfile.Compression$FinishOnFlushCompressionStream.write(Compression.java:59)java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)java.io.DataOutputStream.write(DataOutputStream.java:107)java.io.DataOutputStream.write(DataOutputStream.java:107)org.apache.accumulo.core.data.Value.write(Value.java:163)org.apache.accumulo.core.file.rfile.RFile$Writer.append(RFile.java:388)org.apache.accumulo.tserver.Compactor.compactLocalityGroup(Compactor.java:504)org.apache.accumulo.tserver.Compactor.call(Compactor.java:362)org.apache.accumulo.tserver.MinorCompactor.call(MinorCompactor.java:96)org.apache.accumulo.tserver.Tablet.minorCompact(Tablet.java:2071)org.apache.accumulo.tserver.Tablet.access$4400(Tablet.java:174)org.apache.accumulo.tserver.Tablet$MinorCompactionTask.run(Tablet.java:2158)org.apache.accumulo.tserver.Tablet.minorCompactNow(Tablet.java:2267)org.apache.accumulo.tserver.TabletServer$AssignmentHandler.run(TabletServer.java:2937)org.apache.accumulo.tserver.ActiveAssignmentRunnable.run(ActiveAssignmentRunnable.java:55)org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)org.apache.accumulo.trace.instrument.TraceRunnable.run(TraceRunnable.java:47)org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)java.lang.Thread.run(Thread.java:745){noformat},3259
Rename Method,Add ability to retrieve property value by String key. Normally to retrieve the value for a property we have to get a Property enum instance. Since table arbitrary properties don't have Property enum instances we then can't use the simple get. Instead to retrieve a table arbitrary property we have to create a temporary map and a filtering function.Add the ability to fetch a single value based on the String key.,3261
Extract Method,Get Visibility Metrics from PrintInfo Add the ability to print visibility metrics such as densities of visibilities by locality group and the number of blocks that visibility is in.,3262
Extract Method,speed up WAL roll-overs After reading the proposal on HBASE-10278 I realized there are many ways to make the Accumulo WAL roll-over faster.# Open two WALogs but use only one until it reaches the WALog roll-over size# Rollover consists only of swapping the writers# WALog roll consists of the final close which can happen in parallel# Don't mark the tablets with log entries: they are already marked with the tserver# The tserver can make notes about the logs-in-use in the metadata table(s) as part of opening the log.# The master can copy the log entries to tablets while unassigning them piggybacking on the unassigment mutation.# Tablet servers can remove their current log entries from the metadata tables when they have no tablets using them.There are two issues: # tablets will have an empty file in recovery nearly all the time but the recovery code already handles that case. # presently a tablet doesn't have a marker for a log it did not use. Many more tablets will attempt to recover when it is unnecessary.This would also address ACCUMULO-2889.,3263
Inline Method,Use HostAndPort instead of a String for tracking addresses Noticed in ACCUMULO-3425 that the Thrift RPC factory classes accept {{String}} 's half of the time and {{HostAndPort}} 's the other half of the time.We should consolidate on HostAndPort (since it includes some basic sanity checks that are better than our {{address.indexOf(':')}} that we _sometimes_ do) and let this filter up into server's RPC use and the client API implementation.,3270
Extract Method,Use Java ServiceLoader to identify classes that are launchable by start This is similar to the goals of ACCUMULO-1496 but it should be possible to do it more efficiently with an annotation processor and Java's ServiceLoader. If successful this would supersede ACCUMULO-1496.,3271
Move Method,BatchScanner optimization for AccumuloInputFormat Currently {{AccumuloInputFormat}} produces a split for reach {{Range}} specified in the configuration. Some table indexing schemes for instance z-order geospacial index produce large number of small ranges resulting in large number of splits. This is specifically a concern when using {{AccumuloInputFormat}} as a source for Spark RDD where each Split is mapped to an RDD partition.Large number of small RDD partitions leads to poor parallism on read and high overhead on processing. A desirable alternative is to group ranges by tablet into a single split and use {{BatchScanner}} to produce the records. Grouping by tablets is useful because it represents Accumulos attempt to distributed stored records and can be influance by the user through table splits.The grouping functionality already exists in the internal {{TabletLocator}} class. Current proposal is to modify {{AbstractInputFormat}} such that it generates either {{RangeInputSplit}} or {{MultiRangeInputSplit}} based on a new setting in {{InputConfigurator}}. {{AccumuloInputFormat}} would then be able to inspect the type of the split and instantiate an appropriate reader.The functinality of {{TabletLocator}} should be exposed as a public API in 1.7 as it is useful for optimizations.,3272
Extract Method,Multi data center replication The use case here is where people have multiple data centers and need to replicate the data in between them. Accumulo can model this replication after the way that HBase currently handles the replication as detailed here (http://hbase.apache.org/replication.html). There will be one master Cluster and multiple slave clusters. Accumulo will use the Master-Push model to replicate the statements from the master clusters WAL to the various slaves WALs.,3273
Extract Method,Log Recovery Copy/Sort progress exceeds 100% I regularly notice that the Copy/Sort progress bar for log recovery on the monitor exceeds 100%. We should either* Find out why this exceeds 100% and fix the computationor* Just cap the value so that it just reports 100% and doesn't exceed it.,3285
Extract Method,add configuration changes to the random walk tests Add random configuration changes to the the random walk test.,3286
Extract Method,Support map reduce directly over files Support map reduce jobs that directly read Accumulo files.,3287
Extract Method,Provide examples for different configurations Needed for how I plan to implement the debian packaging stuff.The current site.xml.example and env.sh.example files provides a configuration for a small server machine. However I feel there are many users who are on weaker machines even laptops who want to get Accumulo a test run. So we should provide at least one more .example file for a tiny configuration (1GB footprint). Perhaps we should create multiple configurations for different memory footprints.,3292
Extract Method,Enable A/B testing of scan iterators on a table Classpath contexts are assigned to a table via the table configuration. You can test at scale by cloning your table and assigning a new classpath context to the cloned table. However you would also need to change your application to use the new table names and since we cannot disable compactions you would start to consume more space in the filesystem for that table. We can support users passing in a context name to use for the scan on existing tables.,3297
Rename Method,Improve validation of configuration and arguments with guava Predicates There's a few places in our code which could benefit from additional validation of arguments and configuration by using the built-in Guava Predicates. This makes some checks that we're doing a bit more readable and enables us to do more expressive validation checks.Specifically I see room for improvement in our {{PropertyType}} validators (which currently do very limited regex validation) and to a lesser degree {{o.a.a.core.util.Validator}}.I intend to replace these with proper Predicates to determine their validity.,3299
Extract Method,number of major/minor compactions cannot be changed on-the-fly I changed tserver.compaction.major.concurrent.max in the shell: tablet servers did not change the number of major compactors: I will need to restart the server for this change to take effect.,3300
Extract Method,Duplicated code in IteratorUtil Duplicated code in https://github.com/apache/accumulo/blob/master/core/src/main/java/org/apache/accumulo/core/iterators/IteratorUtil.java#L236{code}public static <K extends WritableComparable<?>V extends Writable> SortedKeyValueIterator<KV> loadIterators(IteratorScope scopeSortedKeyValueIterator<KV> source KeyExtent extent AccumuloConfiguration conf List<IterInfo> ssiList Map<StringMap<StringString>> ssioIteratorEnvironment env boolean useAccumuloClassLoader) throws IOException {List<IterInfo> iters = new ArrayList<IterInfo>(ssiList);Map<StringMap<StringString>> allOptions = new HashMap<StringMap<StringString>>();parseIteratorConfiguration(scope iters ssio allOptions conf);return loadIterators(source iters allOptions env useAccumuloClassLoader conf.get(Property.TABLE_CLASSPATH));}public static <K extends WritableComparable<?>V extends Writable> SortedKeyValueIterator<KV> loadIterators(IteratorScope scopeSortedKeyValueIterator<KV> source KeyExtent extent AccumuloConfiguration conf List<IterInfo> ssiList Map<StringMap<StringString>> ssioIteratorEnvironment env boolean useAccumuloClassLoader String classLoaderContext) throws IOException {List<IterInfo> iters = new ArrayList<IterInfo>(ssiList);Map<StringMap<StringString>> allOptions = new HashMap<StringMap<StringString>>();parseIteratorConfiguration(scope iters ssio allOptions conf);return loadIterators(source iters allOptions env useAccumuloClassLoader classLoaderContext);}{code}I thought I had commented on https://github.com/apache/accumulo/pull/51 about this but maybe I forgot.,3301
Extract Method,Conditional mutation processing performance could be improved. When processing conditional mutations tablets reads are done. The way the current implementation does tablet reads has a lot of overhead. For each condition the following is done :* Opens and reserves iterators files.* Parse table iterators from table config (involves scanning and filtering entire table config)* Merges condition iterators and table iterators* Constructs iterator stack.I created a branch where these operations (except for constructing iterator stack) are done per tablet and/or per batch of conditional mutations. Doing this I am seeing a 3x speed up in conditional mutation processing rates when data is cached.,3302
Extract Method,"TTimeoutTransport repeatedly uses reflection to obtain method Noticed the following in TTimeoutTransport while looking at ACCUMULO-4065:{code}private static InputStream getInputStream(Socket socket long timeout) {try {Method m = NetUtils.class.getMethod(""getInputStream"" Socket.class Long.TYPE);return (InputStream) m.invoke(null socket timeout);} catch (Exception e) {throw new RuntimeException(e);}}{code}We should really just invoke {{getMethod}} once and cache the {{Method}} instance instead of repeatedly getting it every time we create a new connection (which is often).Not sure if this is a ""hot"" enough code path to have a noticeable performance impact but it should be good to fix regardless.",3303
Inline Method,Eliminate constant unnecessary Text wrapping of immutable tableIDs I started looking at ACCUMULO-4138 to see how KeyExtent is used to do overlaps and I noticed a *lot* of {{new KeyExtent(new Text(String) ...)}} calls where that first parameter is the Text version of tableId.It appears we even do this practice so much that KeyExtent has a built-in WeakReference caching of these tableIds so the Java GC we can dedupe them and avoid creating too many.The thing is... a lot of this attempt to optimize appears to be the result of unnecessarily wrapping all these immutable String tableIDs with a Text object yet it doesn't really seem to buy us anything. In most of our API and a lot of internal utilities these are already Strings with references elsewhere in the code... so even if we dedupe the Text objects we're not doing anything about these Strings. Worse we're actually doing some protective copying here and there as we pass around these Text objects using TextUtil etc. This is completely unnecessary because they were immutable before we copied them and wrapped them.In some cases we actually call toString on the text objects to pass them around and they get flip-flopped a few times between String and Text depending on what the internal API accepts.At best using the Text object helps us serialize KeyExtent... but that's questionably beneficial since DataOutput can already serialize with: {{out.writeUTF(String)}} so that's not actually that helpful.The only other benefit I could possibly see is with compareTo for lexicographical comparison but I have a hard time believing Text can compare faster than {{java.lang.String}} and there shouldn't be any difference in the results of the comparison between the UTF-8 encoding of Text and the modified UTF encoding which is native to Java Strings certainly not for the base-36 characters and fixed constants (for special tables) we use in tableIDs.We should just completely strip out all the Text versions of tableId wherever possible. I've already done this as an exercise in the 1.6 branch but it might be potentially risky as these are put in Java maps which don't have strict type checking ({{map.get(Object)}} for instance) and are sometimes compared with {{.equals(Object)}}. For what it's worth the only public API this really touches is {{o.a.a.core.data.KeyExtent}} which was only inadvertently in the public API and has since been moved (IIRC). We can preserve the old methods for compatibility if necessary (deprecated of course).Also getting rid of these Text objects and just sticking with the immutable String objects we'll be able to take advantage of future JVM improvements for String deduplication like http://java-performance.info/java-string-deduplication/,3304
Rename Method,"Create a user level API for RFile Users can bulk import RFiles. Currently the only way users can create RFiles using Accumulo's public API is via AccumuloFileOutputFormat. There is no way to read RFiles in the public API. Also the internal APIs for reading and writing RFiles are cumbersome to use.I am experimenting with a simple RFile API like the following. Below is an example of writing data.{code:java}LocalFileSystem localFs = FileSystem.getLocal(new Configuration());RFileWriter writer = RFileFactory.newWriter().withFileName(""/tmp/test100M.rf"").withFileSystem(localFs).build();writer.startDefaultLocalityGroup();for (int r = 0; r < 10000000; r++) {for (int cq = 0; cq < 10; cq++) {writer.append(genKey(r cq) genVal(r cq));}}writer.close();{code}Below is an example of reading data.{code:java}LocalFileSystem localFs = FileSystem.getLocal(new Configuration());Scanner scanner = RFileFactory.newScanner().withFileName(""/tmp/test100M.rf"").withFileSystem(localFs).withDataCache(250000000).withIndexCache(1000000).build();{code}",3306
Rename Method,TinyLFU-based BlockCache [LruBlockCache|https://github.com/apache/accumulo/blob/master/core/src/main/java/org/apache/accumulo/core/file/blockfile/cache/LruBlockCache.java] appears to be based on HBase's. I currently have a patch being reviewed in [HBASE-15560|https://issues.apache.org/jira/browse/HBASE-15560] that replaces the pseudo Segmented LRU with the TinyLFU eviction policy. That should allow the cache to make [better predictions|https://github.com/ben-manes/caffeine/wiki/Efficiency] based on frequency and recency such as improved scan resistance. The implementation uses [Caffeine|https://github.com/ben-manes/caffeine] the successor to Guava's cache to provide concurrency and keep the patch small.Full details are in the JIRA ticket. I think it should be easy to port if there is interest.,3307
Extract Method,Allow per compaction iterator settings It may be useful to allow the compact command to specify an iterator to be used for that compaction. For example if someone wanted to apply a filter once to a table they could force a compaction with that filter.,3309
Extract Method,Make BatchWriter ConditionalWriter and ScannerBase extend AutoCloseable BatchWriter ConditionalWriter Scanner and BatchScanner all have close Methods. Howerver because they do not implement AutoCloseable they can not be used with try-with-resources.It would be simple to add AutoCloseable to all of these. I think this change should only be made in a minor release because it allows writing code that will not work with older versions.,3310
Extract Method,Stabilize tablet assignment during transient failure When a tablet server dies Accumulo attempts to reassign the tablets it was hosting as quickly as possible to maintain availability. If multiple tablet servers die in quick succession such as from a rolling restart of the Accumulo cluster or a network partition this behavior can cause a storm of reassignment and rebalancing placing significant load on the master.To avert such load Accumulo should be capable of maintaining a steady tablet assignment state in the face of transient tablet server loss. Instead of reassigning tablets as quickly as possible Accumulo should be await the return of a temporarily downed tablet server (for some configurable duration) before assigning its tablets to other tablet servers.,3311
Rename Method,"Replace meaningless method names There are a few abbreviated method names reused throughout the code that are meaningless to new users. Method names such as ""nk"" ""nf"" or ""ane"" are not helpful when trying to determine how a class works. This seems to only occur in Tests but I think it would improve readability a lot especially for newbies. I for one think Tests are a good place to learn how a class works and choosing brevity over clarity greater hinders the readability of a Test.",3313
Move Method,Update to thrift-0.10.0 Thrift just released 0.10.0 so we should update to that for the 2.0.0 branch. It shouldn't have much impact on us (I'll test it out) but it'd be good for future-proofing the 2.0.x releases against later thrift bugfixes.,3316
Extract Method,Modify TableOperations online to check for table online before executing fate operation. The table operations online operation executes as a fate operation. If a transaction lock for the table is currently held (say by table compactions) the online operation will block.This modification essentially changes the behavior of the online operation to a NOOP if the table is currently online. If the current table state is online then the operation returns immediately without queuing a fate operation to set the online table to online. This eliminates the blocking behavior of the operation if the table is already online.,3317
Extract Method,Limit use of environment variables in Java Accumulo should limit its use of environment variables like ACCUMULO_HOME and ACCUMULO_CONF_DIR in Java as environment variables can easily set incorrectly in the shell. Accumulo should only rely on configuration in accumulo-site.xml and accumulo-env.sh.,3318
Extract Method,Improve 'accumulo classpath' command The 'accumulo classpath' command should be me modified to support the following:{noformat}java -cp /path/to/my.jar:$(accumulo classpath) com.my.classjava -cp $(accumulo classpath) -jar /path/to/my.jar{noformat}The current output of the command which lists all jars in a human readable format could be supported by 'accumulo classpath --debug',3319
Rename Method,Simplify Accumulo memory configuration If Accumulo memory configuration can be simplified using percentages then users will no longer need to use {{accumulo create-config}} to create accumulo-site.xml and accumulo-env.sh configuration files. Accumulo should instead ship with simple configuration files that have sane defaults and need limited changes from users.,3320
Rename Method,AccumuloClassLoader should load accumulo-site.xml from classpath Currently it is expects ACCUMULO_CONF_DIR to be set and loads it from that directory.,3321
Inline Method,Back port Iterator improvements The improvements in ACCUMULO-3079 were applied to version 2.0 but some of the non user facing changes could be back ported to older versions. These changes would have to respect existing user iterators and would not include the new iterators introduced in 2.0.,3322
Rename Method,HostRegex balancer should allow migration even when we have pending migrations The HostRegexTableBalancer current halts all migrations when there are pending migrations. I propose fixing this to allow adding additional migrations even when there are pending migrations up to a specified amount.,3323
Rename Method,LocalityGroupIterator very inefficient with large locality groups On one of our systems we tracked some scans that were taking an extremely long time to complete (many hours). As it turns out the scan was relatively simple in that it was scanning a tablet for all keys that had a specific column family. Note that there was very little data that actually matched this column familiy. Upon tracing the code we found that it was spending a large amount of time in the LocalityGroupIterator. Stack traces continually found the code to be at line 128 or 129 of the LocalityGroupIterator. Those line numbers are consistent from the 1.6 series all the way to 2.0.0 (master). In this case the column family being searched for was included in one of a dozen or so locality groups on that table and the locality group itself had 40 or so column families. We see several things that can be done here:1) The code that checks the group column families against those being searched for can quickly exit once if finds a match2) The code that checks the group column families against those being searched for can look at the relative size of those two groups an invert the logic appropriately for a more efficient loop.3) We could create a cached map of column families to locality groups allowing us to avoid examining each locality group every time we seek.,3324
Rename Method,Create WeakReference Map to replace Table.ID constructor Taken from feedback on the PR #279:Could maybe avoid duplicates by making constructor (of Table.ID) private and doing Table.ID.of(tableId) which draws from an internal WeakReference map.If the object deduplication in KeyExtent is still valid this can be pushed down to the Table.ID and Namespace.ID classes replacing the optimization in KeyExtent.,3326
Extract Method,Address some static analysis feedback from Fortify Fortify flagged some things in Accumulo (mostly against 1.7 and 1.8). Actually it flagged a lot of things but there were a few that I noticed which are minor but wouldn't hurt for us to fix.* The {{JarFile}} in {{Jar.java}} is never closed* {{BoundedRangeFileInputStream}} invokes a PrivilegedAction for some reason I can't fathom (been this way since code import -- I think it can be removed).* Numeric validate on the refresh cookie in the monitor* Use {{HttpOnly}} on the cookies we create to mark that we only expect them to be accessed by the browser* We put the request URI back into the page body in DefautlServlet if we can't load the requested element (putting user-controlled info in a http response -- generally bad news). We can just trim the data we write to the browser and log it instead.,3327
Extract Method,Create official Accumulo Docker image While there are some [Accumulo images|https://hub.docker.com/search/?isAutomated=0&isOfficial=0&page=1&pullCount=0&q=accumulo&starCount=0] on DockerHub it looks the majority of the them are designed to run a single-node Accumulo instance in a Docker container for development and testing.It would be great if Accumulo had an official image for running Accumulo processes in containers on a production cluster. The image could be be published as an official image 'apache/accumulo' to DockerHub. In order to make this possible I think work needs to be done to allow configuration to be passed to the Accumulo process in the docker container without using configuration files (as passing files to a running container is hard in Docker). One way to do this is to add an option called {{--upload-accumulo-site}} to 'accumulo init' command which is called outside of Docker by the user. This would set properties in accumulo-site.xml as system properties in Zookeeper during Accumulo initialization. Accumulo processes in Docker containers could be started with minimal configuration by updating 'accumulo <service>' commands to have a {{-o key=value}} option to override configuration. These changes to Accumulo would enable the following commands to start an Accumulo cluster in Docker:{noformat}accumulo init --upload-accumulo-sitedocker pull apache/accumulodocker run apache/accumulo master -o instance.zookeeper.host=zkhost:2181docker run apache/accumulo tserver -o instance.zookeeper.host=zkhost:2181docker run apache/accumulo monitor -o instance.zookeeper.host=zkhost:2181docker run apache/accumulo tracer -o instance.zookeeper.host=zkhost:2181{noformat},3328
Extract Method,No APIs to configure iterators and locality groups for new table In Accumulo 1.7 the ability to set table properties at table creation time was added. For existing tables there are APIs in table operations that allow setting locality groups and iterators for existing tables. When setting table properties at table creation time there is not good API for iterators and locality groups. There should be some way in the API to do this. There may be other things besides iterators and locality groups that should also be supported at table creation time. ,3329
Extract Method,Enable GCM mode for crypto Enable the use of GCM as an optional encryption mode. While this change will allow for GCM it should probably only be used for Java 9 and later. https://docs.oracle.com/javase/9/whatsnew/toc.htm#JSNEW-GUID-71A09701-7412-4499-A88D-53FA8BFBD3D0 http://openjdk.java.net/jeps/246,3330
Inline Method,inconsistent names and duplicate methods in IteratorSettings David Medinets noticed{quote}A Property object used to hold key-value information used to modifythe behavior of an Interator. However these are the methodsavailable:{noformat}getPropertiessetPropertieshasPropertiesaddOptionremoveOptionaddOptionsclearOptions{noformat}Is there a reason why the same concept as two names? I'd like tosettle on one name and standardise.Could we change the names to be something likegetInteratorSettingProperties? I know that some people are annoyed bylonger method names but when searching through a code base haveunique names is handy. Searching for a generically named method - suchas getProperties returns a lot of false positives.{quote},3331
Inline Method,Replace tables in Monitor with DataTables I found this Javascript library: https://datatables.net/ I think this would give us everything we need to display data in the Monitor. It would take some work to get it working but it would eliminate a lot of custom code and give us more features.,3332
Extract Method,Create builder methods for Connector to simplify client API 0,3333
Rename Method,In tablet server start scan authenticates twice The code that handles a start scan RPC call checks authentication twice.  Each call to authenticate takes a bit of time.  It would be nice if it only did it once. At [TabletServer line 479|https://github.com/apache/accumulo/blob/rel/1.8.1/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java#L479] a call to canScan is made which calls authenticate.  Then at [TabletServer line 482|https://github.com/apache/accumulo/blob/rel/1.8.1/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java#L482] a call to check authorizations is made which also authenticates.  ,3334
Extract Method,Add a thrift proxy server Add a thrift proxy server to make integration with other languages besides Java a bit easier. This should work like http://wiki.apache.org/hadoop/Hbase/ThriftApi.,3335
Extract Method,reduce the debugging level for the examples 0,3336
Extract Method,Check if major compaction needed when new file introduced Currently the tablet server scans all tablets every 30 seconds to see which tablets need to major compact. After a tablet minor compacts or imports a file it could check if it needs a major compaction or split. If so it could place itself on the appropriate queue. This would make the system a little more efficient.,3337
Extract Method,Improve scan information available on monitor page The monitor page does not give good awareness with respect to what is actually happening with scans. It would be interesting to know the following.* Number of seeks done on server side* Number of entries read on the server side* Number of entries returned on the servers dieA filtering iterator may do a lot of seeks and drop a lot of data but this is not really shown in anyway on the monitor page. If we displayed the info above then a user could see the filter seeking reading and not returning data. Currently only the amount of data returned by iterators is displayed not the amount of data read by iterators.Also the current monitor page displays scan session which corresponds to a user starting a scan on a tablet server. It does give the user a info about how many seeks the iterators that are running as part of that scan session are doing.,3338
Rename Method,Shell should have way to specify instance name while using ZK hosts from configuration Currently the shell uses HDFSZooInstance unless otherwise specified. It is possible for a user to force a ZooInstance in which case they specify a list of ZKHosts and the instance name. However there is no way to specify an instance name and utilize the list of ZK hosts in the configuration file. We should look into expanding the shell to include this creation option possibly by segregating the zookeeper hosts and instance name into two seperate arguments.,3340
Rename Method,BatchWriters do not track Throwables beyond Constraint Violations Working on ACCUMULO-259 and adapting the Security Random Walk test to account for cached credentials to prevent it from crapping out on ambiguous credentials (credentials which are currently propagating). And I noticed that our client side Writers provide no means of feedback to the client if there is a non-ConstraintViolationException.That is looking at the TabletServerBatchWriter specifically we keep track of which servers had errors which key extents had errors and what ConstrainViolations we saw but we don't actually track the actual Throwables we get aside from ConstraintViolation. We should keep a Collection or Set of the Throwables we see so client code can do proper case checking as well.,3343
Move Method,Remove (deprecate) createUser call with authorizations argument Creating a user depends on a different ACL than granting Authorizations. If the user can do one but not the other it will still create the user but float back an error. This can be confusing to end users so I think we should isolate createUser to just creating the user. They can then be granted authorizations as need be.,3344
Rename Method,Batch Scanner needs timeout The batch scanner needs a user configurable time out. When the batch scanner is used to query lots of tablet in parallel if one tablet or tablet server is unavailable for some reason it will cause the scan to hang indefinitely. Users need more control over this behavior.It seems like the batch scanner could behave in one of the following ways : * Read as much data as possible then throw an exception when a tablet or tablet server has timed out* Throw an exception as soon as a tablet or tablet server times out even if data could still be read from other tablets successfully.The timeout can default to max long to preserve the current behavior.,3345
Extract Method,Batch Writer needs timeout The BatchWriter needs a user configurable timeout. The current behavior when a tablet or tablet server can not be successfully written to is that it will hang indefinitely retrying. The timeout could default to max long to preserve current behavior.,3347
Extract Method,Add option to egrep and RegExFilter for subsequence matching Egrep and RegExfilter use java Matcher.match() which only returns true if the expression matches the entire string. Matcher.find() can detect when an expression matches a subsequence of the string. Need an option to use find in egrep and RegexFilter.This issue was created based on discussion in the following two mailing list threads.* [egrep usage - 1.3.4|http://mail-archives.apache.org/mod_mbox/accumulo-user/201207.mbox/%3CCAP19eqypCNAueE4z6TPU-Jt3g-%3DL2ptLVo9%2Bc_HC_LVA0LOTog%40mail.gmail.com%3E] July* [Re: egrep usage - 1.3.4|http://mail-archives.apache.org/mod_mbox/accumulo-user/201208.mbox/%3CCAGgKvPkHOQ0XVeuvobBD-G6tzR2pb-OSkZPjnsT_qEiSvzY%2Bew%40mail.gmail.com%3E] AugustBased on this discussion -g and --global seem like good candidates for the options to add to egrep.,3349
Extract Method,In FunctionalTest display CLI parameters upon parse error This utility is probably never run by hand but displaying the options is fairly simple.,3350
Inline Method,Use a CLI library consistently to parse parameters in all utilities. In a few utilities the command-line parsing is pretty lazy. There's no usage just an NPE if you don't provide all the magic options on the command-line. In particular Initialize doesn't use an off-the-shelf library for command-line parsing and it really should.See ACCUMULO-744.In addition many command-line utilities can and should be able to read accumulo-site.xml and do not need to provide username/password/instance/zookeeper information by default.,3351
Extract Method,Add support for importDirectory to the mock instance Adding import support for the mock instance is fairly trivial and is useful for testing mapreduce pipelines.I'm attaching a patch for 1.4.1 but forward-porting to 1.5 should be pretty easy.,3353
Rename Method,RFile should compress using common prefixes of key elements Relative keys have proven themselves as a great way to compress within dimensions of the key. However we could probably do better since we know that our data is sorted lexicographically we can make a reasonable assumption that we will get better compression if we only store the fact that a key (or portion of a key) has a common prefix with the previous key even if it is not an exact match.Currently in RFile unused bits from the delete flag byte are being used to store flags that show whether an element of the key is exactly the same as the previous or if it is different. We can change the semantics of these flags to store three states per element of the key: exact match as previous key has a common prefix as previous key no relative key compression. If we don't want to add a byte to store 2 bits for 3 states per element we can just take the ordinal value of the unused 7 bits of the delete flag field and map it to an enumeration of relative key flags.In the case of a common prefix flag enabled for a given element of the current key when reading the RFile we can interpret the first bytes of that element as a VInt expressing the length of the common prefix relative to the previous key's same element. Because this will add at least one byte to the the length of that element we will not want to use the common prefix compression if the common prefix is less than 2 bytes. For less than 2 bytes in common (1 or 0 bytes in common) we'd select the no compression flag for that element.,3355
Extract Method,"table namespaces A large cluster is a valuable shared resource. The current permission system and simple table naming structure does not allow for delegation of authority and safe partitioning within this shared resource.Use cases:# create a namespace (like ""test"") and delegate the {{grant}} permission to tables created in that namespace to a user that would manage those tables. Presently {{grant}} is never delegated.# create simple ""test"" and ""production"" namespaces that are trivial for users to switch between. For example instead of having tables ""test_index"" and ""test_documents"" the client would support ""index"" and ""documents"" with an API to support switching trivially between the the different environments.# create a set of tables in a namespace called ""latest"" This namespace is re-created periodically with a map-reduce job. If code changes inadvertently create a corrupt ""latest"" users can switch to the set of tables known as ""safest"" In this way users can experiment and provide feedback on incremental improvements while have a safe fallback.# two applications hosted on the same cluster that can share a table which has been ""aliased"" into their namespace. Namespace-local permissions are ignored but a (most likely read-only) view of the table is available. This would be helpful for reference tables.# quotas/priorities. Implement namespace-specific priorities and resource allocations. It is reasonable to run namespace-specific queries and ingest on production equipment. Large cluster resources are always limited and often the *only* place where near-production quality software can be run at full scale.",3356
Extract Method,Hadoop 2.0 Support We should start thinking about Hadoop 2 support now that it is Cloudera's recommended distribution and many new Hadoop users will probably be adopting it.When I investigated this first a few months ago it seemed like the biggest barrier to this was that all the Map/Reduce related tests are implemented using pseudo-private constructors from Hadoop 1.0 that are no-longer present in Hadoop 2.0.The main strategy to fix this should probably be to adopt the Map/Reduce cluster test object for testing the various Accumulo input formats instead of instrumenting them directly. I have used this convenience object successfully on tests utilizing MockInstance so I think it should work fine.There may also be some filesystem API issues but I don't think they will be too severe.The other main issue is that we will need to actually deploy on Hadoop 1 and 2 and run the integration tests once we start supporting both so that will be a headache for release testing that we should think through.,3360
Extract Method,Add option to pipe shell commands to a file When working in the shell it would be useful to direct scan output directly to a file for examination outside of the interactive shell.A simple solution is to add a -o/--output <file> option to scan commands in the shell. Other commands may benefit from such a feature also.,3362
Extract Method,Fate operations should be rolled into shell Fate is nifty but the only ways to deal with it are by directly calling some utility jars. We should roll this functionality into the shell to make it easier to manage.,3363
Extract Method,Add Mutation Constructor Accepting Byte Array. I was playing around with reversing the order of my keys in Accumulo. After manipulating my key I had a byte array and I needed to create a mutation. But there is no constructor for Mutation that accepts a byte array. I had to wrap mybyte array with a Text object (not very efficient).,3364
Rename Method,ColumnVisibilities should do more to create normalized representations of expressions ColumnVisibilities offer a `flatten()` functionality which attempts to normalize a given expression. So if I had an expression `b&a&c` and I `flatten`'d it I'd get back `a&b&c`. Through some testing I found that this is applied and not applied depending on how the expression is written. For instance if I had something like `(b&a)&c` I would get back something like `c&a&b`.It's not much more code to provide a correct normalized form of expressions as well do some more work to detect and eliminate expressions that boil down to `expr&expr` or `expr|expr`.I've attached a sample program that shows some output of what the current capability is and what I think it should output.,3366
Rename Method,Improve C++ support for thrift RPC code generation A user emailed me requesting better support for cpp code generation for our thrift RPC. The improved features requested were:# Include cpp namespace# Add --gen cpp line to thrift.sh# Rename major/minor to majors/minors to deconflict with sysmacro.h,3367
Rename Method,Support pluggable encryption in walogs There are some cases where users want encryption at rest for the walogs. It should be fairly trivial to implement it in such a way to insert a CipherOutputStream into the data path (defaulting to using a NullCipher) and then making the Cipher pluggable to users can insert the appropriate mechanisms for their use case.This also means swapping in CipherInputStream and putting in a check to make sure the Cipher type's match at read and write time. Possibly a versioning mechanism so people can migrate Ciphers.,3368
Extract Method,Support Side Inputs in Flink Streaming Runner The Flink Runner supports side inputs for batch mode but its missing support for streaming.,3369
Move Method,Make UnboundedSourceWrapper parallel As of now {{UnboundedSource}} s are executed with a parallelism of 1 regardless of the splits which the source returns. The corresponding {{UnboundedSourceWrapper}} should implement {{RichParallelSourceFunction}} and deal with splits correctly.,3371
Extract Method,"Set default-partitioner in SourceRDD.Unbounded. The SparkRunner uses {{mapWithState}} to read and manage CheckpointMarks and this stateful operation will be followed by a shuffle: https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala#L159Since the stateful read maps ""splitSource"" -> ""partition of a list of read values"" the following shuffle won't benefit in any way (the list of read values has not been flatMapped yet). In order to avoid shuffle we need to set the input RDD ({{SourceRDD.Unbounded}}) partitioner to be a default {{HashPartitioner}} since {{mapWithState}} would use the same partitioner and will skip shuffle if the partitioners match.",3372
Rename Method,Implement the API for Static Display Metadata As described in the following doc we would like the SDK to allow associating display metadata with PTransforms.https://docs.google.com/document/d/11enEB9JwVp6vO0uOYYTMYTGkr3TdNfELwWqoiUg5ZxM/edit?usp=sharing,3374
Rename Method,"Align the naming of ""generateInitialSplits"" and ""splitIntoBundles"" to better reflect their intention See [dev list thread|https://lists.apache.org/thread.html/ac5717566707153e85da880cc75c8d047e1c6606861777670bb9107c@%3Cdev.beam.apache.org%3E].",3375
Rename Method,Basic Java harness capable of understanding process bundle tasks and sending data over the Fn Api Create a basic Java harness capable of understanding process bundle requests and able to stream data over the Fn Api.Overview: https://s.apache.org/beam-fn-api,3376
Rename Method,Use Flink-native side outputs Once Flink has support for side outputs we should use them instead of manually dealing with the {{RawUnionValues}}.Side outputs for Flink is being tracked in https://issues.apache.org/jira/browse/FLINK-4460.,3379
Move Method,Create Dataflow Runner Package Move Dataflow runner out of SDK core and into new Dataflow runner maven module.,3380
Inline Method,Need Source/Sink for Spanner Is there a source/sink for Spanner in the works? If not I would gladly give this a shot.,3388
Rename Method,Add support for bounded sources in streaming mode 0,3389
Rename Method,"withCoder() error in JdbcIO JavaDoc example Follow the JavaDoc of JdbcIO job fails with exception:{code}Exception in thread ""main"" java.lang.IllegalStateException: JdbcIO.read() requires a coder to be set via withCoder(coder)at com.google.common.base.Preconditions.checkState(Preconditions.java:176)at org.apache.beam.sdk.io.jdbc.JdbcIO$Read.validate(JdbcIO.java:329)at org.apache.beam.sdk.io.jdbc.JdbcIO$Read.validate(JdbcIO.java:249)at org.apache.beam.sdk.Pipeline.applyInternal(Pipeline.java:419)at org.apache.beam.sdk.Pipeline.applyTransform(Pipeline.java:350)at org.apache.beam.sdk.values.PBegin.apply(PBegin.java:58)at org.apache.beam.sdk.Pipeline.apply(Pipeline.java:172){code}It requires a Coder provided by withCoder. Need to add it in the example.Other point is does it really need to specify a Coder? Users should register it with CoderRegistry I think.",3390
Move Method,Accumulable MetricsContainers. Make {{MetricsContainer}} accumulable. This can reduce duplication between runners and make implementing metrics easier for runner authors.,3391
Rename Method,Bigtable: improve user agent The bigtable-client-core has changed the way it generates user agent strings to automatically provide the information we were providing manually before. Update the BigtableIO client code to fit the new improved scheme.,3393
Rename Method,Fix use of deprecated Spark APIs in the runner. 0,3394
Inline Method,Thin Java SDK Core Before first stable release we need to thin out {{sdk-java-core}} module. Some candidates for removal but not a non-exhaustive list:{{sdk/io}}* anything BigQuery related* anything PubSub related* everything Protobuf related* TFRecordIO* XMLSink{{sdk/util}}* Everything GCS related* Everything Backoff related* Everything Google API related: ResponseInterceptors RetryHttpBackoff etc.* Everything CloudObject-related* Pubsub stuff{{sdk/coders}}* JAXBCoder* TableRowJsoNCoder,3395
Inline Method,Add a Hadoop FileSystem implementation of Beam's FileSystem Beam's FileSystem creates an abstraction for reading from files in many different places. We should add a Hadoop FileSystem implementation (https://hadoop.apache.org/docs/r2.8.0/api/org/apache/hadoop/fs/FileSystem.html) - that would enable us to read from any file system that implements FileSystem (including HDFS azure s3 etc..)I'm investigating this now.,3403
Rename Method,window support There're several methods TUMBLE/HOP/SESSION introduced in Calcite 1.12 which represent a window-aggregation operation. In BeamSQL it's expected to leverage these methods to determine window function set trigger strategy also handle event_time/watermark properly. ,3406
Rename Method,Move CloudObject to Dataflow runner This entails primarily eliminating Coder.asCloudObject() by adding the needed accessors and possibly a serialization registrar discipline for coders in the Runner API proto.,3407
Rename Method,Create Parquet IO Would be nice to support Parquet files with projection and predicates.,3408
Rename Method,Support for recursive wildcards in GcsPath When working with heavily nested folder structures in Google Cloud Storage it's great to make use of recursive wildcards which the current API explicitly does not support.This code hasn't been touched in 2 years so it's likely that simply no one's gotten around to it yet.,3409
Extract Method,Support custom user Jackson modules for PipelineOptions HadoopFileSystem was added with support for passing Hadoop Configuration through PipelineOptions in #BEAM-2031.This was done by making the ObjectMapper within PipelineOptionsFactory find and load Jackson modules using Jackson supported ServiceLoader pattern.Serializing PipelineOptions requires runners to also use an ObjectMapper which has been similarly configured.,3410
Extract Method,KafkaIO support to use start read time to set start offset This Kafka 0.10.x adds support for a searchable index for each topic based off of message timestamps. It enables consumer support for offset lookup by timestamp.So we can add a start read time to set start offset.,3412
Extract Method,make it easier to specify windowed filename policy It should be easier for Beam users to specify filename policy which understands windowing concepts.Discussion here https://lists.apache.org/thread.html/b53e437894eb511c9161d34fc77c657300b77a7be75f0fab6566b3d6@%3Cdev.beam.apache.org%3E,3413
Extract Method,PCollection as a Table To support method {{registerPCollectionAsTable}} in BeamSQL DSL.This feature makes it possible to break down a complex query into several sub-queries and assemble it by developers.,3414
Rename Method,Make the write transform of HBaseIO simpler HBaseIO imitated the interface of Cloud Bigtable to have an easy migration path for users from/to Bigtable. Bigtable Mutation object does not include the row key so the BigtableIO needed to have a KV with the row key and the Mutation in order to change the data. Hbase does not have this restriction because the row key is part of the Mutation object this issue is to simplify the Write transform even if it will make a small difference,3415
Extract Method,BeamSqlRecordType should migrate to using a builder pattern via AutoValue/AutoBuilder This is a code health and usability issue. Performing the migration now to use AutoValue/AutoBuilder will make it easier for people to create table row types without needing to worry about mutability.,3417
Extract Method,KinesisIO watermark based on approximateArrivalTimestamp In Kinesis we can start reading the stream at some point in the past during the retention period (up to 7 days). With current approach for setting record's timestamp and watermark (both are always set to current time i.e. Instant.now()) we can't observe the actual position in the stream.So the idea is to change this behaviour and set the record timestamp based on the [ApproximateArrivalTimestamp|http://docs.aws.amazon.com/kinesis/latest/APIReference/API_Record.html#Streams-Type-Record-ApproximateArrivalTimestamp]. Watermark will be set accordingly to the last read record's timestamp. ApproximateArrivalTimestamp is still some approximation and may result in having records with out-of-order timestamp's which in turn may result in some events marked as late. This however should not be a frequent issue and even if it happens it should be a matter of milliseconds or seconds so can be handled even with a tiny allowedLateness setting,3418
Extract Method,Reading Kinesis records in the background Currently Kinesis records are read on demand in a runner's thread. We may instead read the records in a background with separate threads and store time in the buffer which will result in a major performance improvement.,3419
Extract Method,Handling Kinesis shards splits and merges Kinesis stream consists of [shards|http://docs.aws.amazon.com/streams/latest/dev/key-concepts.html#shard] that allow for capacity scaling. In order to increase/decrease the capacity shards have to be split/merged together. Such operations are currently not handled properly and will end with errors.,3420
Move Method,TextIO should support watching for new files Motivation and proposed implementation in https://s.apache.org/textio-sdf,3421
Extract Method,Improve error message for missing required options of Beam pipeline 0,3422
Move Method,"ElasticsearchIO breaks for non locally accessible DB We have a ES (2.4) database running in production that is not accessible from dev-machines because of VPS firewall rules. However we do create the Beam graph locally on our machines and submit it to Google Dataflow for execution. However because of this check (line 213){code:java}private static void checkVersion(ConnectionConfiguration connectionConfiguration)throws IOException {RestClient restClient = connectionConfiguration.createClient();Response response = restClient.performRequest(""GET"" """" new BasicHeader("""" """"));JsonNode jsonNode = parseResponse(response);String version = jsonNode.path(""version"").path(""number"").asText();boolean version2x = version.startsWith(""2."");restClient.close();checkArgument(version2x""ConnectionConfiguration.create(addresses index type): ""+ ""the Elasticsearch version to connect to is different of 2.x. ""+ ""This version of the ElasticsearchIO is only compatible with Elasticsearch v2.x"");}{code}the creation of the graph fails because it can't complete the restClient request from our dev machines. This check should probably not happen this early since there is no guarantee that the database is reachable when building the graph{code:java}exception in thread ""main"" java.net.ConnectExceptionat org.apache.http.nio.pool.RouteSpecificPool.timeout(RouteSpecificPool.java:168)at org.apache.http.nio.pool.AbstractNIOConnPool.requestTimeout(AbstractNIOConnPool.java:561)at org.apache.http.nio.pool.AbstractNIOConnPool$InternalSessionRequestCallback.timeout(AbstractNIOConnPool.java:822)at org.apache.http.impl.nio.reactor.SessionRequestImpl.timeout(SessionRequestImpl.java:183)at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processTimeouts(DefaultConnectingIOReactor.java:210)at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvents(DefaultConnectingIOReactor.java:155)at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:348)at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:192)at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:64)at java.lang.Thread.run(Thread.java:745){code}",3423
Extract Method,WindowFnTestUtils should allow using the value in addition to the timestamp of the elements {{WindowFnTestUtils}} relies only on timeStamps for everything related to windows assignment in the test helpers. But when creating a custom {{WindowFn}} (and most likely CustomWindow as well) that {{WindowFn}} might rely on element value in addition to timestamp to decide the windows that will be assigned to the element. To be able to test this kind of custom WindowFn we need versions of the helper methods in WindowFnTestUtils that allow passing {{TimeStampedValues}}.,3424
Extract Method,Apache Apex Runner Like Spark Flink and GearPump Apache Apex also does have advantages. Is it possible to have a runner for Apache Apex?,3425
Extract Method,Introduce Create.ofProvider(ValueProvider) When you have a ValueProvider<T> that may or may not be accessible at construction time a common task is to wrap it into a single-element PCollection<T>. This is especially common when migrating an IO connector that used something like Create.of(query) followed by a ParDo to having query be a ValueProvider.Currently this is done in an icky way (e.g. https://github.com/apache/beam/blob/master/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/datastore/DatastoreV1.java#L615)We should have a convenience helper for that.,3428
Rename Method,Make MinLongFn and MaxLongFn mimic SumLongFn and use BinaryCombineLongFn Ditto for the other 'optimized accumulator' combiner functions.,3429
Extract Method,BigtableIO should use ValueProviders [https://github.com/apache/beam/pull/2057] is an effort towards BigtableIO templatization. This Issue is a request to get a fully featured template for BigtableIO.,3430
Extract Method,Set user-specified Transform names on Flink operations Currently we don't always set a name on the generated operations or we set the wrong name. For example in the batch translation we set the result of {{PTransform.getName()}} as the name which is only the name of the {{PTransform}} itself not the name that the user specified when creating a Pipeline.,3432
Extract Method,improve logs in ElasticsearchIO test utils 0,3433
Extract Method,"Support file scheme in TextIO When users use {{TextIO}} most of the time they provide a full file URI: {{file:/tmp/foo}}. Unfortunately the {{file}} schema is not supported by {{TextIO}} and it fails with ""No handler found"". It's not easy for users to figure it out.We should support {{file}} schema to provide better flexibility to users.",3434
Extract Method,ElasticsearchIO should allow the user to optionally pass id type and index per document *Dynamic documents id*: Today the ESIO only inserts the payload of the ES documents. Elasticsearch generates a document id for each record inserted. So each new insertion is considered as a new document. Users want to be able to update documents using the IO. So for the write part of the IO users should be able to provide a document id so that they could update already stored documents. Providing an id for the documents could also help the user on indempotency. *Dynamic ES type and ES index*: In some cases (streaming pipeline with high throughput) partitioning the PCollection to allow to plug to different ESIO instances (pointing to different index/type) is not very practical the users would like to be able to set ES index/type per document. ,3435
Extract Method,Add serviceEndpoint parameter to KinesisIO KinesisClient can be instantiated with a different serviceEndpoint to the official Amazon one. This allows users to test KinesisIO locally by overwriting the endpointUrl and pointing to an emulator like https://github.com/localstack/localstack or https://github.com/mhart/kinesalite,3436
Rename Method,Update KinesisIO to use AWS SDK 1.11.255 and KCL 1.8.8 The current version of the AWS SDK that Kinesis uses does not include new regions/AZ from AWS. This update solves this as well as include the most recent fixes on the SDK.,3437
Rename Method,Change Filter#greaterThan etc. to actually use Filter This is a good starter task.Right now [{{Filter#greaterThan}}|https://github.com/apache/incubator-beam/blob/315b3c8e333e5f42730c19e89f856d778ce93cab/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Filter.java#L134] constructs a new DoFn rather than using {{Filter#byPredicate}}. We should fix this to make it consistent and simpler.We can also remove deprecated functions in that file and if possible redundant display data.,3438
Move Method,[SQL] Refactor BeamRelNodes into PTransforms BeamRelNode exposes PCollection<BeamRecord> buildBeamPipeline() which builds a pipeline when parsing. It feels like it should instead implement a PTransform<PCollection<BeamRecord> PCollection<BeamRecord>> which would receive a prepared PCollection and apply sub-expressions instead of manually invoking expression evaluation to get the input. And maybe consider building it lazily.,3441
Rename Method,Make MetricQueryResults and related classes more json-serialization friendly When working on this PR [https://github.com/apache/beam/pull/4548] MetricQueryResults needed to be serialized to be pushed to a metrics sink. As they were it required a custom serializer that just calls the name() counter() committed() attempted() ... methods. MetricQueryResults are so close to be serializable with the default serializer just need the accessors to be renamed get* that creating DTO objects with get* methods to just call the non-get methods seems unnecessary.  So just rename public accessors to get* on the experimental API,3443
Rename Method,Add tests for Flink DoFnOperator side-input checkpointing 0,3444
Move Method,Refactor HBaseIO splitting to produce ByteKeyRange objects This allows to reuse the splitting logic for a future SDF-based implementation by reusing it as part of the @SplitRestriction method.,3445
Extract Method,Add HBaseIO.readAll() based on SDF Since the support from runners is still limited it is probably wise to create a first IO based on the current SDF batch implementation in Java to validate/test it with a real data-store. Since HBase partitioning model is quite straightforward it is a perfect candidate.,3446
Extract Method,Portable Flink runner JobService entry point in a Docker container The portable Flink runner exists as a Job Service that runs somewhere. We need a main entry point that itself spins up the job service (and artifact staging service). The main program itself should be packaged into an uberjar such that it can be run locally or submitted to a Flink deployment via `flink run`.,3447
Rename Method,Split IOTestPipelineOptions to multiple test-specific files Currently we have one big IOTestPipelineOptions interface that is used in many IOITs. It contains test specific options that should rather be located next to testing classes not in a generic file. Let's split this. Additionally besides separation of concerns  this will allow adding test-specific @Default and @Required annotations and validate the options better. ,3448
Rename Method,Introducing gcpTempLocation that default to tempLocation Currently DataflowPipelineOptions.stagingLocation default to tempLocation. And it requires tempLocation to be a gcs path.Another case is BigQueryIO uses tempLocation and also requires it to be on gcs.So users cannot set tempLocation to a non-gcs path with DataflowRunner or BigQueryIO.However tempLocation could be on any file system. For example WordCount defaults to output to tempLocation.The proposal is to add gcpTempLocation. And it defaults to tempLocation if tempLocation is a gcs path.StagingLocation and BigQueryIO will use gcpTempLocation by default.,3450
Rename Method,"Consider enabling spotless java format throughout codebase ""Spotless"" can enforce - and automatically restore - automatic Java formatting. Whenever formatting is off it tells a user the exact command to fix it. It isn't (just) about code layout it is about automation. We have pretty strict style rules enforced by checkstyle. The most efficient way to fix up a file is with autoformat. But if the autoformat hits a bunch of irrelevant lines that is annoying for a reviewer and obscures git blame. If we enforce autoformat all the time then it makes sure that autoformatting a particular PR has minimal effects and is always safe to do.",3451
Rename Method,Improve IOChannelUtils.resolve() to accept multiple paths at once Currently IOChannelUtils.resolve() method can only resolve one path against base path. It's useful to have another method with arguments that includes one base path and multiple others. The return string will be a directory that start with base path and append rests which are separated by file separator.,3452
Extract Method,BigQueryIO.Read reimplemented as BoundedSource BigQueryIO.Read is currently implemented in a hacky way: the DirectPipelineRunner streams all rows in the table or query result directly using the JSON API in a single-threaded manner.In contrast the DataflowPipelineRunner uses an entirely different code path implemented in the Google Cloud Dataflow service. (A BigQuery export job to GCS followed by a parallel read from GCS).We need to reimplement BigQueryIO as a BoundedSource in order to support other runners in a scalable way.I additionally suggest that we revisit the design of the BigQueryIO source in the process. A short list:* Do not use TableRow as the default value for rows. It could be Map<String Object> with well-defined types for example or an Avro GenericRecord. Dropping TableRow will get around a variety of issues with types fields named 'f' etc. and it will also reduce confusion as we use TableRow objects differently than usual (for good reason).* We could also directly add support for a RowParser to a user's POJO.* We should expose TableSchema as a side output from the BigQueryIO.Read.* Our builders for BigQueryIO.Read are useful and we should keep them. Where possible we should also allow users to provide the JSON objects that configure the underlying intermediate tables query export etc. This would let users directly control result flattening location of intermediate tables table decorators etc. and also optimistically let users take advantage of some new BigQuery features without code changes.* We could use switch between whether we use a BigQuery export + parallel scan vs API read based on factors such as the size of the table at pipeline construction time.,3453
Extract Method,BigQueryIO.Write: reimplement in Java BigQueryIO.Write is currently implemented in a somewhat hacky way.Unbounded sink:* The DirectPipelineRunner and the DataflowPipelineRunner use StreamingWriteFn and BigQueryTableInserter to insert rows using BigQuery's streaming writes API.Bounded sink:* The DirectPipelineRunner still uses streaming writes.* The DataflowPipelineRunner uses a different code path in the Google Cloud Dataflow service that writes to GCS and the initiates a BigQuery load job.* Per-window table destinations do not work scalably. (See Beam-XXX).We need to reimplement BigQueryIO.Write fully in Java code in order to support other runners in a scalable way.I additionally suggest that we revisit the design of the BigQueryIO sink in the process. A short list:* Do not use TableRow as the default value for rows. It could be Map<String Object> with well-defined types for example or an Avro GenericRecord. Dropping TableRow will get around a variety of issues with types fields named 'f' etc. and it will also reduce confusion as we use TableRow objects differently than usual (for good reason).* Possibly support not-knowing the schema until pipeline execution time.* Our builders for BigQueryIO.Write are useful and we should keep them. Where possible we should also allow users to provide the JSON objects that configure the underlying table creation write disposition etc. This would let users directly control things like table expiration time table location etc. Would also optimistically let users take advantage of some new BigQuery features without code changes.* We could choose between streaming write API and load jobs based on user preference or dynamic job properties . We could use streaming write in a batch pipeline if the data is small. We could use load jobs in streaming pipelines if the windows are large enough to make this practical.* When issuing BigQuery load jobs we could leave files in GCS if the import fails so that data errors can be debugged.* We should make per-window table writes scalable in batch.Caveat possibly blocker:* (Beam-XXX): cleanup and temp file management. One advantage of the Google Cloud Dataflow implementation of BigQueryIO.Write is cleanup: we ensure that intermediate files are deleted when bundles or jobs fail etc. Beam does not currently support this.,3456
Extract Method,Optimize FileBasedSink's WriteOperation.moveToOutput() moveToOutput() methods in FileBasedSink.WriteOperation implements move by copy+delete. It would be better to use a rename() which can be much more effective for some filesystems. Filesystem must support cross-directory rename. BEAM-4861 is related to this for the case of HDFS filesystem. Feature was discussed here: http://mail-archives.apache.org/mod_mbox/beam-dev/201807.mbox/%3CCAF9t7_4Mp54pQ+vRrJrBh9Vx0=uaKnuPZD_qdh_QDm9VXLLsZw@mail.gmail.com%3E,3457
Extract Method,Add MD5 consistency check on S3 uploads (writes) 0,3458
Rename Method,Support Dynamic PipelineOptions During the graph construction phase the given SDK generates an initialexecution graph for the program. At execution time this graph isexecuted either locally or by a service. Currently Beam only supportsparameterization at graph construction time. Both Flink and Spark supplyfunctionality that allows a pre-compiled job to be run without SDKinteraction with updated runtime parameters.In its current incarnation Dataflow can read values of PipelineOptions atjob submission time but this requires the presence of an SDK to properlyencode these values into the job. We would like to build a common layerinto the Beam model so that these dynamic options can be properly providedto jobs.Please seehttps://docs.google.com/document/d/1I-iIgWDYasb7ZmXbGBHdok_IK1r1YAJ90JG5Fz0_28o/editfor the high-level model andhttps://docs.google.com/document/d/17I7HeNQmiIfOJi0aI70tgGMMkOSgGi8ZUH-MOnFatZ8/editforthe specific API proposal.,3461
Rename Method,Portable Flink support for maxBundleSize/maxBundleMillis The portable runner needs to support larger bundles in streaming mode. Currently every element is a separate bundle which is very inefficient due to the per bundle SDK worker overhead. The old Java SDK runner already supports these parameters.,3462
Rename Method,Allow registering UDF with the same method name but different argument list 0,3463
Rename Method,SimpleStreamingWordCountTest does not properly test fixed windows {{org.apache.beam.runners.spark.translation.streaming.SimpleStreamingWordCountTest}} does not properly test {{FixedWindows}}.,3470
Inline Method,"SplittableDoFn SplittableDoFn is a proposed enhancement for ""dynamically splittable work"" to the Beam model.Among other things it would allow a unified implementation of bounded/unbounded sources with dynamic work rebalancing and the ability to express multiple scalable steps (e.g. global expansion -> file sizing & parsing -> splitting files into independently-processable blocks) via composition rather than inheritance.This would make it much easier to implement many types of sources to modify and reuse existing sources. Also it would improve scalability of the Beam model by moving things like splitting a source from the control plane (where it is today -- glob -> List<FileBasedSource> sent over service APIs) into the data plane (PCollection<Glob> -> PCollection<FileName> -> ...).",3471
Rename Method,"Add public TypedPValue.setTypeDescriptor This would give fairly pithy answers to StackOverflow questions sometimes.When choosing between .getOutputCoder() and .getOutputTypeDescriptor() for a transform/DoFn we often choose the type so the coder registry can do its thing.This would also give a similar choice between .setCoder(...) and .setTypeDescriptor(...).And anyhow we have the intention of removing our practice of the ""*Internal"" suffix so this one might be most easily solved by making it public.",3473
Rename Method,Use AutoValue and deal with Document instead of String in MongoDbIO 0,3474
Inline Method,Remove legacy credentials flags related to GCP and adopt application default credentials as only supported default flow Drop the following GcpOptions and use ADC (https://developers.google.com/identity/protocols/application-default-credentials) to clean-up credentials story for GCP:AuthorizationServerEncodedUrlTokenServerUrlCredentialDirCredentialIdSecretsFileServiceAccountNameServiceAccountKeyfileAlso migrate from Apiary Credentials class to Google OAuth Credentials class when available from google-cloud-java.,3476
Extract Method,Validate PipelineOptions Default annotation It shouldn't allow @Override with @Default annotation for example the following is broken:interface A {@Default.Integer(1)Integer getFoo();void setFoo();}interface B extends A {@Default.Integer(-1)@OverrideInteger getFoo();}It is broken because PipelineOptions default values are lazily evaluated. And it will depends on which one of the two following operations happen first:options.as(A.class) and options.as(B.class)If users want to change the default value users should do setFoo(...) explicitly.It shouldn't allow adding Default annotation as well.,3477
Rename Method,Support GroupByKey directly Currently the SparkRunner supports GroupByKey via override with GroupByKeyViaGroupByKeyOnly make it support GroupByKey directly.,3478
Extract Method,ParDo Chaining Current state of Apex runner creates a plan that will place each operator in a separate container (which would be processes when running on a YARN cluster). Often the ParDo operators can be collocated in same thread or container. Use Apex affinity/stream locality attributes for more efficient execution plan.,3479
Rename Method,Use New DoFn Directly in Flink Runner 0,3480
Move Method,Add ability to write/sink results to MongoDBGridfs 0.3 added the ability to read files from gridfs for processing. It would be good to have the Sink side working to allow writing the results there as well.,3482
Rename Method,ProxyManager enhancements As brett pointed out the DefaultProxyManager needs some refactoring especially on these points:- remove cache period- get() ignores the return of getRemoteFile()- make the proxy check for the checksum and metadata files first before parsing the path for an artifact- getArtifactFile should use artifact.setFile() and not return the file anymore.- temp should be setup to deleteOnExit()- rename copyTempToTarget to moveTempToTarget- rename prepare/release checksums to prepare/release ChecksumListeners- use FileUtils.read when applicable,3483
Rename Method,"Search Usability Taken from Bretts email to the list{noformat}- [ ] improve the search results page- [ ] remove metadata files- [ ] merge versions in search results- [ ] for snapshots just show SNAPSHOT not timestamps- [ ] show hits in the results (this may not be possible or needed with better results however)- [ ] existing JIRA complaints- [ ] MRM-732 (tokenizing)- [ ] MRM-495 (weighting)- [ ] MRM-609 (windows bug - may be fixed)- [ ] MRM-933 (hit count pagination completely busted)- [ ] advanced search- [ ] improve appearance and flexibility maybe change to ""add term"" buttons on the default search- [ ] class/package search is still flaky- [ ] might be the analyzer rules etc. for splitting on '.'- [ ] browse improvements- [ ] artifact version list should show basic shared project information rather than having to drill into one version- [ ] snapshot should go to a page that shows a list of versions(go to latest but list previous snapshots){noformat}",3484
Move Method,ProxyConfiguration enhancements According to brett these are the need improvements:- make repoCache a member of the ProxyManager which will remove ArtifactRepositoryFactory- browsable should be a configuration of the webapp- remove field storage for the layout and use plexus to lookup valid values- loadMavenProxyConfiguration should be in a separate reader class,3485
Extract Method,Use cacheFailure configuration in remote repositories Currently a todo in the source code,3486
Extract Method,"Option to force scanning of an artifact/repository regardless of file dates Often I try to browse an artifact and see ""Unable to find project model"" even though I know the artifact is in the repository. By default when Archiva scans a repo it only ""sees"" files added since the last time the scan was run.Currently the only way I know to re-set it is to touch the files on disk so they appear newer or delete the database so that Archiva will start over with scanning neither of which is ideal.I need a way to force scanning of artifact(s) or an entire repository regardless of the file dates.If the 'Last Scanned' date on the Repositories page were configurable that would allow me to set the date far back in time so that all files would appear to be ""new"" and it would scan everything again.Or some way to give it a groupId or groupId+artifactId and have it go fix its database for that subset of the repo contents.",3487
Inline Method,Remove unnecessary maven-proxy classes from the remote-proxy 0,3488
Extract Method,"migrate Archiva's ""browse"" functionality to use the metadata content repository API the first steps to removing direct interaction with the database - see MRM-1025 for more information.A test of the API's implementation will be that we can utilise it directly from the webapp action classes without introducing additional business logic and such that we can comfortably use it in the same way from the XMLRPC module. However we also do not want presentation-related logic (or at least the organisation of the information on screen) in the repository implementation. If that is needed a simpler replacement abstraction for RepositoryBrowsing may be appropriate.",3498
Extract Method,"migrate repository statistics to the metadata content repository currently after every scan repository statistics are persisted to the database. These could easily be stored in the metadata repository. While at present we will keep the ""per-scan"" approach in the future this might better be represented as a regularly stored snapshot / built in versioning of the metadata. It could potentially be adding this information on the fly instead of at scan by adding a callback in the repository API when an artifact / project is created - however this will be approached later.When migrating the functionality it should be offered as a standalone plugin (the only dependencies should be in the webapp that need to render the information and trigger some cleanup).The following changes will be needed:- save the statistics in ArchivaRepositoryScanningTaskExecutor / ArchivaRepositoryScanningTaskExecutorTest- reset statistics in EditManagedRepositoryAction / EditManagedRepositoryActionTest when location changes- query for a prior successful scan in RepositoryArchivaTaskScheduler- retrieve last scan info in RepositoryAction and update corresponding JSP to display information correctly- generate report action to query scan statistics between given dates and aggregate",3512
Extract Method,improve search results page currently the search results are very plain. We should:- provide more information about the artifact- provide more information about the relevance of the result- provide information about how the search hit occurred (what field)- paginate the results,3513
Rename Method,complete artifact browsing currently some features of the artifact display page are not implemented (links to dependencies displaying other information from the pom and so on),3514
Extract Method,implement alternative or improve repository metadata storage The biggest thing to look at is metadata-repository-file. I threw this together with property files quickly and there's no optimisation or even exception handling. We need to look at the right way to approach this - a more robust implementation of a file system store (properties or xml) is definitely workable but would need to be combined with something like a Lucene index (as in Archiva 0.9) to make some of the operations fast enough. What I would like to look at instead is using JCR (with file system persistence - not a database!) to see how well it reacts to a lot of operations. As you can tell from the docs the storage is tailored to living in a hierarchical content repository in whatever form that takes and the storage is isolated behind an API.,3515
Extract Method,improve performance of the browse interface currently the browser reads the entire index to be able to present some pages. It should be able to just read the terms and some of this information should be cached,3521
Extract Method,"Add simple 'CRUD' pages for project-level metadata along with a ""generic metadata"" plugin See for more details:http://mail-archives.apache.org/mod_mbox/archiva-dev/201003.mbox/%3c70CA8566-051F-4CDA-A09D-622641AD8548@apache.org%3e",3522
Extract Method,complete the proxy interface the current proxy interface is not well integrated and the tests are failing. Complete this integration.,3525
Extract Method,Generic metadata should be searcheable in Archiva search 0,3532
Rename Method,Web services for repository merging or artifact promotion I think CRUD for managed repositories should be a pre-requisite of this.,3533
Extract Method,remove use of plexus-spring remove all use of plexus annotations to @Inject annotations.and don't use anymore plexus-spring.done in redback,3534
Extract Method,Expose Archiva services trough REST 0,3535
Extract Method,configure http connection pool values for wagon http to setup resources : maxTotal maxPerroute.,3543
Move Method,add rest method to delete artifact. a rest call to delete an artifact in a repository.,3544
Extract Method,ability to have multiple reports currently the reports are all aggregated into one representation and execution. It should be possible to have separate groups with just the health ones tied to the indexing and the others run on demand.,3545
Extract Method,Allow different implementation of transfer mechanism in the DefaultRepositoryProxyConnectors class We would like to use a different implementation for transferring an artifact from the remote proxy. This particular remote proxy we have is not Maven repository. Our transport library handles the bridging work so we would like to use it instead of the default implementation using Wagon.Could you make a change in the DefaultRepositoryProxyConnectors so that it can be extendable? We would just want to override the transport method which uses Wagon.Please take a look at the submitted patch.Thanks.,3549
Rename Method,ldap configuration editable with the ui. 0,3550
Inline Method,"Chaining user manager implementations It would be really helpful to have a ""chain"" of authentication modules where if the first one fails a second one is queried.The main use-case here is if one has LDAP user auth configured (typical corporate environment) but also needs a few technical users (like a Jenkins instance) which are not in the corporate LDAP. A side benefit for that is that one can hard-code a ""backup-administrator"" which can be used if one accidentally breaks the LDAP configuration or if the LDAP isn't available at all.",3551
Rename Method,path of merged index for group configurable 0,3552
Rename Method,"build merged index for groups with a cron schedule the current merged index for groups is generated ""on the fly"" (with some caching to avoid generation on each request).We can add a cron schedule to generate it.",3553
Rename Method,repogroup merged index ttl configurable per repository group currently this ttl is system property value.It must be configurable per repository group.,3555
Extract Method,"provide mechanism to obtain the latest version of an artifact It will be very useful and convenient for a user to be able to download the ""latest"" version of an artifact. Sonatype Nexus provides such a mechanism please see http://stackoverflow.com/questions/7911620/using-the-nexus-rest-api-to-get-latest-artifact-version-for-given-groupid-artfic and https://maven.java.net/nexus-core-documentation-plugin/core/docs/rest.artifact.maven.redirect.htmlThanks!",3556
Rename Method,Upload (deploy) artifacts to a repository - via a web form (not using wagon) The web interface should allow to upload artifacts to the repository. For M1 one could just ftp the artifacts as neededm but with M2 having to go through the file:deploy plugin is a pain. Archiva could help a lot here,3557
Extract Method,caching repository query interface need to be able to query the repository to see if artifacts exist what versions are available etc. This needs to interoperate with the indexing and where applicable should cache information balancing the need to keep a low memory overhead but avoid repetitive disk reads on metadata files.,3558
Inline Method,just-in-time consumption of repository changes need to have a simple reusable way to be able to trigger consumers for a given artifact just-in-time for things like addition via a proxy removal via purge etc.,3559
Extract Method,Proxy should work transparently with either maven 1.x or 2.x repositories. Have the maven-proxy act as either a maven 1.x or maven 2.x repository depending on which client accesses it. Brett believed this should be managable with aliasing.This was originally posted as a maven-proxy feature at http://jira.codehaus.org/browse/MAVENPROXY-39,3560
Rename Method,Virtual repositories or repository grouping A number of managed repositories can be grouped together with that group having only one url. So you only need to specify that url in the settings.xml file and when Archiva receives a request via that url it would look for that artifact from the repositories belonging to that group. More details are dicussed here:http://www.nabble.com/Archiva-1.1-Roadmap-td15262645.html#a15263879,3563
Rename Method,Support inclusion of pom file when deploying an artifact via the web upload form Currently the web upload form only supports pom generation when uploading M2 artifacts.,3567
Extract Method,Removal of Archiva-Webdav implementation in favor of Jackrabbit-webdav This patch removes plexus-webdav in favor of Jackrabbit's webdav servlet implementation. It is not yet 100% completed and tested. HTTP GET and PUT should work correctly.The following needs to happen before integration:1) New Jackrabbit classes need to be correctly unit tested.2) Webdav properties need to be implemented3) Testing of common webdav clients - Mac OS X Windows Wagon-Dav etc,3568
Extract Method,aggregate indices for repository groups 0,3570
Move Method,Replace Company POM feature with simple Appearence customisation See http://www.nabble.com/MRM-604---Company-POM-for-Appearance-td17395296.htmlThis feature will be replaced with a simple form that allows you to set your organisation image url and name.,3572
Inline Method,When deploying artifacts to the repo they should be added to the index instantly 0,3573
Extract Method,Searching within search results 0,3574
Rename Method,create a second index with a subset of information and have it compressed required for the eclipse plugin. Will need to follow up with Jason on exactly what data is required.,3575
Extract Method,Ability to delete an artifact from the web interface Sometimes when viewing an artifact through the Archiva Web UI I'd like to delete it from the repository.Currently the only way to do this by deleting it from the managed repository filesystem.,3576
Inline Method,allow plugins to handle deletion of artifacts through repository purge currently the repository purge directly calls the database and then the index to cleanup after purging artifacts. This couples the core consumers to those modules and also prevents any plugins that wish to store artifact metadata from doing the same.Note there are also some issues: the cleanup released snapshots doesn't cleanup after itself.These can both be fixed by moving the deletion logic to the respective modules and using an event from the purge to trigger their call,3577
Extract Method,"Patch for several issues while processing poms (effective model expressions resolving storing to database) I'm submitting a bigger patch for serveral issues that drived my crazy after installation of archiva in our company.The patch is for 1.1.1 and 1.1.2 releases. The patch:- fixes problems with expression resolving (${pom.version}) in dependencies- adds support for parent.{groupId artifactId version) properties- fixes issues with jdo detachCopy called from ProjectModelToDatabaseListener while creating effective pom- fixes inconsistency in key format used in effective model cache- add merging parentProject and properties while creating effective pom- ArchivaProjectModel.setOrgin(""filesystem"") was moved to readers (where IMO should be)- adds few new test cases.I will be very happy if you apply the patch on the code base. It will save me a bit of work with merging my changes with new releases in the future.",3578
Extract Method,Make membership management depend on the principal name not the URI of the principal since the principal name is already unique. As per thread on sling-dev -----------------------You are probably right the principal name is sufficient to find theauthorizable. Can you make the patch?Thanks!On Fri Jun 19 2009 at 7:43 AM Ian Boston <ieb@tfd.co.uk> wrote:I was wonderingcurl -F:member=/system/userManager/user/ieb http://admin:admin@localhost:8080/system/userManager/group/g-group1.update.jsonappears to be the way to add members to groupsorcurl -F:member=../../ieb http://admin:admin@localhost:8080/system/userManager/group/g-group1.update.jsonI can see where this is in the code but it looks a bit odd and slightlyhard for a UI developer to work with whencurl -F:member=ieb http://admin:admin@localhost:8080/system/userManager/group/g-group1.update.jsonwould have worked. Was there a reason I am missing this is like it is ?I would like to change it happy to do a patch.Ian----------------------------------,3579
Rename Method,Remove dependency to DS Currently the jcr install requires the declarative services. In order to have minimal dependencies an activator in combination with service trackers should be used instead.,3581
Inline Method,Make script resolution cacheable The ResourceCollector object needs to be enhanced in order to use it as a key for caching the script resolution. Besides the extension selectors etc. it should contain the resource type and resource super type.,3582
Rename Method,Replace MimeTypeService (commons/mime) by [Sling]ServletContext registered as a Service The commons/mime project with the MimeTypeService has been introduced to be able to provide MIME type mapping to the Content Loader in the former sling-content-jcr bundle and a ServletContext to access the MIME type mapping was not available. Extensibility has been reached by defining a MimeTypeProvider service which might be implemented by other bundles to enhance MIME type mapping in the MimeTypeService.With the new Sling API we defined a new ServletResolver interface which in real Sling resolves resource types to servlets registered as OSGi service. These servlets have to be initialized with a ServletConfig and a ServletContext. If we want to provide ServletResolver implementations outside the core project (which I strive to) we have to have a way to provide the ServletContext from the main Sling servlet to the ServletResolver.So I propose ...* to have the main sling servlet (SlingMainServlet) register the ServletContext as a javax.servlet.ServletContext service which would be available to all intersted parties.* to remove the commons/mime project as MIME type mapping may be used by accessing the ServletContext service* drop the MimeTypeResovler interface functionality completely. To extend the registered ServletContext service with new mappings the ServletContext service will be a ManagedService whose configuration will be able to configure MIME type mappings.,3584
Extract Method,Use smaller orderable tasks in jcrinstall's osgi installer See proposal at http://markmail.org/message/a6xx4dawsokl6lpx . This should help improve bundle management as discussed in http://markmail.org/message/ld6tkz6fdseknntx,3586
Rename Method,Recompile jsps on modifications and avoid periodic check Currently the jsp script handler checks a jsp for modifications when the jsp is called. This check has a configurable time intervall.With the latest changes we have resource events and therefore could recheck the jsps on events and avoid the per request check completly.,3587
Rename Method,Use commons dynamic class loader The jsp scripting should use the new commons class loader instead of doing a dynamic import package * and using the repository class loader.,3589
Rename Method,Use new commons dynamic class loader instead of dynamic import package * The new commons dynamic class loader provides a more robust way of dynamic class loading than relying on the dynamic import package *,3591
Extract Method,Ensure recent commons/log bundle builds can be installed on Sling 3 Current builds of commons/log require the availability of org.osgi.framework package 1.4. Since the bundle does not really use any of the 1.4 version functionality we can as well require version 1.3 and thus be able to deploy the recent builds on Sling 3 releases which are based on Felix framework 1.0.4 and hence org.osgi.framework 1.3.Another issue is the registration of the Log Service Panel: Since the version of the Sling Engine bundle installed with Sling 3 (2.0.2-incubator) causes a ClassCastException for any servlet service not registered with either sling.core.servletName or component.name service property the log bundle web console plugin provides a sling.core.servletName property to prevent this error. This property has no further effect than preventing the failure.,3593
Extract Method,SlingFileUploadHandler removes any existing nodes preventing file upload versioning When uploading a file to the location of an existing file the SlingFileUploadHandler removes the old node before creating the new one. That's sometimes necessary for example in the cases where a typeHint is provided for the upload that differs from the type of the existing node but it's not always required. The current behaviour prevents us maintaining a version history for uploaded resources.I'll attach a patch for this in a sec.,3595
Extract Method,Try to encode map keys for value map implementations The current value map implementations (JcrPropertyMap and JrModifiablePropertyMap) try to directly use the provided map key as a property name. If the name contains illegal characters (for a JCR) then setting/getting these properties fails.Therefore an encoding should be used to avoid these cases.We'll use the ISO9075 encoding as a start. Adding this encoding should be compatible to the current implementation as it just adds new use cases.,3597
Extract Method,JsonQueryServlet should return Calendar properties in the same format as the JsonRendererServlet When the JsonQueryServlet returns Calendar properties it doesn't use the same date format as the JsonRendererServlet. In order to have a better consistency between both servlets the JsonQueryServlet should use the JsonResourceWriter.,3599
Rename Method,Replace Resource.getRawData and .getObject methods by better API David brought up an issue on the dev list [1] regarding the Resource.getRawData() method. In short David suggests to replace the getRawData method with a signature which more closely reflects the definition of Sling as a web application framework for JCR. The general consesus on the list is that the getRawData() method is badly named and that we should have a method which shows the JCR integration yet does not tie the API too much into JCR.So I propose the following:> Remove the getRawData() and getObject() methods from the Resource interface> Add new interfaces NodeResource and ObjectResource:// resources backed by JCR nodespublic interface NodeResource extends Resource {Node getNode();}// resources mapped using JCR OCM for examplepublic interface ObjectResource extends Resource {Object getObject();}This way we have the Resource interfaces completely storage-agnostic and provide for a wide range of extensions such as an URLResource which may be backed by a URL such as an entry in an OSGi bundle.[1] http://www.mail-archive.com/sling-dev@incubator.apache.org/msg00906.html,3600
Extract Method,FORM Based Authentication This is a new bundle that provides an implementation of forms based authentication for sling.The login/logout servlets from the org.apache.sling.commons.auth are used.The AuthenticationHandler will use http basic auth credentials if they are on the request otherwise it will use the user/pwd posted from the login form.The login form html is generated by a set of scripts1. login.html.esp - full login page (includes login_body.html.esp for the form markup)2. login_body.html.esp - just the login form which may be useful for drawing the login form for an ajax context3. loginError.html.esp - full login-error page4. loginError_body.html.esp - just the login-error form for login error in ajax contextThe above scripts are included as bundle-resources @ /libs/sling/servlet/defaultThe bundle also has a couple of test scripts to show some examples of usage:1. loginTest.html.esp - shows who is logged in and links to login or logout2. loginTest2.html.esp - shows how a script can check permissions and show a login page if the anonymous user doesn't have permission to see the pageSome examples of usage are:1. http://host:port/path/to/node.login.html - show the login page and then goto http://host:port/path/to/node after authenticated2. http://host:port/path/to/node.login.html?s=.edit.html - show the login page and then goto http://host:port/path/to/node.edit.html after authenticated3. http://host:port/system/sling/logout - invalidate the session and switch back to anonymous user,3605
Inline Method,Display mime types as a tree table The list of MIME types is rather big and displaying them all in a single table gets quite confusing. Since MIME types are hierarchic in nature displaying them in a tree table of two levels enhances the display dramatically.,3606
Rename Method,"Provide ResourceResolver mapping information in a ConfigurationPrinter Currently the resource resolver mapping configuraiton is only available from the ""Jcr Resource Resolver"" page. It would be helpful for support purposes to have this information available as part of the configuration status.",3608
Extract Method,Add structure to SlingRequestProgressTracker messages The SlingRequestProgressTracker messages contain useful timing information structuring them a bit more would allow for parsing them to generate other representations.Here's an example of the current info (from http://localhost:8888/system/console/requests):0 (2009-10-07 11:10:30) Starting Request Processing0 (2009-10-07 11:10:30) Method=GET PathInfo=/index.html0 (2009-10-07 11:10:30) Starting ResourceResolution6 (2009-10-07 11:10:30) URI=/index.html resolves to Resource=JcrNodeResource type=nt:file superType=null path=/index.html elapsed = 6ms6 (2009-10-07 11:10:30) Resource Path Info: SlingRequestPathInfo: path='/index.html' selectorString='null' extension='null' suffix='null'6 (2009-10-07 11:10:30) Starting ServletResolution6 (2009-10-07 11:10:30) Starting resolveServlet(JcrNodeResource type=nt:file superType=null path=/index.html)6 (2009-10-07 11:10:30) Using servlet org.apache.sling.servlets.get.DefaultGetServlet elapsed = 0ms6 (2009-10-07 11:10:30) URI=/index.html handled by Servlet=org.apache.sling.servlets.get.DefaultGetServlet elapsed = 0ms6 (2009-10-07 11:10:30) Applying request filters6 (2009-10-07 11:10:30) Calling filter: org.apache.sling.engine.impl.debug.RequestProgressTrackerLogFilter6 (2009-10-07 11:10:30) Starting org.apache.sling.servlets.get.DefaultGetServlet#06 (2009-10-07 11:10:30) Using org.apache.sling.servlets.get.impl.helpers.StreamRendererServlet to render for extension=null7 (2009-10-07 11:10:30) org.apache.sling.servlets.get.DefaultGetServlet#0 elapsed = 1ms7 (2009-10-07 11:10:30) Request Processing ends elapsed = 7ms,3609
Extract Method,Add the resource node and mapped object defined objects to the sling:defineObjects tag The sling:defineObjects tag defines a number of objects as variables in the JSP e.g. the SlingHttpServletRequest Resource.It would be practical if the Node and mapped Object of the Resource (if the Resource implements the NodeProvider and ObjectProvider interfaces resp.) would also be defined.,3610
Rename Method,ResourceProviderEntry uses iterators rather than maps this becomes expensive with apps with many servlets. The ResourceProviderEntry uses iterators over many servlets this is probably Ok for Resource resolution but when it comes to servlet resolution many are tested and this can be expensive especially if there are many servlets. IMHO the class should be refactored to use trees of maps.This has been discussed on list the intention is to create a contrib version to explore this further not wanting to impact the active version in trunk.,3612
Move Method,Provide helper class for simpler unit testing of sling code When writing unit tests that do a bit of integration in Sling you often need the following:- a JCR repository- registered node types- a JCR-based resource resolver- adapter managers (to support all the adaptTo methods in your code)This covers most of what has to be done to run unit tests for services without an OSGi container running. I have written such a helper class for our product and would like to contribute it to Sling especially to write unit tests for SLING-1131. Patch will follow.,3613
Extract Method,Allow uploading JSON files to create content structures Currently uploading a JSON file will just create the file node.On the other hand it would be useful if uploading to a node with a request extension of JSON the JSON would be unpacked and handled as if it would be a modification request with the JSON data being the content to store.This would be similar to JSON upload supported by CouchDB.,3617
Extract Method,"Add support to set resource to which login is requested Currently the SlingAuthenticator uses the HttpServletRequest.getPathInfo() method to select an applicable authentication handler. In some situations most notably if there is some login servlet or script (e.g. the Sling Engine LoginServlet registered at /system/sling/login) this is incorrect because the path info is the path of the login servlet and not the desired actual resource path.To fix this login servlets should be able to convey a path on behalf of which login should be effected.Since the Engine LoginServlet already supports a ""resource"" request parameter to convey to authentication handlers where to go to after successful login we use a request attribute of the same name to indicate this situation.",3618
Rename Method,Extend Resource interface and provide AbstractResource base class Currently the Resource interface has only bare bones API to access its own local attributes like getPath() or getResourceType().Accessing the resource in the context of its parent or its children is not currently possible and doing so requires getting the resource resolver out of the resource and asking the resource resolver.For convenience we should add the following methods:getParent() -- returns the parent resource (same as ResourceUtil.getParent(this))getName() -- returns the name of the resource (same as ResourceUtil.getName(this))listChildren() -- same as getResourceResolver().listChildren(this)getChild(String) -- same as getResourceResolver().getResource(this path)isResourceType(String) -- same as ResourceUtil.isA(this String)The new AbstractResource class will implement these methods as indicated.Implementors of the Resource interface are then advised to actually extend from the AbstractResource interface which in the future will provide default implementations of any methods added to the Resource interface if it makes sense.,3620
Inline Method,Use tagsoup html parser instead of nekohtml Currently we use the nekohtml parser which is a) buggy and b) largeThe tagsoup parser http://home.ccil.org/~cowan/XML/tagsoup/ is very small and seems to work perfectly(Tika is using this as well instead of nekohtml),3621
Rename Method,Add branding for the Apache Felix Web Console In my whiteboard I have a prototyp branding bundle for the Apache Felix Web Console [1]. Now that the web console support for branding has been published we should propagate this and included it in the base set of our bundles.[1] https://svn.apache.org/repos/asf/sling/whiteboard/fmeschbe/webconsolebranding,3624
Rename Method,Remove direct dependency to web console The new web console provides a way to define plugins without a direct dependency to the web console.,3626
Rename Method,Rename methods of (new) AuthenticationHandler interface Currently the AuthenticationHandler interface defines the following methods:authenticate - extract credentials from requestrequestAuthentication - ask client for credentialsdropAuthentication - forget about current credentialsThe authenticate and requestAuthentication names are historic and date back to some internal code at the time where uthenticationHandler did not exist yet.IMHO these names are not that good. And since we are defining new API anyway this might probaby be a good time to rename the methods.extractCredentials - extract credentials from requestrequestCredentials - ask client for credentialsdropCredentials - forget about current credentialsSee also the discussion at http://markmail.org/thread/bocbtx2q5js4i2gf,3628
Extract Method,Add functionality to redirect to a new request target after successful authentication Some times it would be desirable to get redirected to a new target after authentication was successful.The sling authenticator should be extended to support this functionality.,3631
Rename Method,Allow bundles to contribute values to the script bindings As I described here: http://markmail.org/message/cjsjywo3rsgfujks I'd like to see a way for bundles to contribute script binding values.The proposed interface is:public interface SlingScriptBindingValuesProvider { void addBindings(Bindings bindings); } The Bindings object will be made read-only via a facade.,3633
Move Method,"Add support for login feedback from Authenticator to authentication handler There might be situations (or AuthenticationHandlers actually) desiring to get feedback on the outcome of authentication after providing authentication credentials. At the moment this ""feedback"" is limited to the case of failed login when the SlingAuthenticator calls back into the AuthenticationHandler to request credentials. But this is only indirect feedback in the failure case.I propose to extend the feedback transfer as follows:* Add AuthenticationFeedbackHandler interface with two methods:// called if authentication failed handler is not expected to send response// since SlingAuthenticator will call requestCredentialsvoid authenticationFailed(HttpServletRequest HttpServletResponse AuthenticationInfo)// called if authentication succeeded handler may write into the response// particularly setting a cookie or the like is possible herevoid authenticationSucceeded(HttpServletRequest HttpServletResponse AuthenticationInfo)* Add two methods to the AuthenticationInfo class to pass in a feedback handler:// May be called by the AuthenticationHandler to request feedback on the authenticationvoid setAuthenticationFeedbackHandler(AuthenticationFeedbackHandler)// forward to configured feedback handler ignored if nonevoid authenticationFailed(HttpServletRequest HttpServletResponse)// forward to configured feedback handler or handle redirect request if nonevoid authenticationSucceeded(HttpServletRequest HttpServletResponse)* SlingAuthenticator calls the new AuthenticationInfo methods on success or failure after login* The default behaviour of the AuthenticationInfo.authenticationSucceeded is to redirect to a desired target. This moves the SlingAuthenticator.handleRedirect method to a (probably) new static method which is called by the AuthenticationInfo class and which may also be called by any implementation of the AuthenticationFeedbackHandler.",3635
Rename Method,"Auto-reconnect to the JCR Repository if it is not found or lost Affects the jackrabbit-client bundle accessing a ""remote"" repository. Currently the respective component just tries to access the repository and gives up if it fails.This should be enhanced to:* retry in configurable intervals whether the repository is now available* find out that a repository may have gone and try to reconnect in the same configurable intervalsCare must be taken in the second case to drop (close ?) any sessions still connected with the old disappeared repository.",3636
Extract Method,"microsling Resource resolver and default renderers do not support Properties Currently assuming the /content/testing/foo Node exists and has a ""text"" property requesting /content/testing/foo/text.txt (or any other extension) fails with a ClassCastException as the resolution and rendering chain supports Nodes only.It should be fairly easy to use a JcrPropertyResource when the MicroslingResourceResolver finds an Item that is a Property and let the default renderers handle it.One use case is when working with client-side forms to fill a <textarea> for example with the value of a property based on a relative URL.I don't think we need to handle POSTs to Properties for now but GETs are useful in the above case and for testing as well.",3642
Extract Method,"Support multi-value sling:alias resolution Currently only a single sling:alias value for a resource is honoured even though the sling:alias is defined as both a single-value and multi-value property [1].To reproduce:1. Create a node at /node2. Set a multi-value sling:alias property on /node with values ""alias1"" and ""alias2""3. Browse to http://localhost:8080/alias2Expected result:- The resource at /node is returnedActual result:- A 404 not found. Still browsing to /alias1 returns the /node resource.Mailing list discussion: http://n3.nabble.com/Multi-value-sling-alias-td140425.html#a140425[1] http://svn.apache.org/viewvc/sling/trunk/bundles/jcr/resource/src/main/resources/SLING-INF/nodetypes/mapping.cnd?view=markup#l32",3643
Extract Method,POST Servlet should handle @TypeHint = Reference and value is a path Ifparam = /some/pathparam@TypeHint = Referencethennode.param = UUID of node at /some/path,3645
Extract Method,"Check if parallel property for jobs has value false Currently the parallel property for processing jobs is only checked for existing or non-existing. However this handling is a little bit confusing as for exampleif the parallel property has the value ""false"" the job is still processed in parallel.We should check the value for ""no"" and ""false""",3648
Extract Method,Add a component which allows bundles to configure the session returned by SlingRepository discussion in http://markmail.org/thread/umuk7beuisp6zoqs,3650
Extract Method,Limit the number of parallel jobs Currently jobs can either be processed in parallel or in a series (per topic). However if parallel processing is used as many jobs as are available are processed in parallel and there is no limit.We should change the meaning of the parallel property to:- false: no parallel processing- a positiv number N : parallel processing with max N jobs in parallel- anything else: parallel processingCurrently it is:- false: no parallel processing- anything else: parallel processing,3651
Rename Method,Recompile java scripts on modification and avoid periodic checks Currently the java script handler checks a java file for modifications when the script is called. This check has a configurable time intervall.With the latest changes we have resource events and therefore could recheck the sources on events and avoid the per request check completly.,3652
Rename Method,Web Console Plugin should be a configuration printer The scripting core registers a web console plugin to display the available script engines; this should rather be a configuration printer for the web console,3654
Extract Method,add a method to SlingIntegrationTestClient which accept multiple parameters with the same name currently SlingIntegrationTestClient.createNode() takes a Map<StringString>. This can't support multiple parameter values with the same key.We need a method which accepts a name-value pair list.,3656
Inline Method,"Add replaceAccessControlEntry method to AccessControlUtil ModifyAceServlet and DefaultContentCreator both have a need to merge new privileges for a given principal to an existing resource ACL. Doing so involves some rather complex logic which is easy to get wrong and in the Sakai 3 project we found that quite a few service developers needed the same functionality. This patch moves the functionality to a shared utility method to eliminate any tempatations to copy-and-paste (or worse rewrite incorrectly).Besides consolidating the logic (and removing it from ModifyAceServlet and DefaultContentCreator) this patch introduces a couple of other changes:* The ACE merge is more conservative. SLING-997 broke apart specified and existing aggregated privileges and then tried to recombine them into possibly new combinations. This patch instead maintains exactly what the client specified and (when possible) what was already there but does not create any new aggregates of its own. This better matches Jackrabbit's default behavior should minimize client surprises and eliminates a subtle bug: any candidate aggregate privilege needs to be checked for ""isAbstract()"".* The ModifyAceServlet JavaDoc is corrected and expanded.* Some bad logging format is fixed.",3660
Inline Method,Remove dependency to JCR The commons auth module as independent functionality wrt to JCR therefore we should make it work without JCRWith SLING-1262 implemented we can use the ResourceResolverFactory to login instead of going through the SlingRepository.In addition we should:- drop the setter and getter methods for credentials in the AuthenticationInfo - we keep the constant though- drop the setter and getter method for the workspace in the AuthenticationInfo - we keep the constantWe make the import to the jcr packages optional as these are required for compatibility support of the old engine packages. If someone wants to use this support he'll need the jcr api anyway.,3661
Rename Method,"Compact Syntax for ESP expressions in HTML attributes The current syntax for ESP expressions ( <%= value %>) parallels the JSP syntax but is hard to read when it is used in HTML attributes (<a href=""<%=link %>""><%=name %> </a>). I propose an syntax addition that allows inline ESP expressions in HTML in a more compact way and that is less intrusive to the XML structure. Basis for the syntax were JSP Expression language and XSLT. The above example would be rephrased in the new syntax as: (<a href=""{link}""><%=name %> </a>).",3662
Extract Method,multipart parameter support the multipart parameter support is missing. attached a patch that will enable multipart support.,3665
Inline Method,Scripts should be resolved against the current workspace currently if a non-default workspace is specified in AuthenticationInfo scripts are still resolved against the default workspace.Scripts should be resolved first against the workspace used in the Request (i.e. from AuthenticationInfo) and then a default workspace as a backup.,3668
Extract Method,support for resource paths containing workspace name via configuration it'd be nice to be able to specify multiple workspaces.,3671
Extract Method,Clean up compiler API and use classloading infrastructure The current interface of the commons compiler is unnecessary complicated and does not use all of the features of the commons classloading infrastructure.We can:- remove the CompilerEnvironment interface - this can be handled internally- remove the ClassWriter interface - we have the ClassLoaderWriter interface in the commons classloader- change the options interface to extend a map - this allows us to add new options without changing interfaces/api- the compile unit interface can be changed toCompileUnit {InputStream getSource();String getMainTypeName();}This simplifies the integration with the rest of sling which is resource based.The JavaCompiler interface then just takes an array of compile units an error handler and the options,3674
Extract Method,Mark conflicts between package imports and bootdelegation Currently the Sling Console lists the packages exported and imported for each bundle on request. In case of a package listed in the org.osgi.framework.bootdelegation property such wiring is actually ignored and the parent class loader is used. This situation might be confusing as the results may be unexpected.To help in solving possible issues with this problem wired imports which match any entry in the bootdelegation property should be marked to indicate that respective packages will be loaded from the parent class loader and not from the wired import.,3680
Rename Method,"Create the Sling Launchpad based on microsling-core code Following up on the ""[RT] Shall we merge microsling into Sling?"" [1] and ""µsling 2.0 requirements"" [2] threads on sling-dev we need to merge microsling into Sling.Here the are requirements as discussed in [2] (taking into account Felix's comment about WebDAV and Michael's comment about switching to other JCR repositories):µsling 2.0 is a preconfigured instance of Sling meant to allow web developers to test drive Sling by building scripted web and REST applications backed by a JCR repository.The µsling 2.0 distribution only requires a Java 5 VM to run no installation is needed. Fifteen minutes should be enough to start µsling and understand the basic concepts based on self-guiding examples. µsling should ideally be delivered as a single runnable jar file.Java programming is not required to build web and REST applications with µsling 2.0: both server-side and client-side javascript code and presentation templates can be used to process HTTP requests. Other scripting and templating languages (JSP and BSF-supported ones) can be plugged in easily.The µjax ""application protocol"" and client-side javascript ""JCR proxy"" library make it easy to write powerful Ajaxish JCR-based applications with µsling 2.0.µsling 2.0 is built on the same codebase as Sling it's only a specific configuration of Sling.All µsling 2.0 features are available in Sling applications as long as they are enabled in the Sling configuration.Sling (and µsling as it runs the same core code) uses OSGi to modularize the framework but µsling does not require any OSGI skills and makes OSGI largely invisible to beginners.All Sling features and modules can also be activated in a µsling 2.0 instance by installing and activating the required OSGi bundles.µsling 2.0 passes all the integration tests of the existing microsling test suite (SVN revision 605206) with minor adaptations where needed.µsling 2.0 includes a WebDAV server module to make it easy to copy scripts into the JCR repository.[1] http://markmail.org/message/2s7agnu5kklti6da[2] http://markmail.org/message/atbjzjjp2wflkotb",3681
Extract Method,Update to JCR 2 API Some jcr api has been deprecated with jcr 2.0; we should change calls to deprecated stuff,3697
Extract Method,Allow subclasses of JsonQueryServlet to modify the statement and query type It would be useful if JsonQueryServlet enables subclasses to override the statement and/or query type.,3698
Extract Method,Add package dep. info to the console it would be very helpful if the bundles in the 'installed bundles' list could show a list of the imported packages and maybe mark them 'red' if they cannot be resolved.,3701
Extract Method,Remove the dependency to the Sling JCR API Currently the servlet resolver depends on the Sling JCR API - with the new ResourceResolverFactory in the Sling API we can directly use that and remove the dependency to the Sling JCR API.,3703
Extract Method,Decouple assertJavascript from HttpTestBase The assertJavascript() method in HttpTestBase should be extracted to a separate class to make it reusable independently,3704
Extract Method,contentloader should overwrite file contents not file nodes When a file node is created in the contentloader the underlying jcr:content node is removed and recreated. This is problem as nt:resource is referenceable. Because it is deleted and recreated a new uuid is defined.Instead we should just modify the properties of the jcr:content node and keep the uuid.,3705
Rename Method,Allow access to Node and Property Methods on ScriptableNode and ScriptableProperty I would like to propose access to all jcr Node methods from ScriptableNode and access to the jcr Property.I recently wanted to access the Property.getLength() method from a .esp script and didn't find a good way startingout from my very convenient ScriptableNode.This discussion already talks about a similar issue:http://www.mail-archive.com/sling-dev@incubator.apache.org/msg01481.htmlSo ideally somthing like an automatic getter mapping that I know from earlier rhino projects would mean that I could access the same information through for example prop.length or prop.getLength().I think it would be great if all jcr Property and Node methods would be exposed otherwise we are hiding jcr features fromthe script user. I think maybe the solution also requires a ScriptableProperty.WDYT?,3707
Extract Method,allow the form authentication handler to include the login form as a servlet resource rather than doing a redirect this is intended to be an option with the default being to redirect.,3713
Rename Method,OSGi events should contain a userID when possible. We're building a system that tracks the user's movements in the system.One of the things we're tracking is the OSGi events that are emitted by Sling. These don't contain a userid however.Since this all hapens in the JcrResourceListener which is a JCR Observation Listener getting the userID is fairly straightforward.I'll attach a patch that includes this functionality.AFAICT this is the only location that emits user generated events if I missed a spot I'd be happy to try and patch that one as well.,3716
Move Method,Support loading of initial configurations for ConfigAdmin It would be nice to have support for initial configurations like we have for initial bundles.I think we should just apply the same logic (when to install/update) like we have for bundles and support *.cfg files in the same locations as we have bundles.,3718
Extract Method,The JsonQueryServlet should support the tidy selector to provide pretty printed results SLING-562 provided the ability to apply a tidy rendering for the JsonRendererServlet. The same technique would be useful for formatting the results of the JsonQueryServlet.For example:http://localhost:8888/.query.tidy.json?queryType=xpath&statement=//element(*nt:unstructured),3720
Extract Method,"Cleanup Authentication Info constants and implementation The constants defined on the AuthenticationInfo class in the Commons Auth bundle should be moved as follows:USER = ""user.name""to the ResourceResolverFactory interfacePASSWORD = ""user.password"";to the ResourceResolverFactory interfaceCREDENTIALS = ""user.jcr.credentials"";to the JcrResourceResolverFactory interfaceIn addition support for checking the type of a credentials property is to be removed from the AuthenticationInfo class.Likewise the following constants currently internal to the JcrResourceResolverFactoryImpl class should be moved:AUTH_INFO_WORKSPACE = ""internal.user.jcr.workspace"";to the JcrResourceResolverFactory interfaceand change value to user.jcr.workspace (removing the internal prefix)SESSION_ATTR_IMPERSONATOR = ""impersonator"";to the ResourceResolverFactory interfaceFinally the following constant from the ResourceResolverFactory interface should be changed:SUDO_USER_ID = ""sudo.user.id"";change the value to user.impersonationAt the end use constants should be ensured mostly in the JcrResourceResolverFactoryImpl and JcrResourceResolver classes.",3722
Extract Method,support reading the jackrabbit configuration file from the launchpad archive downstream users of Sling should be able to put a repository.xml in either the launchpad JAR or WAR and get it to work without any additional configuration.,3724
Extract Method,Let UserManager POST servlets return JSON SLING-1336 added the ability to return a JSON response from the SlingPostServlet. The UserManager POST servlets should have this capability as well.,3726
Extract Method,JobStatusProvider should lazy load events when returning The current implementation of the job status provider does a query then loads all jobs and returns a list.If you just want to know how many jobs are available or just want to return the first X jobs the implementation is too expansive.One solution would be to return a kind of range iterator instead of a list and load jobs on demand. There are two potential problems:- if the jcr query does not return a count we can't return a count either without going through the hole result set- it might be that a job can't be loaded because of missing classes; in this cases the job count from the query result is higher than the count of the jobs that get processed. I think we can neglect this,3728
Rename Method,Change the OSGi installer interface As discussed in the mailing list:the current OSGi installer has three methods:- registerResources : which is used to register all resources from theinstaller client like jcr install; this is usually invoked on startup- addResource : adds a resource during runtime- removeResource : removes a resource during runtimeThe api is simple fine and sufficient. However there is a small glitchhere. If a client detects several changes at once like a set of bundlesis removed or updated it has to call addResource or removeResource foreach change separately. The OSGi installer could run a install cycleinbetween those method calls. Causing a part of this process to be donein the first cycle and the other part in the second cycle - now as OSGihas a dynamic nature this isn't a problem after two cycles everything isinstalled as expected. But still I have the feeling that it would benicer if the client could submit several changes add once so maybeinstead of having a addResource and removeResource method just aupdateResources([] addedResources [] removedResources).,3730
Extract Method,Register internal post operations as services for consumption by servlets other than the SlingPostServlet As discussed in [1] it would be useful to have the internal operations of the Sling POST Servlet available as services for other bundles to reuse.[1] http://markmail.org/message/a7vrtyhictf7tv4m,3731
Inline Method,Add state management for resources Currently there is no state management so it is hard to tell if a resource has been installed should be installed uninstalled etc.In some situations this leads to endless loops where something is tried over and over again - although nothing needs to be done anymore or can't be done.If we add proper state management to the resources the installer knows what needs to be done and can act accordingly,3733
Extract Method,"Support for lazy activated bundles The installer waits for an installed bundle to become active - in case of lazy bundles these bundles get only active when they are ""used""Therefore we should relax/change/adapt the logic",3739
Extract Method,Add support for Range requests to the StreamRendererServlet To support some kind of streaming support for the Range HTTP header can be built into the StreamRendererServlet.The Tomcat DefaultServlet [1] can be used as the basis for this implementation.[1] http://svn.apache.org/viewvc/tomcat/trunk/java/org/apache/catalina/servlets/DefaultServlet.java?view=markup,3741
Extract Method,Custom thread pools should get an optional label When a custom thread pool is created through the Java API this pool gets a UUID as the name; this makes it later on difficult to associate with the its usage.Therefore an additional label should be possible.,3743
Extract Method,Prevent Login Request loop Depending on AuthenticationHandler specifics it is conceivable that the Sling Authenticator support may enter an endless redirect loop with the client.Consider this:#1 client provides wrong credentials (e.g. cookie HTTP Basic authentication header)#2 authenticator decides to call AuthenticationHandler.requestCredentials#3 authentication handler sends a redirect to the client#4 client requests redirect target again providing wrong credentials#5 authenticator decides to call AuthenticationHandler.requestCredentials#6 continue with step #3This loop should be broken in the authenticator: As soon as the authenticator recognizes a (potential) redirect loop the authentication handler should not be called again but instead an immediate error response should be sent back.,3744
Rename Method,"Make SlingException a RuntimeException and derive all exceptions of SlingException Assuming lazy consensus to the changes proposed in the mail thread ""Rethinking Exceptions in Sling"" [1] the changes from the whiteboard [2] should be applied.[1] http://www.mail-archive.com/sling-dev@incubator.apache.org/msg01520.html[2] http://svn.apache.org/repos/asf/incubator/sling/whiteboard/fmeschbe/effective_exceptions/api",3747
Extract Method,upgrade GWT to 2.1 GWT 2.1 has been released - http://googlewebtoolkit.blogspot.com/2010/10/announcing-final-release-of-gwt-21.htmlthe contrib module should be upgraded.,3749
Extract Method,Add bolding of declared node type info Following FELIX-2570 it is possible to differentiate declared properties and child node definitions from those inherited with some light HTML.,3750
Extract Method,Adapt Commons Log Web Console panel to JQuery UI functionality The Web Console plugin of the Commons Log bundle still formats the output for the old UI. We should convert this to support WebConsole's JQuery UI use.,3752
Extract Method,"Make Locking Strategy Configurable (for Cluster Usage) The Sling eventing currently uses session scoped locks to prevent two cluster nodes to process the same job.Unfortunately Jackrabbit does currently not support session scoped locks in a cluster.Another way would be to use open scoped locks. We could implement a heartbeat functionality which detects if a cluster node is not available anymore and any other node in the cluster unlocks thelocked nodes. Unfortunately this doesn't work in Jackrabbit either as the lock handling is implemented in a very strict way and only the session which created the lock can unlock it (or the other sessionneeds the lock token) - I'll create an enhancement bug for this in JackrabbitIf no cluster is used at all we could skip the locking completly and therefore reduce the load on the repositoryTherefore we could make a lock manager with a locking mode configuration of ""session scoped"" ""open scoped"" and ""none""As an additional note if one want to use Jackrabbit and the Sling eventing in a clustered environment - one working approach is to disable job execution on all but a single cluster node - this can be done by setting the jobmanager.enabled configuration property for the JobManager",3754
Extract Method,Use time based folder structure for jobs without an id Currently jobs without an job id are stored in an artifical folder structure (to avoid a too large flat hierarchy) - this folder structure is created randomly by generating uuid. To avoid concurrency problems in a clustered environment these folders are never deleted - resulting in a large empty folder structureWe could use a time based folder structure instead and create a new folder every minute - once this folder gets empty and another minute has started we can safely remove the folder.In addition if we include the sling id in the folder name we avoid folder creation problems in a clustered environment.,3756
Extract Method,Make installers pluggable Currently the osgi installer only supports bundles and configurations - we should add service interfaces to allow installation of other resources like deployment packages etc.,3760
Extract Method,Allow resource transformer for processing installable resources In some cases the installable resource is not directly the installable end product for example if a jar is dropped into the installer which is not a bundle a service could bundlize this jar.,3763
Rename Method,Remove dependency to web console Since some versions the web console provides a way to register plugins without directly depending on the web console api - we should use this as well,3765
Rename Method,Launchpad installer should not depend on SCR The current launchpad installer implementation requires SCR - we should rather implement an activator with a service listener. As soon as the osgi installer service and the resource provider is available this service should start it's action,3767
Rename Method,Extend ResourceResolver to make it more flexible As a result of defining a virtual resource tree (SLING-197) we have a need to modify the ResourceResolver API in two respects:(1) Add ResourceResolver.resolve(String absPath)This method behaves exactly as ResourceResolver.resolve(HttpServletRequest) except that the latter method may make use of additional request properties such as request headers or parameters while the resolve(String) method only has the string to work on.Currently the resolve(HttpServletRequest) method does nothing more than use the HttpServletRequest.getPathInfo() to resolve the resource thus both implementations would actually be equivalent.The absPath argument is an absolute path. Resolution fails for relative paths.(2) Support relative paths in ResourceResolver.getResource(String path)Currently this method is defined to throw a SlingException if the path is relative. This should be changed such that the ResourceResolver applies some search path logic to find a resource with the given relative pathThe search path logic is comparable to how *nix systems use the PATH environment variable.This method may then be used by multiple users such as Servlet/Script resolution.(3) Add ResourceResolver.map(String) methodThis method applies the reverse mappings of the ResourceResolver.resolve(String absPath) method to return a path suitable for both resolver() methods. This allows for the creation of link paths for resources.,3772
Extract Method,"Post servlet: Patching multi-value properties As described on the sling list http://sling.markmail.org/thread/xxaaqowtx7jgfo3p  allow patching of multi-value properties:New ""@Patch"" suffix:my:property@TypeHint=String[]my:property@Patch=truemy:property=+value1my:property=-value1When ""@Patch"" is present each of the property values from the request are expected to start with either ""+"" or ""-"" followed by the actual value. These two would represent the operations you want to have executed on the set.",3774
Extract Method,Last modified of java file should not be compared against compiled class if file is modified If a java file is modified:a) on observation event is sentb) if an observation event arrived the last modified of the java file is compared with the last modified of the compiled class (if available)If the last modified of the java file is older no recompilation is done. This creates unexpected results if e.g. a package is downgraded and older versions of a java file are installed (with older last modified values). The new java files would still be used.The last modified check is actually still there to avoid recompilation on startup.We should distinguish between the two cases: do a last modified check on startup but no last modified check based on observation events,3776
Extract Method,Support configuration of the default request parameter encoding As proposed in http://markmail.org/message/mv6jfc26x43keg6i a new configuration property for the Sling Main Servlet should be introduced to overwrite the default request parameter encoding.Currently request parameters are decoded with the encoding provided by the _charset_ request parameter (if set). If the request parameter is not set or does not provide an encoding supported by the platform the default ISO-8859-1 is applied.For applications always only using the same response encoding e.g. UTF-8 specifiying the _charset_ request parameter is tedious. It would be helpful if the default parameter could be configured to something different than ISO-8859-1.,3780
Extract Method,"Support deep folder structure for installation Currently the jcr installer only installs all artifacts directly contained in the ""install"" folder - if there is a nested structure beneath this folder this is not traversed.We should support traversing down the folder structure",3782
Rename Method,When creating the initial configuration use relative path names for the repository configuration and home The Jackrabbit server creates a configuration on initial startup if none is available from the Configuration Admin service. This configuration will contain absolute path names to the repository configuration file as well as the repository home directory. This makes it hard to relocate the Sling Home folder because the configuration will still point to the old location.Instead the default configuration (unless overwritten by some framework property) should use relative path names to be resolved against sling.home once the repository is started/created.,3784
Extract Method,Add limit to job query The JobManager#queryJobs does not provide a limit. To reduce load on the manager we should add an additional method with a limit parameter for use cases where only the first entries are of interest.Though the query is done in memory at the moment it will reduce the load with large queues and a low limit,3785
Rename Method,Clean up content data and request data handling ContentData and RequestData have both a dispose() method which actually is not really used. We could completly remove themIn addition we can remove the stack of ContentData objects from RequestData by handling this locally in the SlingRequestProcessImpl#dispatch methodThis further simplifies the data objects.,3787
Extract Method,Upon the installation of a fragment bundle a refreshPackages call should be made on the host bundle Fragment bundles don't get started but for them to become attached to the host refreshPackages() must be called on the host bundle (assuming that the host bundle has been started).,3789
Rename Method,Properly provide locale inheritance support The ResourceBundle class is intended to be used with inheritance of resources from parent resource bundles. Thus the ResourceBundle.getObject(String) is implemented along these lines:Object obj = handleGetObject(key);if (obj == null) {if (parent != null) {obj = parent.getObject(key);}if (obj == null) {throw new MissingResourceException(...)}}return obj;The JcrResourceBundle.handleObject(String key) method is implemented like this:Object value = // get from the repository resourcesreturn (value == null) ? key : value;That is the key is returned if there is no value. This though breaks the inheritance chain intended by the ResourceBundle.getObject(String) method. We should fix this along these lines:* The JcrResourceBundleProvider provides a root ResourceBundle implementation as follows:- handleObject(String key) always returns the key- getLocale() returns an empty Locale- getKeys() always returns an empty enumeration* The root ResourceBundle is used as the parent for all JcrResourceBundle instances which have no Locale induced parent* JcrResourceBundle.handleGetObject(String key) only returns the value for the given localeThis way we can keep the guarantee that getObject(String key) method of a ResourceBundle provided by the JcrResourceBundleProvider never throws a MissingResourceException.,3790
Extract Method,Launchpad installer does not support nested structure The installer does not support nested structures and only returns resources on the first level.,3792
Extract Method,Preload a configured set of resource bundles from the repository It would be convienent to be able to preload resource bundles from the repository.,3794
Extract Method,AccessManager permissions manipulation services API It would be nice if the jackrabbit.accessmanager bundle expose OSGI service(s) that maps exactly the functionalities of the REST services so that one can use their features also in a programmatic way. This can be useful if an application has to manage permissions without having an explicit request object (ex: from an EventListener) or in the case a user has to manipulate his account (in this case he doesn't have an administrative account so his requests are not permitted to modify users). Also i think that in certain situations it could be just cleaner and simpler to write a servlet or script that directly invoke the methods instead of find the way to invoke the REST services.,3796
Extract Method,Adding support to call a sling resource for JSP page exception handler <%@ page errorPage ... %> At the moment the JSP page exception handler (<%@ page errorPage ... %>) is not supported as the jsp engine can't resolve the path to the jsp if this is a repository resource.See also the user mailing list [0] for more details[0] http://www.mail-archive.com/users@sling.apache.org/msg01369.html,3798
Extract Method,"Add functionality to ignore some parameters from POST requests In certain situations a POST request is accompanied with request parameters that are to be ignored. Currently the Sling POST Servlet has two mechanisms to handle such parameters:- any parameter starting with a colon (:) is ignored e.g. :operation- only parameters starting with ""./"" are considered if at least one parameter has this formatIn certain situations more parameters might be submitted ending in the POST Servlet and then being written to the repository. For example if a user tries to authenticated with form based authentication supplying j_username and j_password parameters then if the Sling POST Servlet is erroneously hit these values might get written to the repository.We should add functionality to specify regular expressions for parameters which are to be ignored (apart from the existing mechanism). The default would be ""j_.*"" to ignore any parameters starting with j_ generally used for authentication",3800
Extract Method,Improve support for OSGi installer by distinguishing between bootstrap and app bundles Currently all bundles are put into the launchpad at /resources/bundles and are installed by the Sling Launchpad Bootstrap InstallerAs we have the Sling OSGi installer we should make use of it and provide a way to just install the bootstrap bundles (like the OSGi installer) through the Launchpad and everything else is handled by the OSGi installer.,3802
Extract Method,Apply some validation to requested redirects after authentication Currently the DefaultAuthenticationFeedbackHandler.handleRedirect and AbstractAuthenticationHandler.sendRedirect methods do not apply any validity checks on the requested redirect target.We should apply some checks to ensure a valid target is accessible within the Sling application. If the target is not valid the methods would redirect to the servlet context root path -- obeying the contract for redirecting the client but not necessairily to the desired target. In any case an ERROR level message is written to the log indicating why the redirect target is not being honoured.This check should be made available to AuthenticationHandler implementations such that they may apply checks to their own redirects.,3804
Rename Method,"ujax post servlet should respond with status page instead of default redirect it is desirable that the ujax post serlvet responds with a status page rather than with a default redirect.this allows clients (javascript+formpost or ajax based) to react better on the modifications.will provide a patch for the post servlet that responds with that status page. the old redirect behavior can still be achieved by sending a ujax:redirect=""*"" input parameter.",3806
Inline Method,Remove check for Sling Engine bundle The Sling settings service currently waits for a Sling Engine bundle on first startup for compatiblity.To reduce startup problems we should remove this extra handling - if an upgrade from an old engine bundle is happening first updating to the Sling Settings 1.0.2 version and then updating to 1.1.0 (or higher) does the trick,3809
Extract Method,Don't copy resources if they are always available (like from the file system) If resources are provided to the installer core they are always first copied into the file system. The basic reasioning behind this are providers like the JCR repository which might be unavailable during installation because of bundle updates etc. To avoid unnecessary round trips and complicated handling of those situations all resources are made available in all cases first - by copying to the file system.If the resources are already in the file system or loaded by a class loader or from within a jar these resources are always available and there is no need for copying them first.,3811
Extract Method,Provide a way to specify additional bootstrap commands It's currently not possible to easily add commands to the bootstrap command file.We should read src/main/sling/bootstrap.txt and add it to a potentially existing bootstrap file,3812
Extract Method,Bundle configurations and sling files with bundlelist The maven launchpad plugin reads configurations from /src/main/config and special sling configuration files from src/main/sling.If a bundle list is a partial bundle list which is used by other bundle lists it would be nice to have these configuration files combined with the partial bundle list. This avoids repeating the configurations in the using bundle list and allows a better separation of concerns.,3814
Extract Method,Add Servlet Container name and version to Sling's server info Sling exposes its own ServletContext implementation to servlets and filters deployed into Sling. This returns a Sling specific string for the getServerInfo method which replaces the information returned from the servlet container. To help with support and for information purposes we should add the container name and version from the servlet container's server info to the Sling server info.For example:* Current Sling server info (sample)Apache Sling/2.X.X (JDK 6.0; Linux 3.2)* Sample platform server infoJetty/6 (JDK 6.0; Linux 3.2; more info; yet more info)* Proposed Sling server infoApache Sling/2.X.X (Jetty/6; JDK 6.0; Linux 3.2),3815
Inline Method,Installer should be able to update itself Currently it's not possible to update the installer core bundle through the installer itself this results in deadlocks,3816
Rename Method,Add a listener to the installer We should add a listener to the installer which allows to keep track of the actions done by the installer.As a first step we could notify the start of an installation cycle the suspension of the installer and a processed resource,3817
Rename Method,Allow for better configuration of sling home folder Currently a few files and folders in the ${sling.home} folder can be more freely located than others:* The org.apache.sling.launchpad.base.jar file is expected inside ${sling.home} hardcoded* The sling.properties file is expected inside ${sling.home} hardcoded* The start folder containing initial bundles to install is expected inside ${sling.home} hardcoded* The Felix framework cache location is configured by the org.osgi.framework.storage property* The location of Configuration Admin configuration is configured by the felix.cm.dir property* The location for File Installer (of the OSGi Install facility) is configured by the sling.installer.dir propertyTo have more flexibility two new properties should be added:* sling.properties (default: ${sling.home}/sling.properties) -- Provides the path and name of the sling.properties file contains the configurable properties used for starting Sling and then becoming Framework properties accessible with the BundleContext.getProperty(String) method. This property itself is also available as a Framework property and will be set to the default value by the Sling Launcher if not already set. The property must not contain a reference to another property but if it is not an absolute path it is resolved against ${sling.home}.* sling.launchpad (default: ${sling.home}) Defines the location of Sling Launchpad related files and folders. At the moment this is the location of the org.apache.sling.launchpad.base.jar library which provides the OSGi Framework and the startup folder which takes bundles to install on framework startup. The property must not contain a reference to another property but if it is not an absolute path it is resolved against ${sling.home}.The defaults are defined such that they account for backwards compatibility.From a little code inspection probably the following areas have to be updated:* The Sling class must ensure the property values* The Sling class (loadConfigProperties method) must be modified to read the sling.properties file from the location indicated by the ${sling.properties} property* The Launcher class must be modified to look for and place the launcher jar in the ${sling.launchpad} folder (instead of ${sling.home})* The BootstrapInstaller class must be modified to expect bundles in the ${sling.launchpad}/startup folder,3819
Extract Method,Support different properties and bootstrap commands for standalone and webapp With SLING-2134 and SLING-2182 it's possible to provide additional Sling properties and bootstrap command entries for a bundle list project.We should enhance this support to allow different properties / commands based on the artifact type,3820
Rename Method,add an SPI interface for injecting custom/alternate PostResponse implementations This was discussed in SLING-2156 but not necessarily tied to that issue.Basically it should be possible to implement an interface which can produce an implementation of the PostResponse interface.,3821
Extract Method,Add configuration to skip session.save in AbstractPostOperation With SLING-1725 the postservlet operations have been exposed as services to be consumed by other servlets.I use that to create two nodes with the ModifyOperation in one request. However the second node creation might fail because of the postprocessors.In that case I want to return an error to the user and have nothing stored. This can't be done at the moment as the AbstractPostOperation always saves independently from its context (so first node gets saved second node not).I think it makes a lot of sense to let the caller of the operation have control over when the session is saved and introduced a flag to skip the save when requested.It is modeled after the isSkipCheckin flag.,3823
Move Method,add .res extension to launchpad defaut servlet the launchpad serlvet should also handle the .res extension for adaptables to 'stream'.the reason is that if a nt:resource node name has no extension it's hard to request it.,3825
Extract Method,Add SlingHttpServletRequest.getRequestDispatcher(String path RequestDispatcherOptions options) method Currently the SlingHttpServletRequest interface only allows the creation of a RequestDispatcher with options if a Resource is available. But it should also be possible to create a RequestDispatcher with options from a path.,3827
Extract Method,Don't pass requests intended to be handled and terminated by Authentication Handlers As discussed on the mailing list [1] the Sling Authenticator should not pass requests which are intended to be handled by Authentication Handlers and then terminated (either by an error or by redirecting the client).[1] http://markmail.org/message/ggsxgaigluwktjyv,3828
Inline Method,Register web console plugin using a ServiceFactory Currently the web console plugin is tried to be directly registered which fails if the javax.servlet api is not available at that moment. This results in a missing plugin if the osgi installer starts before the javax servlet package. Instead of directly trying to register we could register a service factory instead which is the first time queried when the web console is started and then the required api's are available.,3829
Inline Method,UserManager - Convert @scr Java Doc Tags to SCR Annotations The SCR Annotations provides more flexibility and support with respect to referring to actual Java Code thus reducing the risk of copy/paste errors. In additions annotations are fully supported by IDEs and thus provide support while editing.,3831
Extract Method,There should be a way to add multiple Adapter annotations to a class For AdapterFactory instances which support multiple adaptable classes it would be useful to be able to support multiple annotations per class. Since the language spec doesn't allow this we need a container annotation.,3836
Extract Method,Correctly sort resources The sorting of resources pointing to the same artifact is not 100% correct. The digest should only be compared if the artifact is either a snapshot artifact or does not have a version information,3837
Extract Method,Add getters and setters to defined entries For ease of use the SlingBindings and ResourceMetadata classes should be extended with explicit getters and setters for the defined map entries.,3838
Rename Method,JcrResourceListener: Asynchronously post Events to EventAdmin Currently the JcrResourceListener posts events to the OSGi EventAdmin as it processes the JCR events.This may create a considerable delay on the repository observation queue processing because each call to the EventAdmin.postEvent must immediately extract the appropriate EventHandler services from the service registry. During JCR Event processing we should only generate the OSGi events and post pone actual posting of events to a separate thread.,3839
Inline Method,ServerSetup utility - more flexible way of setting up the server-side of integration tests I need a more flexible way of setting up Sling-based servers for integration testing that allows the various phases of the server setup (start runnable jar wait for it to be ready install additional bundles create test content etc) to be selectively run or ignored.This is useful when such server setups are slow for example when testing upgrades from one Sling-based system to another which means starting an old version setting up some content on it stopping it starting the new version running tests on it to check that the test content was preserved or suitably adapted etc.Being able to selectively disable setup tasks allows for restarting tests from saved states which is often much faster than redoing the whole setup when debugging the resulting system or its tests.I'll create the corresponding utilities under testing/tools and use them in testing/samples as an example.,3840
Extract Method,Separate request/access logging from the Sling Engine Currently the request and access log entries are generated in the Sling Engine bundle. The consequence of this is that only requests going through the Sling Main Servlet are actually logged.The fix is to hook the request and access logging infrastructure into the serlvet container as a Servlet API Filter such as to run it as early and late as possible and more importantly on a global level to catch all requests.In addition since this filter has nothing to do with the Sling Engine it would make sense to create a bundle in the commons area along with the commons/log and commons/logservice bundles.,3842
Extract Method,Install bundles in the order of their start level The bundle install tasks are currently sorted by URL - which doesn't provide any advantage. We should rather sort them by sort level this allows bundles with lower start level to be resolved before ones with a higher start level,3843
Extract Method,Handle GET requests by method name not just by request extension Currently the name of the script to call to handle a request is based on the request extension for GET and HEAD requests and on the request method for all other requests. This works fairly well for scripts handling single extensions. But it does not scale well for servlets (and scripts BTW) which are able to handle more than just a single extension such as the launchpad default servlet.Therefore - just for GET/HEAD requests - the script name should just be looked up by request extension and if no script can be found by the request method name just like for all other methods.,3845
Extract Method,Support configurations with old pid With the rewrite of the jcr installer the PID changed from:org.apache.sling.jcr.install.impl.JcrInstallertoorg.apache.sling.installer.provider.jcr.impl.JcrInstallerWe should support potential configurations with the old PID by:- registering a managed service for the old pid- if a configuration arrives with the old pid these values overwrite values from the new PID- log a warning in such casesAll other mechanisms like trying to merge the configs are bound to fail. As there are only a few users having an own configuration for this service anyway this is the best possible solution,3846
Inline Method,ClassLoaderWriter should provide class loader for loading written classes/resources As a follow up to SLING-2445 the ClassLoaderWriter should be enhanced to return a class loader which can be used to load the dynamically loaded classes written through this writer.This writer should use the dynamic class loader as a parent and implement the DynamicClassLoader interface which allows to check if the class loader is still current.The returned classloader should not be cached by clients they should just get the class loader each time they require one. The writer ensures that always a fresh loader is returned.The java and jsp scripting should use this class loader instead.,3848
Rename Method,Replace Resource.getResourceProvider() by Resource.getResourceResolver() Currently the Resource provides access to the ResourceProvider which created the resource. This is not really practical and probably not correct at all because the ResourceProvider is something operating behind the scenes on behalf of the ResourceResolver. Thefore this method should be replaced by a method providing access to the ResourceResolver causing the Resource object to be created by the ResourceProvider.Changes due:Resource:+ getResourceResolver()- getResourceProvider()ResourceProvider:+ add ResourceResolver arguments to all methods creating Resource instances,3850
Extract Method,"Add a ""currentNode"" variable to the scripting context for all languages As discussed in http://markmail.org/message/vokst7wb4k322zdccurrentNode = Resource.adaptTo(Node.class) so the value is null if the Resource does not point to a Node",3854
Rename Method,Implement CRUD based on resources We need full CRUD support based on resources. In general we need some api which allows to create update and delete resources. The method call have to be delegated to the underlying resource providers.,3858
Inline Method,"Remove fallback code for old resource implementations See SLING-2457 for some examples we currently have some ""unusal"" handling of some ResourceUtil/AbstractResource methods which are basically there to support very old implementations of the resource api which predate the 2.1.0 release.As we now extended the resource interface as well as resource resolver and other stuff I think it's time to not support this old stuff anymore and clean up the code",3859
Extract Method,Launchpad war should optionally use an external Repository without requiring a change of bundles It would be useful to allow the Launchpad to use an external Repository (accessed via JNDI or RMI) without having to modify the war file or load/unload bundles.I'll search for a solution along these lines:1) Launchpad includes both the jackrabbit-server (embedded repository) and jackrabbit-client (access to external repositories via JNDI or RMI) bundles but by default the jackrabbit-client does not provide a Repository.2) At startup the Sling class searches the classpath and/or environment for additional configuration properties3) A specific configuration property prevents the jackrabbit-server bundle from providing a Repository and lets the jackrabbit-client provide it.In this way the web container could be setup in advance to define which Repository to use and new releases of the launchpad war file could be dropped in without requiring any configuration or war file changes.,3860
Extract Method,Allow the ThreadPool to process Callable/Runnable and return a Future Currently the sling's ThreadPool [0] is not returning a Future when executing a task.However having a handle on the future result of the computation is useful for cases requiring fine grained synchronization among tasks.For instance one could require to execute tasks mixing parallel and sequential execution (Barrier synchronization). As an example we could take the following queue of tasks:queue: -->(task1)-->(task2 task3 task4)-->(task5)-->(task6task7)Group of tasks = (task1 … taskn)Groups of tasks are executed sequentiallyTasks in a group are executed in parallelThis could be easily implemented based on the Future objects returned by the ThreadPool.Thus it would be nice to enable it.[0] org.apache.sling.commons.threads.ThreadPool,3865
Inline Method,Potential Deadlocks may be caused by AdapterManager The AdapterManager uses a cascade of three synchronized blocks when the AdapterFactoryDescriptor.getFactory() method is called:* synchronized(cache)* synchronized(factories)* synchronized(this)A potential deadlock may happen because the AdapterFactoryDescriptor.getFactory() method asks the Declarative Services runtime for a service which may cause a ServiceFactory service to be instantiated (with a whole cascade of potential secondary actions depending on what happens on this instantiation).The last block can probably simply be removed while the others definitely have to be refactored to not be the cause or participants in deadlock situations. The reason for this is that (a) synchronized blocks must not be too large and (b) no Java locks should be held when calling into the OSGi Framework which happens in the AdapterFactoryDescriptor.getFactory method.,3867
Extract Method,Improvement for the Sling performance tools Added a few improvements to the Sling performance testing tools:1. Added the possibility to run each test method in a class as a performance test; until now a test was represented by a class (for each new test you had to add a new java class)2. Added a PerformanceTest annotation that is used to discover the test methods in a java class3. Added the possibility to provide with the PerformanceTest annotation a few configuration parameters for the test like the warmup time run time  warm up invocations or run invocations; by default the warmuptime and runtime are used but a user can choose not to count on the time but to prefer setting the number of test invocations that he wants to be made during the test run4. Created a new maven project that contains only the sling performance tests and left the framework(tool) related part in the base project as we would like to use the framework for performance testing in other projects also5. Added the possibility to have parameters sent to test suite object added a before suite and after suite method,3870
Rename Method,"Use repository.url.override.property in jackrabbit-client bundle SLING-254 implements a mechanism where the jackrabbit-server bundle uses a system property named ""repository.url.override.property""to override its configuration.The same mechanism should be implemented in the jackrabbit-client bundle to allow people to create Sling-based webapps that can be dropped in web containers and use a container-provided repository without requiring webapp configuration changes.",3874
Extract Method,Add support for run modes We should add run mode support to the launchpad installer like we have in the jcr installer,3876
Extract Method,Manually trigger sync on files/directories We should be able to manually publish a subtree of content as opposed to the sync happening in the background. One use case is where the auto-sync is disabled and the user only wants to publish the changes manually.,3878
Extract Method,Enhance run mode handling Often there is a need to provide different features where the user might have the ability to choose whether a feature should be installed or not.This can easily be done with a run mode so a feature is put into a run mode and if the run mode is activated by the user the feature is installed.However there are cases where two features might be concurrent so either feature A or feature B should be installed. Still run modes can be used by defining two run modes A and B - but in this case if the user does not choose one of the run modes nothing is installed - or if the user selects both run modes both are installed.We could support such use cases with some configuration properties for the settings service:a) we define a property which contains run modes that are usually always activeb) in addition we define a property which contains whether a run mode is active unless another run mode is active.So for the above use case we define run mode A as always active and also set the second property to define that A is not active once B is activated by the userWe could also define a third property which contains a set of run modes that are only choosable on the first startup of the installation so in the in the example above if this property is defined for A and B the user selection of the first startup is preserved,3883
Rename Method,Web console plugin for tenant management Extending the support for Tenant API we need to add a console plugin to create/remove tenants.We also need a pluggable support to register tenant setup handler so that various modules/implementations can be plugged in.,3885
Extract Method,resource access security service for resource providers without backing ACLs Adding a minmal resource access gate as discussed in [1].First step is to define the API interface and a minimal implementation which allows to define READ access (rest of CRUD can follow later)[1] http://markmail.org/thread/4ctczoiy533tquyl,3887
Extract Method,Support of chunked file upload into Sling Use cases: 1. Large file upload - With high speed internet connections advent of cloud and HD going mainstream Sling should support large files (> 2GB) upload.2. Fault tolerant uploads - Sling should provide capability to resume upload from failure point. It should not require client to restart the complete upload process. 3. Faster upload: Sling should support its clients to initiate multiple connection and upload file chunks in parallel.,3892
Extract Method,Add integration test for the error handling mechanism It would be nice to have some integration tests for the error handling mechanism described in http://sling.apache.org/site/errorhandling.html.I am attaching a patch that will contain the skeleton of such tests.Once committed I will extend it further.WDYT?,3894
Rename Method,Make ResourceMetadata read-only when delivered to client code As recently discussed in the mailing list ResourceMetadata is an object which provides additional metadata information about a resource but is not intended to be changed by client code.As ResourceMetadata extends from (Hash)Map it is read/write by default and might potentially be changed by client code.We should update the API docs that this object is read-only and also enforce it in our implementation.It seems so far no one is changing the ResourceMetadata after it has left the resource resolver therefore we can make it read-only after it is returned by the resource resolver.,3896
Move Method,Optionally run Sling on Apache OAK It would be nice to have a runMode of Sling that runs on top of Apache OAK [0][0] http://jackrabbit.apache.org/oak/,3897
Extract Method,"ujax post serlvet should be able to insert new nodes at specified location the current version only supports: ujax:order = 0 which orders a newly created node above it's siblings.suggest to implement the following variants:ujax:order= (""below"" | ""above"" ) [name]example: /a/b/cujax:order=""above b"" -> /a /newnode /b /cujax:order=""above a"" -> /newnode /a /b /cujax:order=""above"" -> /newnode /a /b /cujax:order=""below b"" -> /a/b/newnode/cujax:order=""below c"" -> /a/b/c/newnodeujax:order=""below"" -> /a/b/c/newnodePlease note that JCR does not support the 'below' ordering and it's 'above' handling is different. but i think in scripting environment where you loop over a list. the ""below"" makes sense since you don't need to know the next list item when drawing the 'insert' form.",3907
Extract Method,"Provide JaCoCo agent servlet to expose code coverage data to HTTP clients during Sling Junit Test execution create a simple REST endpoint for the jacoco agent at /system/sling/jacoco.exec to expose:jacoco Agent Status-------------------------------------------------------------------------------------------HEAD /system/sling/jacoco.execContent-Type: application/octet-stream200: jacoco agent is attached and exposed through JMX404: jacoco agent not foundreturn IAgent.getExecutionData(false)-------------------------------------------------------------------------------------------GET /system/sling/jacoco.execContent-Type: application/octet-stream200: execution data returned as response entity404: jacoco agent not foundReset execution data and return jacoco.exec file----------------------------------------------------------------------POST /system/sling/jacoco.exec [optional param "":sessionId"" to set a specific sessionId after reset]Content-Type: application/octet-stream200: agent was reset (with new sessionId if specified) and prior execution data returned as response entity 404: jacoco agent not foundJacoco instrumentation for OSGi is rather limited in the sense that there is one agent per JVM and resetting the execution data resets it across the board which means that concurrent requests to this service on a shared integration test server will need to be restricted during test execution to avoid corrupting coverage data.",3910
Extract Method,Extensible Sling system health checking tool I have created a prototype at https://github.com/bdelacretaz/muppet-prototype that we might want to move to our contrib folder.Muppet (it's like a Puppet but different (*)) allows you to check the health of a system by defining rules that (out of the box) verify things like the presence of specific OSGi bundles JMX MBeans values JUnit tests execution (including scriptable ones thanks to the Sling testing tools) correct disabling of default Sling credentials etc.New rule types can be defined by adding RuleBuilder OSGi services there are several examples in this initial code.I'll add a how-to for this initial version here. Known issues are:-The output does not indicate the value that causes a rule to fail-The servlet output is not JSON yet-Tags on rules would be nice to be able to run just the performance or security rules for example-A rule for checking OSGi configuration parameters would be useful.(*) credits to Joerg Hoh for that one as well as inspiration in https://github.com/joerghoh/cq5-healthcheck,3912
Extract Method,discovery.impl: a resource based implementation of the discovery.api This ticket is about contributing a resource based implementation of the discovery api (see [0]) named discovery.impl to Sling. The implementation is attached as a .tar.gz - its md5 hash is d8891e5401114b2a629d3ff01044a1d6Short description of the discovery.impl:The discovery.impl is an out-of-the-box implementation of the discovery.api using standard features of Sling. The discovery.api provides a view of a topology consisting of a number of individual sling-instances. The instances are loosely coupled except for being part of the topology they do not implicitly or necessarily share anything else. For those instances though that form a cluster - ie when connected to the same repository - the api has an abstraction called ClusterView.The discovery.impl uses two mechanisms for discovering other instances:* it stores information about the local instance at a unique location in the repository. Thus allowing other instances that access the same repository to see and recognize each other.* it connects to 'remote' instances via a plain HTTP POST announcing the instances that it can see and getting back the instances of the counterpartAll of the above is done regularly using a heart-beat - thus allowing to get a view of the currently live instances.The discovery.api additionally supports leader-election within a cluster: it ensures that one and only one instance is elected leader and stays leader until it disappears/shuts down/dies. The discovery.impl uses repository-based voting between the instances of a cluster to establish a common 'cluster view'. Based on an established view the discovery.impl is then able to deterministically elect one of the instances of the view as the leader (namely the one with the lowest 'id').Also to support the PropertyProvider concept of the discovery.api the properties of each instance are propagated to the other instances using the heartbeat as piggyback (either via the repository or via HTTP POSTs for remote instances).To get an idea of the discovery.impl build and add and start the two bundles (org.apache.sling.discovery.api and org.apache.sling.discovery.impl) to your sling installation and open the browser to the provided simplistic 'topology webconsole' athttp://localhost:4502/system/console/topologyPlease let me know if anything needs further explanation details. Looking forward to having this included in Sling!CheersStefan--[0] http://svn.apache.org/repos/asf/sling/trunk/contrib/extensions/discovery/api,3925
Extract Method,Avoid unnecessary classloader creation If a change is done through the class loader writer the class loader is notified of that change - if no classloader exists one is created. This can be avoided - if no classloader has been created until the change there is no need to just yet create one,3932
Move Method,Add inventory printer for JSON output With the new Apache Felix inventory printer we can add json output to web console plugins (or more precisely as an inventory printer).We could output all jobs information as json as well,3934
Extract Method,Replace administrative login by service-based login From the start Sling tried to solve the problem of providing services access to the repository and resource tree without having to hard code and configure any passwords. This was done first with the SlingRepository.loginAdministrative and later with the ResourceResolverFactory.getAdministrativeResourceResolver methods.Over time this mechanism proved to be the hammer to hit all nails. Particularly these methods while truly useful have the disadvantage of providing full administrative privileges to services where just some specific kind of privilege would be enough.For example for the JSP compiler it would be enough to be able to read the JSP source scripts and write the Java classes out to the JSP compiler's target location. Other access is not required. Similarly to manage users user management privileges are enough and no access to /content is really required.To solve this problem a new API for Service Authentication has been proposed at https://cwiki.apache.org/confluence/display/SLING/Service+Authentication. The prototype of which is implemented in http://svn.apache.org/repos/asf/sling/whiteboard/fmeschbe/deprecate_login_administrative.This issue is about merging the prototype code back into trunk and thus fully implementing the feature.,3936
Rename Method,Improve processing performance if job is processed locally Currently the job processing is completely observation based: even if a job is added locally and also processed locally this is not immediately put into the queue. This creates an unnecessary delay between adding a job and processing it which can easily be avoided by directly putting the job into the queue.,3943
Extract Method,[Tooling] show content of .content.xml in project explorer Irrespective of the chosen serialization the content of .content.xml should be shown in the tree structure of the project explorer. This can be achieved using a Navigator Content Extension (NCE) (which is part of the Common Navigator Framework (CNF) of eclipse).,3945
Extract Method,Merged Resource Provider As exchanged once with Felix Meschberger the idea is to implement a custom resource provider with ability to merge multiple resources based on your search paths.For instance if your search paths are/apps/libsHitting /merge/my/resource/is/here will check/apps/my/resource/is/here/libs/my/resource/is/hereThere are some options like:- add/override property- delete a property of the resource under /libs- reorder nodes if availableI intend to submit this patch as soon as possible.Code is currently located at https://github.com/gknob/sling-resourcemerger,3949
Extract Method,Simplified health check services After some prototyping the health check tools are ready for a rewrite that will make them simpler and more OSGi friendly. The functionality will be similar but with much less code more focused on the actual use cases that have emerged during prototyping. The new API is being discussed on list http://markmail.org/thread/i6ib7tgax4cn2sss,3951
Extract Method,Support wildcard (was: regular expressions) in IP whitelist for discovery/topology connectors Currently the IP white listing feature of discovery.impl requires explicit IP addresses or fully qualified hostnames. I more complex setups it can be useful to define regular expressions eg for hostnames eg *.mydomain.com.,3952
Extract Method,[Tooling] support auto-deploy of osgi bundles from eclipse to a running sling launchpad From Eclipse for a bundle type maven project it should be possible to auto-deploy the built bundle into the configured sling launchpad. Ideally but optionally we'd have hot-code replacement of individual classes of that bundle. But auto redeploying the entire bundle is fine as a start.We should look into such tools like Eclipse libra (http://www.eclipse.org/libra/) or Spring-Loaded (https://github.com/SpringSource/spring-loaded) for help on redeploy/hotcode replacement,3954
Rename Method,Improve Karaf integration tests - separate testing support and test into own packages and jars- use Maven Failsafe Plugin for tests- update Pax Exam to 4.9.1- enable test (remove Ignore annotation)- increase timeouts to 300000 ms- test features,3960
Extract Method,Provide a mechanism to install a bundle based on a directory With an IDE integration and incremental builds it would be great to have a mechanism to install/update a bundle based on the output directory of a project.I'll start with a prototype of a bundle which receives post requests containing the path of a local directory. The servlet will then either install or update the bundle from that directoryFor this to work the scr descriptor files of a project should go into a bundle. So configure the scr plugin like this:<plugin><groupId>org.apache.felix</groupId><artifactId>maven-scr-plugin</artifactId><configuration><outputDirectory>${project.build.directory}/classes</outputDirectory></configuration></plugin>,3963
Extract Method,Immutable HealthCheck Results As discussed on list I'll change the Result class to be immutable and allow for both single-value and log-based results:Result is immutable.Result has two constructors one that takes a Status and a message String and one that takes a ResultLog and sets the Result status to ResultLog.getStatus().The ResultLog is a list of messages each with a Status and a message String. It's getStatus() method returns the highest Status that was added to it.,3967
Rename Method,Use service properties for HC meta data and improve JMX registration As discussed in the mailing list we can simplify the health check api by using service properties for all meta data of a HC.In addition the jmx registration bridge needs updates on how to handle if two HC services are using the same name for registrationMail threads:http://mail-archives.us.apache.org/mod_mbox/sling-dev/201308.mbox/%3CCAKkCf4r89JbsVP_-RQK=R304wBL8gqpzAq4Rw3Z9sijWbBo8Yg@mail.gmail.com%3Ehttp://mail-archives.us.apache.org/mod_mbox/sling-dev/201308.mbox/%3CCAKkCf4q79NO26Va542s_QghoF1F1UvsjNyLs6as_2xQczRecpg@mail.gmail.com%3E,3968
Rename Method,Support for progress tracking of jobs and keeping jobs For long-running jobs it would be useful to have some means to track progress which can be shown in a console for the user. This should include the following:* ETA* Completeness value computed from (optional defaults to 1.0) max and current value (e.g. 42% or 23/100)* Log output stream for detailed progress information* Failure reason in case job failedAFAICS this requires a few changes to the existing implementation:* Jobs need additional support for setting properties e.g. max and current progress value* Jobs need to be kept at least for a while after they completed/failed to give access to failure information/log stream,3974
Extract Method,Filter to populate SLF4J MDC with request details With Commons Log moving to Logback its possible to use MDC [1] support of SLF4J in Sling to get better logs and also enable better filtering of logs. The patch adds a new extension module which registers a {{MDCInsertingFilter}}. This filter can extract information from incoming request and add them to the MDCFeatures supported1. By default expose details like Request path Query String etc based on [3]2. Exposes OSGi config where one can define what HTTP Header Parameter or Cookie needs to be added to MDC3. Exposes Session ID USer Information from RequestResolver associated with current thread of executionThis information can then later be exposed via logs like shown below. Here we expose the SessionID as part of log---%d{dd.MM.yyyy HH:mm:ss.SSS} *%level* [%thread] %logger [%X{jcr.sessionId}] %msg%n---The same information can also be used to filter logs based on userurl etc or used in custom Logging event evaluatorFor more details refer to [2][1] http://www.slf4j.org/manual.html#mdc[2] https://github.com/chetanmeh/sling-logback/tree/master/mdc[3] http://logback.qos.ch/manual/mdc.html#mis,3980
Extract Method,APT parser and default servlet I'd like to use a wiki-like format to create interactive docs for the Launchpad.I've done some tests with the doxia-module-apt used by Maven. It provides a customizable APT parser including macros for which we can easily provide our own (OSGI-based if needed) code.I'll create two new modules as a first shot a parser module (simple wrapper around doxia-module-apt) and a default APT servlet to render *.apt files stored in the repository.,3981
Rename Method,Enable Logback to use OSGi services for Filters and TurboFilters Currently the Logback integration enabled use of OSGi services as Appenders [1]. It would be helpful if it can also enabled using OSGi services for Logback Filters and TurboFilters [2]. The filters can be used to precisely extract required logs for certain flow and prove to be very useful for debugging complex issues[1] https://github.com/chetanmeh/sling-logback#appenders-and-whiteboard-pattern[2] http://logback.qos.ch/manual/filters.html,3984
Extract Method,"JsonQueryServlet it would be nice to have a way to specify a jcr query in the url in order to query the underlying jcr repository.the attached patch works as follows:The new query servlet is called if the extension is "".json"" and the selector is set to ""query"". you can pass following parameters in the url:statement: jcr query statement (XPATH or SQL)queryType: xpath or sql (of none is specified xpath is taken)property: specifies which property (relative path) has to be ""put"" into the result set (this parameter can be added multiple times)excerptPath: specifies the relative node path from where the excerpt has to be built.the result is returned as json string. e.g.:[{""name"":""ee0""""rep:excerpt()"":""<excerpt><fragment>geometrixx/components/contentpage ... ee<\/fragment><\/excerpt>""""jcr:path"":""/content/ee0""""jcr:score"":""528""""cq:content/jcr:title"":""ee""}{""name"":""news""""rep:excerpt()"":""<excerpt><fragment> ... geometrixx/components/contentpage ... ... ... ... news about geometrix<\/fragment><\/excerpt>""""jcr:path"":""/content/geometrixx/en/about/news""""jcr:score"":""521""""cq:content/jcr:title"":""News""}]example query call:http://localhost:8080/sling/myhome.query.json?statement=//element(*cq:Page)[jcr:contains(.'sling')]/(rep:excerpt(.))&property=cq:content/jcr:title&excerptPath=cq:content",3985
Rename Method,Improve Authentication Handling The following scheme should improve the authentication handling:Anonymous user accesses content he's not allowed to access- During resource resolution a access control exception is thrown- This exception should be catch at this point and a request for authentication should be send backAuthenticated user accesses content he's not allowed to access- During resource resolution a access control exception is thrown- a 404 should be send back,3987
Extract Method,[Tooling] integrate with decentXml for minimal changes on .content.xml modifications Currently when modifying properties or adding/removing nodes in the content-browser ie in .content.xml the xml gets rewritten and looses formatting such as newlines within an element between attributes (which is the default in .content.xml generated by vault).DecentXml (http://code.google.com/p/decentxml/ 'New BSD License') is a tool which allows to retain the formatting (sax/dom-parsers dont). Integrate with this tool to keep source changes to a minimum.,3989
Inline Method,Allow installing/updating the install support bundle from the servers view Currently the only way to install the support/install bundle is if we're creating a new server when creating a new bundle. this option should be available from the server editor page.Implementation plan:* Extract an OSGiClient and make it part of API/Core* Create an OSGiClientFactory and make it a SCR component+service* Make access to the archetype resources public probably as part of a separate bundle ( a follow-up bug should make this bundle contain all embedded resources )* Implement bundle install in the server page maybe with special logic for SNAPSHOTs,3990
Extract Method,Centralize and improve embedded artifact handling We will probably keep our embedded artifact handling to support cases where users don't have access to artifacts we use and to provide consistency to the user experience. I can imagine at least:- embedding archetypes to generate projects- embedding additional development-time only bundles for the sling installationTo make them reusable from multiple contexts I propose that we1. Package all the embedded artifacts into a single plugin - can be the eclipse.core plugin for now but I'd prefer something like org.apache.sling.ide.artifacts2. Create an OSGi DS component which reads the artifacts using the bundle context3. Do not hardcode versions. Rather read them from the embedded manifest - for bundles or pom.properties - for non-OSGi Maven artifacts. Possibly cache this information lazily once it's retrieved.,3991
Rename Method,Provide a way to schedule jobs The current way of scheduling jobs is not very user friendly: it requires to send an event via event admin. We should add a java api to create scheduled job creation.In addition we could add a new queue type: maintenance queue which is an order queue with the difference of running jobs exactly at the point of time when they are created. So a scheduled maintenance job is executed exactly at the scheduled time - unless another job is currently running.Apart from starting such jobs stopping a job would be another nice addition,3992
Extract Method,Implement support for Feature Flags/Toggles in Sling It would be nice if sling provide support for feature flags (also called toggles) pattern [1].I am thinking the implementation could provide the following.1) Integrate an existing framework such as togglz [2] or implement something similar with UI/Configuration to toggle features.2) Create a jcr property (sling:Feature) which can be added to resource type nodes. Then some conditional logic in <sling:include> and get and post servlets determines if the resource should be rendered if its resource type contains such property.[1] http://en.wikipedia.org/wiki/Feature_toggle[2] http://www.togglz.org/,3995
Extract Method,Add Topology Message Verification to the Discovery service. The discovery service provides support for whitelisting sources of topology information but in a cluster where the topology this creates a re-configuration load of order M*(n*(n-1)) where n is the number of nodes in the topology and M is the number of changes. That load rises rapidly as the number of changes and/or nodes increases. To address this there are 2 proposals.1. To provide an SPI exported from the Discovery Impl bundle that allows implementors to implement whitelisting based on the request. This will need to support creating the request and validating the request.2. Embed functionality within the Discovery Impl bundle that supports validation and encryption of topology requests.,3998
Extract Method,Deprecating JobUtil class? This is a follow up from SLING-3028 based on comments by Stefan Seifert:the JobUtil class is deprecated by 90%. perhaps its better to deprecated it completely and move the remaining 10% to a new location or other classes more suitable. This would solve the inconsistency with the JobUtil.JobPriority enum as well which is not part of the Job interface whereas JobType is part of the Job interface.,4000
Extract Method,Thread pool configuration per queue For better controlling of thread usage wrt queues the queue should be configurable with a thread pool name. If that pool exists the queue will use this one if not it will default to the default eventing thread pool,4001
Extract Method,Avoid duplicated requests to mbeans when creating resources The current implementation is not very optimal it queries attribute values of an mbean over and over again especially when a whole mbean is outputted as a resource tree e.g. when converted to json,4004
Extract Method,Allow refering to OSGi appenders from within Logback config Default Logback config only allows referring to appenders defined within the config file. To enable better integration with OSGi it would be better if one can refer to an appender which is implemented as an OSGi service from within the config.In such case logging system must attach the appender to the logger whenever the appender service is registered,4005
Extract Method,SingAuthenticator: improve Repository sanity check Currently the SlingAuthenticator sometimes behaves in funny ways if the Repository is not available or not accepting logins for some reason. If anonymous access is allowed for example a login box might appear if the repository becomes unavailable which makes things confusing.The attached patch improves the situation by verifying that an admin session can be obtained from the Repository and throws MissingRepositoryException if not.This should allow better handling of Repository problems in higher application layers.I'm not sure about all the implications - can we assume that an admin session is required for things to work?Please review the patch before I apply it or feel free to apply it as I'll be mostly offline until after Easter,4015
Rename Method,Make Sling imports dynamic Currently the imports to sling auth core and sling api resource are mandatory this means the security provider is only active if Sling is running.These imports should rather be dynamic as the provider should also run if only the repository is available,4016
Inline Method,Switch to login page if user is not allowed to access the web console RIght now if the current user is not allowed to access the web console forbidden is returned - we should rather redirect to the login page instead,4018
Extract Method,"Provide a HealthCheckExecutor service Goals:* Be able to get an overall (aggregated) result as quickly as possible (ideally <2sec)* Whenever possible return most current results (e.g. for a memory check)* Provide a declarative way for async checks (async checks should be the exception though) Approach* Run checks in parallel* Make sure long running (or even stuck) checks are timed out* If a health check must run asynchronously (because its execution time cannot be optimized) it should be enough to just specify a service property (e.g. ""hc.async"").See alsohttp://apache-sling.73963.n3.nabble.com/Health-Check-Improvements-td4029330.html#a4029402http://apache-sling.73963.n3.nabble.com/Health-checks-execution-service-td4028477.html",4020
Extract Method,Long startup time with many vanityPath When many vanityPath or alias are present the system take long time to startup  Same when a vanityPath/alias is removed or updated .The reason behind is the usage of a query that updates the global mapentry.I have added a new Test to the performance test suite and this is the outcome{code}0 vanityPath 16ms1 vanityPath 19ms10 vanityPath 70ms100 vanityPath 111ms1000 vanityPath 200ms10000 vanityPath 1173ms30000 vanityPath 3358ms{code},4022
Extract Method,Optimize JcrPropertyMap.escapeKeyName() Escaping use Session.getNamespacePrefixes() method for every single property which is relatively heavy. This method can be called once per instance and stored as member variable which should reduce significantly number of calls for prefixes.,4023
Rename Method,Provide annotation-driven approach to create Model objects I've been working on a model factory approach in my whiteboard and think it is at the state to be formally named Sling Models and moved into extensions.http://svn.apache.org/repos/asf/sling/whiteboard/justin/yamf/Code would be repackage and renamed where appropriate.Seehttps://cwiki.apache.org/confluence/display/SLING/YAMF+-+Yet+Another+Model+Factory,4025
Rename Method,Remove packages after being processed After a replication package it is used cleanup the resources it occupied.,4028
Extract Method,Allow asynchronous package import Let the ReplicationPackageImporter be able to import replication package streams asynchronously this will involve having its own queue.In the future it may be worth moving such functionalities directly into polling agents so that existing queue providers and queue distribution strategy can be used also for package imports.,4030
Rename Method,Bootdelegation support for third party library Requirement:1) Bootdelegate third party library in SLing OSGI framework.2) Bootdelegated classes and resources should be looked up from configured third party jars only. Similar classes from parent classloaders should not leak into system. This case arises usually in Web deployment scenario. Infact we want to bootdelegate RSA library and at the same time prevent interference from parent classloaders in Web deployment model.Approach:1) We extend SlingLauncherClassloader to scan a predefined path (say <launchpadhome>/lib/etx) for jar files and add them to its classpath. This is done at startup. SlingLauncherClassloader will try to load class from its classpath before trying parent classloaders. Currently it is done for org.apache.sling.launchpad.base.jar. 2) The packages can be configured for bootdelegation using sling properties:sling.bootdelegation.class.<any bootdelegated class>=<bootdelegated package>,4032
Rename Method,Reduce memory footprint of JcrResourceListener JcrResourceListener.onEvent(EventIterator) keeps references to all Event instances in the passed iterator. This has turned out as a memory bottleneck e.g. in the scenario where a large sub-tree is copied and NODE_ADDED events for all nodes in that sub-tree are generated. Furthermore Oak will be able to handle arbitrarily large transactions (Session.save). In such cases the EventIterator might contains millions of events.,4034
Extract Method,Implement job acknowledge Currently there is no acknowledge that someone is interested in processing a job. This results in the problemthat jobs might end up locked in the repository for ever if there is noone interested in processing this kind ofjob.This couldbe solved by requiring that a job processor acknowledges the receival of a job and thereby indicates that it will process this job. This could also be used to deny other processors for this job.If a job is not acknowledged during a specified timeout the job will be removed.,4038
Rename Method,Support multiple bundles for jarWebSupport The Launchpad Plugin allows to define the Http Service implementation to be specified with the jarWebSupport property. This property currently only supports a single bundle.Now the Felix Jetty Http Service implementation has been refactored (FELIX-4427) to not export the API anymore. Thus the Http Service API and the Servlet API have to be provided by one or more additional bundles.Hence the jarWebSupport property should be extended to support multiple bundles.,4040
Rename Method,"GET /foo/*.html should return the equivalent of an empty Node Being discussed on sling-dev in the ""GET the magic star"" thread [1].[1] http://www.mail-archive.com/sling-dev@incubator.apache.org/msg03603.html",4045
Extract Method,Avoid shared resource resolver usage The servlet resolver uses a single shared rsource resolver to resolve scripts. Resource resolvers are not thread safe and therefore should not be used at the same time from different threads.Apart from that this creates a bottleneck as the repository implementations (Jackrabbit Oak) synchronize all access in this case - which then ultimately synchronises all requests. This is even more problematic as the servlet resolver is hit alot during a single request,4048
Extract Method,WebConsoleClient should support bundle start levels WebconsoleClient.installBundle(...) needs an optional start level parameter.,4051
Extract Method,Internal refactoring to better support meta-annotations Meta-annotations are currently support for @Source. The code should be more flexible. NOTE - this issue doesn't actually add other meta-annotation support just makes the addition of that support easier in the future.,4052
Rename Method,Facilitate writing of integration tests for multiple instances Facilitate writing of integration tests for multiple instances. This is needed for better testing features like replication or topology discovery.Create a base class for such tests and a way to configure multiple instances and also sample integration test.,4053
Rename Method,Make CompositeHealthCheck use HealthCheckExecutor for parallel execution As the CompositeHealthCheck is used fairly heavily in the well-known product ([1] & [2]) it would be good to make the parallel execution available for the CompositeHealthCheck as well (this would also be in line with the web console that is already using the HealthCheckExecutor). The attached patch - uses the HealthCheckExecutor (shortening the implementation of the CompositeHealthCheck.execute() for quite a bit)- needs to detect cycles in the configuration in a different way (the ThreadLocal does not work anymore)- comes with a unit test that tests both the execution and the cycle detection[1] http://localhost:4502/system/sling/monitoring/mbeans/org/apache/sling/healthcheck/HealthCheck/systemchecks.json[2] http://localhost:4502/libs/granite/operations/content/hr.html/system/sling/monitoring/mbeans/org/apache/sling/healthcheck/HealthCheck/securitychecks,4055
Extract Method,Improve handling of updates to mapping (alias vanity path) The update handling for the mapping including aliases and vanity path is a simple algorithm which simply updates the whole mapping. Especially with large mapping info spread across the repository a simple update of a single property results in the whole mapping info to be recreated.It would be great if this could be improved.Right now only the active mapping is hold in memory - as a change of a single property might cause to activate a totally different mapping than the one which was changed (e.h. when the ordering is changed) the update needs to be more complex or more information needs to be hold in memory.In addition there is more to consider like if the vanity path info is changed only the new value is available - but the old is gone.,4059
Rename Method,Add ability to configure OSGi components from crank files Users should be able to specify OSGi configs inside .crank.txt files.,4063
Extract Method,Integration tests running against a live Sling Launchpad instance There are lots of moving parts - and still quite some bugs - when it comes to transporting content to/from a Sling instance.It would be helpful to create a test harness which validates that the way we transport/read/write/filter content is correct. This will then help when fixing the various bugs we have related to content sync.,4065
Rename Method,Run FindBugs (and fix accordingly) on Sling Replication code Sling Replication code has grown over time and I'd like to a) fix current problems as reported by FindBugs b) add a reporting entry to the pom.xml in order to be continuously able to check eventual problems.,4074
Rename Method,Allow importing content from arbitrary locations The current import wizard only processes the whole project. This is problematic for large projects where import would take a lot of time. Also at times a developer makes a small change in the repository and wants to sync it back to the workspace.To support these scenarios we need to allow importing content from arbitrary locations under the content sync root.,4076
Rename Method,Make RepositoryFactory statefull allow a Repository to be stopped At the moment we have a stateless RepositoryFactory ie it only provides newRepository() which creates a new repository each time. This was fine as long as the repository was stateless - but with the introduction of the NodeTypeRegistry for example the repository initialization is now also heavy-weight.,4077
Rename Method,Allow performance tests to report custom class name to the report logger allow performance tests to overwrite the class name passed in by the junit runner to the report logger. This is useful for test factories and the like.SLING-3294 breaks SLING-2727 because it forces a toString method on the test class. The proposed solution allows this only if the test case implements IdentifiableClassName#testClassName(),4079
Extract Method,Usability improvements in the Sling bundle module wizard A couple of issues can be improved in this wizard* the archetype selection drop-down spans multiple lines and is misaligned * the server selection page should default to 'Add to existing server' if one already exists* the server location should preselect the existing server if only one exists,4081
Rename Method,"Add SlingHealthCheck annotation Add a SlingHealthCheck annotation (similar to SlingServlet). The metatype property should set to true per default since its probably common to configure healthchecks. Usage examples:{code:title=Examples|borderStyle=solid}@SlingHealthCheck(name = BundlesStartedCheck.HC_NAME label = ""Apache Sling Health Check - "" + BundlesStartedCheck.HC_NAME description = ""Checks whether all bundles are started."" tags = ""osgi"")@SlingHealthCheck(name = DiskSpaceCheck.HC_NAME label = ""Apache Sling Health Check - "" + DiskSpaceCheck.HC_NAMEdescription = ""Checks whether enough disk space is available."" tags = ""resources"" configurationFactory = trueconfigurationPolicy = ConfigurationPolicy.REQUIRE){code}",4085
Extract Method,"Add support for ResourceBundle base names From http://markmail.org/message/6ikv4wfabqtnmajk:1) Base NamesThe concept for ResourceBundle base names is derived from the ResourceBundle.getBundle(String baseName) factory method which returns a ResourceBundle for the given base name. While the ResourceBundle.getBundle factory method looks for ResourceBundle implementations (e.g. ListResourceBundle class or a properties file for the PropertyResourceBundle class) the Sling implementation of a newResourceBundle ResourceBundleProvider.getResourceBundle(String baseName Locale locale)method uses an extended query to find the matching resources.The basic query is to find the resources below nodes which have a node type of mix:language and a property jcr:language whose value is the string representation of the Locale. The extended query would also consider a new property sling:basename: A lanugage node is considered if the jcr:language property matches the Locale and the sling:basename property matches the baseName.The sling:basename property is defined in a new mixin node type[sling:Language] > mix:languagemixin- sling:basename (string)- sling:basename (string) multipleThat is the sling:basename may be a single-valued or multi-valued property. Thus a language node may apply to multiple basenames.The base name generally is an application identifier.In addition the SlingHttpServletRequest interface should be extended with:ResourceBundle getResourceBundle(String baseName Locale locale)which allows requests to easily get ""base-named"" resource bundles.",4089
Extract Method,Support modifying of DataSource properties at runtime without restart Tomcat JDBC Pool supports changing the connection pool properties at runtime also. Sling DataSource provider should support that and avoid deregistration of DataSource service as much as possible,4092
Extract Method,JCR Properties view: boolean values are not validated according to the type The property values in the JCR Properties view are not validated according to the boolean property type. It is possible to enter any value for boolean type.,4093
Rename Method,Refine 'connected' state of a (vlt) repository cache node types after disconnection properly Currently the ServerUtil.getDefaultRepository always returns a valid repository with which you can do getNodeTypeRegistry() and that in turn connects to the server and loads the node types. This seemed convenient.But the problem is it doesn't give the user any control when to connect to the server and when not. And it will result in connection errors at places where the user maybe didn't intend or know that a connection would be done.Hence a new simpler schema:* when the server is stopped no 'repository connection' is being established including for the node type registry** hence with a stopped server the node type registry can be null - hence some actions require adjustments for that situation* when the server is started ('connected to') the repository is connected and the node type registry loaded** at this stage the node type registry can be used for various actions including code completion property type display* when the server is stopped again the node type registry is cached and still provided to the various actions (without any server interaction going on though)This should be more intuitive and make initial content-browsing in 'offline mode' simpler,4095
Extract Method,Add Cut / Copy / Paste nodes to content navigator In day-to-day activities being able to manipulate nodes in the content navigator with Cut / Copy / Paste operations is really critical./cc [~fvisser],4097
Inline Method,Better default handling for script selection Currently the ScriptedComponent analyzes the registered scripts and just falls back to a default script called jsp/start.jsp. This default selection should be extended such that request selectors (see ComponentRequest.getSelectors()) contrubute to the script selection as follows:- First apply all registered Script mappings of the ScriptedComponent- Next use the request selectors to find a script below the scripted component node- If the second step does not resolve a script (or if no selectors exist) just use the start.jsp script below the component nodeThe second step needs some more explanations: The selectors of the request are joined with interleaving slashes to form a relative path. This path is appended to the path of the scripted component and the name of script is assumed to be start.jsp. If the path does not exist trailing elements are just removed and such a longest match is tried to be found.Example: (1) request for sample.htmlNo selectors exist the start.jsp script below the component is used(2) request for sample.image.gifSelector is a single entry list [ image ]. First the script image/start.jsp is checked if that does not exist the start.jsp is used.(3) request for sample.image.80x80.gifThe selectors is a two-entry list [ image 80x80 ]. The following scripts are tested in order: image/80x80/start.jsp image/start.jsp start.jsp.,4098
Extract Method,Sling Models: Allow caller to deal with exceptions Currently due to the specification of the adaptTo-method to return null if adaptation is not possible the caller is not notified about any exceptions (because they are caught within the ModelAdapterFactory).This is e.g. necessary to deal with validation exceptions properly (i.e. required field injection not possible). The problem was also discussed briefly in http://apache-sling.73963.n3.nabble.com/Silng-Models-Validation-Framework-td4033411.html.All exceptions either being thrown by the @PostConstruct method or caused by the field/method injection are not propagated but basically swallowed by Sling Models.It would be great to be able to catch those exceptions either in the view or in a servlet filter. I think it should be possible to throw unchecked exceptions in the ModelAdapterFactory.getFactory() method if it is requested (i.e. through a global OSGi configuration flag for Sling Models).WDYT?Would you accept such a patch or do you think this breaks the API (also compare with https://issues.apache.org/jira/browse/SLING-2712?focusedCommentId=13561516&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13561516).If it does not work through the adaptTo SlingModels should provide an alternative way of instanciating models (and propagating exceptions) although this is kind of tricky because it internally relies on adaptTo as well (e.g. in https://github.com/apache/sling/blob/trunk/bundles/extensions/models/impl/src/main/java/org/apache/sling/models/impl/ModelAdapterFactory.java#L647),4100
Extract Method,"[OSGi Installer] Intelligently manage start level during artifact handling OSGi Installer should be made more intelligent in deciding how to handle bundle updates if multiple bundles need to be updated e.g. by lowering the system start level when updating important bundles or when updating a large number of bundles to reduce oscillation of the system and thus reduce update time and increase stability.Some thoughts around this as per [~fmeschbe]:* Bundle installations are not problematic and don't need to be treated specially* Bundle updates and uninstallations may cause the system to oscillated because services may come and go during updates and package refreshes* Some bundles are more central than other bundles. For example the bundle providing the JCR Repository service is ""used"" by almost all of the system. And bundle may be important because a certain threshold if API importing bundles is reached (e.g. the Sling API bundle) or because one of the registered services is used by a certain threshold of consumers.* When at least one important bundle is updated or uninstalled in a single batch the OSGi installer should reduce the system start level before handling the batch and raise the start level after the batch again.* The start level to which the system is reduced is the lower of the start level of the OSGi installer and its helper bundles (probably the Task provides) and the start levels of the important bundle(s) handled in the batch.",4102
Extract Method,"[i18n][Performance] Optimize initialization of ResourceBundle cache I ran into the performance issue described in SLING-2881 (JcrResourceBundleProvider clears the cache on mix:language changes) and updated the SLing I18N bundle to version 2.2.8.However with the preloadBundles flag set to false (the default) the system stalled after being put under load.According to my analysis there were lots of cuncurrent HTTP requests all requesting the i18n ResourceBundle. Because the cache was not yet initialized each of them started loading it from the repository. The system practically came to a hold and did not recover after removing the load (I waited ~10min). I believe it might have eventually recovered as I could not identify a deadlock in a series of thred-dumps.I suggest three improvements:# Change the default of preloadBundles to ""true"". This should avoid having HTTP requests waiting for the cache to be initialized.# Ensure each ResourceBundle is only loaded into cache once (when the cache is empty) in order to limit I/O.# Adjust the queries for languageRootPaths to only return nodes that have a @jcr:language property.The attached patch implements points 2 and 3. It includes a test that verifies correct concurrent behaviour. The test can be run against the current implementation and should fail (it did reliably fail on my machine).Additionally I verified the fix on a local Sling instance using Apache Bench (ab) and finally on a production system. The production system loaded the ResourceBundles for four languages and recovered from that load after a few seconds.Could you kindly review the patch and apply the changes. Thank you.",4104
Extract Method,Add tests and refactor JSON to String conversion There's some code duplication in JSONObject JSONArray and JSONWriter when it comes to creating JSON strings and the test coverage is very low.We should improve this to allow extensions like SLING-3776 to be introduced without breaking existing code.,4107
Extract Method,Allow deep vs shallow replication options Users of replication should have a way to choose whether to replicate a node- deep: including all its subtree- shallow: including just he node propertiesTypically an user editing content visually will trigger a replication of only the edited content (shallow).However an user importing a lot of content will trigger a deep replication after all content is imported.Shallow replication is more targeted and produces the least changes in the system and it is probably best fitted to be the default replication option.,4111
Extract Method,Add configuration option to restrict service user mapper to system users JCR-3802 introduces the concept of system users that distinct from regular user accounts and never have a password set. the API extensions include to following ability to discover if a given User is actually a system user: {{User.isSystemUser}}.It would be good if the service user mapping had a configuration option that would restrict the mapping to dedicated service users i.e. to users which are defined to be system users in case sling is running on a JCR repository that implements jackrabbit API.,4114
Move Method,Create ReplicationTriggers as a HTTP accessible variant of a ReplicationRule ReplicationRules are a way of signaling ReplicationRequest to external users. However they do take their configuration at the time of handler registration.We need a concept that is completely configured via OSGI in order to make it accessible via http. We should be able to expose all rules via http in order to trigger replication from events published by a remote instance.For example the url for a trigger that generates poll requests at a fixed interval will look like this:{code}http://localhost:4503/libs/sling/replication/services/triggers/scheduledpoll{code},4116
Extract Method,Add a resource path injector It is useful to be able to inject a resource by path either a static path or a path defined by a property.,4119
Extract Method,"Sling Models: support adapters for models different from the implementation class currently as adapter (adaption target) only the implementation class itself that is annotated with the @Model annotation is supported (which can be either an interface or a class).if the model is not just a simple model but a class with more complex business logic the following scenario is required:* a ""service"" interface is defined* this service interface ist not directly mapped to injected values but provides higher-level method* an implementation class with @Model annotation is implemented which gets the required values injectd internally but implements the interface for outside accessthis is currently not possible with sling models.the attached patch extends the following features:* extends the @Model annotation with an optional ""adapters"" attribute* if defined only the listed adapters are registered for the adapter factory not the implementation class itself (unless it is listed the ""adapters"" attribute as well)* in the adapters attribute only the implementation class itself or interfaces that it implements or superclasses can be defined* with this the scenario above is perfectly possible* unit tests included which simulate the bundle add/remove usecases which is required to do the indirect implementation class mapping",4120
Inline Method,resourceresolver-mock: Make compatible to JCR resource the attached patch enhances the resourceresolver-mock implementation with this features that are missing to make it behave JCR-like:* support resources for properties* support converting Date objects to Calendar and vice versa* support storing and retrieving binary data using InputStream.class* return value of property jcr:primaryType as resource type when no resource type is definedand it adds unit tests for the whole sling CRUD behavior and these specialities.,4122
Rename Method,Replication transport authenticators should hide the credentials The transport authentication credentials should not be present in the configuration of an agent as it is currently (see SLING-3898).,4130
Extract Method,"Launchpad ControlListener improvement I was involved with a customer report indicating that the ControlListener would not work. The customer issued a stop command upon which the system should have shut down. This seems to not have been the case so the customer tried to issue stop again. This time though the connection to the control port could be opened but the command was not answered.Looking at the situation it looks like the first command issues the shutdown. But this caused a deadlock. The server side socket was terminating and the control thread was waiting for sling to shutdown. While waiting for the shutdown the control thread could of course not accept new commands even though TCP/IP connections were being setup.To prevent such situations I am proposing an extension to the Launchpad ControlListener which listens on a TCP/IP control port for status and stop commands:* Calling stop starts a separate thread to shutdown Sling asynchronously to the ControlListener thread. After having issued a stop command the ControlListener will reply ""STOPPING"" to the status command* Adding a new ""threads"" command to have the ControlListener print a thread dump similar to the JDK jstack command. This includes all threads as well as dumping potential deadlocks. A new Unit test contains two Thread classes which provoke deadlocks to show how the thread dumper reports deadlocks.",4132
Extract Method,ErrorAwareQueueDistributionStrategy should be implemented as an ImporterDistributionStrategy Treating replication errors should be done right after the import not when package queuing is done. Hence we might need to define a different extension point (ImporterDistributionStrategy) and treat the error at that place.,4133
Rename Method,ResourceProviderEntry Should Poll All Viable ModifyingResourceProviders During creation and deletion Currently the ResourceProviderEntry during creation or deletion of a resource will send the creation or deletion request to the first viable ModifyingResourceProvider. This is problematic if multiple ModifyingResourceProviders may be viable for a particular path. Consider the following situation:I create a Resource Provider which listens to /content such that it can participate in resource listing calls on that path. It however does not necessarily handle all resources under /content the root JcrResourceProvider is also expected to handle some resources under that path. This works fine for the various get methods however for modification the custom Resource Provider always gets chosen first and if it is not able to create a resource the creation request does not fall back to less specific providers or the root provider even though those provider might well be able to create the resource. The common pattern across many of the ResourceResolver operations is to poll all viable Resource Providers. For example getResource will iterate over all viable Resource Providers in order of path specificity stopping only if one of them is able to return a resource. The attached patch implements a similar mechanism for the create and delete commands. Create will iterate over all viable ModifyingResourceProviders in order of specificity stopping if one of them actually produces a Resource. Delete will iterate over all viable ModifyingResourceProviders giving each a crack at deleting the resource.,4135
Rename Method,"Testing: Donate sling-mock jcr-mock osgi-mock implementation donate a at suite of mocking libraries to run OSGi/SCR JCR and esp. Sling in a simulated ""in-memory"" environment for unit tests ensuring minimal setup time. it uses either a mocked in-memory JCR or the resourceresolver-mock implementation that is already part of the sling project. additional convenience features like bulk-loading JSON content and binaries into the simulated resource tree via a content loader makes it easy setting up complex text fixtures for your unit tests.",4145
Inline Method,Avoid keeping jobs in memory Currently all jobs for a single instance are hold in memory and put into the queues. This is not optiomal especially for a large amount of jobs. In addition it makes configuration changes queue updates etc. more complicated.We should revert this and let a queue only pick a job from the resource tree when it has a free processing space.,4151
Rename Method,Add notification when a job is added The job notification cover all cases except when a job is added.We shoud also clarify that notifications are only send locally and never across a cluster (using remote events),4160
Extract Method,Clean up code and logging statements We could clean up some unused code based on the refactorings done for the 3.4.0 release.In addition some logging statements might have wrong log level or might be missing.,4165
Extract Method,JCR/Sling/ResourceResolver Mock: Support providing authentication info currently it is not supported to pass custom authentication info to the mocked resource resolver/session.this should be changed to optionally allow this e.g. to simulate two different session/resource resolver with two different users.,4168
Extract Method,Compact and more specific logs from RequestProgressTrackerLogFilter The RequestProgressTracker currently logs each RPT message as a separate log entry. Considering the potentially large number of log entries this can add significantly to the size of log files and the I/O involved.By logging all RPT messages as a single log entry the overhead can be reduced. In addition filtering by extension and request duration (min/max) can further reduce the log size (and simplify analysis).,4171
Extract Method,Allow job consumers to register for a topic and all subtopics Right now a job consumer/executor can only register for a topic or all topics within a package (using /*). However with the event admin based job processors consumers could subscribe for all subtopics regardless of their depths.We should add a new pattern /** to support this.,4172
Extract Method,Create debian/.ubuntu package for sling launchpad standalone jar I’m setting up a development env for a team to work on a sling asset manager app. I wanted to make the env setup simple so I put together a debian/ubuntu package for the org.apache.sling.launchpad-8-SNAPSHOT-standalone.jar.It uses the jdeb maven plugin so it’s cross-platform. Here’s the patch on the off-chance it’s compatible with the upstream policies. If it needs tuning let me know what it’s missing.Comments from dev list:- Do you mind creating a JIRA - my preference would be to have this in contrib instead of the launchpad folder ,4173
Extract Method,"OSGi Mock: Support for ""modified"" SCR lifecycle method currently only the ""activate"" and ""deactivate"" methods are supported. for a service that supports the ""modified"" method as well it is required to support it in the mocks to be able to test it.",4177
Rename Method,Make Content Loader extensible to support new import formats The current Content Loader supports a basic set of import formats which are identified by extension: {{.xml}} {{.jcr.xml}} {{.json}} {{.jar}} and {{.zip}}.There is a [user request|http://mail-archives.apache.org/mod_mbox/sling-users/201412.mbox/%3cD0A6198C.20FBEB%25bruce.edge@nextissuemedia.com%3e] to support custom formats like Adobe Folio and [some ideas how to implement|http://mail-archives.apache.org/mod_mbox/sling-users/201412.mbox/%3cA2AB572F-FA83-4AE2-806E-49CCE87B9FBE@adobe.com%3e]:* we create a new org.apache.sling.contentloader.reader package to be exported at version 1.0* move the ContentCreator (@ProviderType) and ContentReader (@ConsumerType) to that new package* convert the BaseImportLoader into a standalone ContentReader service holder used by the DefaultContentImporter and ContentLoaderService components.* For the ContentReader service interface we define service registration properties for the service to expose the file name extension (and maybe content type) the reader supports.,4180
Rename Method,Improve Sightly JS Provider performance The Sightly JavaScript Use Provider can be optimised such that its JavaScript context is run only for JS Use-API scripts instead of creating the JS context for every Sightly request. The only downside of such an optimisation is that the values provided by the {{SlyBindingsValuesProvider}} will not be available to be used directly in Sightly scripts making them available only for JS Use-API objects.,4184
Rename Method,"JSON representation of Calendar values should preserve timezone Im currently doing some things with dates in Sling that involve timezones and I find that the documentation regarding it is not particularly clear.according to https://sling.apache.org/documentation/bundles/manipulating-content-the-slingpostservlet-servlets-post.html#date-propertiesseveral formats are defined. I found that the only format that saves a provided timezone is the ISO8601 format rest of them relies in a Date object which does not have timezones. Could this be clearly stated?Also the ISO8601 parser is problematic. It relies on the Jackrabbit parser which uses format ""±YYYY-MM-DDThh:mm:ss.SSSTZD"" but according to http://www.w3.org/TR/NOTE-datetime the ISO format does not have milliseconds on it (""SSS""). So it is very hard to find a way to keep the timezone information (I had to dig through the code to figure it out)Could we please replace ISO8601 with the actual format ""±YYYY-MM-DDThh:mm:ss.SSSTZD"" so it is clearer?",4186
Extract Method,Enable dumping of remote server logs in case of test failures In case of large test suite running on CI server its hard to make out which logs were created due to execution of which testcase. This makes determining the cause of testcase failure difficult. Often the server logs are also not avialable once the build is completed and only source of information is system out logs captured via junit framework on client side.This debugging process can be made simpler if the testcase also dumps the server side logs generated while that testcase executes locally upon failure,4188
Inline Method,Support nested config path in BundleListContentProvider BundleListContentProvider does not handle nested config paths. As noted in SLING-4283 this is required to support run mode based config,4189
Rename Method,[Sightly] Reduce the number of repository reads On instances that still use Jackrabbit repository read operations can become problematic if the system is exposed to a high number of requests. Therefore the Sightly Scripting engine should be optimised to reduce the number of repository reads as much as possible.,4191
Extract Method,Improve handling of unregistering ResourceProviderFactories Right now if a ResourceProviderFactory is unregistered the ResourceResolverFactory is first unregistered and then registered again. This is in order to clean up all usages of ResourceResolvers which might have a reference to the unregistered provider factory.This reregistration can lead to nearly the whole system going down and up again. Therefore if there are several unregistrations in a row the system might go down and up several times. If there are circular dependencies this might also result in an endless down/up loop.,4193
Rename Method,The use of finalize in ResourceResolver leads to performance issues Currently there is a finalizer implemented for the ResourceResolverImpl (http://svn.apache.org/repos/asf/sling/trunk/bundles/resourceresolver/src/main/java/org/apache/sling/resourceresolver/impl/ResourceResolverImpl.java).This defers garbage collection. For a more detailed analysis and also some metrics around that have a look at https://issues.apache.org/jira/browse/JCR-2768.A similar approach like in the patch attached to JCR-2768 should be implemented for the ResourceResolverImpl.,4196
Extract Method,Validation: Support validation of resource and all child resources Currently through {{ValidationService.validate}} it is only possible to trigger validation being connected to a single JCR validation model.It would be useful to be able to trigger validation on a resource which would also comprise validation of all child resource (with their own dedicated validation models being determined automatically through their resourceType).,4203
Rename Method,Use sling mocks for the validation core tests The org.apache.sling.validation.core bundle has its own utility mocks in https://svn.apache.org/repos/asf/sling/trunk/contrib/extensions/validation/core/src/test/java/org/apache/sling/validation/impl/setup/ . All of these can be replaced by the new Sling mocks.The benefits would be:- less maintenance for the validation core project- more exposure/coverage for the Sling mocks,4206
Inline Method,The Use-API should return a falsy value if no Use-provider was able to solve the requested object In case no {{UseProvider}} can resolve an identifier the {{UseRuntimeExtension}} should just return a falsy value ({{null}} in this case) instead of throwing a SightlyException.,4208
Rename Method,"Make default bundle location configurable We should make the default bundle location for a new configuration handled by the installer configurable. WIth versions 1.0.x this was null with 1.1.0 we switched to ""?"".We should revert the default to null but provide a way to change that default",4209
Rename Method,Provide Oak features provide features for Oak as we have now for Jackrabbit:* -{{oak-sling}}-* {{sling-jcr-oak}}* {{sling-launchpad-oak}}* {{sling-launchpad-oak-tar}}* {{sling-launchpad-oak-mongo}},4212
Extract Method,In distribution servlets use sling.commons.json to build json responses 0,4217
Extract Method,OSGi Mock: Handle dynamic service references updates currently service references are only injected when registering the services. updates after this e.g. when a another service is referenced matching an optional or multiple references this is currently ignored. we should handle this in the mock context as well at least in a basic way to support unit tests.this is required e.g. to resolve SLING-4434,4218
Extract Method,Schedule jobs n times Currently its only possible to schedule a job either ones or forever; it would be nice to schedule a job n times perhaps also starting at a given time etc.,4221
Extract Method,Provide a way to remove settings artifacts and features through a model If a model is merged into another model it can only add or change existing settings but there is no way to remove a framework setting a configuration an artifact or a complete feature.We should provide some way to do so,4223
Rename Method,"Extend SlingPostServlet to support references to existing repository content After SLING-422 an interesting (for some applications important) side effect of the old SlingPostServlet has gone: With the old implementation it was possible to have files uploaded to the repository in one or more requests and and in a follow up request create another content and move the previously uploaded file to the newly created content.To better illustrate: Assume a CMS where you edit some kind of content. The content is composed of one or more images to be uploaded as image files and a title and a body text field. Now the CMS wants to present a user friendly interface and has implemented some cool file upload dialog which shows upload progress.This feature is now used to pre-upload the image files to a temporary location. Upon sending the rest of the content - the title and body text - the image files should of course also be moved from the temporary location to the final destination in the same place as the title and body text.With the old SlingPostServlet the image files could be moved by simply including :moveSrc/:moveDst parameter pairs for each image file. With the new SlingPostServlet this is not currently possible.To make such things possible again the ModifyOperation of the SlingPostServlet is to be extend such as to recognize special parameters. Similar to the solution proposed in SLING-130 (@ValueFrom suffix to refer to values of another form field) I propose the following parameter name suffixes:@CopyFrom - Copies the items from the repository locations indicated by the parameter value@MoveFrom - Moves the items from the repository locations indicated by the parameter valueExample use: To move an image file uploaded previously to ""/tmp/image000.gif"" to the ""image"" child node the HTML form to submit the content (along with title and text fields) could be defined as:<form method=""POST"" action=""/some/new/content""><input type=""hidden"" name=""image@MoveFrom"" value=""/tmp/image000.gif"" /><input type=""text"" name=""title"" value=""..."" /><input type=""text"" name=""text"" value=""..."" /><submit /></form>If the item referred to in a @CopyFrom/@MoveFrom parameter is a node of type nt:file treatment is special: If the natural type of the destination item is nt:file the addressed node is simply copied or moved. Otherwise the the jcr:content child node is copied or moved. In case of a move the nt:file node is of course also removed.Example use (continued): After processing the request defined by the above form the original item /tmp/image000.gif is gone and the contents is now located in /some/new/content/image.",4230
Move Method,Use a single listener registered for multiple path in JCR installer Sling Jcr installer currently registers one listener per watched folder. On an application like AEM this results in ~150 listeners out of total 210 to belong to Jcr installer.Recently Jackrabbit introduced support for adding [additional path|https://github.com/apache/jackrabbit/blob/trunk/jackrabbit-api/src/main/java/org/apache/jackrabbit/api/observation/JackrabbitEventFilter.java#L232] to listen to as part of JCR-3745. This can be leveraged by the Jcr installer to avoid registering multiple listeners and instead use one listener.The above feature can be used in following form{code}String[] paths = searchPaths.toArray(new String[]{});JackrabbitEventFilter eventFilter = new JackrabbitEventFilter().setAbsPath(paths[0]).setEventTypes(Event.NODE_ADDED |Event.NODE_REMOVED |Event.NODE_MOVED |Event.PROPERTY_ADDED |Event.PROPERTY_CHANGED |Event.PROPERTY_REMOVED ).setIsDeep(true).setNoLocal(false).setNoExternal(true);if (paths.length > 1) {eventFilter.setAdditionalPaths(paths);}JackrabbitObservationManager observationManager = (JackrabbitObservationManager) adminSession.getWorkspace().getObservationManager();observationManager.addEventListener(this eventFilter);{code}This would allow more efficient observation processing and avoid putting load on system as Oak currently maintains one queue per listener.,4231
Extract Method,Enhance SlingPostServlet for multiple operations After implementing SLING-422 the SlingPostServlet can only execute one single operation. It shows that some times it is required to actually evaluate multiple operations. Some examples:(1) A CMS system provides a dialog to modfiy existing content. One such content element is stored in a multivalue property represented in the dialog as a series of checkbox. If the user checks no checkboxes the respective request parameter is not sent to the server and out of the box the property will not be touched. In this case it would be required to remove the respective property before actually modifying the existing content. So in addition to the modification operation a delete operation must be executed.(2) In some administration application an operation such as moving items may be applied to multiple items at once. So having a way of specifying all such items at once would be required.Both use cases require multiple requests after the SLING-422 implementation.The goal is to come up with a solution which is simple out of the box but provides more functionality if really required.,4233
Extract Method,Have vlt packages constructed starting from their closest parent in order to necessitate a smaller set of privileges Trying to build a package for /var/sling/distribution/diff/diff1 requires read privilege on /var. It should work also with an user that has privilege on var/sling/distribution/diff. This can be fixed using the root and mount paths for vlt packages.[1] https://github.com/apache/jackrabbit-filevault/blob/trunk/vault-core/src/test/java/org/apache/jackrabbit/vault/packaging/integration/TestMappedExport.java#L45,4234
Extract Method,Performance: XSSAPI.getValidHref should not be based on HTML filtering Around 3% of the rendering time is spent in getValidHref because it uses AntiSamy (which is a HTML filter) to do the job. This is obviously a hack and adds a lot of unnecessary overhead.,4236
Extract Method,Performance: Use JackrabbitSession#getItemOrNull to speed up JcrResourceProvider#createResource If the current session is a JackrabbitSession JcrResourceProvider should use getItemOrNull (as soon as exported) to save rendering time.See referenced issue and the following mail thread for more information: http://mail-archives.apache.org/mod_mbox/jackrabbit-oak-dev/201504.mbox/%3CD1495A09.3B670%25anchela%40adobe.com%3E,4238
Rename Method,discovery.oak: oak-based discovery implementation When discovery is used in a stack based on jackrabbit oak as the repository the current way of discoving instances somewhat sounds like duplicating work: oak or more precisely documentnodestore itself has a low-level [lease mechanism|http://jackrabbit.apache.org/oak/docs/nodestore/documentmk.html] where it stores information about the cluster nodes including a {{leaseEnd}} indicating at what time others can consider a particular node as dead/crashed. This corresponds pretty much to the discovery.impl heartbeat mechanism. And in a stack which is built ontop of oak-documentMk we could be making use of this fact and delegate the decision about whether a node in a cluster is alive or not to the oak layer. Also with OAK-2597 the relevant information: {{ActiveClusterNodes}} is nicely exposed via JMX - so that can become the new source of truth defining the cluster view.When replacing discovery-owned heartbeats with oak-owned ones there is one important detail to be watched out for: it can no longer easily be determined from another instance in the cluster whether it has this new discovery bundle activated or not. Hence it is not given that when a voting happens that all {{active}} nodes (as reported by oak-documentMk) are actually going to respond. So the 'silent instance due to deactivated discovery bundle' case needs special attention/handling.Other than that given the normal case of all {{active}} nodes having the bundle activated the voting mechanism can stay the same as in discovery.impl. The topology connectors can be treated the same too (by storing announcements to their respective {{/var/discovery/clusterInstances/<slingId>/announcements/<announcerSlingId>}} node. The properties can be handled the same too (by storing to {{/properties}} node. Only thing that gets replaced is the {{heartbeats}}.Note that in order for such an oak-based discovery.impl this oak-lease mechanism must be very robust (it should be so by its own interest already). However there are currently a few issues that should probably first be resolved until discovery can be based on this: OAK-2739 OAK-2682 and OAK-2681 are currently known in this area.,4240
Extract Method,Performance: Consider optimizing MergedResource#getParent and getChild For some pages up to 29% of the rendering time is spent in AbstractResource.getChild. In my case 75% of the resources are MergedResources and the rest are JcrNodeResources (for which I have already opened SLING-4596).Therefore I would suggest implementing the following optimization:The merged resources should be stored in MergedResource and when getChild is called it would use getChild on them directly and merge the child resources. This would have the advantage that the ParentHidingHandler would not have to be called again for the parent resources and could even make SLING-4568 obsolete. Moreover it would leverage optimizations of the merged resource implementations (e.g. SLING-4596 for JcrNodeResource#getChild).A problem with this approach is that possible in JCR (and maybe also some other resource providers) to set ACLs which allow to read children but not their parents. Therefore getChild will in addition have to check for each search path which isn't covered with a merged resource if this path does really not exist.,4246
Extract Method,[discovery] Add a JMX method to start a new voting With SLING-4516 an additional way of controlling the instance ordering in a cluster and thus who will become is leader (which is defined via alphabetical ordering of the {{leaderElectionId}}) is introduced. When the {{leaderElectionId}} changes this does not automatically trigger a new vote however since it conflicts with the discovery API which demands that the leader be *stable*.In order to still support use cases where a leader *knowingly and explicitly* wants to be changed (by an admin) a JMX method for starting a new voting should be introduced. This would simply cause a new {{ongoingVotings}} to be created based on the existing set of instances. It would not fiddle with the way the leader is chosen in any way. All it would do is based on the {{leaderElectionIds}} it would elect a (potentially new) leader.This would support management possibilities around forcing a different or specific leader to some extend.,4247
Move Method,"JST scripting engine: render indexable HTML and separate javascript code The JST scripting engine should output a default HTML rendering meant to be indexed by search engines with a <script> element that points to a separate javascript resource to render the page.The idea is that the javascript code will be cached by client browsers being the same for all resources that have the same sling:resourceType.The HTML rendering should include a meaningful <title> element using the value of a property named ""title"" or ""description"" if present or the node name if not.",4248
Extract Method,LogService logger to implement LocationAwareLogger Some libraries (specifically Spring in this case) requires that the logger is an LocationAwareLogger or it will fail. LocationAwareLogger is a logger that can log the FQCN and extends the Logger interface that is currently implemented in SlingLogger.,4251
Rename Method,Decouple Sightly from the JCR Compiler The current implementation of the Sightly scripting engine depends on the JCR Compiler for generating Java classes from the Sightly script files. However the JCR Compiler can be slow on some systems due to JCR's locking mechanisms.Since Sling also provides the {{org.apache.sling.commons.fsclassloader}} which implements a faster filesystem-based {{ClassLoaderWriter}} it would be better to use a more generic approach for generating Java classes.,4253
Extract Method,"osgi-mock: Support OSGi component name different from implementation class it is possible to define a different component name than the implementation class via SCR. the element ""implemention"" contains the correct implementian class name in this case. osgi-mock should support this.",4254
Extract Method,Allow vlt package builders to be customized with paths for temporary packages One should be able to specify a temporary filesystem folder and a temporary jcr folder where vlt packages should be aggregated and stored.,4260
Extract Method,Crankstart should use the Sling Provisioning Model Crankstart should be converted to use the Sling Provisioning Model that was created in the meantime.This should preserve the following features:Set Initial classpath select framework versionSet defaults for variables override with system propertiesSet OSGi framework properties with variableInstall configs early as soon as ConfigAdmin is availableOSGi configs with typed valuesGuard against multiple factory config creation (CRANKSTART_CONFIG_ID)Ability to register new startup commandsmvn:protocol is optional can also use httplog messages during startup,4263
Move Method,"Log Tracer - To enable logs for specific category at specific level and only for specific request Tracer [1] simplifies enabling the logs for specific category at specific level and only for specific request. It provides a very fine level of control via config provided as part of HTTP request around how the logging should be performed for given category.For e.g. determining what nodes are written for a given POST request can be simply done by including an extra request parameter ""tracer""{noformat}curl -D - -u admin:admin \-d ""./jcr:content/jcr:title=Summer Collection"" \-d "":name=summer-collection"" \-d ""./jcr:primaryType=sling:Folder"" \-d ""./jcr:content/jcr:primaryType=nt:unstructured"" \-d ""tracers=oak-writes"" \http://localhost:8080/content/dam/{noformat}One can also specify actual categories as part of request param itself. For complete details refer to [1] (docs to be moved to Sling site)Refer to [1] for related discussion[1] http://markmail.org/thread/mijnl2mpxu4nzqkb",4268
Rename Method,New Resource Provider API Mail thread from the mailing list:http://mail-archives.apache.org/mod_mbox/sling-dev/201505.mbox/%3C555983ED.1080800%40apache.org%3EStarting mail:The resource provider API has grown a lot over time and when we startedwith it we didn't really think about potential extensions of the api.Today each time we add a new feature we come up with a new markerinterface. There is also the distinction between a resource provider(singleton/stateless) and the factory (creating stateful providers).Although the api is not intended to be used by the average resource apiuser (it's an extension) we put it in the same package. And there aremore minor things.Therefore I think it's time to start a new API that is more future proofand solves the known problems. I've created a draft prototype at [1].During the performance analysis by Joel he found out that getParentcalls to a resource a pretty expensive as in the end these are stringbased. Therefore e.g. the JCR implementation can't simply callgetParent on a node and wrap it in a resource. Therefore I think weshould add a getParent(Resource) method to the resource resolver andhave a better way to handle this in a resource provider.Instead of having a resource provider and a resource provider factorywe define a single ResourceProvider which is a singleton. If thisprovider needs authentication and/or needs to keep state per user thePROPERTY_AUTHENTICATE needs to be set to true and in this case theauthenticate method is called. This one returns a data object which ispassed in to each and every method. If auth is not required the methodis not called and null is passed in as the data object.For authentication providers do not support login administrativeanymore just users and service users.A provider is mounted at a single root - no more support for mounting itat different path at the same time; and a provider always owns the root.So if a provider does not return a resource for a given path no otherprovider is asked. This allows for improved implementations and resourceresolving. If we decided that we need this for compatibility we cansolve it differently.Instead of using marker interface we define the ResourceProvider as anabstract class. This allows us to add new methods without breakingexisting providers.Each method gets a ResolveContext containing the resource resolverthe previously mentioned state data object and other things e.g. theparameter support recently added to the resource resolving. In thefuture we can pass in additional data without breaking the interface.Apart from that the resource provider is similar to the aggregation ofthe already existing marker interfaces. There are two exceptionsobservation and query which I'll handle in different emails.[1]https://svn.apache.org/repos/asf/sling/whiteboard/cziegeler/api-v3/src/main/java/org/apache/sling/api/resource/provider/,4270
Extract Method,Limit the number of job retries by default configuration A job can be configured when the job event is sent with a number of retries indicating how often the job should be rescheduled if something goes wrong.If this configuration is not available in the job event the event is retried forever!The JobEventHandler should get a default configuration which is used in this case instead.,4274
Move Method,SlingConfigurationPrinter should use mode aware and only stream the full logs in the zip version and avoid duplicate information Currently the SlingConfigurationPrinter is not mode aware it produce a log files.txt that contains / combine all the logs in one which is an issue when there are really a lot of logs and large.Also the zip version will contains then both output once combined and once as log attached.To improve this it would be easier to use the mode aware interface ModeAwareConfigurationPrinter and define different output style. In text mode it should only list the log files available a tail of the last n lines could be added.In zip mode it should then attache the logs eventually an option to only add logs that are n days old or n lines  to avoid too large zip files which can also sometime be a problem when trying to analyze a recent issue.I did a quick test based on the current release and that is possible to achieve at least the mode awareness.[~cziegeler] Can you check if that can be done for a future release of common logs ?,4276
Extract Method,Allow to automatically undeploy all deployed bundles after execution of ITs Through the property {{sling.additional.bundle}} it is already possible to deploy bundles before running integration tests derived from {{SlingTestBase}}.It should also be supported to automatically undeploy those bundles after the test has finished.,4278
Extract Method,Provide an option to disable a set of logger via config Logsupport Web Console plugin simplifies configuring the loggers with content assist. At times for debugging purpose users routes logs from set of loggers to a specific file. Post debugging these loggers have to be deleted. It would helpful if UI provides an option to turn off the loggers i.e. have the config be there but not applied,4280
Extract Method,"MockValueMap should implement containsKey MockValueMap inherits ValueMapDecorator which uses an underlying map.Methods using that map do not take care of the deep path features of the MockValueMap.Eg. mockValueMap.containsKey(""path/property"") returns false even when mockValueMap.get(""path/property"" ... returns the property value.MockValueMap should therefore override methods like containsKey.",4281
Extract Method,Slingstart Maven Plugin: Allow to read variables from POM by default provisioning variables can only be resolved by a variables section defined inside the provisioning file.if processed by the slingstart maven plugin it should be optionally possible to reference variables defined within the pom from which the plugin is executed. additionally it should be possible to attach an resolved (effective model) with those variables replaced when storing it as artifact.this is useful if the same property is required within the POM and the provisioning file and avoids haven to define and maintain it in two locations.,4283
Extract Method,osgi-mock: Add support for ComponentContext.getUsingBundle() the method throws UnsupportedOperationException.it should return null if no using bundle is set. optionally it should be possible to set a using bundle for a mocked component context to simulate bundle-scoped services.,4288
Extract Method,Sling Mock: Make compatible with o.a.s.jcr.resource 2.5.0 Currently if using sling-mock 1.4 with a dependencyManagement setting the version of jcr.resource to 2.5 the following error can be observed while executing tests{code}org.apache.sling.testing.mock.osgi.ReferenceViolationException: Unable to inject mandatory reference 'pathMapper' for class org.apache.sling.jcr.resource.internal.helper.jcr.JcrResourceProviderFactory : no matching services were found.at org.apache.sling.testing.mock.osgi.OsgiServiceUtil.injectServiceReference(OsgiServiceUtil.java:313)at org.apache.sling.testing.mock.osgi.OsgiServiceUtil.injectServices(OsgiServiceUtil.java:289)at org.apache.sling.testing.mock.osgi.MockOsgi.injectServices(MockOsgi.java:118)at org.apache.sling.testing.mock.sling.MockJcrResourceResolverFactory.getResourceResolverInternal(MockJcrResourceResolverFactory.java:66)at org.apache.sling.testing.mock.sling.AbstractMockResourceResolverFactory.getAdministrativeResourceResolver(AbstractMockResourceResolverFactory.java:106)at org.apache.sling.testing.mock.sling.context.SlingContextImpl.resourceResolver(SlingContextImpl.java:179)at org.apache.sling.testing.mock.sling.context.SlingContextImpl.load(SlingContextImpl.java:237).....{code},4290
Move Method,Deprecate the asynchronous JavaScript API provided by the Sightly JS Use Provider With the improvement of the API provided by the {{org.apache.sling.scripting.javascript}} bundle the asynchronous promise-based API provided by the Sightly JS Use Provider bundle is redundant.Deprecating the API together with conditionally loading the support for it only when detecting that a Use object actually needs the API namespaces should help in optimising the execution speed for JavaScript Use objects.,4291
Rename Method,Optimise the SightlyJavaCompilerService to provide objects faster The {{SightlyJavaCompilerService}} can be optimised to provide objects faster in its {{getInstance}} method by delaying repository search for POJO objects.,4293
Extract Method,sling-mock: Support HttpSession invalidate new lastAccessedTime maxInteractiveInterval MockHttpSession should support these methods as well:* invalidate()* isNew()* getLastAccessedTime()* getMaxInteractiveInterval()* setMaxInteractiveInterval(),4294
Extract Method,Logic in WhiteboardHandler is hard to follow I find the logic in {{WhiteboardHandler}} hard to follow. I will refactor it to improve readability.,4297
Extract Method,Allow to enable the usage of regular JCR users for service resolvers With SLING-3854 a {{ServiceUserValidator}} interface was introduced. Basically all OSGi services implementing that interface may decide whether certain users can be used as backing user for a call to {{ResourceResolverFactory.getServiceResolver(...)}}. The only implementation of that in Sling is {{JcrSystemUserValidator}} which only allows to use JCR system users.The list of all those services is bound in the {{ServiceUserMapperImpl}} dynamically.If you for example want to use that service to relax the policy being introduced with SLING-3854 (to e.g. allow all users as service users) you may register your own service just returning {{true}} for all users in the only method {{isValid}}. Unfortunately you don't know when your {{ServiceUserValidator}} service is bound (due to the dynamic restart behaviour of services). Therefore other services cannot rely on the fact that your own {{ServiceUserValidator}} is being available at a certain point in time and therefore their call to {{ResourceResolverFactory.getServiceResolver(...)}} may fail if they rely on a non-System JCR user. Therefore this mechanism is not suitable to disable the enforcing of JCR system users.Instead I would propose the following:# allow to configure the {{JcrSystemUserValidator}} via an OSGi property named {{allowOnlySystemUsers}} which by default should be {{true}}.# within the method {{JcrSystemUserValidator.isValidUser}} you either allow all users or leave the current logic in place (in case {{allowOnlySystemUsers}} is {{true}}).Only that way it would be possible to reliably allow all users as service users which is especially helpful during development of a certain feature (although this is probably not a config you would set on a production instance).,4299
Extract Method,"Simplified server-side tests with TeleporterRule I've been working on a prototype at [1] that makes it much easier to create server-side tests using a JUnit Rule to ""teleport"" them to the server._edit: removed obsolete description of {{ServerSideTestRule}}_This can be useful along with something like https://github.com/fizzed/maven-plugins to quickly update the bundle under development when it's modified.[1] https://svn.apache.org/repos/asf/sling/whiteboard/bdelacretaz/test-rules",4304
Extract Method,"sling-mock: Register JCR node types for OSGi bundles in class path for the resource resolver types ""JCR_JACKRABBIT"" and ""JCR_OAK"" node types are required to store data with certain nodetypes or query for it.if in the classpath OSGi bundles are present which define node type definitions in their {{Sling-Nodetypes}} bundle header they should be picked up and registered automatically in the repository.because the correct order of multiple bundles registering node types is not known it should try to re-register it multiple times (up to 5 times) to get the right order ""by chance"".",4308
Rename Method,[HApi] Add java HApi microdata client The hapi tools extension could make use of a java HTML and microdata client to consume html markup annotated using HApi.,4310
Extract Method,Add tests for Sightly editor auto-completion With SLING-4189 Sightly editor auto-completion was added but not tested as I have not found a simple way of testing it ( except by UI-driven tests ).This task is open though to make sure that we eventually add tests for the auto-completion.,4313
Extract Method,Generate OSGi subsystem intermediary file in slingstart-maven-plugin Enhance the slingstart-maven-plugin to support OSGi subsystems as modeled via SLING-5148. The maven plugin should generate an .esa file from the artifacts defined as an OSGi subsystem in the provisioning model. The resulting .esa files should be embedded in the generated archive.,4315
Rename Method,Optimise last modified information retrieval in UnitChangeMonitor The {{UnitChangeMonitor}}'s mechanism for storing the last modified date for Sightly compiled classes can be optimised to provide a better cache creation.,4317
Rename Method,Add Path and PathSet There is often a need to check whether a resource path is equal or a sub path of a given path. We have a lot of different solutions throughout our code base.Therefore I think we should provide some basic functionality directly in the API for reuse.,4319
Extract Method,Remove loginAdministrative() usage from org.apache.sling.bgservlets Counted 1 occurrence,4320
Move Method,ensure a new establishedView (with different syncTokenId) always triggers a TOPOLOGY_CHANGED With SLING-5256 the unerlying mechanisms in ViewStateManager etc ensure that when a topology is passed to {{handleNewView}} that contains the very same instances/properties but differs only in the localClusterSyncId (which is the resource name of the voting in the discovery.impl case) that this then properly generates a TOPOLOGY_CHANGED and a TOPOLOGY_CHANGING first (if required).SLING-5058 was suggesting a different approach to achieving the same: to add a {{viewCnt}}. Now SLING-5058 really becomes obsolete with SLING-5256 - but that should be properly verified first - hence this ticket.,4321
Extract Method,RequestProgressTracker should log microseconds The RequestProgressTracker logs timings as milliseconds. If requests are very fast and in the milliseconds range more resolution is required for fine tuning.,4322
Extract Method,Allow item copy between distribution queues In order to retry items from an error queue we should be able to copy items from an error queue to an original queue.,4323
Extract Method,Support setting the basename for the resource bundle backing the Sightly i18n Extension Currently it is not possible to set the basename for the Sightly i18n extension.In addition to options {{hint}} and {{locale}} the option {{basename}} should be supported.This is not part of the spec (https://github.com/Adobe-Marketing-Cloud/sightly-spec/blob/master/SPECIFICATION.md#123-i18n) but in the Sling context the additional option would be very handy.,4325
Extract Method,osgi-mock: Support OSGi R6 field-based reference bindings in OSGi R6 field references are set directly and not using bind/unbind methods. additionally field references of type List and Collection are supported as well. we have to support this in osgi-mock.for start we keep implementation simple and support a subset of OSGi R6 features:* support field collection types 'service' and 'reference' but not 'serviceobjects' 'map' or 'tuple'* ignore service policy option (acts always greedy),4327
Extract Method,osgi-mock: Support target filtering on DS references references in DS have an optional target attribute to filter service references by their properties. it should be supported in osgi-mock as well.,4329
Extract Method,"osgi-mock: Support OSGi R6 Component propert types for configuration OSGi R6/Declarative Services 1.3 add a new support for passing configuration to lifecycle methods (activate deactivate modified) via ""component property types"" (annotation classes).this should be supported in osgi-mock as well.",4332
Rename Method,sling-mock: Support remote add remote host and remote port in MockSlingHttpSevletRequest 0,4333
Rename Method,Meaningful thread names As discussed on the mailing list\[0] it would be good to assign meaningful names to threads managed by Sling's thread pools.\[0] http://markmail.org/thread/ltq4fdyo5himfwo3,4334
Rename Method,"Create service users and ACLs from the provisioning model As discussed in the ""Removing loginAdministrative how to test that and service username conventions"" thread on our dev list [1] we need to be able to create service users and set the corresponding ACLs from our provisioning model.This should be implemented using distinct utility classes one for the users and one for the ACLs that take simple mini-languages as input. This will allow for reusing these utilities in test code for example.[1] http://markmail.org/message/kcvuhwfdald2dyuz*Edit: high-level requirements*As discussed in the ""SLING-5355 - configs vs. content for ACLs and service users"" thread - http://markmail.org/message/tzno2via2wjckhuc* HR1 - Create service users and set their ACLs as defined in the Sling instance's provisioning model.* HR2 - Create initial paths like /var/discovery so that ACLs can be set on them.* HR3 - Make the full text of the ACL definitions available at runtime for auditing purposes (see Michael Marth's Dec.17 comment in SLING-5355). Also useful for upgrades where merging with conflict detection is needed.",4338
Extract Method,New ResourceBuilder module - fluent API to create content structures As discussed recently on our dev list the {{ContentBuilder}} currently provided by the Sling Mocks library [1] can be very useful in testing. _(edit: while implementing I have diverged from that idea and created a new more powerful {{ResourceBuilder}} API for now that {{ContentBuilder}} stays unchanged)_.In order to make it usable for both client-side and server-side testing (as well as in server-side code) I'm planning to* Extract it into its own module* Define an API that allows for creating nodes and properties importing JSON and other files via the Sling ContentLoader and providing a simple a way to cleanup the test content* Implement (first) a server-side version of that API* Implement (as a second priority) a client-side version that can be used in test run via HTTPThis shouldn't affect the existing Sling Mocks library users except maybe forcing them to rebuild their tests to use the new API.[1] https://sling.apache.org/documentation/development/sling-mock.html#building-content,4344
Extract Method,JcrInstaller should not listen on / for moves The JcrInstaller is currently registering a JCR listener for move events on the root node with setting the 'deep' property to true. If it comes to processing the underlaying Oak observation queue this combination is very inefficient as it does not allow Oak to filter changes by path. AFAIU this move listener is covering the use case where an installer artifact is moved from/to an installer root folder. I therefore wanted to propose to register the move listener on the configured root folders only.,4355
Rename Method,Support renaming of bundles via Sling Provisioning Model Because the Sling OSGi Installer only allows a single OSGi bundle with a given BSN it is sometimes necessary to rename a bundle's BSN to enable it to be installed more than once. To make this renaming simple and do it on the fly we can extend the slingstart-maven-plugin to do this renaming automatically with a configuration like this:{code}org.foo.bar/blah/1.2.3 [rename-bsn=com.adobe.foo.bar.blah]{code}One note in case there are multiple model files that all reference the same artifact. For example with a base model as above. Model A inherits from Base model and has:{code}org.foo.bar/blah/1.2.3{code}without the rename.In this case the rename still happens as the attributes are inherited.,4357
Extract Method,"Provide support for running singleton jobs on non leader cluster nodes also With SLING-2979 support for running singleton jobs on specific instance was provided. In most cases we want to run a job as singleton and not want to ""pin"" it to specific nodes. For this {{scheduler.runOn}} needs to be set to {{SINGLE}}.However per [current implementation|https://github.com/apache/sling/blob/org.apache.sling.commons.scheduler-2.4.14/src/main/java/org/apache/sling/commons/scheduler/impl/QuartzJobExecutor.java#L64] {{SINGLE}} is treated as {{LEADER}}. This effectively causes *all singleton* jobs to get executed on leader only thus putting extra load.For better utilization of cluster resources it should be possible to distribute such singleton jobs on other cluster nodes and still ensure that singleton contract is honoured!We would like to make use of this feature to ensure Oak AsyncIndexTask to run on different cluster nodes (OAK-2749)",4359
Move Method,Review the naming conventions used by JMXReporter to register the Metric MBeans With current setup the JMXReporter would register all metrics under {{org.apache.sling}} domain. [~pfaffm@adobe.com] suggested that this can cause confusion as metrics registered by non Sling bundle would also show up in Sling JMX domain. We should customize the JMX ObjectName logic to account for bundle which has registered the metrics. One approach that can be used is # Expose the MetricService as a ServiceFactory# Use some custom Bundle header else fallback to Bundle Symbolic nameDiscussion thread on mailing list http://markmail.org/thread/sj6yvmyhgze6jn22,4371
Extract Method,"Recording of tracer logs Sling Log Tracer currently provides support for fine grained control of enabling logs for specific request. To make this log more accessible it would be useful to have a feature where the client can also fetch the logs from specific request over HTTP.This feature would work like below# Client sends an HTTP request with header {{Sling-Tracer-Record_}} set to true{noformat}curl -D - -u admin:admin \-H ""Sling-Tracer-Record : true"" \-d ""./jcr:content/jcr:title=Summer Collection"" \-d "":name=summer-collection"" \-d ""./jcr:primaryType=sling:Folder"" \-d ""./jcr:content/jcr:primaryType=nt:unstructured"" \-d ""tracers=oak-writes"" \http://localhost:4802/content/dam/{noformat}# Server includes a request id as part of {{Sling-Tracer-Request-Id}} response headers{noformat}HTTP/1.1 201 CreatedDate: Wed 27 Jan 2016 07:30:22 GMTSling-Tracer-Request-Id: 9b5b01f6-f269-47c3-a889-2dc8d4d7938fX-Content-Type-Options: nosniffX-Frame-Options: SAMEORIGINLocation: /content/dam/summer-collectionContent-Type: text/html; charset=UTF-8Transfer-Encoding: chunked{noformat}# The logs in json format can then be fetched like http://localhost:4802/system/console/tracer/9b5b01f6-f269-47c3-a889-2dc8d4d7938f.json (see [attached output|^tracer-recording.json]. it includes following data for now. It can be extended as per need## RequestProgressTracker logs## JCR Queries madeKey points# Request id is randomly generated# The access to request log is via Web Console Plugin servlet hence its only accessible to admin user account# The request data would only be recorded for those request which have the {{_Sling-Tracer-Record}} header set. The data would be kept in memory for *some time* with the assumption that client would fetch it soon. After some time the data would expire# This feature would need to explicitly enabled via config option# The feature is somewhat similar to 'Recent Request' support. However it exposes a JSON rendition and only keeps the data for request where client requested that. # For this feature dependency is added for Guava Cache to make use of space bound/expiring cache. We can to an extent use {{LinkedHashMap}} but given that Guava is now being used in Sling for Oak it makes sense to make use of its feature. If required can look into embedding minimum required set",4383
Extract Method,Run distribution integration tests with launchpad 8 At the moment the integration tests from distribution module are run with launchpad 7. We should run them against the latest launchpad 8.,4387
Extract Method,Allow discovery of sources of the bundles currently deployed in a Sling runtime For SLING-3605 we need to find out what bundles are installed in the Sling application and identify their source artifacts.This will be a separate bundle with a servlet living under tooling/support.,4396
Inline Method,Remove deprecated features from JCR Base We deprecated several features in JCR Base 2.3.0 and should now remove them,4398
Extract Method,Create new Sightly file wizards With SLING-4076 fixed it would be very convenient to also have some new file wizards:- new Sightly (HTML) file- new Sightly JS Use-Script- new Sightly Java Use-Script,4399
Rename Method,Update to Sling API 2.11 and dependencies Sling API 2.11 and the related new implementations of resourceresolver and jcr.resource introduce deep changes to the resource provider interface and the internal registration processes of the JCR resource provider making it incompatible with sling-mock 1.x.thus we introduce a new version sling-mock 2.x which is compatible with the latest Sling API and the relevant dependencies.the old sling-mock 1.x version is still available at https://svn.apache.org/repos/asf/sling/branches/testing/mocks/sling-mock-1.x,4401
Extract Method,Optimize acquire package operation to make fewer saves Currently the SharedDistributionPackage.acquire is called for each queue and a save into the repo is done each time. We can send an array of queues for which the package should be acquired and do just one save for all queues.,4403
Move Method,Distribution package implementation should be independent of serialization Currently we have different implementations of {{DistributionPackage}} interface in _org.apache.sling.distribution.core_ and _org.apache.sling.distribution.extensions_ some are based on plain files some on FileVault packages (backed by files or JCR content) however it'd be good that packages would be agnostic to serialization type simply based on {{Resources}}.,4404
Rename Method,Use plural for method ValidationModelProvider#getModel(...):Collection<ValidationModel> 0,4409
Extract Method,Pretty print the json dump the current json exports are using the JSONWriter which writes a very compact for of json.this is certainly good for performance but a big pain when developping / debugging.It would be cool if a 'pretty print' flag can be passed to the export that enables niceformatted jsons. eg http://localhost/content/site/en.3.json?tidy=trueI patched the current JSONWriter and JsonItemWriter and added the respective flag to the servlet.WDYT?,4411
Move Method,Distribution packages persisted in files should also have their refs in files 0,4415
Extract Method,Make Thymeleaf TemplateEngine available as service Sling Scripting ({{ThymeleafScriptEngine}}) is strongly tied to HTTP requests. For rendering templates outside of web context service {{org.thymeleaf.ITemplateEngine}} is a better choice.,4418
Extract Method,"special json array import for sling post servlet to post orderered structures it is not possible right now to post an ordered structure through json data [0]. this ticket is for implementing [~bdelacretaz] proposal to use JSON Arrays:{noformat}{""title"": ""The parent node""""SLING:ordered"": [{""SLING:name"": ""first""""title"": ""This comes first""} {""SLING:name"": ""second""""title"": ""This comes second""}]}{noformat}This needs to be triggered by selector or content-type switch so thatthe existing POST behavior is unchanged.I suppose having SLING:ordered should then also set the appropriatemixin on the parent node.[0] http://sling.markmail.org/thread/plov2u7kibscn7sx",4420
Extract Method,Avoid creating a File of a package when it's small enough to fit in memory {{ResourcePackageBuilder}} creates a temporary file holding the actual package stream it'd be good to avoid that when the size of the stream is small enough to quickly fit into memory.,4426
Rename Method,Allow to configure resource resolver mapping Currently the {{ResourceResolverFactoryActivator}} being registered in https://github.com/apache/sling/blob/trunk/testing/mocks/sling-mock/src/main/java/org/apache/sling/testing/mock/sling/ResourceResolverFactoryInitializer.java#L123 is not configured at all.Since resource resolver mapping is a topic which is often implemented in the wrong way it should be possible to setup some mapping for SlingContext to allow to test in a more realistic way.,4430
Extract Method,Expose DistributionContentSerializer Expose {{DistributionContentSerializer}} API from _org.apache.sling.distribution.core_ in order to allow implementation of custom serialization formats (e.g. Avro and Kryo defined in _org.apache.sling.distribution.extensions_).,4431
Extract Method,make sling pipe writer a persistent configuration right now the only way to configure the output of a pipe is to add a json as parameter of the pipe request. Sometimes the output is as important/complex as the pipe itself and should be persisted.This could be also the opportunity to rewrite how writer is managed in a far from ideal if/else block. servlet/plumber should *always* take a writer in account and call it each timedefault being path writer writer should still be overridable through a request parameter,4433
Extract Method,Implement a file system resource provider The idea is to implement a ResourceProvider to access files and folders in the platform filesystem through the virtual Resource tree. This functionality will serve multiple purposes: (1) It shows how to implement a ResourceProvider easily (2) supports simple development and (3) may be used for support.The following use cases for such a resource provider exist (maybe more):Rapid Development-----------------------The goal is to implement a Sling application where the scripts are not stored in the repository but will be loaded from the OSGi bundle which is part of the application. Without filesystem resource provider the scripts have to be stored in the bundle from the start or be stored in the repository and moved to the bundle later. The first approach is tedious as it requires bundle-redeployment on each script change and the second is tedious as it is easy to forget about copying the file out of the repository into the bundle when done.With the filesystem resource provider the scripts may be developed with your favourite IDE persisted at the exact location you later use them anyway (for bundle inclusion or source control system) that is your project location. You simply create a configuration for the filesystem provider which maps the scripts into the resource tree at the exact same location you will later provide them in the bundle.Each change in the script through the IDE is immediately visible in Sling.Support---------Have you ever wanted to access the sling log file of a remote server ? Or wanted to inspect the actual sling.properties file ? Or even wanted to have a look at the actual Configuration Admin configuration files ?The filesystem resource provider is your friend: Create a configuration mapping sling.home into the resource tree and start looking at it.Simple Remote Filesystem Browsing------------------------------------------Browsing the remote filesystem was at the beginning of this idea: I have tons of pictures I would like to access in a Sling application. Instead of copying all pictures into the repository (something which might make sense in the future but not now) the filesystem resource provider allows mapping the folder containing the pictures into the resource tree view.....,4435
Extract Method,JcrPropertyMap shoud allow retrieving of properties the JcrPropertyMap should check for a javax.jcr.Property type class and return the property if exists.,4457
Rename Method,Provide a default Launchpad Oak Tar configuration as Option for Pax Exam * {{org.apache.felix.http}}: {{org.osgi.service.http.port}}* {{org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService}}: {{repository.home}} {{name}}* {{org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexProviderService}}: {{localIndexDir}},4459
Extract Method,"Fail the test by default if there are unresolved bundles TestOptions.baseConfiguration should by default ensure that the test fails for unresolved bundles:{code:java}systemProperty(""pax.exam.osgi.unresolved.fail"").value(""true""){code}",4460
Extract Method,Point to the alternative repository if specified on the command-line This is basically SLING-2847 and SLING-2848 applied to the new module. This will help with the situation where we have SNAPSHOT dependencies which by design aren't resolved from the reactor by Pax-Exam.,4462
Rename Method,"Add pre and post processing hooks to the sling post servlet Currently during a post a SlingPostOperation is selected and then executed. The post operation does ""everything"" performing the changes and saving them.I think we could reintroduce a Changes interface with several implementations like Add Remove Copy etc. This interface has all knowledge of what to change.The SlingPostOperation would then return a list of changes:interface SlingPostOperation {List <Change> prepare(SlingHttpServletRequest);}So the first step when a post comes in is still to select the post operation but then this operation just generates a list of changes without changing anything in the repository.We then introduce pre and post processor interfaces (these are no final names yet:)interface PreProcessor {void process(SlingHttpServletRequest List<Change>);}interface PostProcessor {void process(SlingHttpServletRequest List<Change>);}There can be several pre and post processor registered a property of a processor is used to order them and guarantee an ordered execution.A pre processor can alter the list of changes.When all pre processors are run the changes are applied to the repository by Sling then all post processors are executed.Finally all changes are saved.",4464
Extract Method,Register filter using http whiteboard Instread of using the proprietary way of registering a servlet filter it should be registered with the http whiteboard,4465
Move Method,"Use custom converter adaption from ValueMap to Annotation class this is about the context-aware configuration: https://svn.apache.org/repos/asf/sling/trunk/contrib/extensions/contextaware-configcurrently we use the out-of-the-box functionality of the OSGi converter server to map a resource valuemap properties to an annotation class. this works very well.however some things need to be improved and we need a custom conversion adapter rules for this:* the dynamic proxy created by the converter (see [ConvertingImpl#createProxy|https://github.com/apache/felix/blob/trunk/converter/src/main/java/org/apache/felix/converter/impl/ConvertingImpl.java#L311]) only knows the Map interface not ValueMap thus it accesses directly the ""raw"" type from the value map. all the conversion magic that exists in the JCR value map implementation is not applied. the converter has it's own magic but it will not always produce the same results as the JCR mapping magic. thus we need an adaption rule from ValueMap to <any annotation class> which used the valuemap get methods with the type required for the property as second argument.* problem: the converter service currently supports explicit mappings from type A to B not mapping from type A to any type. most of the rule method variants are currently not implemented in the felix converter impl. i will post a question for this issues on the felix mailing list.once we have this custom conversion rule in place we can do further improvements:* create our own sling-variant of ""ConversionException"" and make sure it is thrown in all relevant cases (on conversion on property accesS) instead of the built-in one from the conversion service* support nested configurations and nested configuration lists - when access to a subresource is detected (does currently not work valuemap returns null) adapt the subresource to valuemap and convert it.",4468
Move Method,Remove deprecated jcr resource API The jcr resource API has been deprecated for some time and not used anymore in our codebase we should now remove it from the jcr resource bundleWe should also remove the deprecated PathMapper,4474
Rename Method,[SCD] support deep property filters 0,4476
Extract Method,Improve auth requirement whiteboard implementation The current auth requirement whiteboard implementation could be improved: if a service registration is modified currently all existing registrations are removed and then all new registrations are added - which can result in a lot of churn going on. The implementation can be improved to make a diff between the old and the new array and only apply the diff.,4478
Rename Method,"Context-Aware Config: Introduce ""bucket name"" parameter in ConfigurationResourceResolver follow-up from discussion: http://apache-sling.73963.n3.nabble.com/context-aware-config-why-sling-configs-node-tt4064393.htmlwe want to introduce a ""bucket-name"" parameter in ConfigurationResourceResolver interface to make it explicit each high-level configuration-like resolver should define one.",4480
Rename Method,Context-Aware Config: Adapt defaulting strategy as discusesd in mail thread http://apache-sling.73963.n3.nabble.com/contextaware-config-Default-configuration-and-naming-td4064399.html we should change:* default folder name to /conf* rename the {{sling:config}} property name to {{sling:config-ref}}* and change the way default configuration is detected by following the example belowfor a content structure like this{noformat}/content/tenant1 - context path /content/tenant1/region1 - context path /content/tenant1/region1/site1 - context path /content/tenant1/region1/site1/page1{noformat}the configuration is looked up at this paths (in this order){noformat}/conf/tenant1/region1/site1/conf/tenant1/region1/conf/tenant1/conf/global/apps/conf/libs/conf{noformat},4481
Extract Method,"Teleporter.getService() should wait for required services the JUnit TeleporterRule provides a getService method to get OSGi services for the test run. this may return null for services that are not yet started.as a workaround some integration tests implement their own ""WaitFor"" patternhttps://svn.apache.org/repos/asf/sling/trunk/bundles/extensions/repoinit/it/src/test/java/org/apache/sling/repoinit/it/WaitFor.javait should be supported by the teleporter rule directly.",4485
Extract Method,Not sling pipe a not sling pipe working as a reference sling pipe would be useful:- output nothing if referred pipe has any output- output input if referred pipe has no output,4487
Extract Method,Sling Models: Automatic Registration of Sling Models via bnd Plugin currently packages with sling model classes have to be registered with a {{Sling-Model-Packages}} bundle header. when such a bundle is deployed the classpath gets scanned and all models of this package (or subpackages) is registered. the developer has to add this header manually to his bundle (e.g. via pom/maven-bundle-plugin).we should additionally provide an alternative for model class registration:* introduce a new header {{Sling-Model-Classes}} which lists all class names with sling models* when such a bundle is deployed both headers are supported. for bundles with only {{Sling-Model-Classes}} no class path scanning is required.* we add a bnd plugin which scans the project class files on compile time for classes with sling model annotations and automatically add fills the {{Sling-Model-Classes}} header* thus when using this bnd plugin no manual configuration of any bundle header is required* the bnd plugin should work with both maven-bundle-plugin and bnd-maven-plugin (or when using bnd directly)this approach is quite similar as recently discussed in SLING-6025,4492
Rename Method,sling-mock ContentBuilder: Support creating resources with object vararg parameter similar to ResourceBuilder sling-mock's ContentBuilder should support creating resources with an object vararg array specifying the properrties additionally to supporting a Map<StringObject>.,4493
Extract Method,ResourceBuilder: Support Map for resource properties additionally to supporting providing resource properties via an object varargs array a Map<StringObject> with properties should be accepted as well.,4495
Rename Method,Context-Aware Config: Property Inheritance/Merging currently the context-aware config implementation supports resource inheritance but not property inheritance that means no properties gets merged in the resource inheritance chain.there was a long discussion on the mailing list about this topics with arguments to support this and other not to support thishttp://apache-sling.73963.n3.nabble.com/Context-Aware-Configs-Merging-tt4063382.htmlthe goal of this ticket is to support it but make it configurable so it can be switched on and off.,4496
Rename Method,"Context-Aware Config: Make resource inheritance for configuration collections configurable currently and automatic merging/combining of configuration resource items takes place. example:{noformat}/conf/site1/feature/a/conf/site1/feature/c/conf/global/feature/b/libs/conf/feature/c{noformat}this returns abc when config resource collection for ""feature"" is requested. c is from /conf/site1/feature/c and not from /libs/conf/feature/c.this is inconsistent to the support for properties (where currently no such ""merging"" is supported) and can have undesired effects.furthermore it is problematic when saving configuration collections via the ConfigurationManager interface (SLING-6026). when storing a set of config resources for /conf/site1 they are stored as children of /conf/site1/feature. but when reading them again they are automatically merged with the others from the fallback paths and the user has no possibility to prevent this.so we should either disable this merging on paths or implement it correctly with giving the user control when merging should take place or not (confmgr has a special property for this).",4497
Extract Method,"slingstart-maven-plugin: Allow to start a quickstart JAR based on a provisioning model even for non ""slingstart"" packagings Currently the {{slingstart-maven-plugin}} can only start a server based on textual model definitions in case the maven module is of packaging ""slingstart"" (https://sling.apache.org/documentation/development/slingstart.html#starting-a-server). For ITs it is often beneficial to have them in the same module as the tested classes itself (which in most cases have packaging {{bundle}}). Therefore it would be nice if even for other packaging values all model definitions below {{src/main/provisioning}} would be considered during the goal {{start}} (which must first build the quickstart.jar out of the models and then start it).Compare also with the readme in https://github.com/apache/sling/blob/trunk/testing/samples/bundle-with-it/pom.xml#L196. This would be especially helpful for ITs leveraging the {{TeleporterRule}}",4502
Rename Method,"Hamcrest: Simplify ResourceMatchers method signatures the resource matching methods of ResourceMatchers should be simplified (before doing the 1.0.0 release):* eliminate ""resource"" prefix from method names because resource is already in the ""ResourceMatchers"" class names and the child methods do not contain this prefix as well* allow to specify property maps either as map or as object vararg array (similar to resource builder and sling mocks)existing sling projects using this snapshot release will be updated as well.",4506
Move Method,Handle empty tracer config header gracefully In case tracer config header is null request fails with following exception{noformat}08.10.2016 14:47:43.920 *WARN* [qtp840310891-62] org.eclipse.jetty.servlet.ServletHandler /content/geometrixx-outdoors/en/women.htmljava.lang.IllegalArgumentException: A header cannot be an empty string.at org.apache.sling.commons.osgi.ManifestHeader.parse(ManifestHeader.java:116)at org.apache.sling.tracer.internal.TracerSet.parseTracerConfigs(TracerSet.java:71)at org.apache.sling.tracer.internal.TracerSet.<init>(TracerSet.java:50)at org.apache.sling.tracer.internal.LogTracer.getTracerContext(LogTracer.java:250)at org.apache.sling.tracer.internal.LogTracer$TracerFilter.doFilter(LogTracer.java:356)at org.apache.felix.http.base.internal.handler.FilterHandler.doHandle(FilterHandler.java:108){noformat},4509
Rename Method,"Ability to specify TTL for separate health check Currently there is no ability to specify TTL for separate health check.Ex.: in my case ""hc"" validating against repository about 3-5 minutes therefore I couldn't specify TTL globally to not impact on other ""hc"" results. This ""hc"" I couldn't execute by scheduler to prevent CPU from high loading also results for this check remains relevant for an 1-3 hours.Therefore if it make sense not only for me I've added ""[pull request|https://github.com/apache/sling/pull/180]"" for this functionality:* If property ""hc.ttl"" specified within HC - it will be used as TTL for result otherwise cache will use default TTL value",4511
Extract Method,Restrict access for principal everyone and move configuration to repoinit Currently {{everyone}} can {{read}} from {{/}} (configured in {{OakSlingRepositoryManager}}).Access for {{everyone}} should be restricted:* {{read}} should be restricted to {{/content}}* configuration of principals and ACLs should be done with _repoinit_# -Change path from {{/}} to {{/content}} in {{OakSlingRepositoryManager}}- (/) (-[r1764259|https://svn.apache.org/r1764259]-)# Fix modules (samples) relying on _unrestricted_ {{read}} access# Move configuration of ACLs to _repoinit_ (/)discussion on [dev@|https://lists.apache.org/thread.html/36908ed62ac93c63cad594a897f8abceb93f08da5bcea30dbce98e58@%3Cdev.sling.apache.org%3E],4512
Rename Method,Context-Aware Config: Config reference should be detected in ContextPathStrategy currently the DefaultContextPathStrategy checks for the availability of the sling:config-ref prop but it does not return it's value this is done in the DefaultConfigurationResourceResolvingStrategy.this is as bit inconsistent leading to configure the lookup of SLING-6149 in multiple places and makes it more difficult to define scenarios where the config references is build on a conventions-based pattern (e.g. derived from content path) instead of an explicit sling:config-ref property.the logic itself and other implementation details of both strategies remain untouched.,4518
Rename Method,Improve MapEntries implementation Looking at the MapEntries implementation it does the same thing in some cases several times during handling change events.We should optimize this and also verify that everything is handled by tests. The current unit tests check more single methods but not the whole functionality,4520
Extract Method,Provide better filter options to search for job Currently it's only possible to search for all jobs activate jobs and filter by an exact matching topics.This can be enhanced to allow pattern matching for the job topic - like the OSGi eventing doesand to provide a map of properties that must match as well. This map acts like a search template.,4524
Extract Method,Use Jackrabbit/Oak globbing support We recently switched back to use a jcr listener (SLING-6138) unfortunately Jackrabbit does not support glob patterns atm.Once JCR-4044 is fixed we should use it.If the JR api will not support glob support we have to come up with a different solution as today either- switch back to oak observer- or only register a single jcr listener on root and do the filtering internally,4532
Extract Method,Installer: Persist optional error text per resource Currently the OSGi installer web console will just expose a state like „Ignored“ or „Install“ in case a bundle/config/package could not be installed. It would be very helpful (especially in the context of SLING-5014) to additionally persist some error text on the last action if it was not successful. This error text should be exposed through the web console. That way one could easily determine (even afterwards) why a specific artifact was not installed by the installer.See also the discussion at http://www.mail-archive.com/dev@sling.apache.org/msg60872.html.,4535
Inline Method,Use official OSGi annotations in org.apache.sling.resourceresolver The ResourceResolverFactoryActivator should use consistenly PropertiesUtil in favor of casts.Also the use of default values in both the SCR annotations and the activate() method should be harmonized.,4537
Move Method,Allow for creating users with repoinit it seems it's not possible to create a user through the repoinit. This would be very useful for sample apps and testing. For example theslingshot sample app currently needs an admin user to create the sampleuser accounts. And therefore slingshot needs to be in the whitelist foradmin usage - which is not a good thingI suggest we add:create user {name}create user {name} {password}delete user {name}If no pw is provided for create user we create a random pw,4539
Extract Method,"[log] Perform initial configuration from framework properties synchronously {{LogbackManager}} uses {{LogConfigManager}} to support traditional logging configuration including initial (global) configuration from framework properties. Once everything is setup the {{LogbackManager.configChanged()}} method is called to initiate logging for the first time.Unfortunately {{configChanged}} is processed asynchronously leading to initial configuration to be applied only later - in some special use cases even *after* the complete application has already started.I proposed to replace the call to {{configChanged()}} by a call to {{configure()}} which actually implements the configuration change *before* the {{started}} flag is set to {{true}}.Proposed patch:{code}Index: src/main/java/org/apache/sling/commons/log/logback/internal/LogbackManager.java===================================================================--- src/main/java/org/apache/sling/commons/log/logback/internal/LogbackManager.java (Revision 1767024)+++ src/main/java/org/apache/sling/commons/log/logback/internal/LogbackManager.java (Arbeitskopie)@@ -1678 +16713 @@registerWebConsoleSupport();registerEventHandler();+ // initial configuration must be done synchronously (aka immediately)+ addInfo(""LogbackManager: BEGIN initial configuration"");+ configure();+ addInfo(""LogbackManager: END initialconfiguration"");++ // now open the gate for regular configurationstarted = true;- configChanged();}public void shutdown() {{code}",4541
Extract Method,Allow configuration of fallback properties in DefaultContextPathStrategy Currently the DefaultContextPathStrategy only looks for the property sling:configRef.If other properties should be looked for e.g. for migration from Adobe's confmgr this can be done with an additional ContextPathStrategy - but this has a performance drawback.We should allow to define additional property names which are used in the order of appearance. Once a value for such a property is found its value is used and the others are not queried anymore. The sling:configRef property is always the first one used and does not need to be configured - which also means it is not possible to deviate from this rule.,4542
Rename Method,Support relative references in DefaultConfigurationResourceResolvingStrategy If the value from the sling:confRef property is relative the parenthierarchy of the context resource is traversed up to find a contextresource with an absolute property. Once found these are concatenated.For example/content/a+ sling:confRef=/conf/a/content/a/b+ sling:confRef=bIf you try to find configurations for /content/a/b it searches in/conf/a/b first then /conf/a,4543
Extract Method,Provide separate queues for specific jobs It would be great to be able to run specific jobs in a separate queue instead of using the main queue for all jobs.This queue is running the jobs one after the other.,4544
Extract Method,Implement LoginAdminWhitelist in JCR Base JCR Base should provide a default implementation {{AbstractSlingRepositoryManager#allowLoginAdministrativeForBundle(Bundle)}} with a configurable {{LoginAdminWhitelist}}. Implementation that want a different logic can then still choose to overwrite the method.cc [~cziegeler] [~bdelacretaz],4546
Extract Method,Support Sling-Models-Classes bundle header As discussed in SLING-6048 we should support a new bundle header Sling-Models-Classes for explicit class listing.,4549
Extract Method,"Clarify description of Content-Disposition-Filter configuration From the description of the Apache ""Sling Content Disposition Filter"" component (https://github.com/apache/sling/blob/02fb326a008418c51482090814e4bff3cac657c7/contrib/extensions/security/src/main/java/org/apache/sling/security/impl/ContentDispositionFilter.java#L52) it is not clear that under all circumstances the {{content-disposition:attachment}} is only then set if on the current resource either a {{jcr:data}} or {{jcr:content/jcr:data}} property is found. That is important information when you want to understand/configure the filter correctly.",4551
Rename Method,JcrInstaller.counters should be accessed in a thread-safe manner The JcrInstaller.counter thread is a final long array. However that does not ensure that changes performed are safely published. Since they are potentially accessed from multiple threads including unit tests this can lead to random-looking hard-to-debug issues.,4553
Move Method,"ResourceResolverImpl.isResourceType() should compare relative resource types (and ignore any search path prefixes) Currently the following two expressions return false{{ResourceResolverImpl.isResourceType(<Resource with resourceType=""sling/some/type""> ""/libs/sling/some/type""}}{{ResourceResolverImpl.isResourceType(<Resource with resourceType=""/libs/sling/some/type""> ""sling/some/type""}}Since it cannot always be influenced whether the given resource is absolute or relative (because both usually works from a rendering perspective when you talk about the current request's resource) both cases should actually return {{true}}.See also the related discussion at http://www.mail-archive.com/dev@sling.apache.org/msg62351.html",4555
Extract Method,Server-Side Tests: Use ServiceTracker to wait for services rather than polling Inside {{ServerSideTeleporter#getService}} the {{BundleContext}} is polled for the presence of a service every 50ms. Instead a {{ServiceTracker}} could be used to wait for the service.,4557
Extract Method,Context-Aware Config: Properly support nested configuration classes in SPI and Mangement API currently nested configuration classes are supported in the configuration resolver but are not properly supported in the AnnotationClassConfigurationMetadataProvider SPI implementation and the Management API implementation.,4559
Rename Method,Context-Aware Config: Access to Inheritance Properties in Management API SPI the configuration management API (and the related configuration persistence SPI) should provide access (read/write) to inheritance-related properties e.g. for controlling resource collection inheritance and resource property inheritance.because both are depending on the resource resolving strategy the APIs should only route through these control properties without interpreting them themselves.,4563
Extract Method,"Allow to extend LoginAdminWhitelist with multiple configurations As [discussed on the mailing list|http://sling.markmail.org/thread/7xfcefaufczvsdgk] it would be desirable to allow multiple configurations to contribute to the {{LoginAdminWhitelist}}.This issue is marked *blocker* as the current implementation was not yet released thus allowing arbitrary changes without backwards compatibility headaches.I propose to remove the {{whitelist.bundles.default}} and {{whitelist.bundles.additional}} properties and replace them by ""additional configurations"" that each allow to provide a list of whitlisted bundle symbolic names.In the main configuration for {{LoginAdminWhitelist}} I propose to retain the flag to bypass the whitelist completely.I am uncertain whether we really need the whitelist regexp for testing as it is fairly simple to list a hand full of required bundles. If we keep it I suggest to make its metatype private.Optionally we could consider the possibility to allow configuring a list of required ""additional configurations"". I would leave this until we find a real requirement for this as it would complicate the implementation.",4565
Rename Method,"osgi-mock sling-mock: Add support for Context Plugins * osgi-mock should provide a OsgiContextBuilder as well* the context builders should accept a list of context callbacks for each lifecycle phase* introduce a generics ContextCallback which can be used with all mock variantsa better solution for all this is add support for ""context plugins"".",4567
Extract Method,Context-Aware Config: Sling Mock Context Plugin using the new mock context plugin feature from SLING-6359 we should provide a plugin for setting up context-aware configuration in the unit test environment.,4571
Rename Method,Context-Aware Config: Delete Configurations via ConfigurationManager currently it is not possible to delete singleton configurations - only to clear all properties.we should add a delete method to ConfigurationManager API and to the related ConfigurationPersistenceStrategy SPI.,4574
Extract Method,Log a warning in case a resource resolver is closed by the Sling RR Finalizer thread Currently all dangling resource resolvers which are no longer referenced are being automatically closed by Sling in https://github.com/apache/sling/blob/trunk/bundles/resourceresolver/src/main/java/org/apache/sling/resourceresolver/impl/CommonResourceResolverFactoryImpl.java#L100. Unfortunately this happens silently.Since all resource resolvers should be closed explicitly in the code (and not rely on this thread to clean them up) there should be an according WARN in the log whenever an unclosed ResourceResolver is detected and closed.A similar logging statement can be seen in https://github.com/apache/jackrabbit/blob/trunk/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/SessionImpl.java#L1372.Of course it would be very helpful if the stack trace where the resource resolver was opened would be logged as well.See also the related discussion at http://www.mail-archive.com/dev@sling.apache.org/msg62779.html.,4576
Rename Method,Context-Aware Config: Support default values in ConfigurationResolver for ValueMaps currently default values as defined in the configuration annotation classes are only supported in ConfigurationResolver when accessing them via this Annotation class. when using ValueMap only the real data stored in the repository is returned.in ConfigurationManager the default values are also supported in the ValueMaps so we should support this in ConfigurationResolver as well for consistency and to make them available e.g. in Sightly templates via SLING-6384.,4578
Extract Method,ContentLoader shouldn't commit changes or at least allow to disable auto commit The {{ContentLoader}} always automatically persists changes made to the given {{ResoureResolver}}. This makes it hard to use for test on classes implementing transactional changes. Example: Having high-level APIs that do changes on the {{ResourceResolver}} allowing to automatically commiting them (PageManager AssetManager in AEM as implementation on top of sling). But to keep it abstract lets say I have a class {{SpecificBinaryFileSetResource}} which has a method {{addBinaryFile}}. The goal is to implement a mock for that so I'm using {{ContentLoader}} to create a binary file in the {{ResourceResolver}}. This will automatically commit the changes. Now lets extend the {{addBinaryFile}} to accept a boolean parameter to not automatically commit those changes (maybe because I want to make multiple changes rolling them back on error). This isn't not supported so far when using ContentLoader.,4580
Rename Method,Implement support for date and number formatting for HTL Version 1.3 of the HTL Specification extends the functionality of the {{format}} option by providing support for also formatting dates and numbers - https://github.com/Adobe-Marketing-Cloud/htl-spec/blob/1.3/SPECIFICATION.md#122-format.,4584
Extract Method,Add provider for status information about timed events Currently it is only possible to start and stop timed events (by sending events)A new interface TimedEventStatusProvider should be added which allows to query if something is scheduled.,4587
Extract Method,Allow for specifying oak restrictions with repoinit Allow for specifying oak restrictions with repoinit. Currently repoinit allows one to ADD remove ACLs but there is no way to specify oak restrictions.http://jackrabbit.apache.org/oak/docs/security/authorization/restriction.html,4589
Move Method,Remove dependency to jcr.resource API As the jcr.resource API is deprecated we should replace it's usage with the Resource API,4595
Extract Method,Remove dependency to org.json Some IDE code is using org.json. We have to replace this. This is the list of files using that code:./tooling/ide/api/src/org/apache/sling/ide/osgi/impl/HttpOsgiClient.java:import org.json.JSONArray;./tooling/ide/api/src/org/apache/sling/ide/osgi/impl/HttpOsgiClient.java:import org.json.JSONException;./tooling/ide/api/src/org/apache/sling/ide/osgi/impl/HttpOsgiClient.java:import org.json.JSONObject;./tooling/ide/api/src/org/apache/sling/ide/osgi/impl/HttpOsgiClient.java:import org.json.JSONTokener;./tooling/ide/eclipse-test/src/org/apache/sling/ide/test/impl/helpers/ExternalSlingLaunchpad.java:import org.json.JSONArray;./tooling/ide/eclipse-test/src/org/apache/sling/ide/test/impl/helpers/ExternalSlingLaunchpad.java:import org.json.JSONObject;./tooling/ide/eclipse-test/src/org/apache/sling/ide/test/impl/helpers/ExternalSlingLaunchpad.java:import org.json.JSONTokener;./tooling/ide/impl-resource/src/org/apache/sling/ide/impl/resource/transport/GetNodeContentCommand.java:import org.json.JSONArray;./tooling/ide/impl-resource/src/org/apache/sling/ide/impl/resource/transport/GetNodeContentCommand.java:import org.json.JSONObject;./tooling/ide/impl-resource/src/org/apache/sling/ide/impl/resource/transport/ListChildrenCommand.java:import org.json.JSONObject;,4596
Inline Method,Remove getAdministrativeResourceResolver from Sling Validation 3 occurrences in# {{o.a.s.validation.impl.resourcemodel.ResourceValidationModelProviderImpl}}# {{o.a.s.validation.impl.ValidationModelRetrieverImpl}} (most probably the latter should not deal with resource resolvers at all (compare with http://www.mail-archive.com/dev@sling.apache.org/msg64770.html))# {{o.a.s.validation.impl.ValidationServiceImpl}} to retrieve the search paths (for relativizing resource types),4600
Move Method,JCR Content Parser for different usecases around file system resource provider and sling mocks (see related tickets) we need to parse content structures from files in the file system e.g. in JSON format or JCR XML format.we should put this code in a new commons library so it can be reused from the different projects.,4605
Move Method,Improve logging in HealthCheckMBeanCreator 0,4607
Extract Method,DefaultValidationFailure and DefaultValidationResult should use ValidationContext as parameter {{ValidationContext}}:{noformat}public DefaultValidationFailure(@Nonnull ValidationContext validationContext @Nonnull String messageKey Object... messageArguments){noformat}instead of {{location}} {{severity}} and {{ResourceBundle}} from {{ValidationContext}}:{noformat}public DefaultValidationFailure(@Nonnull String location Integer severity @Nonnull ResourceBundle defaultResourceBundle @Nonnull String messageKey Object... messageArguments){noformat},4608
Inline Method,Support CAConfig Impl 1.3.0 some classes in the CAConfig 1.3.0 implementation have moved - we need to update the mock plugin to support this changes in unit tests.as before the mock plugin should support all CAConfig Impl version since 1.0.,4613
Rename Method,Context-Aware Config: Separate exception when persist failes due to missing access rights when persisting configuration failes due to missing access rights (e.g. read-only access) the implementation should throw a dedicated exception to be handled separately in upper layers (e.g. configuration editor GUI).,4617
Rename Method,Replace usage of org.apache.sling.commons.json.* and org.json in launchpad relevant bundles Following the deprecation of org.apache.sling.commons.json (SLING-6536) we need to replace its usage everywhere else (at least if we want to be able to release other modules that depend on it). This is the umbrella issue for getting this done. The idea is to create sub-issues with patches for individual components review the patches and when all are done: close this issue. General discussions and problems should go to this issue and specific ones on the sub-issue in question.,4618
Move Method,SlingInfoServlet is overengineered The SlingInfoServlet is a little bit overengineered. It is using an extensible provider concept however the provider interface is private there is only a single implementation and in fact this is not extensible at all.,4622
Inline Method,Migrate to R6 annotations clean up dependencies We should migrate to the R6 annotations and check whether we can remove some of the dependencies.,4623
Extract Method,Migrate to R6 annotations clean up dependencies 0,4624
Rename Method,Align accessors in API We currently have {{getValidationModel(...)}} and {{getModel(...)}}/{{getModels(...)}} but should use one or the other.Also we should remove _Validated_ from {{ValidationModel#getValidatedResourceType()}}.,4625
Extract Method,Drop workspace support We dropped workspace support from most other modules long time ago it seems we missed the servlets post module. We should remove it here as well as it is not needed anymore.,4630
Extract Method,Enable Rhino debugger by framework property Currently the server side Rhino debugger is enabled by placing a file in the /tmp directory (see SLING-193).This is not very appropriate and should be replaced by a framework property which when set will enable the Rhino debugger.,4633
Move Method,Context-Aware Config: Make filtered property names for configuration persistence configurable currently a hard-coded list of filtered property names is applied in the configuration manager implementation and the default configuration persistence implementation:{noformat}jcr:primaryTypejcr:mixinTypesjcr:createdjcr:createdByjcr:lastModifiedjcr:lastModifiedByjcr:uuid{noformat}this list should be made configurable because in upstream applications more properties might need to be filtered e.g. to track replication status.the possibility to filter a list of property names by name or regex should be made accessible in the ConfigurationManager API that is part of the Management API so other configuration persistence applications may use it as well.,4635
Extract Method,add support for multivalues expressions in write pipes configurations for now an expression in a MV in a write pipe configuration will systematically be reproduced as is. WritePipe.computeValue reproduce the expression as is if the passed object is a string.In case of a String MV we should compute the expression for each String,4636
Extract Method,Request to allow the health check servlet to directly query a single health check by name AMS has a request to be able to access an individual health check by nameFor example: http://localhost:4502/system/health/named/Sling%20Get%20Servlet.jsonAnd have it return the results for only this named health check.,4637
Extract Method,Migrate to R6 annotations clean up dependencies We should check the dependencies and move to the R6 OSGi annotations,4638
Extract Method,FSResource: Support node descriptor files for folders and binary files for binary files and folders it is possible to add additional content definition files with the same name which add additional mixins or properties to the imported nt:file/nt:folder resource or change the JCR primary type.this has to be supported by fsresource as well.with this we should also add support for the XML descriptor file format (see SLING-6828).,4639
Extract Method,Migrate to parent pom 32 0,4641
Extract Method,"JCR Content Parser: Support tick as well as double quote when parsing JSON files currently the content parser supports JSON files only in ""strict format"" plus the support for comments in JSON files.we should also support ticks for quoting names and string values as in SLING-6871 for the content loader.",4643
Rename Method,Bundle resource provider: support mounting of JSON files I think similar to SLING-6440 we should support mounting of JSON files through the bundle resource provider (we don't need to support other file formats as xml or vault),4644
Extract Method,Implement RequestDispatcher.forward Currently the SlingRequestDispatcher class implementing the RequestDispatcher interface inside Sling does not implement the forward method. Since this method may be used in certain environments this should also be implemented.,4650
Extract Method,Adjust AbstractSlingRepository2 to reflect SLING-6963 Splitting off the adjustment of {{AbstractSlingRepository2}} from SLING-6963 in order to make sure the enhancement for the serviceusermapping bundle can be treated separately from the (optional) changes to the {{AbstractSlingRepository2}}.IMHO it makes sense to also reflect the enhancement in the _jcr.base_ module to actually illustrated the intention inside the Sling code base.,4651
Rename Method,Add SlingScriptHelper.forward method SLING-692 implements the RequestDispatcher.forward method. The SlingScriptHelper interface should now be enhanced with a forward method.,4653
Extract Method,Access control setup of repository-level permissions (i.e. null path) If I am not mistaken it is currently not possible to create access control setup for principals at the 'null' path which according to JSR 283 is to be used to setup repository level permissions such as - node type definition management (i.e. registering new node types)- namespace management (i.e. registering new namespaces)- privilege management (i.e. registering new privileges)- workspace management (i.e. creating/removing workspaces)All of these operations are not bound to a path (like e.g. removing an item or creating a new version for a given node) but instead take global effect on the whole JCR repository... that's why permissions for these operations cannot be granted at a given path.In the default authorization model shipped with Jackrabbit and Oak the -null- path access control policy is stored with a dedicated _rep:repoPolicy_ node located with the root node and For service user definitions we need to be able to define entries for the -null- path policy for the reasons explained above. Thanks for extending the repo-init accordingly.,4656
Extract Method,"Support mixins in repoinit ""create path"" statements The repoinit ""create path"" statement currently supports nodetypes but no mixins we should add support for them.The current create path syntax is like{code}create path (sling:Folder) /var/discovery(nt:unstructured)/somefoldercreate path /one/two/threecreate path /three/four(nt:folk)/five(nt:jazz)/six{code}Where the first bracketed statement before the path is the default nodetype for all subpaths and each subpath can have a specific nodetype.To add mixin support I suggest the syntax of these examples for these bracketed statements:{code}(sling:Folder mixin mix:A mix:B)(nt:unstructured mixin mix:C)(mixin mix:A)(mixin mix:A mix:B){code}The last two forms without a nodeteype meaning ""set mixins only but keep the default nodetype"" which in this example{code}create path (sling:Folder) /var/foo(mixin mix:B){code}means /var/foo is of type sling:Folder with mixin mix:Bwhereas in this example{code}create /var/bar(mixin mix:C){code}/var/bar uses the default type defined by /var's type with mix:C added.",4658
Extract Method,Rename metrics file on configuration change The metrics reporter introduced in SLING-7055 overwrites an existing metrics file when the configuration is changed. It would be more useful to keep an existing file by renaming it. Otherwise valuable metrics data gathered so far is lost.,4660
Extract Method,few pipe builder improvements needed there are a few improvements that would be cool after a few usages of pipe builder:- add #build(String) method that allows to build a pipe *not* in a random path in case the location does matter (for now searching & moving the executed pipe is a bit tedious)- allow .with() to work before first subpipe this would affect the container pipe allowing properties there- add runWith(String...) that is a shorter version of run(Map),4662
Rename Method,Sling Query support for Java 8 Sling Query while written in Java 7 uses a number of concepts introduces in Java 8 eg. Predicate and Function. Now we can replace these interfaces with the native java.lang.* types.Also it should be possible to transform the SlingQuery to the Java 8 stream.,4665
Extract Method,Add PipeModel add a pipe model allowing quick content aggregation with no java code,4667
Extract Method,Add configuration to default GET servlet to produce other kinds of responses. it would be desirable to configure the default behavior for the default GET servlet. eg:report=dump -> wil generate some sort of dump (as it does today)report=dirlisting -> will generate a dir listing of the subnodes.report={number} -> send an error eg: 403,4668
Rename Method,Process resource detection and installation in sequence instead of asynchronously Currently the RepositoryObserver's resource detection loop and the OsgiController's installation/update/delete loop run in their own separate threads.Running the detection and installation operations in sequence would make debugging and testing easier and there are no downsides except maybe slightly slower processing of installed bundles and configs - I think the tradeoff is worth it and it's simple to implement: I'll add a executeScheduledOperations() method to the OsgiController interface which will be called by the RepositoryObserver at the end of its repository observation loop.,4670
Extract Method,"Improve the Sling Embedded repository config to make it more flexible As discussed in [1] it would be necessary to make the Sling embedded repository configuration more flexible. Use cases for this can be:* Work with another persistent manager for nodes (rather than default one but already supported by Jackrabbit. See [2]).* Custom login module and access manager.* Change the name of the repository.The propossal is having three new properties:## The name of the JCR repository. Default is ""jackrabbit"".# sling.repository.name = ## The JCR repository home directory. Default is sling.home/sling.repository.name.# sling.repository.home = ## The JCR repository url config file (repository.xml). Default is repository.xml in# bundle Embedded JCR Repository# sling.repository.config.file.url =These properties can be set in sling.properties as system properties or as web app init-params. The properties are not mandatory. The default beahaviour is a ""jackrabbit"" repo under sling home if the properties are not set.[1] http://markmail.org/search/Customizing+the+Sling+embedded+repository [2] http://markmail.org/message/2mtvc3egw5omcrbd?q=Customizing+the+Sling+embedded+repository",4671
Extract Method,Create filesystem provider configurations for initial content on sling install The maven sling plugin could be improved to generate file system provider factory configurations for initial content.During a sling:install the plugin will read the generated manifest extract the initial content directives and uses them toPOST new configurations to the Felix webconsole,4672
Extract Method,"ValueMapDecorator does not support arrays. albeit the JcrPropertyMap supports arrays the wrapper does not and yields to unexpected problems when using it.eg: i use the following code to get a detached copy of a jcr property map:Node content = node.getNode(""jcr:content"");ValueMap props = new JcrPropertyMap(content);// create detached copyValueMap properties = new ValueMapDecorator(new HashMap<String Object>(props));and the following does not work:String[] values = properties.get(""myProp"" new String[0]);although it works on the JcrPropertyMapps: will provide a patch",4685
Rename Method,OpenID AuthenticationHandler An implementation of AuthenticationHandler for authenticating users against OpenID providers. Includes basic UI for login & logout.,4687
Extract Method,Support getting versioned resources by using uri path parameters Getting versioned content should be support thorough uri path parameters like /something/hello;v=1.1For jcr based resources the value of the version should either point to a version name or label.In order to not change our existing apis we introduce a new utility method which removes all uri path parametersand returns a map of these. Every resource provider could use this utility method and then decide to act on theseparameters.If a requested version does not exists a 404 is returned.If the requested node does not directly point to a versionable node the algorithm checks the parent hierarchy until a versionable node is found and tries to get the version of this node and then goes down the path again. If the versionable node does not have the requested version or the child a 404 is returned.,4688
Extract Method,"Add HTTP PUT support for maven-sling-plugin In addition to the standard POST that is tailored for the felix webconsole in the maven-sling-plugin install goal it can also be useful to support simple PUT uploads of the bundle jar. This is useful in combination with the jcrinstall extension where you could PUT the jar file via Jackrabbit's or Sling's WebDAV into a folder that is watched by jcrinstall.Therefore I implemented this feature using PUT for the install goal and DELETE for the uninstall goal. There is a new config option ""sling.usePut"" that must be set to ""true"" for using PUT or DELETE - by default it is false so that the POST behaviour stays default.To make the configuration of the plugin in a multi-module POM structure simpler I also added a new config ""sling.urlSuffix"" that (if present) will be appended to the ""sling.url"" (in all cases POST PUT and DELETE). That allows you to specify a base path in your parent POM (sling.url=http://localhost:8080) and define a different path for each project (sling.urlSuffix=/libs/myproject/install) because with jcrinstall you can place them at different locations in the repository if you wish.There is also a ""sling.mimeTypeForPut"" option to set the content-type for the PUT request (by default it is ""application/java-archive"").Just for reference since the config names are different if you specify them in the pom (using the java field name in the Mojo rather than the defined expression....) here is a sample configuration:<plugin><groupId>org.apache.sling</groupId><artifactId>maven-sling-plugin</artifactId><version>2.0.3-incubator-SNAPSHOT</version><configuration><slingUrl>http://localhost:8080</slingUrl><slingUrlSuffix>/libs/myproject/install/</slingUrlSuffix><usePut>true</usePut></configuration></plugin>",4690
Rename Method,"Use the sling-api from SLING-28 in microsling Once we agree on SLING-28 microsling can be ""ported"" to this API so that both microsling and Sling use it.Make sure to tag http://svn.apache.org/repos/asf/incubator/sling/whiteboard/microsling/ before doing this",4691
Inline Method,"microjax - lightweight Ajax client and servlet for microsling As discussed on the mailing list [1] I'll contribute to microsling the experimental ""r-jax"" stuff [2] that we wrote together with David Nuescheler.The main changes are:1) The default microsling servlet needs to handle POSTs in a more clever way to provide useful services to the javascript client2) For GETs the default microsling servlet needs to provide a JSON representation when the json extension is used. This will be extensible to XML and other output formats.[1] http://thread.gmane.org/gmane.comp.apache.sling.devel/721[2] http://www.day.com/maven/rjax/",4694
Rename Method,Refine initiaition of the authentication process Currently the authentication process can only be initiated by explicitly calling a login page provided by some AuthenticationHandler implementation bundle. There is no way to initiate the authentication process from within a servlet or script (e.g. to have the user log in a 404/NOT FOUND error handler).To support this kind of functionality the existing SlingAuthenticator.requestAuthentcation method should be publicly accessible through Service interface. Servlets or scripts which want to request authentication from the client for the current request may then call this service method.This method applies the same authentication handler selection algorithm for the given HttpServletRequest object as it does for finding the authentication handler in the authenticate process. This ensures that for a given request the appropriate authentication handler is called which is then able to initiate authentication appropriately for example by drawing a form.For full details refer to http://cwiki.apache.org/SLING/authentication-initiation.html,4697
Extract Method,"Add suppport for TCP/IP based control connection for Sling standalone app Currently the Sling standalone application can only be stopped by stopping the system bundle or by killing the Sling process. To better control Sling control connection support based on TCP/IP would be nice. This way the same sling standalone application may be used to start sling to check a running sling instance and to stop a running sling instance.The following command line arguments are added:""start"" - Sling opens a TCP/IP server socket for the control conneciton""status"" - Checks whether a Sling instance is listening on a TCP/IP socket""stop"" - Stops a Sling instance listening on a TCP/IP socketnone of the above - starts Sling without listening on a TCP/IP socketThe socket to listen on (start option) or to send the command to (status stop) is configured with the ""-j"" command line option. This option takes an argument of the following form:host:port -- host name or ip address and port number of the socketport -- port number on localhost (InetAddress.getLocalHost()) of the socketnone or -j not specified -- defaults to port 63000 on localhostNote that setting any host name or IP address which is reachable from remote systems may be considered as security issue since the connection is not secured by password or such. For this reason the default interface to listen on is local host and the server socket is only created if explicitly asked for with the start parameter.",4698
Extract Method,MimeTypeService: expose default mime types config in osgi web console please consider exposing the default mime types currently being defined in launchpad's web.xml as an OSGi configuration accessible through the web console or repository based content configuration.,4702
Move Method,Allow ATOM Syndication Format output Having a JSON output for Sling is quite nice but many web applications require RSS/Atom support these days. Having an extension module that provides a tag library for creating Atom feeds and atom entries out of JCR nodes would be very helpful.,4704
Rename Method,Storm Cassandra connector 0,4705
Rename Method,Support AND OR and NOT operators in StormSQL This jira proposes to compile AND OR and NOT operators to Java source code.,4708
Rename Method,Have the IConnection push batches instead of buffering them The messaging layer currently buffers tuples and waits for one or more threads to take the tuples and route them where they need to go.This adds an extra thread that a tuple has to go through before it can be processed and places the tuple in a LinkedBlockingQueue waiting for one of the threads to be ready to process the data.,4709
Rename Method,Reduce Thread Usage of Netty Transport When users start to create large topologies the storm netty messaging layeruses lots of threads. This has resulted in OOMs because the default ulimit on most linux distros is around 4000 processes. It looks like the messaging layer wants to have one thread per server it is connected to so that means the total number of other workers in the System.For one particular case we saw.1 (Curator delay thread)1 (Curator Event Processor)1 (Finalizer)1 (GC???)1 (Storm messaging recv thread asking netty for messages)1 (Thread pool polling on a Synchronous queue???)1 (ZK Connection)1 (ZK epoll)2 (???)2 (Netty epoll)6 (Timer Thread)15 (Disruptor consume batches)104 (Netty Thread pool taking messages to be sent)and this process was dieing with OOMs because it could not create any more netty threads.Looking at the code it appears that come from two different things. First The Client code is using it's own thread pool for each Client instead of sharing a thread pool but also the protocol itself blocks the thread in takeMessages() if there are no messages to send.So we need to make the thread pool shared between all of the clients and modify the protocol so that takeMessages does not block. But with it not blocking we also need a way to have Client.send write directly to the Channel in some situations so that the messages still are sent.,4710
Extract Method,Support distributed deployment in StormSQL StormSQL compiles the SQL statements into Java classes. The Trident topology executes these classes in order to execute the SQL statements.These classes need to be properly distributed through Nimbus so that the topology can be run in distributed mode.,4711
Rename Method,port backtype.storm.utils-test to java junit test migration,4714
Rename Method,port backtype.storm.daemon.worker to java https://github.com/apache/storm/tree/jstorm-import/jstorm-core/src/main/java/com/alibaba/jstorm/daemon/worker as an example,4717
Extract Method,port backtype.storm.testing to java lots of helper functions/macros for running tests. Some might need to stay in cojure with a java equivalent that can be used when other tests are ported over,4718
Move Method,port backtype.storm.worker-test to java Test Worker,4719
Rename Method,Trident should support writing to multiple Kafka clusters Current it is impossible to instantiate two instances of the {{TridentKafkaState}} class that write to different Kafka cluster. This is because that {{TridentKafkaState}} obtains the the location of the Kafka producer from configuration. Multiple instances can only get the same configuration in the {{prepare()}} method.This jira proposes to introduce a configuration class like {{TridentKafkaConfig}} to allow multiple instances of {{TridentKafkaState}} to write to different Kafka clusters.,4720
Extract Method,Support writing to Kafka streams in Storm SQL This jira proposes to add supports to write SQL results to Kafka streams.,4721
Extract Method,Support EXPLAIN statement in StormSQL It is useful to support the `EXPLAIN` statement in StormSQL to allow debugging and customizing the topology generated by StormSQL.,4722
Rename Method,"Load Balancing Shuffle Grouping https://github.com/nathanmarz/storm/issues/571Hey @nathanmarzI think that the current shuffle grouping is creating very obvious hot-spots in load on hosts here at Twitter. The reason is that randomized message distribution to the workers is susceptible to the balls and bins problem:http://pages.cs.wisc.edu/~shuchi/courses/787-F07/scribe-notes/lecture07.pdfthe odds that some particular queue gets bogged down when you're assigning tasks randomly is high. You can solve this problem with a load-aware shuffle grouping -- when shuffling prefer tasks with lower load.What would it take to implement this feature?----------sritchie: Looks like Rap Genius was heavily affected when Heroku started running a ""shuffle grouping"" on tasks to its dynos:http://rapgenius.com/James-somers-herokus-ugly-secret-lyrics50x performance degradation over a more intelligent load-balancing scheme that only sent tasks to non-busy dynos. Seems very relevant to Storm.----------nathanmarz: It's doing randomized round robin not fully random distribution. So every downstream task gets the same number of messages. But yes I agree that this would be a great feature. Basically what this requires is making stats of downstream tasks available to the stream grouping code. The best way to implement this would be:Implement a broadcast message type in the networking code so that one can efficiently send a large object to all tasks in a worker (rather than having to send N copies of that large message)Have a single executor in every topology that polls nimbus for accumulated stats once per minute and then broadcasts that information to all tasks in all workersWire up the task code to pass that information along from the task to the outgoing stream groupings for that task (and adding appropriate methods to the CustomStreamGrouping interface to receive the stats info)----------sorenmacbeth: @nathanmarz @sritchie Did any progress ever get made on this? Is the description above still relevant to Storm 0.9.0. We are getting bitten by this problem and would love to see something like this implemented.",4723
Rename Method,DRPCSpout should attempt reconnect if on fail it cannot reach client 0,4724
Extract Method,Optimize Kryo instaces creation in HBaseWindowsStore 0,4725
Rename Method,Cap on number of retries for a failed message in kafka spout The kafka-spout module based on newer APIs has a cap on the number of times a message is to be retried. It will be a good feature add in the older kafka spout code as well.,4728
Extract Method,Add simple equi-join support in storm-sql standalone mode Provide simple equi join support in storm sql standalone mode.,4729
Extract Method,Upgrade Jetty and Ring Jetty 7 is EOL  upgrade to Jetty 9 & Ring could also support it.,4731
Rename Method,Blacklist Scheduler My company has gone through a fault in production in which a critical switch causes unstable network for a set of machines with package loss rate of 30%-50%. In such fault the supervisors and workers on the machines are not definitely dead which is easy to handle. Instead they are still alive but very unstable. They lost heartbeat to the nimbus occasionally. The nimbus in such circumstance will still assign jobs to these machines but will soon find them invalid again result in a very slow convergence to stable status.To deal with such unstable cases we intend to implement a blacklist scheduler which will add the unstable nodes (supervisors slots) to the blacklist temporarily and resume them later.,4732
Extract Method,Supervisor V2 can use a lot more memory When launching a worker the supervisor will read in the complete topology to get a small amount of data out of it. This can be very large. In the past we would launch the workers one at a time so we only needed enough memory to read it in once now we are doing it in parallel and lots of these tend to show up very close to one another. We need to find a way to make it so that we read the data one and ideally cache it someplace that everyone can share.,4734
Extract Method,Let ShellBolts and ShellSpouts run with scripts from blobs It would be nice to be able to use the scripts and executable files distributed through the blob store rather then through the resources directory in a jar.This is nice because it allows you to use a tgz that preserves the execute bit on files like scripts. The biggest issue here is that ShellProcess switches the current working directory for the process over to the code dir (where the storm jar is extracted). And there is not simple way for a child process to find its way back to the blobs. I will add in a new option to not change the CWD.,4735
Extract Method,[Storm SQL] Support Avro as input / output format Support Avro as Input / Output format.Try to generalize regardless of data source but if it can't be it should address all of data sources. If some datasources are not compatible to Avro it should be documented.,4736
Extract Method,Dynamic scheduler configuration loader It would be useful if scheduler configuration for multitenant or resource aware scheduler could be loaded and updated dynamically through a local file change or through an update to a configuration archive in an Artifactory repository.,4738
Extract Method,Expose a method to override in computing the field from given tuple in FieldSelector org.apache.storm.cassandra.query.selector.FieldSelector should give a way to customize computing field value from tuple.,4739
Extract Method,Send activate and deactivate command from ShellSpout When deactivate and activate are called on ShellSpout those calls should be send to the corresponding ShellProcess as multilang commands. *For Example:*When a ShellSpout polls some data from any source those resouces can be gracefully allocated or deallocated on (de)activation. Otherwise there is no possibility to react on those events via multilang support.,4741
Extract Method,Support lists of childopts beyond just worker The following worker childopts configuration options all support both a string value and a list of strings value:{code}WORKER_CHILDOPTSWORKER_PROFILER_CHILDOPTSWORKER_GC_CHILDOPTSTOPOLOGY_WORKER_CHILDOPTSTOPOLOGY_WORKER_GC_CHILDOPTSTOPOLOGY_WORKER_LOGWRITER_CHILDOPTS{code}Currently the following childopts configuration options only support strings:{code}NIMBUS_CHILDOPTSLOGVIEWER_CHILDOPTSUI_CHILDOPTSPACEMAKER_CHILDOPTSDRPC_CHILDOPTSSUPERVISOR_CHILDOPTS{code}Please could lists be supported across all childopts options as it makes configuration management and building easier using automated tools such as Chef and Puppet.,4743
Extract Method,Record version and revision information in builds The effect shown below {code: title=For Subversion project}$ storm version Storm 0.9.2-incubating-SNAPSHOTSubversion https://github.com/apache/incubator-storm/trunk/storm-core -r 1959Compiled by somebody on Wed Feb 19 11:23:38 CST 2014From source with checksum 9347aded8a39f3ddf8e8f2f9bf56186f{code}or {code}$ java -classpath storm-core-0.9.2-incubating-SNAPSHOT.jar backtype.storm.utils.VersionInfoStorm 0.9.2-incubating-SNAPSHOTSubversion https://github.com/apache/incubator-storm/trunk/storm-core -r 1959Compiled by somebody on Wed Feb 19 11:23:38 CST 2014From source with checksum 9347aded8a39f3ddf8e8f2f9bf56186f{code}{code: title=For Git Project:}$ storm version _ storm-current storm versionRunning: ... ...Storm 0.10.0-SNAPSHOTSubversion https://github.com/caofangkun/apache-storm.git -r ffba148cc47a92185fa1a5db11f72982de10f106Branch storm-243Compiled by caokun on Thu Jan 8 10:47:48 CST 2015From source with checksum 97e7c942939e3e82dcb854b497991a51{code}For UI ,4744
Rename Method,Configs should have generics Config since the beginning has not really had generics it has just been a Map. We should really have it be consistent everywhere a {{Map<String Object>}}This will reduce the number of warnings in the code base by a lot.,4745
Move Method,"Lambda support In the past If we want print tuples we need to write the following code:{code}class PritingBolt extends BaseBasicBolt{@Overridepublic void execute(Tuple input BasicOutputCollector collector) {System.out.println(input);}@Overridepublic void declareOutputFields(OutputFieldsDeclarer declarer) {// nothing}}builder.setBolt(""bolt2"" new PritingBolt());{code}Now with this patch:{code}builder.setBolt(""bolt2"" tuple -> System.out.println(tuple));{code}The above is just an simplest demo. This patch provides some new methods in TopologyBuilder to allow you to use Java8 lambda expression:{code}setSpout(String id SerializableSupplier<?> supplier)setSpout(String id SerializableSupplier<?> supplier Number parallelism_hint)// receiving tuple and emitting to downstreamsetBolt(String id SerializableBiConsumer<TupleBasicOutputCollector> biConsumer String... fields)setBolt(String id SerializableBiConsumer<TupleBasicOutputCollector> biConsumer Number parallelism_hint String... fields)// receiving tuple and never emitting to downstreamsetBolt(String id SerializableConsumer<Tuple> consumer)setBolt(String id SerializableConsumer<Tuple> consumer Number parallelism_hint){code}Here is another example including the three interface usage:{code}// example. spout1: generate random strings// bolt1: get the first part of a string// bolt2: output the tuplebuilder.setSpout(""spout1"" () -> UUID.randomUUID().toString());builder.setBolt(""bolt1"" (tuple collector) -> {String[] parts = tuple.getStringByField(""lambda"").split(""\\-"");collector.emit(new Values(parts[0]));} ""field"").shuffleGrouping(""spout1"");builder.setBolt(""bolt2"" tuple -> System.out.println(tuple)).shuffleGrouping(""bolt1"");{code}",4746
Extract Method,Implement auto credential plugin for Hive Currently the auto credential plugins are available for hdfs hbase. This jira is to implement a similar auto credential plugin for Apache Hive. This plugin fetches a delegation token using HCatClient and distributes to workers. This builds on top of STORM-2482,4747
Extract Method,AbstractAutoCreds should look for configKeys in both nimbus and topology configs 0,4748
Extract Method,the method of getting the nimbus cilent doenot accept timeout parameter The method of getting the nimbus cilent cannot receive timeout parameter. When the storm cannot be reached it takes a lot of time to wait for the result.,4750
Extract Method,We need a drpc-client command line. We have no simple way to send a DRPC request from the command line. We really should have one for debugging/testing at a minimum and also as an example of what works to use DRPC.,4751
Rename Method,Spout throtteling metrics are unusable When helping someone debug an issue with backpressure I realized that the metrics we are collecting in the spout are mistakenly being multiplied by the rate even though we are not sub-sampling them. This results in the values being by default 20 times higher then they should be. Thinking about how I would use the metrics to debug an issue also showed that some of them. skipped-max-spout and skipped-throttle correspond to about 1 ms of sleep but skipped-inactive corresponds to about 100 ms of sleep. And the 1 ms sleep is configurable so it could be different from one topology to another and even the code around it is pluggable so it could be doing anything from not sleeping to sleeping a random amount of time.I think we just need to scrap what we have been doing and record how long we sleep for and use that as the metric instead.These metrics also don't appear to be documented anywhere so I am going to change what they mean and document them to actually be useful and correct.,4752
Extract Method,Enhance stateful windowing to persist the window state Right now the tuples in window are stored in memory. This limits the usage to windows that fit in memory and the source tuples cannot be acked until the window expiry. By persisting the window transparently in the state backend and caching/iterating them as need we could support larger windows and also windowed bolts with user/application state.,4753
Extract Method,Improvements for access to topology Listing a user in topology.users means that user can see the topology's storm UI view logs but also affect the topology kill it restart a worker do profiling or heap dumps. We want to give some users access to UI and logs but not let them impact the topology.We are proposing to add in some new configs for TOPOLOGY_UI_USERS and TOPOLOGY_UI_GROUPS and then split the get operations off from the others in SimpleACLAuthorizer,4754
Rename Method,Apply new code style to storm-sql-kafka 0,4755
Rename Method,Update config validation check to give better information As part of submitting a topology we need to serialize the config as JSON. The check right now is rather bad. it just calls Utils.isValidConf and throws some generic Exception about problems.https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/utils/Utils.java#L978-L980It would be much better to have a new function/method that would check if they are equal and if not it would walk through them looking for which configs are different when it finds one it includes which configs and how they are different in the exception.,4756
Rename Method,For debugging allow users to tell the scheduler which nodes they would prefer In some cases with debugging it would be nice to let the user tell the scheduler that it wants to run on host X and not run on host Y.This is mostly for the case where we saw an odd issue with a topology and it was running on a specific host. So to unblock the user giving them the ability to avoid a specific host is helpful at the same time we may want to reproduce the issue and causing the topology to be scheduled on that bad node is helpful.,4758
Extract Method,Topology submission or kill takes too much time when topologies grow to a few hundred Now for a storm cluster with 40 hosts [with 32 cores/128G memory] and hundreds of topologies nimbus submission and killing will take about minutes to finish. For example for a cluster with 300 hundred of topologiesit will take about 8 minutes to submit a topology this affect our efficiency seriously.So i check out the nimbus code and find two factor that will effect nimbus submission/killing time for a scheduling round:* read existing-assignments from zookeeper for every topology [will take about 4 seconds for a 300 topologies cluster]* read all the workers heartbeats and update the state to nimbus cache [will take about 30 seconds for a 300 topologies cluster]the key here is that Storm now use zookeeper to collect heartbeats [not RPC] and also keep physical plan [assignments] using zookeeper which can be totally local in nimbus.So i think we should make some changes to storm's heartbeats and assignments management.For assignment promotion:1. nimbus will put the assignments in local disk2. when restart or HA leader trigger nimbus will recover assignments from zk to local disk3. nimbus will tell supervisor its assignment every time through RPC every scheduling round4. supervisor will sync assignments at fixed timeFor heartbeats promotion:1. workers will report executors ok or wrong to supervisor at fixed time2. supervisor will report workers heartbeats to nimbus at fixed time3. if supervisor die it will tell nimbus through runtime hookor let nimbus find it through aware supervisor if is survive 4. let supervisor decide if worker is running ok or invalid  supervisor will tell nimbus which executors of every topology are ok,4759
Extract Method,Add caching of some blobs in nimbus to improve performance In nimbus we read the topology and the topology config from the blob store all the time. Even multiple times for a single thrift call. It would really be great if we could cache these instead of rereading them all the time. (We have found it is a cause of the UI being slow in some cases).,4762
Rename Method,Refactor storm-kafka-client KafkaSpout Processing Guarantees 0,4763
Inline Method,Clean up configs in topology builders There are a lot of topology builders that are storing an array of maps for configs. But then the array gets smashed together into a single map when the topology is actually submitted. This makes the code really confusing and is completely unnecessary.,4764
Extract Method,Give users the option to disable the login cache Currently we cache login using [LoginCacheKey title|https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java#L57] But the LoginCacheKey failed to work correctly when we use TGT cache instead of keytab. The proposed solution is to add an option to jaas.conf so that user can disable the login cache if needed.,4765
Rename Method,Integration test should shut down topologies immediately after the test The integration test kills topologies with the default 30 second timeout. This is unnecessary and delays the following tests because the killed topology is still occupying worker slots. When the integration test kills topologies it tries sending the kill message to Nimbus once and may fail quietly. This breaks following tests because the default Storm install has only 4 worker slots and the test topologies each take up 3. When a topology is not shut down it prevents the following topologies from being assigned.,4766
Extract Method,RAS Constraint Solver Strategy We have a use case where a user has some old native code and they need it to work with storm but sadly the code is not thread safe so they need to be sure that each instance of a specific bolt is in a worker without other instances of the same bolt. It also cannot co-exist with other bolts for a similar reason. I know that this is a fairly strange use case but to help fix the issue we wrote a strategy for RAS that can do a simple search of the state space trying to honor these constrains and we thought it best to push it back then to keep it internal.,4767
Extract Method,"Expose IEventLogger to make event logging pluggable For the first time ""Event Logger"" feature is designed to make implementation pluggable so that's why IEventLogger exists but we didn't have actual use case other than just writing them to the file at that time so we just simplified the case. Now we have use case which also write events to file but with awareness of structure of event so that it can be easily parseable from log feeder. We would want to have custom IEventLogger to represent event as our own format in this case. There's another issue as well: EventInfo has `ts` which stores epoch but it's defined as String not long.",4768
Rename Method,Add Kerberos support to Solr bolt Update Solr bolt to work with Kerberized Solr clusters. Instructions for the SolrJ clients are here: https://lucene.apache.org/solr/guide/6_6/kerberos-authentication-plugin.html#KerberosAuthenticationPlugin-UsingSolrJwithaKerberizedSolr,4769
Inline Method,Use new wait strategies for Spout as well STORM-2306 introduced a new configurable wait strategy system for these situations * BackPressure Wait (used by spout & bolt) * No incoming data (used by bolt) There is another wait situation in the spout when there are no emits generated in a nextTuple() or if max.spout.pending has been reached. This Jira is to transition the spout wait strategy from the old model to the new model. Thereby we have a uniform model for dealing with wait strategies.,4770
Inline Method,Decouple storm-hive UTs with Hive The unit tests in storm-hive are running Hive instance and interact with Hive. This creates unnecessary directories and more likely easier to be broken. Given that they're UTs it would be better to decouple Hive from storm-hive tests.,4771
Extract Method,PartitionedTridentSpoutExecutor should use getPartitionsForTask https://issues.apache.org/jira/browse/STORM-2407 added a method to the IOpaquePartitionedTridentSpout.Emitter interface called getPartitionsForTask which is now used to delegate partitioning between tasks (previously the partitioning was hard coded to round robin). If we want to be able to delegate partitioning I don't see a reason not to make the same change on the IPartitionedTridentSpout.Emitter interface where partitioning is still hard coded to use round robin. E.g. compare https://github.com/apache/storm/blob/4137328b75c06771f84414c3c2113e2d1c757c08/storm-client/src/jvm/org/apache/storm/trident/spout/PartitionedTridentSpoutExecutor.java#L131 to https://github.com/apache/storm/blob/4137328b75c06771f84414c3c2113e2d1c757c08/storm-client/src/jvm/org/apache/storm/trident/spout/OpaquePartitionedTridentSpoutExecutor.java#L131,4772
Extract Method,Cache the storm id to executors mapping on master to avoid repeat computation Now nimbus will collect all the topologies's conf/topology-ser/storm-base to compute in a scheduling round which is a very heavy work. The scheduling will still take to minutes even we now change to RPC heartbeats and assignment distribution. So i decide to redesign the scheduler so we can only schedule the topologies that need to: that have dead workers or not enough number workers. Here i checkout out the code and found that the id->executors mapping is computed every time for every topology which is really a heavy computation and totally not that necessary because this mapping is fixed invariable for a topology unless we rebalance or kill it. So i refactor the code a little here and this is more powerful after the scheduler is resigned for delta-scheduling[ which is very lightweight even there are thousands of topologies on one cluster.] For now this is enough for us.,4773
Extract Method,"Topology name needs to be validated at storm-client *Current Behavior :* Execute topology with invalid topology name is throwing exception after uploading the jar. *Improvement :* Validating topology name at client side before uploading the jar.     {code:java} 2018-06-05 16:16:19.461 o.a.s.d.n.Nimbus pool-21-thread-53 [INFO] Uploading file from client to /manu/Git/storm/storm-dist/binary/final-package/target/apache-storm-2.0.0-SNAPSHOT/storm-local/nimbus/inbox/stormjar-22979659-5176-46fd-9027-ffdde13f595a.jar 2018-06-05 16:16:20.596 o.a.s.d.n.Nimbus pool-21-thread-35 [INFO] Finished uploading file from client: /manu/Git/storm/storm-dist/binary/final-package/target/apache-storm-2.0.0-SNAPSHOT/storm-local/nimbus/inbox/stormjar-22979659-5176-46fd-9027-ffdde13f595a.jar 2018-06-05 16:16:20.624 o.a.s.d.n.Nimbus pool-21-thread-29 [INFO] Received topology submission for test-[123] (storm-2.0.0-SNAPSHOT JDK-1.8.0_162) with conf {topology.users=[null] topology.acker.executors=null storm.zookeeper.superACL=null topology.workers=3 topology.submitter.principal= topology.debug=true topology.name=test-[123] topology.kryo.register={} storm.id=test-[123]-7-1528195580 topology.kryo.decorators=[] topology.eventlogger.executors=0 topology.submitter.user=mvanam topology.max.task.parallelism=null} 2018-06-05 16:16:20.624 o.a.s.d.n.Nimbus pool-21-thread-29 [INFO] uploadedJar /manu/Git/storm/storm-dist/binary/final-package/target/apache-storm-2.0.0-SNAPSHOT/storm-local/nimbus/inbox/stormjar-22979659-5176-46fd-9027-ffdde13f595a.jar 2018-06-05 16:16:20.624 o.a.s.b.BlobStore pool-21-thread-29 [ERROR] 'test-[123]-7-1528195580-stormjar.jar' does not appear to be valid. It must match ^[\w \t\._-]+$. And it can't be ""."" "".."" null or empty string. 2018-06-05 16:16:20.625 o.a.s.b.BlobStore pool-21-thread-29 [ERROR] 'test-[123]-7-1528195580-stormconf.ser' does not appear to be valid. It must match ^[\w \t\._-]+$. And it can't be ""."" "".."" null or empty string. 2018-06-05 16:16:20.626 o.a.s.d.n.Nimbus pool-21-thread-29 [WARN] Topology submission exception. (topology name='test-[123]') java.lang.IllegalArgumentException: test-[123]-7-1528195580-stormconf.ser does not appear to be a valid blob key at org.apache.storm.blobstore.BlobStore.validateKey(BlobStore.java:66) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]{code}    ",4774
Inline Method,Add admin command to get a zookeeper shell At times ZK might get messed up or a user may just want to see what storm is doing with it and it would be nice to have a simple command line tool that pops us into ZK where we can start to look for issues.,4775
Rename Method,Refactoring methods in components for Supervisor and DRPC This is a supplement issue page to STORM-3099 separating out refactoring work from metrics addition. A few misc bug discovered during refactoring have been incorporate in this issue as well. See links for more information.,4776
Extract Method,Extend metrics on Nimbus and LogViewer Include but not limited to Logviewer 1. Clean-up time 2. Time to complete one clean up loop Time. 3. Disk usage by logs before cleanup and After cleanup loop. ( Just like GC.?) 4. Failures/exceptions. 5. Search request Cnt: By category - Archived/non-archived 6. Search Request - Response time 7. Search Request - 0 result Cnt 8. Search Result - open files 9. File partial read count 10. File Download request Cnt/ And Size served 11. Disk IO by logviewer 12. CPU usage ( for unzipping files) Nimbus Additional: * Topology stormjar.ser/stormconf.ser/stormser.ser file upload time. * Scheduler related metrics would be a long list generic and specific to different strategies. * Most if not all cluster summary can be pushed as Metrics. * Restart cnt * Nimbus loss of leadership !/jira/images/icons/emoticons/help_16.png|width=16height=16align=absmiddle! * UI not responding ([https://jira.ouroath.com/browse/YSTORM-4838]) * Negative resource scheduling events ([https://jira.ouroath.com/browse/YSTORM-4940]) * Excessive scheduling time !/jira/images/icons/emoticons/help_16.png|width=16height=16align=absmiddle!,4777
Extract Method,Use custom Callback in KafkaBolt Currently {{KafkaBolt}} completely encapsulates its {{KafkaProducer}} so there's no way to inject a custom Callback when sending a message to Kafka. This change will add a method that allows the injection of a Callback function that will be passed into {color:#333333}{{KafkaProducer.send()}}{color}. The reasoning behind this change is to expose any exceptions that occur during Kafka publishing so they can be logged and have metrics built around them. This would provide more customized ways we could build alerting around Kafka publishing failures.,4778
Extract Method,Improve security of credentials push When pushing credentials to a topology most of the checks we do right now are to verify that the topology is allowing a given user to do the push but we also need to protect the user from pushing to the wrong topology.   This is really only an issue if a user has the push setup on some kind of a cron like job and the topology is down (which should be rare) but to eliminate any race conditions we should have nimbus either verify that the topology is owned by the same user as the one doing the push or have an optional user that the client expects the topology to be owned by.,4779
Extract Method,secure Impersonation in storm Storm security adds features of authenticating with kerberos and than uses that principal and TGT as way to authorize user operations topology operation. Currently Storm UI user needs to be part of nimbus.admins to get details on user submitted topologies. Ideally storm ui needs to take authenticated user principal to submit requests to nimbus which will than authorize the user rather than storm UI user. This feature will also benefit superusers to impersonate other users to submit topologies in a secured way.,4782
Extract Method,lack of static helper registerMetricsConsumer for backtype.storm.Config In backtype.storm.Config there is the concept in design to use regular maps while providing only helpers to operate on that map.Most of the methods are using this concept by providing static versions.For example {{registerSerialization}} method.{code}public static void registerSerialization(Map conf Class klass) {getRegisteredSerializations(conf).add(klass.getName());}public void registerSerialization(Class klass) {registerSerialization(this klass);}{code}However recently added metrics interface doesn't follow this approach.Unfortunately this breaks the concept of using HashMap as configuration container by user. And it is still only HashMap inside storm.Also not having static version of {{registerMetricsConsumer}} while introducing metrics feature breaks the configuration code in clojure that previosly was just a map.Without static helper this code should be instance of {{backtype.storm.Config}} class and it is not possible to keep configuration as simple as in pseudocode example below in submitter call.{code}(let [topology-config (doto { TOPOLOGY-DEBUG falseTOPOLOGY-STATS-SAMPLE-RATE 0.01TOPOLOGY-RECEIVER-BUFFER-SIZE 32TOPOLOGY-TRANSFER-BUFFER-SIZE 4096TOPOLOGY-EXECUTOR-RECEIVE-BUFFER-SIZE 2048 };; using static helper to add serialization class to config.storm.backtype.Config/registerSerialization EXMPLSerializer serializer)](StormSubmitter/submitTopologytopology-nametopology-config(mk-topology))){code}Proposed solution to create a static version of `registerMetricsConsumer` they way it is done by all other {{backtype.storm.Config}} helpers.,4783
Rename Method,add storm-jdbc to list of external connectors. There have been several questions in the apache mailing list around how to use storm to write tuples to a relational database. Storm should add a jdbc connector to its list of external connectors that has a general implementation to insert data into relational dbs as part of a topology.,4784
Rename Method,Storm Trident support for sliding/tumbling windows 0,4786
Extract Method,SimpleACLAuthorizer should provide a way to restrict who can submit topologies SimpleACLAuthorizer currently allows anyone with a valid kerberos ticket to submit topologies. There are cases where storm admins want to allow only selected users to submit topologies. I am proposing nimbus.users config option if its added to storm.yaml only the listed users can deploy the storm topologies. cc [~revans2],4787
Extract Method,"storm-jdbc should support customer insert queries. Currently storm-jdbc insert bolt/state only supports to specify a table name and constructs a query of the form ""insert into tablename values(???)"" based on table's schema. This fails to support use cases like ""insert into as select * from"" or special cases like Phoenix that has a jdbc driver but only supports ""upsert into"". We should add a way so the users can specify their own custom query for the insert bolt. This was already pointed out by [~revans2] during the PR review and we now have concrete cases that will be benefited by this feature.",4788
Extract Method,"[multilang] Introduce overflow control mechanism It's from STORM-738 https://issues.apache.org/jira/browse/STORM-738?focusedCommentId=14394106&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14394106A. ShellBolt side controlWe can modify ShellBolt to have sent tuple ids list and stop sending tuples when list exceeds configured max value. In order to achieve this subprocess should notify ""tuple id is complete"" to ShellBolt.* It introduces new commands for multi-lang ""proceed"" (or better name)* ShellBolt stores in-progress-of-processing tuples list.* Its overhead could be big subprocess should always notify to ShellBolt when any tuples are processed.B. subprocess side controlWe can modify subprocess to check pending queue after reading tuple.If it exceeds configured max value subprocess can request ""delay"" to ShellBolt for slowing down.When ShellBolt receives ""delay"" BoltWriterRunnable should stop polling pending queue and continue polling later.How long ShellBolt wait for resending? Its unit would be ""delay time"" or ""tuple count"". I don't know which is better yet.* It introduces new commands for multi-lang ""delay"" (or better name)* I don't think it would be introduced soon but subprocess can request delay based on own statistics. (ex. pending tuple count * average tuple processed time for time unit average pending tuple count for count unit)** We can leave when and how much to request ""delay"" to user. User can make his/her own algorithm to control flooding.In my opinion B seems to more natural cause current issue is by subprocess side so it would be better to let subprocess overcome it.",4789
Extract Method,LocalState should not use java serialization If we want to do rolling upgrades we need to move away from java serialization as it does not provide a good rolling upgrade path. For zookeeper we have been using thrift and I think we want to continue down that path here too.,4790
Rename Method,Storm ElasticSearch connector It would be nice to provide storm driver for elasticsearch just like it does for hive redis and so on.,4791
Extract Method,HDFS Bolt can end up in an unrecoverable state The body of the HDFSBolt.execute() method is essentially one try-catch block. The catch block reports the error and fails the current tuple. In some cases the bolt's FSDataOutputStream object (named 'out') is in an unrecoverable state and no subsequent calls to execute() can succeed.To produce this scenario:- process some tuples through HDFS bolt- put the underlying HDFS system into safemode- process some more tuples and receive a correct ClosedChannelException- take the underlying HDFS system out of safemode- subsequent tuples continue to fail with the same exceptionThe three fundamental operations that execute takes (writing sync'ing rotating) need to be isolated so that errors from each are specifically handled.,4792
Extract Method,List#indexOf(groovy.lang.Closure closure) and friends to groovy jdk find(groovy.lang.Closure closure) and findAll(groovy.lang.Closure closure) methods are really useful. It would be also good to haveList#indexOf(groovy.lang.Closure closure)List#indexOf(int startIndex groovy.lang.Closure closure) // startIndex tells from which index to start lookingint[] List#indicesOfAll(groovy.lang.Closure closure) // return type may also be a List<Integer> or whateverList#lastIndexOf(groovy.lang.Closure closure)in the groovy jdk as it is sometimes important to know where the elements fullfilling the given condition are in the list.,4793
Move Method,Change the signature of GroovyResourceLoader.loadGroovyFile() to support URLs GroovyResourceLoader.loadGroovyFile() should return URLs instead.This is much more friendly to those not using the filesystem but also using JARs possibly even remote JARs.This method will be renamed and refactored to return a URL:GroovyResourceLoader.loadGroovySource(),4794
Rename Method,provide a feature to declare TestCases as 'notYetImplemented' and thus expected to fail provide notYetImplemented() as in HtmlUnit and use it right away for the Groovy build itself,4795
Extract Method,Patch to provide acess to Script Object in GroovyScriptEngine Patch to provide acess to Script Object in GroovyScriptEngine. This is to allow dependency injection (and multiple return values) from scripts through acess to setProperty getProperty.,4799
Extract Method,Additional helper methods for DOMCategory to make it feel more like XmlParser/Slurper The attached patch and testcase extends DOMCategory to also support:node.text()node.name()node.parent()nodelist.iterator()While not strictly necessary these extensions make DOM processing closer to XmlSlurper and XmlParser usage.There are some other discrepancies between DOMCategory and XmlSlurper and XmlParser usage but I will make this the subject of a subsequent issue.,4801
Rename Method,Implement groupBy() for Maps as well as for Lists The method groupBy() works only for lists. It should work for Maps as well.The code for this functionality and the respective test is commented out in DefaultGroovyMethods and GroovyMethodsTest.public static Map groupBy(Map self Closure closure) {final Map answer = new HashMap();for (final Iterator iter = self.entrySet().iterator(); iter.hasNext();) {groupCurrentElement(closure answer iter);}return answer;},4802
Extract Method,Allow the addition of MetaBeanProperty instances to MetaClassImpl we need to be able to add MetaBeanProperty instances to MetaClassImpl allow dynamic properties to be added. (Patch attached),4803
Extract Method,Improve docs for Collection.groupBy and add new Collection.collate(Closure) [Small breaking change] The documentation for Collection.groupBy is a little ambiguous in that it does not make it absolutely clear that the return value is a Map of keys pointing to values that are ArrayLists.A new method Collection.collate(Closure) would be a nice addition to allow collation of a collection of objects into a map keyed on some value provided by a closure -without- returning a List for every value. This might be implemented so:{code}java.util.Map collate(Closure collator) {def result = [:]this.each() {result[collator.call(it)] = it}}{code},4804
Rename Method,SQL pipeline improvement groovy.sql.Sql is very usefull for SQL pipeline as {code}/* Fetch Person */sql.eachRow('select * from person') { person -> /* Fetch unit */...process person data ...if ( person.status == 'active' ) {sql.eachRow('select * from unit where unit_id = ? and seq = ?' [ person.unit  ] ) { unit ->... process unit data .../* For each Job fetchs job description */[ person.job0 person.job1 person.job2].each { jobDes -> sql.eachRow('select * from job where job_id = ?  [ jobDes ] ) {... process job description ...}} else {....}}{code}But in groovy.sql.Sql#eachRow(String List Closure) preparedStatement is *ever evaluated* at each _iteration_. It would be very easy to add a PreparedStatement Cache feature to improve speed and avoid useless evaluation.Maybe could we create a new top wrapping statement as 'persistent' for example :{code}sql.persistent() {sql.eachRow('select * from person') { person -> ....}}{code}as {code}public void persistent(Closure clos) {setPersistent(true)clos.call();setPersistent(false);closeResources();}{code}Therefore we could have {code}public void eachRow(String sql List params Closure closure) throws SQLException {Connection connection = createConnection();PreparedStatement statement = null;ResultSet results = null;try {log.fine(sql);if ( isPersistent() ) {statement = (PreparedStatement) cache.get(sql);if ( statement == null ) {statement = connection.prepareStatement(sql);cache.put(sql statement);}} else {statement = connection.prepareStatement(sql);}...{code},4805
Move Method,provide security sandbox for executing scripts It would be great to have a secure sandbox in which scripts could be evaluated; restricting both the kinds of scripts available and the packages that can be used,4806
Extract Method,printf should work with any PrintStream and support sprintf printf currently sends to System.outIt should work for System.err or PrintStreams to files etc.It should also be available via strings (e.g. as per sprintf),4807
Extract Method,GDK: Add eachDirMatch and eachDirRecurse on File on File the GDK has following methodsvoid eachDir(closure)void eachFile(closure)void eachFileMatch(filter closure)void eachFileRecurse(closure)for coherence it would be good to have:void eachDirMatch(filter closure)void eachDirRecurse(closure) See discussion in dev mailing list:http://www.nabble.com/Adding-eachDirMatch-and-eachDirRecurse--tf3473673.html#a9694265,4808
Extract Method,It would be useful to have a DGM Socket.withObjectStreams to complement Socket.withStreams Wii currently have:socket.withStreams { input output -> ... }I have a similar suggestion to mirror this for Object variants:socket.withObjectStreams { ois oos -> ... }This would make setting up proxy type objects over sockets very streamlined.,4809
Extract Method,"I need to get the metaData even when I get no results/I'd like an easier way to get the metaData from the Sql class. I was not able to find a way to get sql metadata even when there are no rows returnedwith the current Groovy sql stuff so I subclassed groovy.sql.Sql and added a newversion of eachRow and rows such that each takes an additional ""metaClosure"" sothat I can feed the metaClosure the result set's meta data. that way even if I don't have any rows returned I can be sure to have the meta data. And I don't have to delve into the row processing closure to get the meta data from the first row or anything like that.I made my own subclass and added the following methods:public List rows( final String sqlfinal Closure metaClosure )public void eachRow( final String sqlfinal Closure metaClosurefinal Closure rowClosure )So now I can:def metaDatadef results = sql.rows( query ) { metaData = it }I didn't make a groovier version of the metadata class the way there's a GroovyRowResult and GroovyResultSet class. I was very impressed with how easy it was to do what I needed to do.I have attached the methods I added but they are not in the approved groovy format.",4810
Rename Method,force SwingBulder to execute builder methods in EDT to avoid problems with GUIs and to elt the user have a simple way of creating GUIs wihtout thinking about the EDT SwingBuilder should invoke all builder methods (frame dialog ...) in the EDT.,4811
Extract Method,"Dynamically added static getters can't be called as properties String.metaClass.""static"".getSomething << { -> ""something"" }assertEquals(""something"" String.something)This fails with ""No such property: something for class: java.lang.Class""",4812
Extract Method,Allow customisation of closure variable resolving strategy It just needs something likedef c = {}c.resolveStrategy = Closure.DELEGATE_FIRSTorc.resolveStrategy = Closure.DELEGATE_ONLYorc.resolveStrategy = Closure.OWNER_FIRSTc.delegate = fooc.call(),4813
Extract Method,"SimpleTemplateEngine (and poentially other TemplateEngines) should allow caller to specify classloader SimpleTemplateEngine doesn't allow the caller to specify a parent classloader. When it creates a GroovyShell it just uses the loader for that class. {code}Index: src/main/groovy/text/SimpleTemplateEngine.java===================================================================--- src/main/groovy/text/SimpleTemplateEngine.java (revision 6601)+++ src/main/groovy/text/SimpleTemplateEngine.java (working copy)@@ -698 +6914 @@}public Template createTemplate(Reader reader) throws CompilationFailedException IOException {+ return createTemplate(GroovyShell.class.getClassLoader() reader);+ }++ public Template createTemplate(ClassLoader parentLoader Reader reader)+ throws CompilationFailedException IOException+ {SimpleTemplate template = new SimpleTemplate();- GroovyShell shell = new GroovyShell();+ GroovyShell shell = new GroovyShell(parentLoader);String script = template.parse(reader);if (verbose) {System.out.println(""\n-- script source --"");{code}Other template engines should have consistent features with this enhancement.",4814
Extract Method,"Add support for ""method missing"" behaviour to Groovy 0",4815
Extract Method,Add JLine support to groovysh Hiya I'd like to add JLine support to groovysh. JLine ( http://jline.sourceforge.net/ ) for those that don't know is a really cool little library similar to editline/readline which allows Java CLI apps to have a very rich buffer editing/history/completion that most modern interactive CLI tools generally have.Its a tiny library (~60k) which is highly portable working well on most modern Windows and POSIX compatible UNIX systems. For POSIX systems it leverages the stty command to initialize the terminal in unbuffered mode and on Windows uses a tiny .dll to get the same affect. It works very very well and so far I've not heard of anyone having problems using it. It also has a fallback mode which if on the off case that neither stty or the .dll bits work that it will simply behave like normal Java console input/output... though I've yet to actually see a case where that fallback mode was kicked in.JLine is also BSD licensed so no worries about tainting the distribution with GPL muck... ;-)The change is relatively small and IMO the benefit is large.,4817
Extract Method,Change sleep() variant with closure to enable continued sleep The following is possible with the new sleep variant:,4818
Extract Method,The iteration method inject() should be provided for Object and Map as well as for Object[] and Collection Most iteration methods are implemented for Object and Map as well as for Collection and Object[].,4819
Extract Method,increase Closure performance by using a specialized Closure MetaClass Closures generated by the Groovy compiler are of special nature things that the runtime could use to optimize call performance. Instead the runtime uses the standard MetaClass which contains special logic for Closures in general but not special to generated Closures.,4821
Inline Method,MetaClass.getMetaMethods() only returns methods from DGM should return all meta methods 0,4822
Extract Method,"change the groovyc ant task to enable embedding the javac ant task instead of using the jointCompilation property and providing the options for the compiler in a command line like manner it would be nice to be able to reuse the ant javac task to run the joint compilation process.Example {code:xml}<groovyc srcdir=""${testSourceDirectory}"" destdir=""${testClassesDirectory}""><javac source=""1.4"" target=""1.4""/></groovyc>{code}",4823
Extract Method,AllTestSuite should supports excludes pattern as well as current includes pattern This would allow tests not to be included in a suite according to a supplied pattern,4824
Extract Method,"GPath: allow attribute reference using unquoted selectors in XmlParser Nodes making it consistent with XmlSlurper The suggested way to access XML attributes from the results produced by XmlParser is to use the form x.'@attrName' . Note the quotes around the attribute name.For XmlSlurper style of GPath you can leave off the quotes and write x.@attrNameThis doesn't appear to be supported in the XmlParse style. The Groovy in Action book has a table (Table 12.4) on page 410-411 which showsthese variations and restrictions.The form ""x.@name"" is also used in general in Groovy to specify direct field access see Groovy in Action on page 203 section titled""Field access with .@"".Somehow it appears that for XmlSlurper results you can use the "".@name"" form to access an attribute.It would be nice if the "".@name"" form (where you don't have to ""quote"" the @name) worked also for XmlParse result ""Nodes"".",4825
Extract Method,Expose GSE on GroovyServlet to subclasses so they may add extra configuration Exposing the GSE in a protected method will help tweaking its CompilerConfiguration by subclasses of GroovyServlet OR by an external strategy like GroovyServletConfigurator that can be set as an init-param (no subclass needed in this case).,4826
Extract Method,allow script base class to be specified on a GroovyShell or GroovyClassLoader e.g. to allow a bunch of functions to be added to the current script class.InteractiveShellScript could add some shell-related stuff.GroovletScript could add some servlet/groovlet related stuffDavid's EJB container telnet client that uses groovy could add some EJB-container helper methods.the GAP dynaop stuff could have a DynaopScript to add helper methods etc.we should have some kind of CompilerConfig object or something which we can set on a GroovyShell or a GroovyClassLoader to allow this kinda thing to be overloaded,4827
Extract Method,"Process .getText() GDK method blocks on some operating systems. Pulled from the USER email group on 11/14/2007:> i am executing maven reports inside a certain folder with> {code}new File('.').eachDir{//ignore config-dirsif(it.name.startsWith("".""))returnprintln ""Executing site:deploy task für $it/pom.xml""def processOutput=""mvn.bat -f $it/pom.xml site-deploy"".execute().textprint processOutput}{code}> > in some cases somehow the process hangs i don't know why because the processOutput only gets displayed after > the process exited. how is it possible to print out continously the stdout/stderr of the process to the console > with ""anyCommand"".execute()?And this is from the JavaDoc for java.lang.Process:""Because some native platforms only provide limited buffer size for standard input and output streams failure to promptly write the input stream or read the output stream of the subprocess may cause the subprocess to block and even deadlock.""Two simple solutions come to mind:(1) .getText() actively reads and buffers stderr and stdout in two separate threads.(2) Subclass java.lang.Process so that stderr and stdout are streams are overridden and read into buffered streams using separate threads.And for extra credit there are issues with synchronizing output between stderr and stdout. If you are reading both in separate threads and printing to console or a log file the outputs can come in the wrong order depending on when the threads were executed and in what order. Maybe there is a better way to solve this problem.Perhaps there is an easy way to pipe both into a StringBuffer??? I am calling this a ""critical"" bug because being able to run external processes simply and elegantly is pretty central to any scripting-language-type uses and the current implementation is clearly broken (but in a subtle way).",4828
Extract Method,Extending XmlNodePrinter to support namespaces A modified XmlNodePrinter (file and patch) as well as a unit test case are attached.One potential change to what I've uploaded: make namespace awareness optional (like in XmlParser). If I should add that then I would recommend adding namespace prefixes only when namespace awareness is on. (In 1.5 namespace prefixes are printed without the xmlns: attribute -- which leads to the printing of xml that cannot be parsed by SAX.),4830
Rename Method,rename map methods to collect to be more consistent map() could be reserved for a method which creates a Map. Using 'collect' means we're consistent with Ruby & Smalltalk,4831
Extract Method,Allow subclasses of XmlParser to customize creation of new Nodes This patch adds a new createNode() method to XmlParser to allow subclasses to customize the creation of new Nodes. This allows subclasses to store additional information in the node tree in a way that does not impact the memory footprint of the base implementation.,4832
Rename Method,Allow JUnit 4 tests to be run by the groovy command Groovy allows JUnit 3.8.x tests and GroovyTestCases to be invoked from the command line. It would be good to also support JUnit 4 tests in this way. The attached patch does this.Pros and Cons:(-) It 'simply' saves the '{{JUnitCore.main(MyTest.name)}}' added to the bottom of a script which manually invokes the runner. So we are adding code to the Groovy codebase for only a small saving(+) It does allow the script to be left as a test class that IDEs would run using their runners and doesn't require them to be polluted with the {{JUnitCore}} manual runner code which makes no sense to a IDE runner(+) Given that Groovy 1.5 is offering Java 1.5 support it does round out nicely JUnit 4 support which is often used in the jump to Java 1.5 and hence improves the case for using Groovy as your testing language for Java 1.5 projects (e.g. making TDD and BDD easier out of the box)(-) Are we showing favouritism vs other alternatives e.g. TestNG(+) We already have a testng runner but no direct JUnit runnerI am happy to apply it but wanted to attach it here so that discussion can occur around whether it should be a 1.5.2 or 1.6 feature (or not included).Paul.,4833
Extract Method,sum min and friends should work on the same types that collect each and their friends work on. [123].sum() works nicely.But [123].iterator().sum() fails.In contrast this code[123].iterator().each {println it}works just fine.On aesthetic grounds there shouldn't be any difference in what works and what doesn't. More importantly there are situations where an iterator is available (due to pre-existing API's) but a collection is not.,4834
Extract Method,"Support the multiple namespaces with NamespaceBuilderSupport The major limitation when using the BuilderSupport is it doesn't support the multiple namespaces which is very common.E.g. I want to create the XML message to invoke Google calendar{code:xml}<entry xmlns='http://www.w3.org/2005/Atom' xmlns:gCal='http://schemas.google.com/gCal/2005'><content type=""html"">Tennis with John April 11 3pm-3:30pm</content><gCal:quickadd value=""true""/></entry>{code}I cannot build the message with current NamespaceBuilderSupport for there is only one namespace could be specified.So I create one NamespaceBuilderSupport then I can create the message in Groovy.{code}def builder = new NamespaceBuilderSupport (DOMBuilder.newInstance());builder.namespace('http://www.w3.org/2005/Atom');builder.namespace('http://schemas.google.com/gCal/2005' 'gCal')def data = builder.entry {""gCal:quickadd""(value:""true"")title(""Tennis with John"")content(type:'html' ""Tennis with wen Bing in PKU on February 5 3pm-3:30pm"")}{code}And the reference impl is as following.{code}public class NamespaceBuilderSupport extends BuilderSupport{private Map<String String> nsMap = new HashMap<String String>();public NamespaceBuilderSupport(BuilderSupport builder) {super(builder);}protected NamespaceBuilderSupport namespace(String namespaceURI) {nsMap.put("""" namespaceURI);return this;}protected NamespaceBuilderSupport namespace(String namespaceURI String prefix) {nsMap.put(prefix namespaceURI);return this;}protected Object getName(String methodName) {String prefix = """";String localPart = methodName;int idx = methodName.indexOf(':');if (idx > 0 ) {prefix = methodName.substring(0 idx);localPart = methodName.substring(idx + 1);}String namespaceURI = nsMap.get(prefix);if (namespaceURI == null) {namespaceURI = """";prefix = """";}return new QName(namespaceURI localPart prefix);}@Overrideprotected Object createNode(Object name) {return name;}@Overrideprotected Object createNode(Object name Object obj1) {return name;}@Overrideprotected Object createNode(Object name Map map) {return name;}@Overrideprotected Object createNode(Object name Map map Object obj1) {return name;}@Overrideprotected void setParent(Object obj Object obj1) { }}{code}Thanks in advance. I hope it could be helpful.",4835
Extract Method,Support Half-Mocks Half-Mocks are mocks for classes with dynamic methods or properties. One example are Grails domain objects where you would use the implemented domain specific methods but want to mock the GORM methods.,4836
Extract Method,Plus on Collection tries to keep similar type of original collection but not for LinkedList return LinkedList with DGM.plus for Collection if original is a linkedlist,4837
Extract Method,Intersect on Collection should try to keep similar type of original collection (like plus) Intersect on a Set should return a Set etc. as per plus,4838
Extract Method,"Preserve line/column information for user-defined occurrences of variables thissuper and literals nulltruefalse Currently user-defined occurrences of variables this super and literals null true false are represented as singletons in the AST (VariableExpression.THIS_EXPRESSION ConstantExpression.NULL etc.). This means that their line/column information is lost causing troubles for tools relying on that information (IDEs code analyzers transformations etc.). It isn't even easy to detect that line/column information is missing for these expressions as they will return ""arbitrary"" values.Attached is a patch that changes AntlrParserPlugin so that it creates a new instance of VariableExpression/ConstantExpression for every user-defined occurrence of the variables/literals mentioned above thus preserving line/column information. The singletons can still be used for internal purposes (like representing an implicit this in a method call); however in order to test for the presence of one of the expressions it is now necessary to call one of the new methods isXXXExpression (e.g. myVar.isThisExpression()) instead of comparing against the singleton (e.g. myVar == VariableExpression.THIS_EXPRESSION). This keeps changes to the code base to a minimum while avoiding the problem of defining equality of ASTNode's by way of overriding equals(). All affected code in groovy-core has been adapted accordingly. Groovy-core builds successfully (ant install) after applying the patch on my local machine. Initial manual tests indicate that the patch works as intended. Of course an automated test would be preferable - is there already an easy way to get to an expression's AST node from a unit test? If not I might write a transformation that allows to capture a declaration's AST node in a certain compiler phase.Please have a look at the patch and give me some feedback. Thanks!",4839
Extract Method,sql.rows() and sql.eachRow() not consistent in behaviour? Groovy allows this:sql.rows(someQuery).each { Map row -> def item = newItemThatTakesAMap(row) }But not this:sql.eachRow(someQuery) { Map row -> def item = newItemThatTakesAMap(row) } Shoudn't sql.eachRow also implement the Map interface?Best regards.,4840
Extract Method,"Global transforms should only be executed once in IntelliJ IDEA In IntelliJ IDEA when I build a Groovy project that has a dependency containing a global transform for some reason the transform is executed twice. When I compile with groovyc or run FileSystemCompiler in IDEA the transform is only executed once (as expected). I've studied the transform implementation (specifically ASTTransformationVisitor) but haven't found the cause of the problem. All I know is that the transform is invoked twice on the same ModuleNode instance. SourceUnit reports phase ""conversion"" instead of ""semantic analysis"" but I guess that's because SourceUnit.phase is no longer incremented after phase conversion. To reproduce the problem I've attached a jar containing a transform that will create a file transformlog.txt in your user home and append ""transform invoked\n"" every time the transform is invoked. (What's the preferred way to produce debug/info output in a transform and will this output also be shown in IDEA?) In IDEA add the jar as a dependency to any Groovy project and rebuild. To build with groovyc type ""groovyc -cp ./transformtest.jar:<your_path>/groovy-all-1.6-beta-2-SNAPSHOT.jar <any_groovy_file>"". Don't forget to delete transformlog.txt between compiler invocations.",4841
Extract Method,GenericTypes in AST for local variable Generic for local variable weren't represented in the AST. Now they are.The ClassNode which is representing the type is now built using:{code}type = makeType(node);instead oftype = makeTypeWithArguments(node);{code}in APP; method declarationExpression.For GenericType Node the CST types TYPE_PARAMETER_ and TYPE_ARGUMENT are used to set the line/col infos .,4842
Extract Method,"[ [a:1 b:2 c:3] ].flatten() -> [ 1 2 3 ] but should be [ [a:1 b:2 c:3] ] Updated list#flatten/set#flatten so that maps are unadulterated.Going with this approach means that maps can safely be used in lists. This is commonly done as a kind of ""pseudobean"" for unit testing -- it's how I first encountered this behavior.",4843
Extract Method,It would be good to be able to name threads when using the DGSM start variants As well as Thread.start { ... } it would be good to have a variation that allow thread naming. This would make it easier to perform subsequent thread management tasks.,4844
Rename Method,Groovy should provide an @Immutable annotation to make creating immutable objects easy 0,4845
Extract Method,"Groovy Sql API should support stored procedures that have output parameters and return a result set If you have a stored procedure that has output parameters AND returns one or more result sets you can't get all of the output using the Groovy SQL API. You can get just the output parameters by doing something like:{code}sql.call(""{ call someProc(??) }""[Sql.out(Sql.INTEGER.type)Sql.out(Sql.VARCHAR.type)]) { param1 param2 ->println ""returned ${param1} ${param2}""}{code}or you can get a single resultset but not the output parameters by doing something like this:{code}sql.query(""{ call someProc(??) }""[null null]) { resultSet ->println ""returned ${resultSet}""}{code}but you can't do both and if the stored proc returns multiple result sets you are out of luck as well. The Sql.call method should probably use the CallableStatement getResultSet and getMoreResult methods to get all of the result sets and and pass them as parameters to the closure.",4847
Extract Method,Include transaction closure in standard api When doing a set of write operations sometimes a statement-set should follow transaction semantics. Thus a closure like withTransaction{} inside *groovy.sql.Sql* class would be very handy.For discussion see also http://www.nabble.com/groovy.sql.Sql-and-doing-SQL-statements-in-one-transaction-td2594039.html#a20562605,4848
Extract Method,Open up AntrlParserPlugin for extension While an error recovery parser is a good idea it is currently not supported by the groovy grammar. The Groovy Eclipse team has come up with a solution of patching the grammar and adding their own AntrlParserPlugin implementation that is a subclass of the original AntrlParserPlugin. But the current implementation of AntrlParserPlugin makes coding subclasses rather cumbersome specifically because the ast field is private and that the error recovery code must be injected in the middle of the execution of parseCST()This feature proposes opening up AntrlParserPlugin for extension by initially modifying the accessibility of the ast field (or provide protected mutator) by subclasses and refactoring parseCST() to a template method.I'll provide a patch shortly and would love to have it merged before 1.6 ships as this patch would not only help the Groovy Eclipse plugin but also other specialized tools (like SwingPad) that can reapt the benefits of an error recovery parser.,4850
Extract Method,MarkupBuilder performance MarkupBuilder#escapeXmlValue uses an algorithm for XML escaping that has unneccessary space and time requirements.The attached patch (for trunk) while having less memory consumption seems to increase the performance to factors between 5 and 25.,4851
Extract Method,Improved class loading for AST transformations Currently groovyc loads AST transformations with the same class loader that is also responsible for loading compile dependencies. This can lead to problems for compiler clients that provide a custom class loader for loading compile dependencies as such a class loader might not be able to meet the requirements for loading AST transformations. The most prominent example is GMaven which uses a custom class loader to prevent classpath pollution without having to fork groovyc. Unfortunately this also means that GMaven cannot compile any programs that make use of AST transformations.This patch allows for compiler clients to optionally provide a separate class loader for loading AST transformations thereby decoupling the class loading of AST transformations from that of compile dependencies. This makes it possible for compiler clients such as GMaven (and possibly Gradle) to support AST transformations and paves the way for further class loading improvements in future versions of groovyc.The patch is fairly small and tries to keep changes to the existing codebase to an absolute minimum. Apart from applying moderate changes to two transform-related classes it only adds a few lines of code to class CompilationUnit. Existing compiler clients will see no change in behavior. Please consider this patch for inclusion in Groovy 1.6 or a 1.6 maintenance release. Also attached is the corresponding GMaven patch. Any feedback is welcome.,4853
Extract Method,"Support multiple labels on the same statement Unlike Java Groovy doesn't support multiple labels on the same statement - instead the AST builder discards all but the first (\!) label: class MultipleLabels { static void main(args) { label1: label2: while (true) { break label1 // OK break label2 // Compile error: ""break to missing label"" } } }For a discussion see: http://www.nabble.com/Multiple-labels-on-the-same-statement-td21543898.html",4854
Extract Method,"ObjectGraphBuilder should support multiple associations for the same class It doesn't seem possible to do something like this:class Company {String nameEmployee mdEmployee cio}class Employee {String name} def builder = new ObjectGraphBuilder()def company = builder.company( name: 'ACME' ) {md( name: 'Billy Bob')cio( name: 'Joey Johns')}} Where the same class is associated mutliple times by different fields.Why not use the field name as the builder nodes and have ObjectGraphBuilder determine the class using reflection?I have attached a patch that does this. Unfortunately I had trouble building groovy-core so I wasn't able to run the ObjectGraphBuilder or add extra tests.Anyway I just wanted to see if anyone thought this was a good idea then maybe I can finish it off.This patch slightly changes the meaning of the classNameResolver - it would only apply to ""root"" classes. Maybe it should be renamed and the docs updated.",4855
Rename Method,Add stripMargin() to multi-line strings As explained in the following thread http://www.nabble.com/GString.stripMargin---td21679146.html Scals provides a stripMargin() method to its multi-line strings which is very helpful in overall situations. For something fancier not covered by the default behavior the user will have to roll its own impl.Link to Scala api docshttp://www.scala-lang.org/docu/files/api/scala/runtime/RichString.html#stripMargin,4856
Extract Method,"Patch to enhance String class with a find method that takes regular expressions Groovy makes working with regular expressions much easier than Java but there are still a couple of holes in the 1.6 implementation.I've attached a patch that adds a couple of simple methods that make regular expressions much easier to use.One of the most common use cases is to search a string for a regular expression pattern. If a match is found then do something with the matched value.Currently in groovy the recommended way to do this is to create a matcher and then use indexes to work with any matches that might be found:assert ""10292"" == (""New York NY 10292"" =~ /\d{5}/)[0]If you try to do that on a string that doesn't actually match the regular expression you'll get an IndexOutOfBoundException. To be safe you need to check matcher.find() (not matches as that requires the entire string to match!) to see if the string is actually in there:def m = (""New York NY"" =~ /\d{5}/)[0]def zip if (m.find()) {zip = m[0]}It also has inconsistent behavior if the regular expression happens to have capture groups in it. Then it returns an array containing the match and the capture groups forcing you to index into that array to actually get the match you want:assert ""c"" == (""foo car baz"" =~ /(.)ar/)[0][1]Groovy has already added closure aware replace method to the String class. The patch adds a complimentary find method to string that will return the string matched by the closure without needing to worry about matcher objects and array indexes. You can either call it without a closure to get the full found match back (even if it has groups in it):assert ""10292"" == ""New York NY 10292"".find(/\d{5}/)It safely returns a null if the match isn't found which can clean up boilerplate safety checks quite a bit. The user can check for null using groovy truth if they want to:def zip = ""New York NY"".find(/\d{5}/) // returns nullif (zip) { ... }If you want to work with capture groups or manipulate the value you can pass a closure to the find method that will be passed the full match as well as any capture groups (just as the collection based regular expression methods work):// no capture groups only the match is passed to the closureassert ""bar"" == ""foo bar baz"".find(/.ar/) { match -> return match }// one capture groupassert ""b"" == ""foo bar baz"".find(/(.)ar/) { match firstLetter -> return firstLetter }// many capture groups all passed to the closure after the full matchassert ""2339999"" == ""adsf 233-9999 adsf"".find(/(\d{3})?-?(\d{3})-(\d{4})/) { match areaCode exchange stationNumber -> assert ""233-9999"" == matchassert null == areaCodeassert ""233"" == exchangeassert ""9999"" == stationNumber return ""$exchange$stationNumber""}The patch also includes a number of unit tests to exercise and demonstrate the functionality.I get the most traffic on my blog to a post that I did explaning regular expressions in groovy and I think that people struggle a bit with the current implementation. I think this patch makes working with regular expressions much easier and more intuitive. ",4857
Extract Method,DataSet classes could support batch operations Perhaps as described here:http://markmail.org/thread/ciai2znkexfnrwot,4859
Extract Method,It would be good if @Delegate didn't delegate @Deprecated methods It is very rare when you would want @Deprecated methods be the subject of delegation. So it would be nice if they weren't carried over by default with a switch to force them over in those rare cases.,4860
Move Method,Enhance Swing classes and models with additional methods Some swing models do not follow a naming standard for common operations like get(int) set(intelement) clear() nor they provide an iterator() method (for those that make sense). This is a request to enhance Swing classes and models with additional methods that bridge the gap with useful collection/groovy methods.This is a port of http://jira.codehaus.org/browse/griffon-44,4861
Extract Method,"Allow subclasses of groovy.sql.Sql to get ResultSet In order for applications to perform optimally where multiple large databases are in use it can be important to iterate through a result set without using a closure. One use case is a merge of records from multiple databases: the company has several divisions and each has a an order database each day the orders for each SKU must be totalled across all divisions.The desired solution is to submit a query to each of the databases in parallel.String query = ""select SKU sum(Qty) from OrderItems group by SKU order by SKU""The results from the various DBs are then combined using a merge algorithm.This cannot be accomplished with the current API since it's impractical to load the results of the queries into List objects as is done by the rows(...) methods.The attached patch provides a path to implementing the desired solution by adding new protected methods. It also refactors to consolidate logic and remove an exposure to NPE. In addition to new protect methods a static method to create a List from a ResultSet is made public and the log field is made static.The patch includes a new test case added to SqlTest.groovy",4862
Extract Method,"grape resolve -ivy (... ... ...)+ could generate ivy dependencies Please add an option for grape to generate ivy dependencies from the command line just like -ant or -dos options.For example this command :grape resolve -ivy ""org.apache.poi"" ""poi"" ""3.5-beta6"" ""org.apache.poi"" ""poi-ooxml"" ""3.5-beta6""Could generate the following output :<dependency org=""org.apache.poi"" name=""poi"" revision=""3.5-beta6""/><dependency org=""org.apache.poi"" name=""poi-ooxml"" revision=""3.5-beta6""/>",4863
Extract Method,Enhance consistency of regex/pattern DGM methods Most regex/pattern based DGM methods try to allow both the regex as a String or as a Pattern.Here are the counter-examples:{code}StringString replaceFirst(Pattern pattern String replacement)boolean matches(Pattern pattern)Object splitEachLine(String regex) {match -> ... }String replaceAll(String regex) {match -> ... }String replaceAll(Pattern pattern String replacement)FileObject splitEachLine(String sep) {match -> ... }ReaderObject splitEachLine(String sep) {match -> ... }InputStreamObject splitEachLine(String sep String charset) {match -> ... } Object splitEachLine(String sep) {match -> ... } {code}A special case is 'minus' which is particularly bad as you _can_ provide a regex string but this will _not_ be treated as a regex since that would conflict with String.minus(String).,4864
Extract Method,Provide more control for @Grapes to exclude transitives or adjust classloader The current @Grab and @Grapes annotations provide a simplified way to bring in dependencies but there are some limitations:* you get all transitive dependencies* there is no way to exclude a particular unwanted transitive dependency* you sometimes don't get the right classloaderWe should consider enhancing support to overcome these limitations.,4865
Extract Method,"Round out eachRows() and rows() variants taking metaDataClosure. Currently Sql has these variants of eachRows() and rows():{code}eachRow(String sql Closure closure)eachRow(String sql Closure metaClosure Closure closure)eachRow(String sql List params Closure closure)eachRow(GString sql Closure closure)rows(String sql)rows(GString sql)rows(String sql Closure metaClosure)rows(String sql List params){code}This patch rounds out support for metaDataClosures so that they can be used in variants taking params and GStrings.I've needed this on most of these methods on one occasion or another and I don't see any reason why each variant should not be supported:{code}eachRow(String sql Closure closure)eachRow(String sql Closure metaClosure Closure closure)eachRow(String sql List params Closure closure)eachRow(String sql List params Closure metaClosure Closure closure)eachRow(GString sql Closure closure)eachRow(GString sql Closure metaClosure Closure closure)rows(String sql)rows(String sql Closure metaClosure)rows(String sql List params)rows(String sql Closure metaClosure List params)rows(GString sql)rows(GString sql Closure metaClosure){code}I have expanded the tests to cover all of these variants. I've renamed some of the test methods to name them according to the signatures of the methods they test. This is a more systematic naming convention which seems to be required with all of these variants.I noticed a couple of tests with misleading names. For example currently in trunk:{code}void testRowsGString() {def sql = createSql()def table = 'PERSON'GString query = ""select * from $table""def result = sql.rows(query.toString())assert result.size() == 3}{code}Either I am misunderstanding this test or else it's wrong. By calling toString() on the GString the test is not passing it as a GString and not exercising the GString variant of the rows() method. Perhaps this test has been modified from a previous form.In my patch I've corrected this for the rows() GString test but left alone another test method called testFirstRowWithGString(). If I'm right we should fix that too. (I cam come back to it with another patch if I'm right.)I created the patch using git diff but I tested it on svn trunk (on Linux) using GNU patch thus:{code}patch -p1 -i metaclosure.patch{code}It works for me. Please let me know if there's anything I can do to make patches in a more convenient format for you.I hope you find this agreeable!John Hurst",4866
Move Method,MarkupBuilder alignment of special mkp commands compared with StreamingMarkupBuilder StreamingMarkupBuilder supports:mkp.commentmkp.pimkp.xmlDeclarationThey can be emulated using just mkp.yieldUnescaped but MarkupBuilder should also support these convenience methods to ease learning about the two approaches.,4867
Extract Method,AST Transforms: @GroovyASTTransformationClass annotation should allow parameters of type Class[] not just String[] The annotation @GroovyASTTransformationClass marks another annotation as an AST Transformation. The annotation has a required parameter a String[]. The parameters stores the fully qualified class name that will be invoked during the transformation phases. As a convenience the parameter type should be Class[] instead of String[]. It allows programmers to specify a class not a String that represents a class. We cannot remove String[] because that would break backwards compatibility. We could add a 2nd parameter and then require 1 (and only one) of the two parameters to be specified. The was originally requested by Venkat Subramanium at 2gx 2009.,4868
Extract Method,Add Reader support to GroovyShell and GroovyCodeSOurce 0,4869
Extract Method,"ObjectGraphBuilder: add a general purpose bean factory SwingBuilder sports a {{bean()}} capable of wiring an existing bean instance into the build process. OGB can't do that.It will be great to have this feature as a developer will be able to tweak any bean instance using the builder syntax{code}Company acme = ... // fetch company from a webservice perhaps??ogb.bean(acme) {// add a few employees moreemployee(name: ""Duke"")employee(name: ""Tux"")}{code}",4870
Rename Method,add Map.synchronized() List.synchronized() methods We can do map.immutable() we should also be able to do map.synchronized() and list.synchronized().This is just a trivial helper method for map = CollectionUtils.synchronizedMap(map) and ditto for List - it just saves mucho typing & importing.,4871
Extract Method,"Implicit this reference for inner classes should be handled automatically Currently an inner class can only be instantiated in the following way: new MyInnerClass(this). If ""this"" is not passed explicitly a NPE in propertyMissing() occurs. This is highly irritating. groovyc should take care of handling ""this"" just like javac. I have classified this issue as an improvement because according to Jochen non-anoymous inner classes are not yet officially supported.",4872
Extract Method,Make the Groovy truth value of NaN be false Similar to null and empty-string have Groovy evaluate NaN as false rather than true.{noformat}def result = a?.b?.calcX(...)if (result) { // now I have a valid non-null number}{noformat}This is how JavaScript treats NaN and like other Groovy design choices it just makes sense.Discussed in this threadhttp://old.nabble.com/Shouldn't-the-Groovy-truth-value-of-NaN-be-false--tt27348256.htmlThat is if NaN was false the following should work:assert !(Double.NaN)assert !((Double.NaN as Boolean) == true)assert ((Double.NaN as Boolean) == false),4873
Extract Method,Groovy equality for Sets and Maps Currently Groovy equality extends to arrays and lists but not to sets and maps. Example:{code}assert [1] == [1L] // passesassert [1] as Set == [1L] as Set // fails{code}This is surprising and ungroovy. It would be nice to have Groovy equality for sets and maps too. The drawback is that it would make set/map equality inconsistent with lookup results (i.e. two sets are equal but Set.contains() yields different results). On the other hand the same problem already exists for Groovy list equality. For maps one could avoid the (lookup) inconsistency by only using Groovy equality for values. It would be good to have a discussion about this proposal.,4874
Extract Method,Groovy's Sql classes could support batch operations by leveraging the java.sql.Statement.executeBatch() command. Perhaps as described here:http://markmail.org/thread/ciai2znkexfnrwot,4875
Extract Method,Statically imported properties Describe a class{code}class Foo {static def getBar(){}static def setBar(def bar){}}{code}and a script{code}import static Foo.barprint barprint getBar(){code}Now 'print getBar()' throws MissingMethodException. IMHO 'getBar()' should be resolved to Foo.getBar() without explicit importThe same thing is with setters and aliased imports.{code}import static Foo.barsetBar(2){code}{code} import static Foo.bar as bazsetBaz(2)print getBaz(){code}{code}import static Foo.getBarprint bar{code}{code}import static Foo.setBarbar = 2{code},4877
Inline Method,"BigDecimal division is currently implemented using pre Java 1.5 code The implementation in BigDecimalMath for divide was done prior to Java 1.5. We can improve the implementation making use of newer facilities.Proposed implementation:{code}// ...// This is an arbitrary value picked as a reasonable choice for a precision// for typical user math when a non-terminating result would otherwise occur.public static final int DEFAULT_MINIMUM_PRECISION = 10;// ...public Number divideImpl(Number left Number right) {BigDecimal bigLeft = toBigDecimal(left);BigDecimal bigRight = toBigDecimal(right);try {return bigLeft.divide(bigRight);} catch (ArithmeticException e) {// set a DEFAULT precision if otherwise non-terminatingint precision = Math.max(Math.max(bigLeft.precision() bigRight.precision()) DEFAULT_MINIMUM_PRECISION);return bigLeft.divide(bigRight new MathContext(precision));}}// ...{code}Example:{code}println 100.0G / 3.0Gprintln 100000000.0G / 3.0Gprintln 0.000000000000000000000000000000001 / 3println 0.000000000000000000000000000000001 / 8println ((0.000000000000000000000000000000001 / 8).toPlainString())println 1000 / 10println 100000000000000000000000000000000000 / 3println 100000000000000000000000000000000000 / 8println 1E35 / 8println ((1E35 / 8).toPlainString())println 100000000000000000000000000000000000 / 0.000001println 100000000000000000000000000000000000.0000000 / 0.000001println ((100000000000000000000000000000000000.0000000 / 0.000001).stripTrailingZeros())try {// use divide direct to override Groovy behavior1.0G.divide(3G)} catch(ArithmeticException ae) {println ""Caught: $ae.message""}{code}Output from running example:{noformat}33.3333333333333333.333.333333333E-341.25E-340.00000000000000000000000000000000012510033333333333333333333333333333333333.3125000000000000000000000000000000001.25E+34125000000000000000000000000000000001.00000000000000000000000000000000000E+41100000000000000000000000000000000000000000.01E+41Caught: Non-terminating decimal expansion; no exact representable decimal result.{noformat}",4878
Extract Method,Open up ObservableList and ObservableMap for extension Both ObservableList and ObservableMap are closed for extension. It would be great if those classes expose some of its functionality to subclasses like their collection delegate and event firing methods.,4879
Extract Method,Collection and Map functions This improvement adds the following methods to Set:union - for completeness a synonym for plus (to match intersect)This improvement adds the following methods to Map:Set domain() - a synonym for keySetSet codomain() - returns a set of unique valuesSet range() - a synonym for codomainMap domainRestrict(Set s) - returns a map of those entries whose keys appear in set sMap rangeRestrict(Set s) - returns a map of those entries whose values appear in set sSet image(Set s) - returns a set of unique values of a map whose domain appears in set s. Note image is equivalent to map.domainRestrict(s).range()Map invert(Map m) - returns a map with the key value relationship inverted.,4884
Rename Method,Assert a closure call should look like As per the following conversation:http://groovy.markmail.org/search/?q=Power%20assert%20question%20when%20asserting%20closure%20calls#query:Power%20assert%20question%20when%20asserting%20closure%20calls+page:1+mid:bzbavvzceem3dd2z+state:resultsWe should make closure calls look like method calls.So for:{code}def closure = { it }assert closure(false){code}instead of:{code}assert closure(false)|ConsoleScript1$_run_closure1@575fa5{code}we should get:{code}assert closure(false)|false{code}If one really wans to see the toString() of the closure he could still do:{code}assert closure.call(false)| || falseConsoleScript2$_run_closure1@f1584a{code},4885
Extract Method,Add multiple file extensions support in compiler This is regarding the need Groovy++ has to support file extensions other than *.groovy - which can really be useful to others as well.I am submitting here a revised version of the patch - it now does not put entries like following in org.codehaus.groovy.transform.ASTTransformation where they don't belong{noformat}#files:gppgrunit{noformat}These can now be defined in a separate file {{org.codehaus.groovy.source.Extensions}} and the classes listed in such files need to implement the following interface to register the extensions with the compiler through which it calls back to collect the file extensions of caller's interest.{code}public interface SourceExtensionSupport {Set<String> getExtensions();}{code}Review comments welcome.,4886
Extract Method,Parser to support method declarations in scripts When creating scripts it'd be cool to be able to create methods in the script. e.g.foo(x) {println(x)}[1 2 3].each { foo(it) }This kinda thing might impact the parser implementation somewhat. The AST can handle the above right now - we just need the parser to handle it. e.g. in ASTBuilder the datatypeDeclaration() method currently checks for class interface or statement. Making it detect a method declaration and calling module.addMethod(MethodNode) would fix this.Though I'm leaving it up to you if this is gonna be easy & possible,4887
Extract Method,"add paging to groovy.Sql The patch provided adds offset and maxRows arguments to the eachRow methods. It could also be applicable for other methods but I left it at this for now.The implementation looks for ""scrollable"" results and applies the ResultSet.absolute() method. For ""forward only"" result sets next() is invoked offset number of times. The maxRows arg is implemented by only calling ResultSet.next() maxRows number of times.I was not able to run the tests (I have a dev mailing list thread on this topic) but I've included them in this patch. They're not all that comprehensive but should be correct. Again I'm not able to run these tests but I was able to verify this implementation through other means.",4888
Extract Method,implement a way for direct method calls MethodCallExpression already got a set/getMethodTarget method to enable the AST writers to do direct method calls but the logic behind it is not implemented.,4889
Extract Method,Add createTempDir() to File It would be nice if it were possible to easily create a temporary directory. The Java File class has a method which creates a temporary file but it doesn't just create the File object but also the file on disk. Instead it might create the object and then the user could decide at that point whether to create a file or directory from the name. The most straight forward approach would be to add a new method called createDir(). The current workaround is to create the file delete the file and then make the directory.,4890
Extract Method,includes' attribute in EqualsAndHashCode This is to continue the late discussion in [GROOVY-2879|http://jira.codehaus.org/browse/GROOVY-2879] which has stopped since the issue was closed.So the use case for 'includes' attribute usefulness in @EqualsAndHashCode is a typical JPA Entity which should implement the equivalence on the natural key basis and usually the natural key is one or two properties while the entity can contain dozens of properties and all of them have to be listed in 'excludes' at the moment.Example:Entity Person where firstName and lastName represent the natural key we don't want anything else in equals(){code:title=Using current implementation}@Entity@EqualsAndHashCode(excludes = 'idageemailaddressoccupationspousecontactsinterestsrelativesfriendswhatever')class Person {@Id int idString firstName lastNamedef age email address occupation spouse contacts interests relatives friends whatever} {code}{code:title=Proposed}@Entity@EqualsAndHashCode(includes= 'firstNamelastName')class Person {@Id int idString firstName lastNamedef age email address occupation spouse contacts interests relatives friends whatever} {code},4891
Extract Method,rename DGM collectAll to collectNested - keep original as an alias for the time being with a view to deprecating/removing eventually 0,4892
Extract Method,SwingBuilder binding updates should happen inside the EDT if the target is an UI component 0,4893
Rename Method,@Memoized AST Transformation for Methods The whole idea is similar to existing great @Lazy annotation but it differs in concept: instead of being applied to fields it is applied to methods thus providing a wider field of use. When applied to getters it serves as an alternative to @Lazy but applied to other methods it provides what @Lazy can't. Thus it eliminates the need for heavy refactoring in certain situations by simply letting the user add the annotation to the method.Here is a suggestion of how it could work:{code}@CachedT createX() {new T(1 2 3)} {code}gets transformed into:{code}private T $createX$resultT $createX() {new T(1 2 3)}T createX() {T $result_local = $createX$resultif ($result_local != null)return $result_localelse {synchronized (this) {if ($createX$result == null) {$createX$result = $createX()}return $createX$result}}}{code}This whole thing could be extended to cache different results of a method depending on its arguments but it's a topic for a discussion.,4894
Extract Method,"SwingBuilder - add more components to support ButtonDemo For a test of the relative size and ease of a Groovy swing applicaiton I though of re-implementing the SwingSet from the JFC demos in groovy. It wound up being about a third of the size of the original!To do a really groovy job I needed to add some more items and concepts to the SwingBuilder.jar.notable changes:- add more methiods from javax.swing.Box for glue struts and rigid areas- add buttonGroup case for AbstractButtons so we don't have to dive into the model to set a button group on a checkBox or radioButtontweaks:- add support for adding components to a tabbed pane explicitly- introduce some ""pass through nodes"" where the widget is not nesscicarily created but provided by the user (an elaboration of the widget() node from GROOVY-333) they are widget action and tableModel.- refactor code where attribures are set as java beans proeprties on the widget.This patch may contain stuff from the GROOVY-333 patch. I don't thuink it's been applied yet.Also ButtonDemo.groovy is the portion of the Buttons tab from SwingSet in ButtonDemo.java sans the images.",4895
Extract Method,Unnecessary castToType when boxed type subclasses or implements the target type With the following code :{code}int method() { 1 }void test() {int i = method()println i}test(){code}The compiler produces an uncessary castToType() call to convert a boxed int (aka Integer) to Object.,4896
Move Method,Make TupleConstructor work with enums The intent would be to make the following example work. Currently the {{TupleConstructor}} annotation is ignored and unless an explicit constructor is added the example will fail.{code}@groovy.transform.TupleConstructorenum Operator {PLUS('+') MINUS('-')String symbol}assert Operator.PLUS.next() == Operator.MINUS{code},4898
Extract Method,@ToString optionally exclude fields with null values Particularly on objects with a high number of fields having the ability to exclude fields which are null from the output of the generated toString would be a nice to have.,4899
Extract Method,"Implement a ""lowest upper bound"" algorithm For type inference it is important to determine the lowest upper bound of two types. The current implementation is called _findCommonSuperClass_ which is fine but limited. For example if two types do not have a common superclass but implement the same interface _findCommonSuperClass_ would return _Object_ where it could return the interface.The idea is to replace the current implementation with a smarter one which computes the lowest upper bound (LUB) of two classes.",4900
Extract Method,Bytecode optimizations: make use of LDC for class literals Class literals are currently loaded using generated {{$get$class$}} methods which increase bytecode size and may prevent some optimizations. In most situations though we may use the {{LCD}} bytecode instruction to load the class literal.,4902
Extract Method,GroovyDoc error for @EqualsAndHashCode Looking at the GroovyDoc for @EqualsAndHashCode the GroovyDoc here is missing methods:http://groovy.codehaus.org/gapi/groovy/transform/EqualsAndHashCode.htmlbut they can be seen in the JavaDoc:http://groovy.codehaus.org/api/groovy/transform/EqualsAndHashCode.htmlThere is a glitch in GroovyDoc processing and/or some formatting within the EqualsAndHashCode source file that needs tweaking.,4903
Extract Method,Process should have a method called closeStreams() Java doesn't close streams until finalization which can result in too many open files errors sometimes. To avoid this problem one needs to proactively close all the streams of a Process when it is finished. This currently requires 3 method calls so I propose a simple one be added to Process called closeStreams() instead.,4904
Extract Method,Add base script option to groovyc and its ant task Please provide a way to tell groovyc and its ant task which base script should compiled scripts extend from as it is already possible using GroovyShell.In Gaelyk project we are precompiling a scripts to gain better runtime preformance. Currently we can only precompile pure groovlets and groovy templates as they don't need to extend any special base script but we would like to be able to also able to precompile other script which relies on having particular base script (routes plugins).,4905
Extract Method,"@DelegatesTo annotation for IDEs/TypeChecker Add an annotation for closure parameters which allows to document how methods are resolved in the closure.For example:{code}exec {executable = ""foo""args ""bar"" ""baz""}class Project {ExecResult exec(@DelegatesTo(ExecSpec) Closure closure) { ... }}{code}",4906
Extract Method,remove runtime dependency on asm-util 0,4907
Extract Method,"Support for DSL type checking Improve support for DSL type checking:* allow the user to customize the behaviour of the type checker* provide means to support static compilation* provide higher level API for ""simple"" customizations",4908
Extract Method,"Provide a way to assemble an annoation consisting of several other The idea is to give an alias annotation representing a group of annotations. If the alias is encountered it is replaced by the annotations it ""contains""",4911
Extract Method,@ToString should allow caching of toString values For an immutable object a hashcode value & toString value only need to be computed once and cached. Then the hashcode and toString methods can just return the cached values.The @ToString and @EqualsAndHashCode ast transformation should have options to allow for caching. For example:@ToString(cache=true)@EqualsAndHashCode(cache=true)Also the @Immutable annotation should use the @ToString and @EqualsAndHashCode transformation with this caching capability enabled.,4915
Extract Method,DGM.collectMany() overload that accepts Iterable {{DGM.collectMany()}} should have an overload that accepts {{Iterable}} or even {{Object}} like {{DGM.collect()}} does.,4916
Extract Method,"Try iterating or transforming Object into Collection in DefaultGroovyMethods like count() sort() and other Collection methods. Lets say we have following Utils class (XML taken from groovykoans.org):{code}class Utils {static String MOVIES_XML = """"""<movie-catalog><movie id=""6""><title>Total Recall</title><year>1990</year></movie><movie id=""4""><title>The Terminator</title><year>1984</year></movie><movie id=""5""><title>The Expendables</title><year>2010</year></movie><movie id=""1""><title>Conan the Barbarian</title><year>1982</year></movie><movie id=""3""><title>Predator</title><year>1987</year></movie><movie id=""2""><title>True Lies</title><year>1994</year></movie><movie id=""7""><title>Kindergarten Cop</title><year>1990</year></movie></movie-catalog>""""""static def getMovies() {new XmlSlurper().parseText(Utils.MOVIES_XML).movie}}{code}I would like to count movies with id greater than 5.This doesn't work (throws MissingMethodException):{code}assert Utils.movies.count{it.@id.text() > '5'} == 2{code}But this does:{code}assert Utils.movies.iterator().count{it.@id.text() > '5'} == 2{code}This works too:{code}assert Utils.movies.collect().count{it.@id.text() > '5'} == 2{code}The same behaviour applies to Utils.movies.sort\{it.@id.text()\}.Also collectEntries collectMany sum join and many other Collection methods have this. Only collect() method works as expected.Something like this would be useful in GroovyDefaultMethods:{code}public static Number count(Object self Closure closure) {return count(collect(self) closure);}public static List sort(Object self) {return sort(collect(self));}public static List sort(Object self Closure closure) {return sort(collect(self) closure);}//etc...{code}I noticed also that groovy.util.slurpersupport.NodeChildren doesn't implement the Iterable class. Maybe changing {code}public abstract class GPathResult extends GroovyObjectSupport implements Writable Buildable {...}{code}to{code}public abstract class GPathResult extends GroovyObjectSupport implements Writable Buildable Iterable {...}{code}would solve some problems.",4917
Extract Method,Support AST transforms on the field generated by @Field in scripts The following snippet from a script won't currently work:{code}@Lazy @Field Sql db = new Sql(...){code}We copy over non-AST transforms when creating the field but not AST ones. This is intentional at the moment to align with limitations in how annotations are collected and traversed but we could attempt to add them after the fact to the collected known transforms.,4918
Extract Method,"Support for dynamic method calls in static compilation It might be interesting to instruct the compiler that some method calls in Groovy are to be made dynamically even if in a {{@CompileStatic}} section.For example if we want to enable {{@CompileStatic}} on Grails controllers and still want to be able to call dynamic finders method calls starting with ""find"" should be recognized as dynamic calls.The idea is to write a type checking extension for this to instruct the compiler:* which methods are meant to be called dynamically* what is the expected (inferred) return type of such a callThe latter is very important in order to keep the ""grooviness"" of code otherwise you would need to add explicit casts for each dynamic method call.",4920
Rename Method,New Groovy JDK methods - to improve consistency From my August 2004 email:I've been having a quick look at the Groovy JDK i.e. DefaultGroovyMethodsand DefaultGroovyStaticMethodsAs there are so many methods in these classes it can be a little hard totake in all at once. So I've knocked up a quick script (in groovy)that collatesthe methods against the objects to which they become attached.hacky script -> http://javanicus.com/groovy/MungGroovySourceCode.groovyThe result is a pretty table with the methods down the left hand sideand the Classes to which they are attached along the top. Where a methodhas been implemented at the crossover I have placed a small graphicwhich if you hover your cursor over will give you a bit more detailabout the method.http://javanicus.com/groovy/GroovyJDKCrossReference.htmlI've had a quick look for methods that I thought would be defined andfrom my quick inspection of abs() thru to leftShift() so far I believethat the following candidates are available(this is by no means an exhaustive list...)* Some possible missing (non static) methods-Collection.asImmutable()-Object.asImmutable()Object.asSynchronized()-Set.count()--Byte[].eachByte()--File.filterLine()--InputStream.filterLine()--List.findIndexOf()--Collection.flatten()--Date.getAt()- // to get the year etc... (In conjunction with a Calendar?)-Reader.getText()--Collection.intersect()--BufferedWriter <<-Object[] <<CharSequence <<-File <<--Map << // another Map--Process <<- // to the process.out-Socket <<- // to the socket.out-Object[].max()--Object[].min()-Collection.minus()Map.minus() // could compare RHS with key || value-Object[].minus()--URL.newInputStream()--URL.newReader()-CharSequence.padLeft()CharSequence.padRight()Object[].pop()CharSequence.putAt()Collection.putAt() // !always ordered but we have-Collection.getAt()--Date.putAt()- // e.g. easy access to components of DateMatcher.putAt()InputStream.readBytes()URL.readBytes()-URL.readLines()-CharSequence.reverse()-Object[].reverse()-SortedMap.reverse()SortedSet.reverse()Collection.reverseEach() // like Collection.each() this could be indeterminate...-Map.reverseEach()- // we have Map.each()...Matcher.reverseEach()Object.reverseEach()Object[].reverseEach() // perhaps foo.reverseEach() should be foo.reverse().each()Object.rightShift() // so you can do things like... foo >> log-Object[].sort()--InputStream.splitEachLine()--URL.splitEachLine()-OutputStream.withWriterAppend()-BufferedWriter.write()--OutputStream.write()-File.writeLine()OutputStream.writeLine()* and I'm not quite sure what the difference between append() and << issupposed to be perhaps File.append() will become File <<* As we have {{eachByte()}} would it not be a good idea to also have {{eachCharacter()}} for the Readers...thanksJeremy.P.S. This is based entirely on existing methods and existing Owner Objectswould be nice to think of what other Objects and methods we could include(current owner objects come from java.lang.* java.util.*java.util.regex.* java.io.* and java.net.*),4921
Extract Method,Closure to SAM interface coercion doesn't handle contravariant types correctly When coercing a closure to an interface where the method takes a contravariant type Groovy should create an implementation that accepts the most special type.{code}class TestCase {interface Action<T> {void execute(T thing)}static class Wrapper<T> {private final T thingWrapper(T thing) {this.thing = thing}void contravariantTake(Action<? super T> action) {action.execute(thing)}void invariantTake(Action<T> action) {action.execute(thing)}}static <T> Wrapper<T> wrap(Callable<T> callable) {new Wrapper(callable.call())}static Integer dub(Integer integer) {integer * 2}static void main(String[] args) {wrap {1} contravariantTake {dub(it) // fails static compile 'it' is not known to be Integer}wrap {1} invariantTake {dub(it) // passes static compile 'it' is known to be Integer}}}{code},4922
Extract Method,Allow @BaseScript on import or package definition It would be nice if {{@BaseScript}} could be used on a package/import definition like this:{code}@BaseScript(MyScript)import groovy.transform.BaseScriptprintln 'ok'{code},4924
Rename Method,Reflection API for Traits There needs to be some kind of reflection API for traits so that programs can reason about traits at runtime. For example one thing I need to know for Spock is whether a method that according to Java reflection is declared in class X is actually declared in that Groovy class or inherited from a trait.,4925
Extract Method,DOMCategory should not trim whitespace by default DOMCategory trims whitespace from returned text by default. This reduces its usefulness in many scenarios and since trimming is easy to do after the fact offers very little utililty. See also GROOVY-5360.,4926
Extract Method,Allow to implement and register a type checking extension as a subclass of TypeCheckingExtension Type checking extensions implemented via TypeCheckingDSL can be registered via CompilationCustomizer. It would be nice if one could also extend the less magic TypeCheckingExtension and somehow register that.,4927
Extract Method,Move methods from Collection to Iterable Move many methods from Collection to Iterable.Added these methods to Iterable.contains(Object)containsAll(Object[])eachPermutation(Closure)inject(Closure)head()tail()asCollection()plus(Iterable)plus(Object)multiply(Number)intersect(Iterable)toSet()Deprecated this method from Collection.eachPermutation(Closure)Did not add these methods to Iterable because GPathResult conflicts.Read https://github.com/groovy/groovy-core/pull/444 for the detail.toListString()toListString(int)I also added this method.DefaultGroovyMethodsSupport.createSimilarCollection(Iterable)I cannot move these methods because they exists in Objectand Groovy cannot handle properly if I move them from Collection to Iterable.grep(Object)grep()collect(Closure)collect()collect(Collection Closure)find(Closure)find()findResult(Object Closure)findResult(Closure)findAll(Closure)findAll()split(Closure)inject(Closure)inject(Object Closure)The pull request is here.https://github.com/groovy/groovy-core/pull/444,4928
Extract Method,Move collate() and permutations() from List to Iterable I added collate() to Iterable and moved permutations() from List to Iterable.This patch depends and related to https://github.com/groovy/groovy-core/pull/444  which moves methods from Collection to Iterable.The patch is here.https://github.com/groovy/groovy-core/pull/449,4929
Extract Method,Add ability for layouts to inherit the model The markup template engine layouts use a model which is independent from the original model. It would be nice to have an option to inherit the model from the parent template in order to avoid lots of code duplication.,4930
Extract Method,Add disjoint minus and toSpreadMap to Iterable Added these methods to Iterable.disjoint(Iterable)minus(Object)minus(Iterable)toSpreadMap()Added this method to Collection.minus(Collection)The pull request is here.https://github.com/groovy/groovy-core/pull/476,4931
Inline Method,Some DGM Collection improvements Highlights of the proposed improvements:* The {{unique()}} DGM methods follow Java's default of mutate in place when called on for instance a {{Collection}} (but also has a boolean flag for non-mutating behavior). There is no {{Iterable}} variant of the method but if there was one a mutate flag wouldn't make sense for non-Collection iterables. The {{Iterator}} variant has no flag and it wouldn't make sense anyway. There is no array variant but if there was one it wouldn't make sense to have a mutate flag. Rather than complicating {{unique}} further this improvement adds Iterable Iterator and array variants of {{toUnique}}. This method always returns new collection-like structures.* A similar situation exists for {{sort()}} as for {{unique}}. This improvement adds Iterable Iterator array and Map variants of {{toSorted}}. This method always returns new collection-like structures.* Some non-Iterable variants can be deprecated in favor of Iterable variants for: {{collate}} {{containsAll}} {{tail}} {{take}} {{multiply}} on Iterables {{disjoint}} {{minus}} on Iterables.* The non-released {{List}} variants for {{init()}} {{takeRight}} and {{dropRight}} have been elided (they provided only a marginal performance benefit compared to extra API complexity) and {{Iterator}} variants have been added.* The returned collection type is kept similar to the original where possible for the Iterable variants of: {{tail}} {{init}} {{take}} {{takeRight}} {{drop}} {{dropRight}} {{takeWhile}} {{dropWhile}} {{multiply}} on Iterables.* There are two new {{addAll}} variants on Collections: {{addAll(Iterator)}} and {{addAll(Iterable)}}.* Where possible Iterator variants of the above mentioned methods have been modified (where necessary) so that they don't create extra copies of collection-like structures (i.e. Iterators are used instead of logic involving for example {{toList}} followed by operations on the list). This makes them work better with (possibly infinite) streams of data e.g.:{code}int a = 1def infiniterator = [ hasNext:{ true } next:{ a++ } ] as Iteratorassert infiniterator.drop(3).dropWhile{ it < 9 }.toUnique{ it % 100 }.init().tail().take(3).toList() == [10 11 12]{code},4932
Extract Method,GroovyServlet: generate binary content Allow groovy servlets to generate binary content.,4933
Extract Method,Base64 URL Safe encoder The EncodingGroovyMethods support encodeBase64 and also support the MIME encoding using the chunked parameter but is no support the Base64 URL Safe encoding which implementation maybe trivial.,4934
Extract Method,Include inherited properties with @ToString The {{@ToString}} annotation only includes properties that are directly declared on the current class and not any properties that are inherited. I have a small hierarchy of Spring Data {{@Documents}} that inherit from a common abstract base (with ID status key and the like) and it is most natural for the {{@ToString}} to include all properties not just the ones declared on the concrete classes. {{includeSuper}} is a usable workaround but awkward because the base class is never instantiated and splitting the properties up is a bit confusing.It would be helpful to be able to tell {{@ToString}} to include inherited properties when generating the {{toString()}}.,4935
Extract Method,Optimize primitive type initialization in @CompileStatic The static compiler should be able to optimize constant initialization in some cases. For example if we have:{code}double x = 2.5{code}The static compiler creates an intermediate BigDecimal although a direct constant initialization could be done.,4937
Extract Method,groovydoc's help option is misleading When you specify the help option to groovydoc{noformat}groovydoc -help{noformat}It sounds like by default only protected and public fields and methods are included in the documentation.{noformat}-protected Show protected/public classes and members (default){noformat}But this is not true as it generates documentation for public protected and package classes and members. This what the package option does:{noformat}-package Show package/protected/public classes and members{noformat}I kind of think that both options should be removed and that there should be a statement below how to use the groovydoc tool that documents the default behavior.{noformat}usage: groovydoc [options] [packagenames] [sourcefiles]By default groovydoc generates documentation for public/protected/package classes and members....{noformat}Another issue is the public option which explicitly states that it _only_ generates documentation for public classes and members.{noformat}-public Show only public classes and members{noformat}This is not true either as it generates documentation for both public _and_ package classes and members.,4938
Extract Method,Optimize foreach style loops on arrays The static compiler should be able to optimize foreach style loops if the collection type is an array type:{code}int[] arr = ...for(int x: arr) { ... }{code}Currently it relies on an iterator making it much slower than it could be.,4939
Rename Method,@AnnotationCollector could provide more control over how annotations are added in the presence of existing explicit annotations When expanding a meta annotation alias into its annotation collection it is sometimes useful to be able to control how annotations are added in particular in the presence of existing explicit annotations. This issue proposes adding an annotation parameter to {{@AnnotationCollection}} which lets the addition of collected annotations be controlled in more flexible ways. The following modes are proposed for @AC:{code}DUPLICATE: Annotations from the annotation collection will always be inserted. After all transforms have been run it will be an error if multiple annotations (excluding those with SOURCE retention) exist.PREFER_COLLECTOR: Annotations from the collector will be added and any existing annotations with the same name will be removed.PREFER_EXPLICIT: Annotations from the collector will be ignored if any existing annotations with the same name are found.PREFER_COLLECTOR_MERGED: Annotations from the collector will be added and any existing annotations with the same name will be removed but any new parameters found within existing annotations will be merged into the added annotation.PREFER_EXPLICIT_MERGED: Annotations from the collector will be ignored if any existing annotations with the same name are found but any new parameters on the collector annotation will be added to existing annotations.{code},4940
Extract Method,TupleConstructor should allow a mechanism to switch off automatic parameter defaults When using @TupleConstructor it automatically provides defaults for the parameters. This is particularly useful behavior since it allows parameters which can remain as their default (user provided or null/0/false) can be left out of the constructor. In particular a no-arg constructor corresponds to the case of all arguments left out. This is exactly the kind of constructor which Groovy needs for default named-argument processing.Having said that there are times when all is required is a single simple constructor having all of the classes properties. This issue allows a parameter {{defaults=false}} to be set to enable this new behavior.For this class:{code}@TupleConstructorclass Person {String first lastint age}{code}The following normal constructor is produced:{code}Person(String first=null String last=null int age=0) { this.first = first //etc... }{code}After Groovy's compilation phases are complete the following constructors are visible to Java:{code}Person(String first String last int age) { //... }Person(String first String last) { this(first last 0) }Person(String first) { this(first null) }Person() { this(null) }{code}Adding {{defaults=false}} as a parameter to TupleConstructor means that just the first of these will be produced. If any of the properties has a default value when this setting is made false an error will be flagged.,4941
Extract Method,groovy.util.Node.depthFirst should provide a way to specify pre-order post-order ordering Please provide a way to specify the ordering in depth 1st traversal.,4942
Extract Method,InvokerHelper formatting methods have inconsistent API in class org.codehaus.groovy.runtime.InvokerHelper there are methods used to print out Objects. The methods sometimes have a maxsize verbose or safe argument but they do not have them consistently. I suggest changing the private API so that methods for all Collection/Map types have the same API and same functionality.See https://github.com/apache/incubator-groovy/pull/96,4943
Extract Method,ObjectRange methods duplicate functionality and should not rely on size() The methods iterator() step() get() size() and subList() all step through the range all having their own stepping implementation which makes the class more complex than it needs to be.Also get() subList() and iterator() rely on method size() which in non-numeric cases does a brute-force iteration over all elements. This causes an unnecessary performance overhead in several cases (even if the value of size() is cached after first computation). The overhead may be arbitrarily large and in cases the size() exceed MAX_INTEGER the reliance leads to errors.(Also see GROOVY-2972 GROOVY-5426).I suggest unifying the stepping semantics and not relying on size(). PR will follow.,4944
Extract Method,"@Builder should have an option to include superclass properties If one annotates a groovy class with @Builder and that class extends from another class then the generated builder does not support setting the parent class properties.This is especially problematic when mixin groovy builders in java code.e.g. the following class shows what will and will not compile{code:java}// Animal.groovyimport groovy.transform.builder.Builderimport groovy.transform.builder.SimpleStrategy@Builder(builderStrategy = SimpleStrategy)class Animal {String colorint legs}// Pet.groovyimport groovy.transform.builder.Builderimport groovy.transform.builder.SimpleStrategy@Builder(builderStrategy = SimpleStrategy)class Pet extends Animal {String name}// PetTest.javaimport org.junit.Test;import static org.junit.Assert.*;public class PetTest {@Test public void createPet() {// Pet pet = new Pet().setColor(""white"").setLegs(4).setName(""Bobby""); does not compilePet pet = (Pet) new Pet().setName(""Bobby"").setColor(""white"").setLegs(4);assertTrue(pet.getLegs() == 4);}}{code}",4945
Extract Method,Add support for Iterable with Closure to JsonBuilder/StreamingJsonBuilder JsonBuilder and StreamingJsonBuilder currently support passing a {{Collection}} and a {{Closure}} the closure will be applied to each item in the collection. An improvement would be to support the more general {{Iterable}} type.See https://github.com/apache/groovy/pull/203,4946
Extract Method,Groovy could implement an @AutoImplement transform Groovy provides numerous facilities for dynamically creating 'Proxy' implementations of interfaces e.g.:{code}def emptyIterator = [hasNext: { false }] as Iterator{code}There is special support for Closures maps of Closures SAM method coercion and various proxy generator classes. Typically such dynamic creation is exactly what is required e.g. a one-off usage object or a testing stub of some kind. But other times compile time creation of such classes would be useful. This proposal suggests a transform to reduce boilerplate code for a number of common scenarios where code can be created. The proposal has numerous configuration options but doesn't try to support everything that the dynamic options provide. E.g. no map of Closures is supported; you _can_ just create a class manually in that case.The transform allows the above example to be as follows:{code}@AutoImplementclass EmptyStringIterator implements Iterator<String> {boolean hasNext() { false }}{code}which provides a method having signature '{{String next()}}' with an implementation that returns the default value for the return type ({{null}} for {{String}}).Alternatively we can make it throw an exception as follows:{code}@AutoImplement(exception=UnsupportedOperationException)class EmptyStringIterator implements Iterator<String> {boolean hasNext() { false }}{code}This would in fact be a closer match to the initial dynamic case shown above.,4947
Extract Method,"MissingMethodException should limit argument types in getMessage() The getMessage() method in MissingMethodException uses InvokerHelper.toTypeString(Object[] arguments) to generate the types of each argument. When this exception occurs with a large parameter list this can create a very large String. The format method which is used 2 lines later to print the values uses a 40 character limit I'd like to propose to use similar logic for the types as well.Here is a trivial code snippet from groovysh - this log message can grow unbounded:{code}groovy:000> missing(*(""Test"".getBytes()))ERROR groovy.lang.MissingMethodException:No signature of method: groovysh_evaluate.missing() is applicable for argument types: (java.lang.Byte java.lang.Byte java.lang.Byte java.lang.Byte) values: [84 101 115 116]Possible solutions: toString() toString(){code}",4949
Rename Method,Not public constructors for groovy.transform.Immutable anotated class groovy.transform.Immutable annotation generates only public constructors for annotated class.I want to be able to create class with constructors with another modifiers e. g. private constructors and define static method create/build/from which will be the only way to create my immutable class.The pull request will be provided.,4950
Rename Method,"Groovy TemplateServlet NPE fixed and totally revamped Hello Groovy dev teamhere is my revamped version of the TemplateServlet class. I didn't produce a diff because I use another code formatter than the Groovy project team. You must sit in front of wide screens... :) Fixes changes and improvements:* Fixed NPE in log()-calls by calling super() in init().* Rewrote template source file finding according to the JspServletfound in Apache/Tomcat.* Added ""setVariables(ServletBinding)"" method allowing extension to bind their own variables.* Implemented a simple template cache using WeakHashMap.* Javadoc'ed a lot.* Removed 1.5 syntax... ;-)",4951
Extract Method,add suppressed exceptions for with[Auto]Closeable methods As with the try-with-resources statement if multiple exceptions are thrown the exception from the closure should be returned and the exception from closing should be added as a suppressed exception.Currently the withCloseable/withAutoCloseable method return the closure exception and log a warning if an exception is thrown on the call to {{close()}}. With this improvement the exception from {{close()}} would also be added as a suppressed exception to the exception thrown from the closure.,4952
Extract Method,groovy-starter.conf could be extended to support configscript groovy-starter.conf could be extended to support statements like: {code} configscript !{groovy.home}/conf/autofinalconf.groovy configscript !{user.home}/.groovy/autoimportbuilder.groovy configscript !{user.home}/.groovy/compilestaticeverywhere.groovy {code},4953
Extract Method,DefaultGroovyMethods missing Array support The methods every any and collect support instances of Iterable and Iterator but do not include equivalent methods for support arrays.,4954
Extract Method,Support native lambda in static compilation mode Here is the PR: https://github.com/apache/groovy/pull/654,4955
Extract Method,Final variable analysis doesn't account for early exit for try/catch/finally An example with early return from a method: {code} def method(String foo) { final str try { str = foo.trim() } catch(e) { println e return null } int exitCode = str.isInteger() ? str.toInteger() : null exitCode //str.isInteger() ? str.toInteger() : null // this doesn't trigger the error } println method(null) {code} And a slight rearrangement of above: {code} def method(String foo) { final str try { return foo.trim() } catch(e) { str = '-1' } int exitCode = str.toInteger() // The variable [str] may be uninitialized exitCode } println method(null) {code} ,4956
Extract Method,DGM#intersect should provide a variant with a comparator There is a performance penalty using NumberAwareComparator when that feature is not required.,4957
Extract Method,Migrate Groovyc to picocli Migrate {{org.codehaus.groovy.ant.Groovyc}} from commons-cli to picocli.,4958
Extract Method,Migrate groovy.ui.GroovyMain to picocli Migrate groovy.ui.GroovyMain from commons-cli to picocli.,4959
Extract Method,Annotate generated methods with @Generated The `@Generated` annotation was added a couple of releases ago to indicate that a method/class was generated by the compiler (most likely via AST transformations). This annotation is already applied to methods defined by `GroovyObject` however it has yet to be applied to many of the generated methods added by core AST transformations.   See the following links for more context [https://github.com/apache/groovy/pull/617#issuecomment-336249772] [https://github.com/jacoco/jacoco/pull/733]  ,4960
Extract Method,implement list.putAt(Range) to work like list.putAt(splice) list = [012345]assert list[01] == list[0..0]assert list[23] == list[2..5]list[2..5] = [x]assert list == [01x]list[2..2] = [2345]assert list == [012345],4961
Extract Method,Update commons.solr to Solr 4.4 Some initial checks have showed that this update comes with API changes in Solr that will affect several components:This I am currently aware of are:* solr.core: OsgiSolrCore OsgiResourceLoader* Lucene based NLP components: as the API for creating Analyzer instances changed* TopicEngine: need to adapt the unit tests to use the new API to create CoreContainer,4965
Rename Method,Add Lookup Cache to EntityLinking Engine The EntityLinkingEngine should cache results of lookups on the EntitySearchers.Entities are often reoccurring in analyzed Documents. Because of that caching results for look upped tokens should provide considerable performance improvements as tatistics shows that ~90% of the processing time for the EntityLinking engine is contributed by the entity look-up. So if 20% of all Entity mentions are about reoccurring Entities the processing time should be reduced by about 18%.The cache will use the list of search string as key and a list of returned Entities as value. The cache will only collect look-up results for the currently analyzed document. EntityLinking statistics will be updated to include the cache hit percentage.This issue affects both the trunk (1.0.0-SNAPSHOT) as well as the stable 0.12 releasing branch.,4966
Extract Method,Add FallbackMode URI Pattern and URI Prefix support to the DereferenceEngine The generic implementation of the DereferenceEngine should be extended by the following options:1. FallbackMode: If enabled it will not dereference Entities for those already some triples are present in the Enhancement Structure2. URI Prefixes: Allows to configure prefixes of URIs that should be dereferenced3. URI Patterns: Allows to configure regex pattern for URIs that should be dereferenced.An Entity will only be scheduled for dereferenceing if* !fallbackMode OR no triples are available* prefixes.isEmpty() && patterns.isEmpty()* the entity URI matches any of the prefixes or patternsThis allows to have multiple Engine instances with different field and ldpath configurations for different data sets (e.g. Geonames DBPedia SKOS ...). It also allows to avoid necessary calls Dereferencer that might need much more time to check if they can dereference an Entity.By default fallback mode should be enabled prefixes and patterns will be empty.,4967
Rename Method,Upgrade to OpenNLP 1.5 The OpenNLP 1.5 release makes it easier to train new custom models.,4968
Extract Method,Add support for custom entity references to the Dereference Engine Currently the EntityDereferenceEngine only support `fise:entity-reference`. This will allow users to parse the list of properties that do reference to Entities in the configuration.The `enhancer.engines.dereference.references` property will be used. Supported values are String (single value string[] and Collection<String>). QName configurations (`{ns:}:{local-name}`) will be supported if a NamespacePrefixService is present.If the configuration is not present `fise:entity-reference` will be assumed as default.EntityDereferencer implementations will not need to be addapted because of this extension.,4969
Extract Method,"Allow EnhancementEngines to get chain scoped enhancement properties Currently the EnhancementEngines can only retrieve the merged view over all Enhancement Properties (as defined by STANBOL-488). This includes the ""chain scoped"" properties - defined by the Enhancement Chain configuration as well as the ""request scoped"" properties - defined for a single enhancement request.This is required for implementing EnhancementEngines that only want to react of properties provided by Chain configuration but to not want to allow overriding those by Enhancement Requests (e.g. the configuration of an external service endpoint or user/pwd for an external service).",4970
Extract Method,Enhancer benchmark tool I'm working on a benchmark tool for the stanbol enhancer where content is fed to the enhancer and the output is checked according to a set of expectations.Here's an example benchmark - the syntax might evolve as we implement this but it explains the general idea. To run the benchmark we'll POST a plain text document containing one or several such benchmarks and get an HTML document back with the results.= INPUT =Bob Marley was born in Kingston Jamaica= EXPECT =# Comments such as this one are ignored# EXPECT defines groups of predicate/object matchers that we expect to find in the output# Each group applies to one given enhancement: for the expectation to succeed at least# one enhancement must match all lines in the grouphttp://fise.iks-project.eu/ontology/entity-reference URI http://dbpedia.org/resource/Kingston%2C_Jamaicahttp://purl.org/dc/terms/creator STRING org.apache..*ReferencedSiteEntityTaggingEnhancementEngine# Groups are separated by empty lines here's a new onehttp://fise.iks-project.eu/ontology/entity-type URI http://dbpedia.org/ontology/MusicalArtisthttp://fise.iks-project.eu/ontology/entity-reference URI http://dbpedia.org/resource/Bob_Marleyhttp://purl.org/dc/terms/creator REGEXP org.apache.*EntityMentionEnhancementEngine= COMPLAIN =# COMPLAIN statements are similar to EXPECT but we expect them *not* to be fulfilled by# the enhancer output. ,4971
Extract Method,Rule management must support an offline mode The ontologymanager/ontonet and rules components should provide heuristics for loading recipes and networked ontologies from an offline resource possibly hijacking dependencies so that they resolve locally recursively.,4974
Move Method,Jena-based reasoner Add Jena rule based reasoner as a new separate inference module inside the reasoners package;Implement the Reasoners OWLApi interface to adapt to Jena rule based inference engine with OWL lite profile;Remove HermiT fro within the reasoners module,4976
Rename Method,Integration tests framework Create a framework for running tests at build time by starting our runnable jars and testing them via http.,4978
Rename Method,Processors should become OSGI Components Bridge processors should become OSGI components and implement Processor interface. In this way it will be possible to plug any other Processor implementation based on a specific need.,4980
Rename Method,"Add support for selective ontology library (lazy) loading When supplied an ontology registry OntoNet currently loads all the ontologies from all the libraries referenced by this registry. This is an overkill if a user only wants to manage a single library and load it into a scope.Registry management should be configurable to support laziness in that the actual ontology resources are not loaded until the corresponding model is ""touched"" (e.g. a request is made for the OWL ontologies contained in a library).Add the possibility to provide the IDs of the libraries to load and avoid loading other libraries both in the Java and REST API.",4984
Extract Method,Make root ontology management implicit in ontology spaces The root ontology of each ontology space (e.g. {scopeid}/core/root.owl and {scopeid}/custom/root.owl) is managed as an actual OWLOntology object just like actual ontologies loaded in the scope.However this does not bring any benefit and creates clutter when managing import statements and serialization of ontology scopes and spaces for RESTful services.Root ontologies can be generated on-the-fly for GET services on scopes and spaces so that import statements can be swiftly moved across spaces without worrying about reloading the whole imports closure. Also since they do not map to actual files we can remove .owl extensions (which should be reserved for included ontologies).As a consequence OntologySpace#setTopOntology() methods would no longer make sense and should be deprecated in the API then removed altogether. On the contrary getTopOntology() could still be used to dynamically generate root ontologies from the existing resources.,4987
Rename Method,Clerezza converters to handle TripleCollection instead of MGraph OWL transformation APIs such as OWLAPIToClerezzaConverter transform to and from MGraph objects. This is unnecessary as there is no need to modify graphs internally and developers might have to obtain MGraph objects just for the sake of passing them to this API.Replacing MGraph with TripleCollection to accommodate Graph and MGraph alike will make the transformation API more versatile.,4989
Rename Method,Enabling the SCR factory for the configuration of multiple Refactor engine components via Felix console In order to have multiple Refactor engine components active during the enhancement (each one with its mapping recipe) it would be appropriate to enable the SCR factory in the Refactor engine for the configuration of this component via Felix console.,4996
Move Method,Google Refine Reconciliation Service support Add support for the Reconciliation Service API to the Stanbol Entityhub RESTful APIThe Google Refine ReconciliationServiceApi allows to reconcile String values with Entities. The documentation of the service can be found at [1].The Entityhub is very well suited for implementing this service as it can execute those queries very efficiently based on the SolrYard implementation.[1] http://code.google.com/p/google-refine/wiki/ReconciliationServiceApi,4998
Extract Method,"Allow to load any kind of OpenNLP Model with the OpenNLP service Currently the OpenNLP service provides only Methods to load models following the default naming conventions of OpenNLP. (e.g. getSentenceModel(String langauge) would try to load the model with the name ""{language}-sent.bin"").This suggests to add an additional Method that allows to load any type of model from any file available via the DataFileProvider framework<T> T getModel(Class<T> modelType String modelName Map<StringString> properties)",4999
Extract Method,"Stanbol NLP processing This issue covers the NLP processing components as discussed in http://markmail.org/message/qxusiup3mim2lhpxGoals=====1. provide a modular infrastructure for NLP-related thingsMany tasks in NLP can be computationally intensive and there is no ""one fitsall"" NLP approach when analysing text. Therefore we wanted to have a NLPinfrastructure that can be configured and wired together as needed for thespecific use case with several specialised modules that can build upon eachother but many of which are optional. 2. provide a unified data model for representing NLP text annotationsIn many szenarios it will be necessary to implement custom engines building onthe results of a previous ""generic"" analysis of the text (e.g. POS tagging andchunking). For example in a project we are identifying so-called ""nounphrases"" use a lemmatizer to build the ground form then convert this tosingular nominative form to have a gramatically correct label to use in a tagcloud. Most of this builds on generic NLP functionality but the last step isvery specific to the use case.Therefore we wanted also to implement a generic NLP data model that allowsrepresenting text annotations attached to individual words or also to spans ofwords.",5000
Rename Method,Sentiment Summarization EnhancementEngine An EnhancementEngine that consumes word level Sentiment annotations and sums them up to* Noun Phrases* Sentences* the whole DocumentNotes: * As this EnhancementEngine is expected to create Enhancement we will need to define how SentimentAnnotation should be represented by using the Stanbol enhancement Structure.* This Engine should also support the detection of negations (e.g. this was not a nice trip the weather was not as bad as the forecast suggested),5002
Extract Method,Improve the parameter and response types; parameter validation of REST services of Contenthub The parameter validation in the RESTful sevices of Contenthub is not very effective now. Existence check on the mandatory parameters should be done. Accordingly the services also should return correct status codes. Furthermore types of parameters (e.g FormParam QueryParam etc) of services should be consistent where possible.,5003
Rename Method,"Make it possible to define expansion of RdfViewable by resource method According to https://svn.apache.org/viewvc/stanbol/trunk/commons/web/rdfviewable-writer/src/main/java/org/apache/stanbol/commons/web/viewable/ldpath/writer/impl/RdfSerializingWriter.java?view=markup&pathrev=1447265:""the expansion can be widened by using the query parameters xPropObj and xProSubj""It would be nice if I could specify that a bit more fine grained on the server side. From what I get templatePath is used for the HTML export to have more influence on how the result values looks like. I would like to have a similar functionality for RDF representations.I do something similar in a framework for interfacing RDF based data. In there I use so called Recipes to define on which rdf:Properties I'm interested in. The current vocab can be found here: http://vocab.netlabs.org/recipe. ",5007
Extract Method,"Improve Header Mediator to add/remove transport headers Currently header mediator is used for set or remove headers from the current SOAP infoset.With this Header mediator is improved to set or remove headers from transport as well. Though this can be achieved also with property mediator using header mediator will make configuration more readable and clearer.Syntax:{noformat}<header name=""qname"" (value=""literal"" | expression=""xpath"") [action=""set""] [scope=default|transport]/>{noformat}Example:{noformat}<header name=""Accept"" value=""text/html"" scope=""transport""/>{noformat}Please note that this will not affect already existing configurations as null/default scope still mean altering SOAP headers.",5008
Rename Method,Update Synapse to Axis2 - 1.3 Final release Hi FolksPlease see enclosed a patch to update Synapse to latest Axis2 1.3 final release. There is one test failure. So i added an exclude in modules/core/pom.xml.thanksdims,5009
Extract Method,Should be able to define seperate policies for incoming and outgoing messages It should be able to specify different policies for incoming and outgoing messages in proxy services,5010
Extract Method,"It should be possible to re-use the JMS connection information from the axis2.xml JMSSender when specifying JMS EPRs The JMSSender now supports the definition of connection factory entries as follows:<transportSender name=""jms"" class=""org.apache.synapse.transport.jms.JMSSender""><parameter name=""myQueueConnectionFactory""><parameter name=""java.naming.factory.initial"">com.swiftmq.jndi.InitialContextFactoryImpl</parameter><parameter name=""java.naming.provider.url"">smqp://10.25.51.38:18000/type=com.swiftmq.net.JSSESocketFactory;timeout=10000</parameter><parameter name=""transport.jms.ConnectionFactoryJNDIName"">QueueConnectionFactory</parameter><parameter name=""java.naming.security.principal"">xxxadapter</parameter><parameter name=""java.naming.security.credentials"">xxxadapter</parameter></parameter></transportSender>Thus it should be now possible to refer to this information when specifying a JMS EPR as:<syn:endpoint name=""myJMSEndpoint""><syn:address uri=""jms:/QueueName@Somewhere?transport.jms.ConnectionFactory=myQueueConnectionFactory""/></syn:endpoint>instead of specifying the IC class url username and password etc",5011
Rename Method,Allow XPath expressions to be specified relative to envelope or body via an attribute This would make XPath expressions simpler without consideration for SOAP 1.1 or 1.2 or REST etcDefault could be envelope (i.e. what we have now - for backward compatibility) and an optional attribute could specify if it should be relative to the body,5012
Extract Method,Add the ability to configure the fault detial dynamically in the MakeFault mediator Add the ability to configure the fault detial dynamically in the MakeFault mediator,5014
Move Method,Remove duplicate code in Axis2FlexibleMEPClient and MessageHelper For the moment the removeAddressingHeaders method of the org.apache.synapse.util.MessageHelper#237 duplicates the code at removeAddressingHeaders method of the org.apache.synapse.core.axis2.Axis2FlexibleMEPClient#532 which sould be merged.The method detachAddressingInformation is also having the same behavior on the MessageHelper and Axis2FlexibleMEPClient.These duplicate code should be removed and merged to a single method.,5015
Rename Method,Dynamic load balancing There are some limitations in the current load balancer implementation. e.g. if we have 2 identical services in 2 different worker nodes which are fronted by a synapse load balancer instance. In such a case we need to provide 4 endpoints in the synapse.xml file. As can be seen this is not a very scalable solution. Hence I have implemented an dynamic load balancing mechanism where the application members are discovered at runtime and the endpoint do not need to be statically specified in the synapse.xml file.Currently the application endpoints are calculated by replacing the IP and port of the incoming request with that of the member to which this request will be forwarded to. I have only tested with HTTP/S for the moment. More details about the concept & design can be found here: http://afkham.org/2008/06/fault-resilient-dynamic-load-balancing.html,5016
Extract Method,Static load balancing across a static group With the current load balance endpoint implementation we need to provide the endpoint of the services. We should be able to simply provide the information of the application group members and load balance across this group.,5017
Extract Method,Simplify the way logging is done in mediators Mediators can log messages to three different logs: the usual log with category set to the class name the trace log and the service log. While this is extremely useful and should be preserved the way mediators have to be coded to leverage these logging facilities could be improved. Indeed the following problems with the current situation can be identified:* Code using the logging methods defined in AbstractMediator can't be reused in anything else then mediators. A random example is the SpringMediator#buildAppContext method. The code in this method can quite easily be reused in another mediator (provided that both mediators have a common base class) but not e.g. in a Startup implementation. The reason is that it uses the traceOrDebug method from AbstractMediator.* In general when the code in a mediator is split into several methods or when reusing a method in several mediators it is required to pass traceOrDebugOn and traceOn from one method to the other. This is quite annoying.* For someone who starts writing new mediators it is not obvious how to correctly use the various logging methods. In addition the current implementation doesn't enforce a consistent use of the logging facilities one of the reasons being that the log and trace attributes in AbstractMediator are accessible to subclasses. E.g. there are mediators that simply call log.error thereby bypassing the TRACE and SERVICE logs.To improve the situation the proposal is to:(1) Introduce an interface called SynapseLog with* a set of logging methods such as error info traceOrDebug auditWarn etc. (mainly equivalent to what is defined now in AbstractMediator);* a set of corresponding isXxxEnabled methods following the pattern in commons-logging.(2) Add a getLog(MessageContext) method to AbstractMediator that returns an appropriate implementation of the SynapseLog interface. Mediator implementations would call this method at the beginning of the mediate method and exclusively rely on the returned object to send messages to the logs. If the code in the mediator is split into several methods or if it shares a common method with another mediator this object would be passed as argument to these methods (instead of traceOrDebugOn and traceOn).(3) Create a SynapseLogAdapter class that implements SynapseLog and that delegates calls to a single org.apache.commons.logging.Log instance. Instances of this class would be used by code that is executed outside of a mediator but that needs to call a method shared with some mediator implementation.To summarize the general idea is to expose a set of (Synapse specific) logging categories in a well defined interface and to completely hide the underlying implementation(s) behind this interface. Note that the proposal can be implemented without breaking any existing code: it can coexist with the existing logging methods in AbstractMediator which would later be tagged as deprecated before potentially being removed in some future version of Synapse.,5018
Extract Method,Improve the performance of the XSLT mediator XSLTMediator performance can be further improved by using various methodologies and the idea is to enable a configuration point where a user can specify the optimized methodology for his configuration.Some of the methodologies will be Stax based Source StreamSource and StreamResult and so on... It is better if we can write an AXIOMSource and AXIOMResult pair which should give the best performance :-)Also we can use features to enable these methodologies and it is obvious that only one of these set of features can be used for a given XSLT config and we might introduce a priority level so that if two of these features are used highest priority one will be used.,5022
Rename Method,Adding the tracing capability to the mediator hi allAdding a new feature for enable tracing capability to each of the mediatorendpoint and proxy service Then it is flexible to trace specific information which local to a particular mediator. thanksRegards indika,5024
Extract Method,Source code of HessianMessageFormatter/Builder could be improved and a test cases added Attached you find the second part of my changes to the HessianMessageBuilder/HessianMessageFormatter. It hopefully makes the HessianMessageFormatter and HessianUtil more readable adds JavaDoc and test coverage. No functional change. Please review and commit if everything is fine!,5025
Extract Method,Send Synapse generated Hessian faults with HTTP status 200 If synapse generates faults using the fault mediator using the nhttp transport they are currently always send using HTTP status 500. Hessian clients requires all messages to arrive with HTTP status 200 (no differentiation between normal messages and fault messages). HTTP 500 is reserved for real internal server errors.The attached patch introduces a message context property to advice the nhttp transport to use a HTTP 200 status code. For the Hessian case the message builder set this advice and the HttpCoreNIOSender picks up this advice in case of fault messages. The logic to detect the proper HTTP status has been extracted to a separate private method to improve readability.I think this approach is even better than the approach discussed on the dev list modifying the FaultMediator to set just another property with the value of the HTTP status. The FaultMediator stays transport independent and the property is directly evaluated from the transport.Regarding the naming and place of the new Constants in NhttpConstants I would appreciate a review. The same applies to the extracted status code logic which should not change the existing bahavior. I just tried to make it a bit more readable also changing some comments.Feedback welcome!,5026
Rename Method,"Enhanced JMX-support for Synapse Hicurrently it is only possible to use Java's ""out-of-the-Box JMX solution"" configured via system properties and/or property files which is sufficient for many cases.Anyway there are the following advantages of using the according Remote API to create configure and deploy the management agent (server connector) programatically:1) easier configuration for average users2) exporting RMI server's remote objects over a certain port to allow passage through a firewall (very important for enterprise deployments) possibility to configure a specific network interface (also sometimes important for enterprise deployments in multi-homed systems 3) possibility to use a custom JMXAuthenticator to handle own credential configuration (including use of Secret-API to encrypt passwords in plain text files if required/prefered over setting os permissions accordingly.",5027
Extract Method,Implement Support for Multiple SSL Configurations Currently Synapse can have only one SSL configuration in the HTTPS transport sender (defined as a parameter of the HTTPS transport sender in the axis2.xml). However there could be an instance where we want the sender to use different SSL configurations to connect to different endpoints. So we should be able to specify multiple SSL configurations at transport level and refer to such configurations at endpoint level.,5028
Extract Method,XSLT Mediator possibility of creating multiple Templates by mutiple threads With the current code multiple threads can create the XSLT template after seeing that the cache has expired. Also we are always doing a synchronization for creating the template which can be costly.,5029
Extract Method,Using UserTransaction interfacr instead of TransactionManager interface in tx mediator I am not sure why the TransactionManager interface was used when implementing the tx mediator. But the documentation says when we need to move the tx mangement boundries to application server we need to use TransactionManager interface. How ever using this(TransactionManager) with more that one data sources caused me resource enlist exception. So I just switched to UserTransaction Interface and then it works fine with more than one datasources. I am attaching the changes as a patch. (If anybody interested I can attach the configuration files used.),5030
Extract Method,Make the MultiXMLConfigurationSerializer More Reliable Currently this is how the MultiXMLConfigurationSerializer (MXCL) saves the configuration to the file system:1. Backup the existing synapse-config directory2. Create a new synapse-config directory and the necessary child directories3. Serialize the configuration to the files (this can fail due to an IO error - eg: running out of file handles in the system)4. Delete the backupIf step 2 or 3 fails MXCL tries to restore the backup. But in most practical scenarios when IO operations fail they fail in chunks. Therefore most of the time the restoration operation never succeeds. This can leave Synapse with either no or partially done synapse-config directory. We can make the process more reliable implementing the following strategy:1. Create a temp directory2. Serialize the entire configuration the temp directory (leave the original synapse-config directory intact)3. Once the serialization is completed move the temp directory as the new synapse-config directoryThis way if something goes wrong during serialization the original synapse-config directory won't be affected.,5031
Rename Method,Implement Ability to Intercept Admin Level Messages in the FIX Transport Currently FIX transport can only intercept application level messages. It would be most useful to have the ability to intercept admin level messages as well. Basically we should be able to add custom fields to the outgoing admin level messages.,5032
Move Method,Implement a URL Rewrite Mediator Implement a mediator that can rewrite URLs efficiently based on a set of user defined rules.,5033
Extract Method,Add Serialization Tests for Endpoints Currently there are no Serialization Tests for Endpoints.Since there are bunch of EndpointSerializers in synapse core it will be better to have same tests to keep the correctness of the code and monitor it.,5034
Move Method,Improve Synapse ServerWorker to have pluggable HTTP Get request processors Currently Synapse doesn't support a mechanism of adding custom Http Get processors. I am working on an improvement which will enhance Synapse in such a way that a user can write his own Http Get processor and work with it against Synapse. Regards Heshan.,5035
Extract Method,"Supporting Dynamic registry keys in Mediators Currently synapse supporting only static registry keys with mediators. For an example XSLT Mediators only allows static keys when creating the mediator. With this patch synapse will able to provide support for dynamic keys like xpath expressions. In that case user will able to use XPath kind of approach to dynamically generate the key instead of a static key. With the improvement synapse supports both static and dynamic keys as in following example.<!-- using static key ---><xslt key=""xslt-key-req""/><!-- using Dynamic key --><xslt key={xpathExpressionToEvaluateKey } />Also with this improvement XSLT Mediators will able to support multiple template caching.",5036
Extract Method,"Function-Templates for Synapse Configuration Implementing Function templates in Synapse can be used to reduce lot of complexity as well as eliminate redundancy at synapse configuration. At the moment a large portion of ESB configuration are at the micro level. Because of this users have to individually configure each and every mediator to achieve some high level task. But what we need is a high level abstraction that users can easily use to model such scenarios .For example if we consider following scenario <proxy name=""SplitAggregateProxy""><target><inSequence><iterate expression=""//m0:getQuote/m0:request""preservePayload=""true""attachPath=""//m0:getQuote""xmlns:m0=""http://services.samples""><target><sequence><send><endpoint><addressuri=""http://localhost:9000/services/SimpleStockQuoteService""/></endpoint></send></sequence></target></iterate></inSequence><outSequence><aggregate><onComplete expression=""//m0:getQuoteResponse""xmlns:m0=""http://services.samples""><send/></onComplete></aggregate></outSequence></target></proxy>Here what user really wants to look at is three configuration parameters 1. Two XPath expressions2. An endpoint addresswe can hide a lot of this complexity by introducing a 'template' configuration that parameterize (like a function does) a known pattern such as this one.Parameterization can be done using xpath expressions..Function templates will look like following in synapse config1)<template name=""[func name]"">/**parameters for this function template**/<parameter name=""p1"" />*/**any sequence**/<any sequence>+</template> -->A Template is an extension of Sequence  hence template body can contain any sequence in general--> any mediator/comp inside template body can refer to its parameters just like a normal function does by xpath function scope (ie:-func) variableie:- <aggregate><onComplete xmlns:m0=""http://services.samples"" expression=""$func:p2""><log level=""full"" /><send/></onComplete></aggregate>OR<log level=""custom""><property name=""p1-value"" expression=""$func:p1""/><property name=""p2-value"" expression=""get-property('func''p2')""/></log>2)To invoke a template in synapse we can define a Invoke mediator which will be in the following form. Using a invoke mediator from within any sequence of Synapse we would be able to execute a template with the passed on values for parameters such as p1p2etc.... :<invoke target=""[target func template]""><parameter name=""p2"" value=""{any xpath} | plain-value""/>*</invoke>",5037
Rename Method,Upgrading quartz version used in synapse-task Current synapse-task implementation has used quartz 1.6.0 version which is something old. The quartz latest stable version 2.1.1 seem to have many functional improvements and new features over 1.6.0 version. So it will be a good idea to upgrade quartz version to 2.1.1 to have future enhancements in synapse-task.,5038
Rename Method,"OCSP/CRL Certificate Validation Feature for Synapse. Please find the implementation of the feature along with unit tests and a working sample in the attached ""certificate_validation_feature.zip"" file. This feature can be plugged into both NHTTP and Passthru transports. For more information please read the README.ThanksJeewantha",5039
Extract Method,MessageInjector task improvements In the current implementation of MessageInjector only main sequence can be invoked.When we want to invoke a different sequence other than the main we have to filter the message coming from the task at the main sequence and direct those messages to that sequence.It is good if we can have following functionality in the MessageInjector.* Ability to invoke a named sequence without going through the main sequence* Ability to invoke a proxy service,5040
Rename Method,Allow bookie garbage collection to be triggered manually from tests The current gc tests rely on waiting on a timeout for gc to run. It'snever certain whether it has run or not or if it's still running. This patch allows tests to trigger a gc run and gives the clienta future to know when it has completed. The gc algorithm is unchangedI but now it runs in a scheduled executor rather than as aThread.This work was originally done by Ivan Kelly and I am just pushing it back to open source,5041
Extract Method,Optimize handling of masterKey in case it is empty On each request client and bookies are exchanging the ledger masterKey which is a 20 bytes MAC digest of the ledger password.For each request there is a considerable overhead in allocating byte arrays when parsing the add/read requests. If the client is a passing an empty password we should optimize the data path to skip all allocations (related to the masterKey) and instead rely on a static byte array.,5042
Extract Method,add configuration support for BK As Ivan's comment on BOOKKEEPER-39 we use lots of system properties in BK now. It's better to use a proper configuration object to manager them.,5043
Extract Method,Message bounding on subscriptions In hedwig messages for a subscription will queue up forever if the subscriber is offline. In some usecases this is undesirable as it will eventually mean resource exhaustion. In this JIRA we propose an optional change to the subscription contract which allows the user to set a bound on the number of messages which will be queued for its subscription while it is offline.,5044
Move Method,Delay ledger directory creation until the ledger index file was created The index file creation is delayed until absolutely necessary after BOOKKEEPER-137. but we don't delay the creation of parent directories of index file it would hurt performance.,5045
Extract Method,Provide tools to read/check data files in bookie server We have written some tools to read/check data files (including index file journal files entry log files) in bookie server which helps user finding/debugging issues. would like to contribute them back.,5046
Extract Method,Separate write quorum from ack quorum There are use cases for bookkeeper that may require submitting add requests to a write set and returning upon receiving a confirmation from an ack set. The ack set must be a subset of the write set. An important special case is writing to all and returning upon hearing from a majority. Another important use case is avoiding *s* slow disks by writing to *f + s + 1* and returning upon receiving *f + 1* responses.Currently the write set and the ack set are the same for a ledger. Internal changes to support these cases include changes to LedgerHandle and PendingAddOp. We also need to add a call to the client API to accept different sizes for the write set and the ack set upon ledger creation.It is also open for the discussion the need to implement a new distribution schedule. So far it looks like we can reuse the round robin implementation we currently have. We would need to implement a new one if for example the initial bookie of an add operation must be always the same.,5047
Inline Method,Benchmarking improvements from latest round of benchmarking Improvements to the benchmark harnesses from the latest round of benchmarking which we have done.,5048
Extract Method,Hedwig: provide a subscription mode to kill other subscription channel when hedwig client is used as a proxy-style server. In some case we need to hedwig-client as proxy server to provide messaging service to other users.client -> proxy server 1 -> hedwig\> proxy server 2 />when client would connect to either proxy server to receive messages the proxy server would setup subscription channel to hedwig server.we just want client to be simple so when the channel between client and proxy server is broken client will try to connect to proxy servers thru VIP. it might connect to other proxy server. for example first time client connects to proxy server 1 but the client found the connection is broken it connects to proxy server 2. when proxy server 2 tried to setup subscription channel to hedwig hedwig found that this subscription has existed before occupied by proxy server 1.the panic here is that proxy server 1 only disconnect old subscription channel only when it detected the channel between client and itself is broken. The detection might be delayed due to several reasons. so it might increment the latency that messages are pushed to real client.so we try to introduce a subscription mode called CREATE_OR_ATTACH_OR_KILL mode.when a subscriber use this subscription mode it would kill old existed subscription channel. when using this subscription mode we would turn off auto-reconnect functionality in hedwig client and just tell client about the channel disconnected event so client could do its logic when channel is detected.in order to provide some admin tool for admin guys to debug/operate we provide ADMIN mode. if a subscriber attach to a subscription using ADMIN mode its subscription channel would never be killed then it is safe to guarantee admin operations.,5049
Extract Method,Create Bookie format command Provide a bookie format command. Then the admin would just have to run the command on each machine which will prepare the bookie env+Zookeeper paths (znodes):+- ledger's root path- bookie's available path+Directories:+- Journal directories- Ledger directories,5050
Extract Method,Add length and offset parameter to addEntry <from email to dev list>I'm having an issue with the LedgerHandle#addEntry api.[1] best illustrates it. I'm buffering namenode transactions in the stream and only transmitting when either flush is called or I have enough data to pass my threshold. This means I have a byte buffer in my class which I fill up as new transactions come in. When I transmit I set this buffer as an entry to bookkeeper. I.e. N whole namenode transactions will be contained in 1 single bk entry. The problem is this byte buffer (DataOutputBuffer in this case). I reuse the same buffer over and over. But this buffer has a fixed size. If I transmit before it is full the whole buffer size will be transmitted anyhow. If the buffer is being reused this will retransmit old transactions out of order. For example in the first use the buffer fills with [abcde] and adds this as an entry and resets the byte buffer. Then transaction f is added and flushed in this case [fbcde] is not transmitted. What I need is the ability to set offset and length in the byte[] passed to addEntry. Is there a reason this wasn't added in the initial implementation? If not and if you agree this is a valid usecase ill open a JIRA and add this functionality. Im getting around this now by doing an extra Array.copyOf which is less than ideal.</from email>,5051
Extract Method,Provide support for ZooKeeper authentication This JIRA adds support for protecting the state of Bookkeeper znodes on a multi-tenant ZooKeeper cluster.Use case: When user tries to run a ZK cluster in multitenant mode where more than one client service would like to share a single ZK service instance (cluster). In this case the client services typically want to protect their data (ZK znodes) from access by other services (tenants) on the cluster. Say you are running BK HBase or ZKFC instances etc... having authentication/authorization on the znodes is important for both security and helping to ensure that services don't interact negatively (touch each other's data).Presently Bookkeeper does not have support for authentication or authorization while accessing to ZK. This should be added to the BK clients/server that are accessing the ZK cluster. In general it means calling addAuthInfo once after a session is established,5052
Rename Method,CompositeException message is not useful Exceptions logged via slf4j don't actually have their toString method called so the current behaviour of overriding toString for CompositeException is rarely/never triggered in client code.Composing a better `message` field for CompositeException would make it loggable.,5053
Extract Method,Let hub server configure write quorum and ack quorum. since we support ack quorum in BOOKKEEPER-208 it would be better to let hub server could configure it.,5054
Extract Method,stopServingSubscriber in delivery manager should remove stub callbacks in ReadAheadCache Currently each subscriber would insert stub callback to wait newly published messages or scanning result. for waiting scan result it was OK. the callback would be triggered and removed after scan callback arrived. but if it was wait newly published it would be a problem. if sub/closesub/sub become frequent closesub doesn't remove the installed callback so the stub callbacks are accumulated which cause the memory increased finally OOM.it would be better to remove its installed stub callback when closesub.,5056
Extract Method,Avoid Journal polluting page cache writing data into journal which force the data in os buffer cache being used for hot reads which could have negative affect on performance.similar solution is as CASSANDRA-1470.,5057
Rename Method,allow application to recommend ledger data locality For application like hbase WAL it will be useful if application like hbase can give a hint to bk about application's preferred ledger location. In that way application can fail over to specific machines where one of the ledger replica is located; the recovery time will be faster. Another scenario is hbase's support for hot standby region server where read request can be served from a different machine other than the active region server. That requires the hot standby region server to read from ledger. If the ledger is on the same machine as standby region server the performance will be better.,5058
Extract Method,Stats for AutoRecovery Idea of this JIRA is to provide jmx interfaces to get the statistics of the auto recovery activities.,5060
Extract Method,Move fence requests out of read threads move fence requests out of read threads to address TODO.,5061
Extract Method,Add TryReadLastAddConfirmed API add TryReadLastConfirmed to read last confirmed without coverage checking as for readers which polls LAC they just need LAC.,5062
Extract Method,Reorder read sequnce We should reorder the read sequence base on location bookie availability for latency consideration.,5063
Extract Method,Recovery tool doesn't remove cookie after recovering one bookie we ran bookie recovery tool to recover bookie after encountered hardware issue. but the recovery tool doesn't remove the cookie after it finished recovery. so when we fixed the bookie and added it back to the cluster it couldn't start because the cookie exists on zookeeper but not on disks.we want the tool could delete the cookie after recover a bookie or maybe provide another tool to remove the cookie for a bookie.,5064
Extract Method,support journal rolling now bookkeeper is writing a single journal file so the journal file has no chance to be garbage collected and the disk space keeps growing.,5065
Extract Method,"change throttle in GarbageCollector to use either ""by entry"" or ""by byte"" Current bookie compaction in GarbageCollector has setting: 'compactionRate'. It is throttling and limiting the compaction by entries. But from a bandwidth perspective it would be good that we could throttle and limit compaction by bytes which would really reflect the bandwidth of disk. So in this enhancement we added another ""by bytes"" option when doing compaction in GarbageCollector:""boolean isThrottleByBytes"": true when use by bytes false when use by entries;""int compactionRateByEntries"": by entries number of concurrent entries;""int compactionRateByBytes"": by bytes number of bytes of entries before flush.",5066
Extract Method,Added versioning and flags to the bookie protocol There is no concept of versions in the BookKeeper protocol at the moment. This patch addresses that. ,5067
Extract Method,Add tracing and stats to OrderedSafeExecutor for debugging slow tasks Porting a change form the Twitter branch to improve stats and logging in OrderedSafeExecutorThese changes have been helpful for us in debugging latency issues in Bookkeeper server/client Summary of changes is* add a config option for op stats* add stats for task execution time task pending time* add a config option for logging a warning when an op takes longer than x micros* add toString implementations for submitted tasks so make it easier to track down slow ops* start using Builder for OrderedSafeExecutor* add a very simple test to make sure that the slow op logging path is exercisedMost of this came from Sijie originally with some changes from me,5068
Rename Method,Dispatch individual callbacks from journal in different threads Currently the journal is sending all the responses from a single thread after the entries in a batch are synced. Since a thread pool has been configured it is better to spread the send-response tasks to all the available threads.,5069
Extract Method,Read ledger entries from the bookie shell Bookie Shell should have a tool to read ledger entries from the bookkeeper cluster with optional arguments of startEntryId and endEntryId.Solution:We implement readEntries() in BookKeeperAdmin and return an Iterable. While iterating through it we fetch individual entries instead of fetching all entries at once. Also if the lastEntryId is not specified we read entries till we get a NoSuchEntryException,5070
Rename Method,Bookkeeper and hedwig clients should not use log4j directly Using log4j directly requires that any application using bookkeeper or hedwig clients have to configure log4j. We should use something like commons logging[1] or slf4j[2].[1] http://commons.apache.org/logging/index.html[2] http://www.slf4j.org/,5072
Extract Method,Create a generic (KV) map to store ledger metadata We have introduced ctime into ledger metadata through https://issues.apache.org/jira/browse/BOOKKEEPER-879.In the same token we would like to introduce createrId also. This can be a 128 bit UUID. Caller can write tools to group ledgers by createrId in the future we can even enhance this to run queries based on createrId. ,5073
Rename Method,Upgrade protobuf to 2.6 I had to update protobuf definition for some internal experiments and found that working with protobuf 2.4 is rather inconvenient. It cannot be installed with brew on mac and building it on mac always result is build errors hence leaves an option of switching to linux to run protoc.I decided to upgrade to 2.6 instead. It is compatible with 2.4 on wire and shaded so should not create any problems. All tests passed.Please ignore changes in java files in attached patch during review; these are auto-generated.,5075
Extract Method,Provide an option to delay auto recovery of lost bookies If auto recovery is enabled and a bookie goes down for upgrade or even if it looses zk connection intermittently the auditor detects it as a lost bookie and starts under replication detection and the replication workers on other bookie nodes start replicating the under replicated ledgers. All of this stops once the bookie comes up but by then a few ledgers would get replicated. Given the fact that we have multiple copies of data it is probably not necessary to start the recovery as soon as a bookie goes down. We can probably wait for an hour or so and then start recovery. This should cover cases like planned upgrade intermittent network connectivity loss etc. The amount of time to wait can be an option and the default would be to not wait at all(i.e. retain current behavior).Of course if more than one bookie goes down within a short interval we could decide to start auto recovery without waiting.,5076
Extract Method,Provide an option to add more ledger/index directories to a bookie Addition of new ledger or index directories to an existing bookie is disallowed via the cookie check in the bookie start path. Any attempt to add new storage is rejected and the bookie doesn't come up. We have a need to add additional storage to a bookie. This jira is for providing an option to add additional storage to a bookie.,5077
Rename Method,Assing read/write request for same ledger to a single thread When entries for the same ledger are processed by the bookie we should avoidthe reordering of the request. Currently if multiple read/write threads areconfigured the requests will be passed to the executor and writes for sameledger will be spread across multiple threads.This poses 2 issues:# Mutex contention to access the LedgerDescriptor# If the client receives add-entry acks out of order it has anyway to waitfor the acks of previous entries before acknowledging the whole sequenceto the application. In practice the reordering is increasing the latencyexperienced by the application.,5078
Extract Method,Bump zookeeper version to 3.5 in DL we need to leverage the asynchronous version of 'multi' in zookeeper. so this jira is to bump the zookeeper version to 3.5 to support async multi.,5079
Rename Method,Package datagen scripts with standalone SystemML distribution The random data generation scripts in scripts/datagen are a useful way for people to become familiar with SystemML assuming they generate data files in a manner that can easily be consumed by the algorithms being executed.Right now the scripts/datagen directory is not packaged into the standalone distribution. We should probably include this directory.,5080
Extract Method,Crc file created for each output file when run in standalone mode When a DML script is executed in standalone mode (e.g. ./runStandaloneSystemML.sh algorithms/datagen/genLinearRegressionData.dml -nvargs numSamples=1000 numFeatures=50 maxFeatureValue=5 maxWeight=5 addNoise=FALSE b=0 sparsity=1000 output=linRegData.csv format=csv) a crc file is created for each file.For example with the above script the following files were written to the file system:linRegData.csv.linRegData.csv.crclinRegData.csv.mtd.linRegData.csv.mtd.crcThis litters the file system with a lot of basically unnecessary files. If several scripts are executed the number of crc grows rapidly.There should be a mechanism for one of the following:1) don't generate crc files in the first place2) delete generated crc files3) have an option to generate crc files with option by default set to false,5081
Extract Method,"New Unary Aggregate Operations: ""Variance"" & ""Standard Deviation"" For numerical stability we normally compute standard variation (and variance) via the second central moment. However our central moment is currently limited to input vectors without support for matrices. This requires users to ""mis-use"" parfor for data-parallel computation as follows which causes unnecessary performance problems:R = matrix(0 rows=1 cols=n);parfor(i in 1:n) {R[1i] = sqrt(n/(n-1)*moment(A[i]2));}There are two ways of addressing this: generalize moment to matrices or add a new builtin unary aggregate for standard deviation or variance. We go with the latter because it is such a common operation. Note that the only reason why moment is not part of the unary aggregate framework is the scalar 'order' parameter (for standard deviation this is constant 2).",5082
Extract Method,JMLC API: Support for text analytics usecases I am working on text analytics use case (e.g. document classification entity extraction).I would like to use the JLMC interface at scoring time but can't find the right method in org.apache.sysml.api.jmlc.PreparedScript.For entity extraction I need features associated with every token in the document. In this case the features are conceptually represented as a table with 3 columns: - tokenID (Integer) - consecutive integer numbers representing the position of the token in the document (entity extraction is essentially a problem of classifying every token as Begin_Entity Inside_Entity Outside_Entity and hence the order of tokens in the document is important)- featureName (String): name of the feature for example whether the token is a capitalized word or the surface form of the token etc- featureValue (Integer): an integer in this case always 1 since I do not include features that are absent.For document classification the order of tokens in the document may or may not be important. In the simplest case assume the order is not important. For each document we just use the surface form of each token in the document as feature name and the number of times that surface form appears in that document as feature value. So the features are: conceptually represented as a table with 2 columns: - featureName (String): the surface form of the token- featureValue (Integer): the number of times the surface form appears within the documentEssentially for both use cases I would like to pass to JMLC a table with a schema where each column has a known basic datatype (I can think of String Integer Float Boolean). Is this possible ?,5085
Extract Method,Read matrix from URL The ability to read a matrix from a URL can increase usability of SystemML by eliminating the need to have already existing local or cluster data when invoking SystemML.For instance rather than doing something like:{code}wget http://example.com/ml/matrix.csv./runStandaloneSystemML.sh example.dml -nvargs X=matrix.csv{code}Someone could do:{code}./runStandaloneSystemML.sh example.dml -nvargs X=http://example.com/ml/matrix.csv{code}One way that this can be useful is to have a small set of example data for each algorithm sitting on a server. Documentation could show a single command that can be invoked for each algorithm that references the server-based example data. This would mean that a user can essentially copy/paste a single command from the documentation and it would run using the server example data.,5086
Extract Method,Cleanup exception handling apis/compiler/runtime 1) Remove unnecessary exceptions (e.g. various subclasses of CacheException DMLUnsupportedOperationException AppException)2) Fuse unnecessary deep exception hierarchies (e.g. DMLParseException)3) Reinvestigation uncaught vs caught exceptions,5087
Rename Method,Compare Performance of LeNet Scripts With & Without Using SystemML-NN This JIRA issue tracks the comparison of the performance of the LeNet scripts with & without using SystemML-NN. The goal is that they should have equal performance in terms of both accuracy and time. Any difference will be indicate areas of engine improvement.Scripts:* [mnist_lenet-train.dml | https://github.com/apache/incubator-systemml/blob/master/scripts/staging/SystemML-NN/examples/mnist_lenet-train.dml] - LeNet script that *does* use the SystemML-NN library.* [lenet-train.dml | https://github.com/apache/incubator-systemml/blob/master/scripts/staging/lenet-train.dml] - LeNet script that *does not* use the SystemML-NN library.*Current Status - Forced Singlenode:*Equal performance when running the scripts in standalone mode with the {{-exec singlenode}} flag 20GB of memory and using data inputs in the SystemML binary format -- see {{run.sh}} and {{perf.sh}} for information.Results:- Run #1:|| Script | Time (s) | Accuracy ||| mnist_lenet-train.dml | 2987.400704441 | 99.32% || lenet-train.dml | 2816.369435579 | 99.28% |- Run #2:|| Script | Time (s) | Accuracy ||| mnist_lenet-train.dml | 2847.790531812 | 99.16% || lenet-train.dml | 2950.520494210 | 99.18% |So same accuracy and same runtime in singlenode mode!*Current Status - Spark Local:*The two scripts now have the same performance in Spark local mode (non-singlenode) equivalent to the performance in forced singlenode mode due to the creation of only CP jobs!---To fully reproduce I basically created a directory placed the two attached bash scripts in it grabbed a copy of the NN library and placed it into the directory ran the examples/get_mnist_data.sh script from the library to get the data (placed into examples/data) then used the attached convert.dml to create binary copies of the data for both scripts then ran run.sh. Also I copied examples/data to the base directory as well. Adjust the {{EXEC}} and related variables in {{perf.sh}} to switch between standalone Spark memory sizes explain stats etc.,5088
Rename Method,Update MLContext Matrix 'as' methods to 'to' The MLContext Matrix class allows for easy conversion to different formats (DataFrames RDDs etc) using several 'as' methods. Updating these to be 'to' methods would more closely follow Spark's 'to' conversion method conventions.The documentation should also be updated.,5089
Inline Method,Add additional classes for MLContext Frame support Create and implement Frame classes to enhance MLContext API Frame support. Classes/enums to create include: Frame FrameFormat FrameMetadata FrameSchema and BinaryBlockFrame. Add support for these classes in MLContextUtil MLContextConvertUtil Script MLResults and any other related classes. Most code should be quite similar to the code for Matrix support.,5090
Extract Method,"Performance: Improve Vector DataFrame Conversions Currently the performance of vector DataFrame conversions leaves much to be desired with regards to frequent OOM errors and overall slow performance. Scenario:* Spark DataFrame:** 3745888 rows** Each row contains one {{vector}} column where the vector is dense and of length 65539. Note: This is a 256x256 pixel image stretched out and appended with a 3-column one-hot encoded label. I.e. this is a forced workaround to get both the labels and features into SystemML as efficiently as possible.* SystemML script + MLContext invocation (Python): Simply accept the DataFrame as input and save as a SystemML matrix in binary form. Note: I'm not grabbing the output here so the matrix will literally be written by SystemML.** {code}script = """"""write(train ""train"" format=""binary"")""""""script = dml(script).input(train=train_YX)ml.execute(script){code}I'm seeing large amounts of memory being used in conversion during the {{mapPartitionsToPair at RDDConverterUtils.java:311}} stage. For example I have a scenario where it read in 1493.2 GB as ""Input"" and performed a ""Shuffle Write"" of 2.5 TB. A subsequent stage of {{saveAsHadoopFile at WriteSPInstruction.java:261}} then did a ""Shuffle Read"" of 2.5TB and ""Output"" 1829.1 GB. This was for a simple script that took in DataFrames with a vector column and wrote to disk in binary format. It kept running out of heap space memory so I kept increasing the executor memory 3x until it finally ran. Additionally the latter stage had a very skewed execution time across the partitions with ~1hour for the first 1000 paritions (out of 20000) ~20 minutes for the next 18000 partitions and ~1 hour for the final 1000 partitions. The passed in DataFrame had an average of 180 rows per partition with a max of 215 and a min of 155.cc [~mboehm7]",5093
Extract Method,"Add api support for creating multiple resources in a single request Add the ability to create multiple resources of the same type by providing an array of resource properties in the http body.For example to create multiple service resources for the cluster named ""mycluster"":POST htp://myHost:8080/api/v1/clusters/mycluster/services[ {""ServiceInfo"" : {""service_name"" : ""PIG""}}{""ServiceInfo"" : {""service_name"" : ""OOZIE""}}{""ServiceInfo"" : {""service_name"" : ""HIVE""}}]",5095
Extract Method,role_command_order.json should not be at stack level In current stack definitions role_command_order.json is at the stack level.For example: HDP/2.2/role_command_order.jsonService definitions are all nicely separated into different directories like HDP/2.2/services/{HDFS|YARN} but not the role_command_order. It would be neater to separate role_command_order per service and would be very useful while adding a new service to Ambari.Looking for something as below- HDP/2.2/services/HDFS/role_command_order.json- HDP/2.2/services/YARN/role_command_order.jsonAmbari server while starting should merge all role_command_order.json and create dependencies accordingly.This is extremely useful for custom services which are potentially added after the cluster install.,5096
Inline Method,Principal creation for Active Directory accounts should be configurable The properties used to create accounts in an Active Directory related to principal creation should be configurable such that a user may specify the required fields and their values (with variable replacement).This may be done using a simple structure like XML or JSON however a template facility (like Jinja2) may be more useful since conditional paths may be built in. The template should be stored in the {{kerberos-env}} configuration.An example of a need for a conditional path in a template is related to _service_ accounts vs _user_ accounts. A _service_ account (such as nn/\_HOST@REALM) should have the {{servicePrincipalName}} field set to the service's principal where this value shouldn't be set for a _user_ account (such as hdfs@REALM).,5097
Extract Method,"Test principal and keytab required for service check should be created as part of kerberos service check action Intercept call to execute SERVICE_CHECK for KERBEROS service and (if necessary) create and distribute test user keytab.It will be hard coded that the test user information is taken from the ""smokeuser"" identity in the relevant clusters Kerberos descriptor.",5098
Rename Method,"Views: min and max version Add support for including optional min max or min+max Ambari version in view.xml. This can include exact versions or variables. A few examples:< min-ambari-version>1.7.0</min-ambari-version>< min-ambari-version>1.7.*</min-ambari-version>< max-ambari-version>1.*</max-ambari-version>On deploy ambari should validate the view meets the min/max specified and if not not deploy and produce an error in the log with the details (for example: ""view requires minimum ambari version 1.7"")",5100
Extract Method,Update ConfigurationResourceProvider to handle Kerberos Administrative Credentials as a special case Certain configuration settings need to handled in special-case scenarios. For example short-lived settings to be stored per request or session scope. Or secure data the must not be stored in the Ambari database.An example of this type of data is the administrative credentials used to manage a KDC server. This _configuration_ data is short lived (per session) and sensitive. Therefore it must be handled as a special case. To determine that a configuration request contains this data the {{type}} of the configuration is to be used. For this specific case a configuration {{type}} of *_kerberos_admin_identity_* will trigger the special case to secure and store the administrative credentials in a file. Ideally if the _session_ data was available (see AMBARI-8426) a session-based encryption key would be created and stored in session. That key would then be used to encrypt the data from this request. The encrypted data and key would then be retrieved from the _session_ decrypted and used as needed. ,5101
Extract Method,Create ability to disable protocols for https connections in Ambari. Create ability to disable protocols for https connections in Ambari. Port our patch from EAR-660 to 1.7.0 and trunk.,5102
Rename Method,Views: CapScheduler service endpoint for operator - Expose endpoint for operator setting- cleanup unused code- up'd version to 0.2.0,5103
Extract Method,FreeIPA Support in Ambari FreeIPA Is a powerful tool for unifying identity kerberos credentials across a cluster.A great value add for ambari would be to provide support for using FreeIPA to kerberize services. This would allow for 1) better HCFS interoperability because first class GID/UID is critical for certain file systems (GlusterFS Lustre and any other file system which uses kernel / FUSE apis for determining identity)2) better enterprise interoperability. Because of the fact that FreeIPA makes it easy to interop with different identity solutions (like active directory) it would make ambari easier to adopt for various enterprises.3) broadens ambaris scope. Now ambari could also allow people to setup the users of their clusters and at least some of the security features of their clusters all from one interface (no more manual handling of TGTs and such - it could all be done quite easily via the ambari UI which could make calls to underlying FreeIPA clients).,5104
Extract Method,Define Components in terms of xml We should define components in terms of xml rather than java code.,5105
Extract Method,View: Pig property checking should log more explicit failure If I incorrectly set the properties for the Pig view obviously I can't use the view. But the error is very cryptic.,5107
Extract Method,Call to fetch metrics takes over 20 seconds CPU is consumed by NagiosPropertyProvider and JMXPropertyProvider.For JMXPropertyProvider execution of ConfigHelper.isStaleConfigs() takes 10-15% of the time now. isStale could be easily cached with Guava Cache. isStale may be changed in 4 cases:1. Configuration change2. ConfigGroup created/deleted3. Start/restart report receiving4. Host registerationSo we could invalidate appropriate recorrd in cache in these points*API call*:{code}http://<server>:8080/api/v1/clusters/c1/components/?ServiceComponentInfo/category=MASTER&fields=ServiceComponentInfo/VersionServiceComponentInfo/StartTimeServiceComponentInfo/HeapMemoryUsedServiceComponentInfo/HeapMemoryMaxServiceComponentInfo/service_namehost_components/HostRoles/host_namehost_components/HostRoles/statehost_components/HostRoles/maintenance_statehost_components/HostRoles/stale_configshost_components/metrics/jvm/memHeapUsedMhost_components/metrics/jvm/HeapMemoryMaxhost_components/metrics/jvm/HeapMemoryUsedhost_components/metrics/jvm/memHeapCommittedMhost_components/metrics/mapred/jobtracker/trackers_decommissionedhost_components/metrics/cpu/cpu_wiohost_components/metrics/rpc/RpcQueueTime_avg_timehost_components/metrics/dfs/FSNamesystem/*host_components/metrics/dfs/namenode/Versionhost_components/metrics/dfs/namenode/DecomNodeshost_components/metrics/dfs/namenode/TotalFileshost_components/metrics/dfs/namenode/UpgradeFinalizedhost_components/metrics/dfs/namenode/Safemodehost_components/metrics/runtime/StartTimehost_components/metrics/hbase/master/IsActiveMasterServiceComponentInfo/MasterStartTimeServiceComponentInfo/MasterActiveTimeServiceComponentInfo/AverageLoadServiceComponentInfo/RevisionServiceComponentInfo/RegionsInTransitionmetrics/api/cluster/summaryhost_components/metrics/yarn/QueueServiceComponentInfo/rm_metrics/cluster/activeNMcountServiceComponentInfo/rm_metrics/cluster/unhealthyNMcountServiceComponentInfo/rm_metrics/cluster/rebootedNMcountServiceComponentInfo/rm_metrics/cluster/decommissionedNMcount&minimal_response=true&_=1400808845240{code},5108
Rename Method,Moving Clusters and Nodes container objects into controller Moving Clusters and Nodes container objects into controller. Currently they are part of client/entities.,5109
Extract Method,"Provide ability to rebalance HDFS Use case example: put cluster in maintenance add 5 nodes hit rebalance take out of maintenanceRebalance should optionally allow the user to override the ""threshold"" (a percentage value).",5110
Rename Method,Validate required fields including passwords for blueprint cluster creation For blueprint creation validate all non-password required properties have been set in the blueprint.For cluster creation via a blueprint validate that all required password properties have been set in configuration or that a 'default_password' property has been included in the request. Password properties can be set in the blueprint cluster or host group configurations or as part of the cluster create call as either cluster or host group properties.,5111
Extract Method,Allow custom configuration properties to be specified in blueprint Allow a user to specify custom properties in a blueprint. The UI currently allows custom properties to be set and these will eventually be exported to a blueprint.Currently if a property in a blueprint isn't specified in the stack configurations it is ignored.,5112
Extract Method,"Add stack extension support for pluggable services h1. Proposal on inheritance rules for schema ver 2.0 service metainfoh2. Package and repository informationPer-stack repository information will not be a subject of stack extension (just as it is done now). At the same time if osSpecifics section (containing per-service repo and package descriptions) is present at metainfo of inherited service it completely overrides appropriate information of a parent service. Deletion of inherited osSpecifics section is not possible because doing that makes no sense (service will not be able to install).h2. Command scripts and custom commands- Command script definition at child overrides parent's command script definition. - Custom command script definition in child overrides parent's custom command script definition with the same command name.h2. Hooks script files and templatesWe allow reusing python scripts and templates from parent stack(s). The limitation is that only full ""package"" directory may be overridden. Implementation of resolving hooks is pretty similar.How it is done: When reading stacks on startup ambari-server determines what service folders/""package"" folders/""hooks"" folders are missing (inherited from parent) and appropriately adjusts ServiceInfo. When sending execution commands server populates ""service_metadata_folder"" and ""hooks_folder"" variables (located at commandParams section) with appropriate paths.h2. To be done in a separate jira(s)A complete list of stack extension functionality that is not in scope of current jira (please feel free to append):- per-file overrides for scripts and templates. We get a list of stacks (search paths) from the server. We look up for every file we are running via PythonExecutor (hook script command script template file) at this list of stacks (starting from the child stack) . Files that are imported by scrips using ""import"" statements can not be overridden.- (maybe) packaging files that will be downloaded by an agent to tar files. So for every stack there will exist few ""package.tar"" files (one per service) and one ""hooks.tar"" file (one per stack). Every file is a separate download unit.- exposing files from the server to an agent (resolving authentification issues)- stack cache management (downloading stacks from the server to an agent on first use and cache invalidation)With proposed stack extension rules child stack metainfo should be lightweight enough in cases of minor changes (like adjusting repository information or package names or editing file templates).",5113
Extract Method,Run Various Requests in Parallel so that one does not wait on the other. Run stages that belong to different requests and affect different sets of hosts at parallel so that one does not wait on the other. So for example starting/stopping components on different hosts should be done in parallel if possible.,5114
Rename Method,Create new stack with Gluster support for 1.3.2 HDP version This feature extends the ability for Ambari to support an Hadoop Compatible File System outside of HDFS. For this stack definition we are introducing the use of GlusterFS but it provides a good road map for other Hadoop Compatible File Systems to also be able to leverage Ambari. The feature/patch does not remove the HDFS patch but simply provides an alternative when selecting the services within the installer.,5115
Extract Method,Add ability to MasterHostResolver to resolve by namespace at a time The upgrade behavior which figures out the restart order for the Namenodes needs to become namespace aware to support NN federation in Ambari.,5117
Extract Method,Infra Manager: hdfs upload support for archiving Infra Solr Upload exported document from infa solr to a specified hdfs server. hdfs configuration should be defined in the ambari-infra-manager.properties file for each jobs.,5121
Extract Method,"each service should be able to implement server actions package them add a jar to be loaded during EU xsi:type=""server_action"" tasks defined in EU/RU upgrade pack xml files currently can only classes from Ambari source code. This limites the server action that custom services / services from mpacks can perform. This JIRA proposes a way to allow each service to implement server action classes package them in a jar to be loaded during EU 1. Each service can have a server_actions (support stack inheritance) such as /var/lib/ambari-server/resources/stacks/HDP/2.5/services/ZOOKEEPER/server_actions. a) The dir contains jar for the server action classes. /var/lib/ambari-server/resources/stacks/HDP/2.5/services/ZOOKEEPER/server_actions [root@~ server_actions]# ll total 8 -rw-r--r-- 1 root root 7510 Oct 30 10:49 test_full.jar 2. upgrade pack can then invoke the server action as shown below <!-- server action test without specifying a service in the execution stage-->< group xsi:type=""cluster"" name=""TEST_A"" title=""Test abc""> <execute-stage title=""aaaa""> <task xsi:type=""server_action"" class=""org.apache.ambari.server.serveraction.upgrades.SATestWithoutService""/> </execute-stage> </group> <!-- server action test with a service specified in the execution stage-->< execute-stage service=""ZOOKEEPER"" component=""ZOOKEEPER_SERVER"" title=""Parame terizing Zookeeper Log4J Properties""> <task xsi:type=""server_action"" class=""org.apache.ambari.server.serveraction.upgrades.SATestWithService""> <summary>zkpr test</summary> </task> </execute-stage> ",5122
Extract Method,Should be able to switch the extension version to which a stack version is linked Currently the only way to switch the extension version to which a stack version is linked is to perform the following actions: * Stop the extension services * Reregister (delete) the extension services * Unlink the old extension version * Link the new extension version * Add the extension services back The REST API should allow us to do an update action on the extension link. In this manner if we were running an upgrade from HDP 2.6.1 to 2.6.3 (or something similar) which both use the HDP 2.6 stack version then we could perform the extension link switch as a manual step during the upgrade process.,5123
Extract Method,Create a new rest resource for handling LDAP (and possibly other) ambari configuration Develop support for handling LDAP (and possibly other ambari ralated) configuration as a rest resource.,5124
Extract Method,"Add Kerberos HTTP SPNEGO authentication support to Accumulo This is a subtask of AMBARI-14384 ""Ambari Metrics doesn't use SPNEGO to authenticate"".In a Kerberos enabled cluster with SPNEGO enabled on Hadoop APIs Ambari Metrics Collector web-console will be Kerberos HTTP SPNEGO enabled too. But Accumulo sinks clients of Ambari Metrics Collector currently do not support Kerberos HTTP SPNEGO authentication.e.g. /var/log/accumulo/tserver_<tserver-host>.debug.log:2017-06-06 23:07:39918 [timeline.HadoopTimelineMetricsSink] INFO : Received WWW-Authentication header:Negotiate for URL: http://<metrics-collector-host>:6188/ws/v1/timeline/metrics2017-06-06 23:07:39927 [timeline.HadoopTimelineMetricsSink] INFO : No live collector to send metrics to. Metrics to be sent will be discarded. This message will be skipped for the next 20 times.",5125
Rename Method,"Hosts tab: clicking on red badge should not toggle ""Alerts"" filter Clicking on the red badge in the Hosts tab should not toggle the ""Alerts"" filter on the Hosts page (clicking anywhere in Hosts tab should go to Hosts page with ""All"" selected).",5126
Extract Method,Change HostStackVersionResourceProvider to be able to install packages on single host not belonging to any cluster This improvement will create the possibility of preinstalling packages on a host before the host is added to a cluster for given stack and components. A new request parameter 'components' has to be added to HostStackVersionResourceProvider to be able to pass the list of components we want to install packages for.,5127
Extract Method,rolling restart datanode cluster name show null when rolling restart services ( eg:DataNode ) cluster name in audit log show null2016-09-13T17:29:22.033+0800 User(admin) RemoteIp(127.0.0.1) Operation(Request from server) RequestType(POST) url(http://localhost:8080/api/v1/clusters/amabri/requests) ResultStatus(202 Accepted) Command(RESTART) Cluster name(null)2016-09-13T17:29:22.042+0800 User(admin) Operation(_PARSE_.ROLLING-RESTART.DATANODE.1.1) Status(IN_PROGRESS) RequestId(15),5128
Rename Method,Controller marks nodes unhealthy upon command execution failures AMBARI-171 handles retries of commands on the agent side. This jira makes the controller aware of nodes where repeated tries of any command execution failed and marks such nodes unhealthy. The nodes are put back in the healthy state when the agent is restarted.,5129
Extract Method,Add ability to start and stop all services from Services page ,5131
Rename Method,Fix misnamed Zookeeper connect strings in Log Search Variables/properties holding zookeeper connect strings are misnamed as zk_host or zk_hosts which may be misleading. Variable / property names fixed.,5132
Rename Method,Support loading of logs to S3 Upload logs to S3 so it can be archived and analyzed offline.,5133
Rename Method,Ambari Should Suspend Alerts Notifications During Upgrade Ambari reports alerts and triggers notifications during a stack upgrade. In most cases alert notifications should be suppressed during the upgrade to prevent false positives. However some alerts such as those which don't related to the cluster should remain fully operational:- Host disk space- Upgrade not finalized- Ambari Server Performance,5134
Extract Method,"Provide Ability To Pass JPA / EclipseLink Properties to the DataSource There is currently no way to pass JPA or EclipseLink specific connection/DataSource properties in from {{ambari.properties}}. Although there exists {{Configuration.getDatabaseCustomProperties}} these are actually _driver specific_ properties and not properties for EclipseLink.For example if I wanted to pass in to the JDBC Driver {{foo=bar}} then I could set{{server.jdbc.properties.foo=bar}} and this will get translated into {{eclipselink.jdbc.property.foo=bar}}.However if I wanted to set any of the EclipseLink or JPA specific DataSource properties (see http://www.eclipse.org/eclipselink/api/2.6/org/eclipse/persistence/config/PersistenceUnitProperties.html) I would not be able to.Proposal is to add something similar to the custom driver properties:{code}server.persistence.properties.eclipselink.jdbc.batch-writing.size=25{code}Which could get translated into{code}properties.put(""eclipselink.jdbc.batch-writing.size"" 25){code}",5135
Extract Method,"Automate creation of Ambari Server proxy users (secure/non-secure clusters) principal and keytab setup of JAAS (secure clusters) The aim of this improvement is to automate the following: - creation of proxy users for Ambari server necessary for views (Files Hive Pig Tez etc) - creation of Ambari Server principal and keytab and setup of JAAS which is currently a manual step documented here: http://docs.hortonworks.com/HDPDocuments/Ambari-2.1.0.0/bk_Ambari_Security_Guide/content/_optional_set_up_kerberos_for_ambari_server.htmlIn case of a non secure cluster Ambari proxy user will be set up for the user account Ambari Server is running as. This is specified in *ambari-server.properties* by *ambari-server.user* and can be adjusted by running 'ambari-server setup'. Stackadvisor is responsible for configuring proxy users both for secure / non-secure cluster wizard or blueprint based deployments. Therefore in case of blueprint based deployments proxy users will be only created if ""config_recommendation_strategy"": ""ALWAYS_APPLY"" in Cluster template. The following proxy users will be configured by stackadvisor: {code} hadoop.proxyuser.${ambari_proxy_user}.groups=* hadoop.proxyuser.${ambari_proxy_user}.hosts=* hadoop.proxyuser.hcat.groups=* hadoop.proxyuser.hcat.hosts=* webhcat.proxyuser.${ambari_proxy_user}.groups=* webhcat.proxyuser.${ambari_proxy_user}.hosts=* yarn.timeline-service.http-authentication.proxyuser.${ambari_proxy_user}.hosts=* yarn.timeline-service.http-authentication.proxyuser.${ambari_proxy_user}.users=* yarn.timeline-service.http-authentication.proxyuser.${ambari_proxy_user}.groups=* {code} For a secure (eg. securityType=KERBEROS) cluster proxy user will be setup based on Ambari Server principal. A new identity 'ambari-server' will be added to default kerberos descriptor where principal name is specified which can be modified either in Kerberos Setup wizard screen or by submitting a custom kerberos descriptor in Blueprint case. By default principal name is: {code}ambari-server-${cluster_name}@${realm}{code} Generate principal & keytab is set in JAAS configuration file. Generation of Ambari Server principal and keytab can be enabled / disabled by setting config property *create_ambari_principal* = true / false in kerberos-env config. ('Create Ambari Principal & Keytab' on Keberos Setup wizard screen). This is enabled by default.There is a new functionality in Kerberos related handling of configurations recommended by StackAdvisor properties marked with delete flag by StackAdvisor are removed from configuration when running Enable Kerberos wizard. This is necessary to be able to remove old Ambari proxy users in non-secure mode.In a scenario where multiple Ambari servers are managing a single cluster only the _operation master_ Ambari server will be affected. All other Ambari server instances will need to be manually updated. Meaning the Ambari server keytab file will need to be manually distributed to the _other_ Ambari server hosts. Also the _other_ Ambari servers' JAAS files will need to be manually updated either by editing the {{/etc/ambari-server/conf/krb5JAASLogin.conf}} file or by executing {{ambari-server setup-security}} and selecting option #3 {{Setup Ambari kerberos JAAS configuration}}.",5136
Extract Method,Custom services need a way to integrate in the upgrade process Currently the upgrade is defined as a series of xml files specific to the current stack version and the target stack version. Each upgrade xml defines the overall sequence of the upgrade and what needs to be done for each service. Custom services need to be able to specify their upgrade process and how those steps fit into the upgrade process.,5137
Extract Method,Decrease the load on ambari database after cluster creation Decrease the load generated by queries that are periodically executed after cluster creation.Queries executed frequently:{code}SELECT t0.service_config_id t0.cluster_id t0.create_timestamp t0.group_id t0.note t0.service_name t0.user_name t0.version t0.stack_id FROM serviceconfig t0 WHERE ((t0.cluster_id = @P0) AND (t0.create_timestamp = (SELECT MAX(t1.create_timestamp) FROM serviceconfig t1 WHERE (((t1.service_name = t0.service_name) AND (t1.cluster_id = @P1)) AND (t1.group_id IS NULL))))) {code}{code}SELECT MAX(t0.skippable) MIN(t1.start_time) MAX(t1.end_time) t1.stage_id SUM(CASE WHEN (t1.status = @P0) THEN @P1 ELSE @P2 END) SUM(CASE WHEN (t1.status = @P3) THEN @P4 ELSE @P5 END) SUM(CASE WHEN (t1.status = @P6) THEN @P7 ELSE @P8 END) SUM(CASE WHEN (t1.status = @P9) THEN @P10 ELSE @P11 END) SUM(CASE WHEN (t1.status = @P12) THEN @P13 ELSE @P14 END) SUM(CASE WHEN (t1.status = @P15) THEN @P16 ELSE @P17 END) SUM(CASE WHEN (t1.status = @P18) THEN @P19 ELSE @P20 END) SUM(CASE WHEN (t1.status = @P21) THEN @P22 ELSE @P23 END) SUM(CASE WHEN (t1.status = @P24) THEN @P25 ELSE @P26 END) SUM(CASE WHEN (t1.status = @P27) THEN @P28 ELSE @P29 END) SUM(CASE WHEN (t1.status = @P30) THEN @P31 ELSE @P32 END) FROM stage t0 host_role_command t1 WHERE ((t0.stage_id = t1.stage_id) AND (t0.request_id = t1.request_id)) GROUP BY t1.request_id t1.stage_id HAVING (t1.request_id = @P33){code}{code}SELECT DISTINCT t0.task_id FROM host_role_command t0 hosts t1 WHERE ((((t1.host_name = @P0) AND (t0.role = @P1)) AND (t0.status = @P2)) AND (t1.host_id = t0.host_id)) ORDER BY t0.task_id{code}{code}SELECT DISTINCT task_id FROM host_role_command WHERE ((role = @P0) AND (status = @P1)) ORDER BY task_id{code}{code}SELECT COUNT(task_id) FROM host_role_command WHERE (status IN (@P0@P1@P2@P3@P4@P5)){code},5138
Move Method,Create a stack flattener We need to be able to flatten inheriting stacks into a single stack. It also generates the whole configuration for each role including the client role.,5141
Extract Method,"Add ability to restart all host components with stale_configs=true with one API request Provide ability to filter host components using a predicate that allows bulk ops like RESTART of stale configs with a simple API call.Example:{code}{""RequestInfo"": {""context"": ""Restart stale""""operational_level"": ""host_component""""command"": ""RESTART""}""Requests/resource_filters"": [{""hosts_predicate"": ""HostRoles/stale_configs=true""}]}{code}This API call should allow user to perform custom command restart on all host components with stale configurations.",5142
Rename Method,"Allow client to specify a ""context"" value for asynchronous requests This context value will be added to the associated ""request"" resource to give some context as to what the request was doing. The context value will only have meaning for asynchronous requests and will be ignored for synchronous requests. This is a request from the UI team.",5143
Extract Method,Make Hosts table update dynamically ,5144
Rename Method,Improve error checking for blueprint resource creation Improving error checking to avoid NPE errors on blueprint resource creation when request is missing,5145
Rename Method,Ambari API: Add additional query operator 'isEmpty' for categories Add additional query operators including 'isEmpty' to determine if a category contains any properties.For example: ?category.isEmpty()Should return true if there are no properties in the category named 'category'; otherwise it should return false.,5146
Extract Method,Ambari API: Support explicit predicate grouping Provide support for explicit predicate grouping using brackets.Example: a=1&(b=2|c=3),5147
Rename Method,Improve Agent Registration and Heartbeat json Improve data coming back from agent registration to include rpm queries more flexible way to add directories,5148
Extract Method,API support for cascade delete of a specified cluster We need to be able to allow the user to reconfigure the cluster if installation fails.It would greatly simplify UI logic if the API supported cascade delete of the cluster (delete the cluster and all of its sub-resources).This way the UI can simply issue DELETE on cluster and proceed as if performing a fresh cluster install. This minimizes code changes on the UI side to support AMBARI-1193.,5149
Extract Method,RU - Install repo should batch the distribution of bits to prevent a Denial-of-Service attack Distribution of bits will call yum commands simultaneously (in this case on 1300 hosts) and is bound to fail for large clusters.Batch the distribution of bits to prevent a Denial-of-Service attack.,5150
Rename Method,RU Improvements. Part 1* Important: Before running HDFS Finalize run a Server Action similar to FinalizeUpgradeAction that will confirm that all host components have been upgraded to the version. This action is to inform the user so it may fail and the user can skip. [Update] Just modify FinalizeUpgradeAction to do this work.* /api/v1/clusters/c1/hosts/host_name/host_components/component_name to show the current version field,5151
Inline Method,Add dispatch counter to JMX destination view make the dequeue counter be the number of messages acked,5153
Extract Method,allow a MessageTransformer to be registered with a producer or consumer to help transform a message going onto the bus or coming off the bus For example a user may wish to use ObjectMessage in their code - but in deployment use a TextMessage with XStream or JAXB as the marshalling.,5154
Extract Method,support selectors in virtual destinations to allow a message to be dispatched to multiple phyiscal queues if it matches the selector ,5155
Extract Method,support for BlobMessage interface to support in-band and out-of-band file transfer Some new API like this...{code}public class ActiveMQSession {// send a local file or stream over JMSpublic BlobMessage createBlobMessage(InputStream inputStream) {...}public BlobMessage createBlobMessage(File file) {..}// send a remote URL over JMSpublic BlobMessage createBlobMessage(URL url) {...}}public interface BlobMessage extends Message {// access the remote resource// or for local resources force creation of temporary file// so this resource can be parsed multiple times etcURL getURL();InputStream getInputStream();}{code}For further discussion seehttp://www.nabble.com/support-for-FileMessage--tf2641673.html#a7373916,5156
Rename Method,AMQ-975 provide a way of setting the timeToLive from the point in time the message is received by the broker to avoid clock sync issues The JMS default is to use timeToLive relative to the client send; which means things get converted to GMT and can suffer from clock sync issues.A useful alternative could be to set the timeToLive on a message relative to the broker's clock when it receives it. That way there's no need to rely on a properly sync'd clock.So we could either * use a new header - ActiveMQBrokerTimeToLive or something.* use a negative time to live value to indicate its the time to live relative to the broker (rather than relative to the client)Am wondering if the negative timeToLive value would break any existing software? (Currently we tend to ignore any TTL values less than or equal to zero),5157
Rename Method,provide a new simple API so end users can view available destinations and query them to find their statistics (such as queue depth etc) From the dev list..On 3/9/07 Dhawan Vikram (LNG-DAY) <vikram.dhawan@lexisnexis.com> wrote:> Hi> Is there a direct way using OpenWire Client API to get the number of> consumers attached to a Queue at a given point of time? I know it can be> done using JMX API but I was wondering if it can be done using Open Wire> client api.4.2 has a command agent...http://activemq.apache.org/command-agent.htmlwhich can be used for sending simple text commands to the broker and getting replies - such as to enquire about queue depths and the like.There's also advisory messages...so you can listen to the operation of the broker seeing new destinations consumers producers etc.However we've never yet wrapped up a little easy API folks can use on the client to easily view the available destinations and get their stats etc.Something like the following...{code}Collection<ActiveMQQueue> connection.findQueues()Collection<ActiveMQTopic> connection.findTopics()// look up stats like queue depth etcDestinationStats getDestinationStatistics(ActiveMQDestination);{code},5158
Extract Method,XStream message transformer that works in both directions Enhanced transformer that could be configured to work in both directions. It contains 'reverse' property which is false by default. In this case it works exactly as it is working now. In case that it is true it changes direction of transformation ... producer transforms from text to object messages and consumers transforms form object to text ... I needed it for use with the Rest API in order to intercept and transform messages submitted through POST method.,5159
Extract Method,"AMQ-1883 Maven plugin could locate activemq-data directory in target dir I'd like the Maven plugin to coerce the activemq-data directory to the build output (target) directory so that it will be deleted when doing a ""clean"". Useful for integration tests where we don't need the activemq-data directory to survive between broker instantiations.The attached patch will cause the activemq-data directory to be located in the existing configured ""outputDirectory"" which defaults to ""target"". This essentially changes the current behavior.",5160
Move Method,Make WebConsole run as a standalone WAR The WebConsole is a really nice thing however you should be able to run it as a standalone war connecting to a remote broker (another vm or another server). This improves the following (IMO):* stability of the broker itself as the web-console can f.e. eat up all available memory (try looking at a large queue) or do other bad things* deployment (many companys have a standard deployment process for war-files as well as preconfigured application server to put them into)* backward-compatibility (webconsole per se does not depend on a 4.2 broker it runs just fine with 4.1 and 4.0)* support for master-slave configurations with auto-failover (f.e. with a failover jmx syntax as well as the existing failover syntax for the jms connection)Related to this bug/improvement is the current mean behaviour of the web-console with non-default named brokers (sometimes using the web-console will end up in a second broker beeing started).,5161
Extract Method,AMQ Should Print a Warning (Or Throw an Exception) If Messages Are Received to a Non-Started Connection The following scenario can occur:(1) Create a javax.jms.Connection but don't start it(2) Create a MessageConsumer on the Connection and subscribe to a Queue(3) Send messages to the QueueIn this case (turning on trace logging) the messages will be received by the Connection and they will silently be discarded. To the poor bloke who forgot to call Connection.start() it looks like the messages have simply disappeared and the universe is torturing him. He then must endure the horror of tracing through his code trying to find out why some cruel god is consuming his messages only to discover six hours later that he forgot to call Connection.start() and thus the punchline of the cruel joke that is his life is delivered and he's prompted to thoughts of self destruction and spilling the blood innocents.To prevent this scenario it would be nice if AMQ printed a warning (or better yet threw an exception) indicating that activity is occurring on a Connection that has not been started. In this case the programmer could clearly see that he forgot to call Connection.start() happily add the line and continue on into the brave future!,5162
Rename Method,Display the establised Neteowork Connector Bridges via JMX ,5163
Rename Method,Allow to view connections and consumers in the WebConsole This patch enables viewing of open connections and active consumers on queues in the activemq web-console.It also contains a minor fix enabling the use of password protected connections to remote brokers.,5164
Inline Method,Exclusive consumers are now selected up front when the consumer gets registered. ,5165
Extract Method,Add the option for a backup channel that is already connected for the Failover transport By having an already established transport as backup in the fault tolerant transport if a failure occurs - then failover can be accomplished more quickly,5166
Rename Method,Improvements/Bug Fixes for LDAP Discovery Mechanism (LDAP Network Connector) These are much needed improvements and bug fixes for the AMQ-358. The original patch worked for only a subset of cases. I have updated the original network connector I submitted with the following features and fixes.- detection and handling of multiple LDAP entries pointing to the same upstream broker- anonymous binding support (not everyone wants to put their login credentials in an XML file!)- LDAP server failover support- general logging improvements- fix bug that only allowed a single discovered network connector from a broker- persistent search capabilities allowing a broker to stay in sync with the LDAP server entries (only works for LDAP servers who support the extensions defined in [draft-ietf-ldapext-psearch-03.txt|http://www.ietf.org/proceedings/01aug/I-D/draft-ietf-ldapext-psearch-03.txt]),5167
Extract Method,Improve DestinationViewMBean so that the sendTextMessage opersation allows optional username and password The DestinationViewMBean's sendTextMessage() method does not provide you with the option of specifying a username and password; therefore if authentication services have been enabled for the broker you're precluded from using the sendTextMessage() method.,5168
Extract Method,AMQStore - enable transactions to sync write by default Add an option parameter to enable transactions to sync write to disk by default,5169
Extract Method,JMX DestinationViewMBean.browse() returns CompositeData which does not expose the user properties other than as a String in addition some other properties should be exposed such as JMSXGroupID and JMSXGroupSeq,5170
Extract Method,TcpTransportServer.bind() should use serverSocket.setReuseAddress(Boolean.TRUE) for tests that restart brokers or for apps that require fast restarts using reuseAddress for server sockets makes sense.The default should be true with the option to control it through transport Options.This will help the robustness of DuplexNetworkMBeanTest which does a bunch of broker restarts to check for leaked MBeans,5171
Extract Method,AMQ-2046 ActiveMQTextMessage toString() method to call getText() Currently toString on newly consumed message shows text field has the null value because getText is never called.,5172
Extract Method,Shutdown broker if default message store cannot access the disk Default message store should behave the same as the journaled message store (see https://issues.apache.org/activemq/browse/AMQ-2038),5173
Extract Method,masterConnectorURI using failover We have a use case of configuring two pair of master/slave as follows:A-master/A-slaveB-master/B-slaveand for client(producer/consumer) we use failover://(tcp://A-master tcp://B-master) so A-slave and B-slave only is used to replicate the data their master receives. In this case we want to use failover://(tcp://A-master) as masterConnectorURI for A-slave similar to B. so that when A-master goes down and gets restarted A-slave will be able to reconnect so we don't need to do anything with A-slave when A-master is down and up.Attached patch is based on tags/activemq-5.2.0 intended to address this please review it and I will appreciate it can be applied to the trunk ( tags and trunk has the same previous file). Please let me know if you have any question regarding this. Thank youpatches are at https://issues.apache.org/activemq/browse/AMQ-2070AMQ-2070 and AMQ-2071 are related because if you cannot use failover as uri in masterConnectorURI and you start the slave before master you will have issue. failover takes care of that.,5174
Extract Method,Make pooled session implement XASession in case we need very specific management ,5175
Extract Method,Allow suppression of duplicate queue subscriptions in a cyclic network topology in a cyclic network of brokers (where each broker knows about each other) it is possible to have a cyclic graph and multiple routes across the network. this occurs because some brokers pick up the second order advisories that arise from a broker responding to an advisory from another broker. The result is that a consumer on one broker can manifest itself as multiple consumers on brokers across the network. Network priority gives precedence to the shortest route when it is configured. This enhancement would ensure that there is only one route for a given destination and makes the network more deterministic and a little simpler. With small numbers of brokers in the network this is often what you want.When topics are involved the duplication leads to duplicate messages so duplicates for topics are suppressed by default on trunk and will be in 5.3. For queues as the message goes to just one consumer there is no duplicate issue just some indeterminism in how a message is routed through the network. This indeterminism is a means of fault tolerance and can be a good thing so this feature is enabled via configuration for queues.see some more background on the topic case @ https://issues.apache.org/activemq/browse/AMQ-2030,5177
Extract Method,Forwarded message cannot be distributed to the original broker I have a simple cause which can cause dispatch problem:1. setup a network of broker1 broker2 bridged by multicast discovery2. make a producer send 5 msg to queueA to broker23. make a consumer to consume from broker1 queueA ( make it slow so it only consumer 1 msg) but make sure all 5 msg from broker2 are forwared to broker14. stop the consumer to broke1 restart it to consume from broker2 queueA5. the 4 msgs originally published to broker2 and forwarded to broker1 and has not yet been consumed will stuck on broker1 and will not forwarded to broker2 for the consumer to consume. Here is an solution: it checks forwarded to broker( eg broker1) to see whether it has any active consumer it will be able forward the message back to the original broker when there is no active consumer on the forwarded to broker.,5179
Extract Method,Improve scalability of stomp+nio transport Currently stomp+nio transport still uses one thread per client. Use selectors to minimize number of threads and improve scalability.,5181
Extract Method,separate thread pool per Usage requires lots of threads when separate MemoryUsage defined for each destination Have a system with couple of hundreds of queues configured each with separate MemoryUsage (separate SLA enforcements).Each (Memory)Usage has separate ThreadPoolExecutor (corePoolSize: 1 maximumPoolSize: 1 ...) used for notifying interested listeners when usage changes or drops below 100% which basically means that for each started queue with MemoryUsage additional thread is created e.g named:Main:memory:queue://queueX:memory Usage Thread PoolThis starts a few hundred threads which increase system load.Is it possible to share some thread pool for all MemoryUsages or somehow decrease number of threads required for usage monitoring?BTW executor will be created even when no listeners are registered on given Usage (it will run following runnable iterating over empty list in fireEvent()):Runnable listenerNotifier = new Runnable() {public void run() {for (Iterator<UsageListener> iter = listeners.iterator(); iter.hasNext();) {UsageListener l = iter.next();l.onUsageChanged(Usage.this oldPercentUsage newPercentUsage);}}};,5182
Extract Method,don't use Kaha for creation of temporary files The FilePendingMessageCursor uses Kaha as the persistent engine for non-persistent messages flushed to disk when memory limits run low. To reduce the use of file descriptors - use KahaDB instead,5184
Rename Method,AMQ-2904 Failover connection recovery needs a new command to indicate recovery completion that can gate dispatch on a recovered consumer Unconsumed messages at a consumer need to be rolledback on recovery as they can get redispatched in arbitrary order. see - https://issues.apache.org/activemq/browse/AMQ-2573As operations are in progress like a send transaction the rollback cannot happen till the send transaction commit completes so it must be async with the failover interruption. Dispatch needs to be gated on completion of the outstanding operations as it currently is with the resolution to AMQ-2573However there is the possibility that the broker starts to dispatch to that consumer/connection before recovery is complete and can block the receipt of messages the response to the send commit for example as the dispatch is waiting for the send to complete so that any unconsumed messages are rolledback in advance of dispatch. With asyncDispatch=false and optimizedDispatch it is possible to simulate this. The solution requires two wireformat changes An indication on a connection that it is recovering (this can be propagated to a consumer) and an indication that recovery is complete such that dispatch on a recovered consumer can complete. An additional AckMode AckRecoveryComplete could do it.Thus dispatch would be gated such that it cannot interfere with outstanding work that needs to be restored and completed inorder to correctly clear unconsumed and delivered messages.,5185
Rename Method,Make JDBC store resilient on broker sequence id order Currently if the message is sent in a transaction there's a chance that messages are added to the cursor out of order (regarding broker seq id). The problem with JDBC store is that it does message recovery based on this seq id which can lead to all kind of problems (such as orphaned messages in the database).The solution is to refactor JDBC store to use its own seq generator for recovering purposes and replace broker seq id with message id for all other operations,5186
Extract Method,Update client connections with information about a cluster of networked brokers Currently it is up to the client to decide which broker(s) it should connect to. It would be beneficial to allow clients to be informed of brokers joining/leaving a cluster of networked brokers and optionally load balance across them.,5187
Rename Method,Make fileserver app jetty-neutral Currently the app uses some classes from jetty-util. Also we'd want to trim the size of the war as it doesn't need all these jars in the WEB-INF/lib folder.,5188
Move Method,Localize Spring-related classes in a separate module We should try reducing Spring dependency on the broker core (and other modules) and move all Spring-related classes in a separate module.,5189
Extract Method,"NPE in WriteTimeoutFilter with nio - add support for soWriteTimeout to nio transport config that shows the problem:{code}< amq:transportConnectors><amq:transportConnector name=""openwire"" uri=""nio://0.0.0.0:61616?transport.soWriteTimeout=5000&transport.soTimeout=5000""/><amq:transportConnector name=""stomp"" uri=""stomp+nio://0.0.0.0:61618?transport.soWriteTimeout=5000&transport.soTimeout=5000""/>< /amq:transportConnectors>{code}In the event that a write times out with nio the npe appears in the log:2010-04-12 18:17:03159 ERROR org.apache.activemq.transport.WriteTimeoutFilter$TimeoutThread.run(WriteTimeoutFilter.java:177) [WriteTimeoutFilter-Timeout-1] - WriteTimeout thread unable validate existing sockets.java.lang.NullPointerException: nullIssue is nio transport doe not support narrow to a TcpBufferedOutputStream.class a little refactor and implementation is needed.",5190
Extract Method,"additional support for dynamic uri's in FailoverTransport In some environments would be very convenient to allow dynamic addition of transport uri's to a FailoverTransport. This is currently supported for a network of brokers but this doesn't help in certain scenarios.Here's one scenario that I'm interested in: a shared-storage master-slave configuration of brokers with multiple clients. Since only one master broker is active at a time (and the master is not aware of the other slave brokers) it cannot communicate new broker url transports to connected clients. If clients must use tcp/ip protocols (i.e. multicast/discovery isn't an option) then there's no way for a client to dynamically ""learn"" when new brokers have been deployed. You must update the client's configuration and restart all connections with an updated list of transport uri's.I have a patch which will allow FailoverTransport to read new transport uri's from a file. The file is only read during doReconnect() processing. So new processing is only driven when client has lost a connection/is establishing a new connection. To use the new feature use a failover uri like the following:failover:(tcp://localhost:61616tcp://localhost:61626)?updateURIsFile=/YourListOf/TransportUrisor evenfailover:()?updateURIsFile=/YourListOf/TransportUrisWhere the file contents would look like this:tcp://localhost:61616tcp://localhost:61626If a new broker is added to your configuration just append a new transport uri to that file.Patch includes a new test which tests this new feature -- FailoverUpdateURIsTest.java.Comments?",5191
Extract Method,Fanout: unable to apply parameters to discovered brokers There is currently no way to apply parameters to discovered brokers when using the fanout transport. The discovery transport allows this by calling {{DiscoveryTransport.setParameters(Map)}} in {{DiscoveryTransportFactory.createTransport(CompositeData)}}. For example the following URI would apply a connection timeout of 3 seconds to all discovered TCP transport brokers& nbsp;&nbsp;&nbsp;&nbsp;{{discovery:(multicast://default)?connectionTimeout=3000}}The corresponding fanout URI would not apply the connection timeout to discovered TCP brokers& nbsp;&nbsp;&nbsp;&nbsp;{{fanout:(multicast://default)?connectionTimeout=3000}} This functionality is requested as discovered brokers may become unreachable. In those circumstances the default 30 connection timeout for the TCP transport can cause a considerable delay.The attached patch is a proposed solution where both transports that use the discovery transport failover and fanout have a common code path in {{DiscoveryTransportFactory}} for creating a discovery transport. This code path sets the parameters consistently for both transports. (Although it relies on a new static method in {{DiscoveryTansportFactory}} and {{TransportFactory}} objects do not have static methods {{FanoutTransportFactory}} was already making a static call to {{DiscoveryAgentFactory}}). The patch also has two new test cases one for apply parameters and another for a minor fix where {{DiscoveryTransport}} should cache the added URI after the parameters are applied so that the same URI is removed in {{onServiceRemove()}}.This issue is created after discussion on the ActiveMQ Users discussion board& nbsp;&nbsp;&nbsp;&nbsp;[http://old.nabble.com/Applying-Parameters-to-Discovered-Brokers-td29239157.html],5192
Rename Method,Implement custom brokerId assignment strategy In network of brokers duplicate route detection is done by checking broker ids. After the restart the broker ids change which can cause duplicate routes and messages stuck due to reached ttl.We need to provide a mechanism for users to configure how their broker ids are assigned and make sure that they stay the same after the restart. Which will ensure correct duplicate route detection even after some of the broker in the mesh is restarted.,5193
Extract Method,Allow the option of a DLQ per durable subscription DeadLetterStrategy From https://issues.apache.org/activemq/browse/AMQ-2584 - with durable subscriptions sharing the DLQ there will be duplicate sends to the dlq if more than one durable sub rejects the message. These durables are suppressed provided they are not already acked in which case the duplicate can hang about. The audit=false option for the DLQ works around this but it begs the question can I know which durable subscription refused a message.To facilitate this having a DLQ pre durable sub is a nice option. It can use the clientId and subscriberName as the postfix so ACTIVEMQ_DLQ.ClientId-SubscriberName - If the subscriber changes subsequent messages can go do a different DLQ. These destinations would need to be manually deleted when no longer needed.This will require additional methods in org.apache.activemq.broker.region.policy.DeadLetterStrategy so will need to a version update.,5194
Extract Method,ActiveMQInputStream should allow to specify a timeout like MessageConsumer.receive() does When using ActiveMQInputStream you are not able todo some kind of polling consuming. As soon as you call ActiveMQInputStream.read() it will block until it receive a message. So it could block forever. Sometime it would be usefull to allow some kind of polling even when using a stream.,5195
Extract Method,Fire advisory when network bridge is starter/stopped ,5196
Extract Method,NetworkConnector initialization should be backed by an executor If there are many network connectors and a slow network starting serially means that the last network connector may have to wait for N slow connection establishment processes. If the connection initiation fails fast it is not really a problem as the start process can move on quickly.It should be possible to start network connectors using an executor such that they can start in parallel. Fast connections can be up an running immediately.,5197
Extract Method,"Stomp Frame should mask passcode header in toString output so it does not pollute the log Logging of stomp CONNECT frame includes the raw passcode. This should be masked with ""****""so instead of:{code}22011-03-18 09:38:30634 [38.171.43:40701] WARN TransportConnection - Failed to add Connection ID:xx.xx.38456-99-2:3 reason: java.lang.SecurityException: User name or password is invalid.2011-03-18 09:38:30634 [38.171.43:40701] WARN ProtocolConverter - Exception occurred processing:CONNECThost:big77accept-version:1.01.1passcode:barlogin:foo{code} it should be:{code}22011-03-18 09:38:30634 [38.171.43:40701] WARN TransportConnection - Failed to add Connection ID:xx.xx.38456-99-2:3 reason: java.lang.SecurityException: User name or password is invalid.2011-03-18 09:38:30634 [38.171.43:40701] WARN ProtocolConverter - Exception occurred processing:CONNECThost:big77accept-version:1.01.1passcode:*****login:foo{code}",5198
Extract Method,"Enable PropertiesLoginModule JAAS module to optionally cache values in memory Currently PropertiesLoginModule will load users.properties and groups.properties files on every connection. While this is OK in most cases in some (Stomp where connections come and go frequently) this can cause performance issues.We should provide configuration parameter ({{reload=false}} which will make module to cache values of these files in memory. The example config is here:{code}activemq-domain {org.apache.activemq.jaas.PropertiesLoginModule requireddebug=truereload=falseorg.apache.activemq.jaas.properties.user=""org/apache/activemq/security/users.properties""org.apache.activemq.jaas.properties.group=""org/apache/activemq/security/groups.properties"";};{code}",5200
Extract Method,"Cannot use <SslContext> tag in blueprint configuration Couldn't use the <SslContext> tag in blueprint configuration: setting a string attribute of the SslContext element resulted in the error ""Cannot convert String to Spring Resource type"" when I tried to use it in a blueprint file. It turns out that the Java source defines the property to be of Spring Resource type not String.Pulling the Resource type out of the api and pushing it into the implementations will keep the schema String type matching the api which is more natural and will work easily with blueprint.",5201
Rename Method,"Implement ""exactly once"" delivery with kahaDB and XA in the event of a failure post prepare With XA 2PC a camel route jms 'to' jdbc should ensure exactly once delivery to jdbc. In the event of a failure after prepare where commit to jdbc is done the jms message must remain with a pending ack till the commit outcome is relayed from the transaction manager.Current versions of geronimo will correctly retry the commit in a timer thread so activemq eventually gets the commit outcome after recovery. (btw: it looks like howl will not persist a commit outcome per NamedXAResource so after a failure of the TM it may consier the transaction completed and the message may still be pending need to check that!)At the moment ActiveMQ does a heuristic rollback after recovery which leads to message redelivery in error.With the fix an acked message remains pending awaiting the outcome. On commit the message is acked. On rollback the message is available for redelivery.",5202
Extract Method,Broker does not check for expired persistent topic msgs. When using topics with durable subscriptions where subscribers disconnect for a while there is no task that checks for expired messages on the durable subscription.In case where subscribers are disconnected for hours and message are still sent with a TTL it may happen that either the producer gets blocked (in case of flow control) or the broker runs out of persistent storage (no flow control) just because of msgs that are already expired.Similar to queues there should be a periodic task that checks for expired msgs on durable topic subs. This task should also be configurable using a property similar to [expireMessagesPeriod|http://activemq.apache.org/per-destination-policies.html].,5204
Rename Method,Make use of remote port in Connection MBeanNames optional useful when ephemeral range cycles quickly With fast connection close/creation (like stomp) the client side ephemeral port range can result in duplicate mbean names when close is async. Potential of failed mbean registration due to port reuse.{code}DEBUG ManagedTransportConnection - Failure reason: javax.management.InstanceAlreadyExistsException: org.apache.activemq:BrokerName=XXXXXXType=ConnectionConnectorName=stompViewType=addressName=/X.X.X.X_52170{code}Make the registration of an address type mbean configurable so that this case can be avoided. In the main it is handy to see the remote address in the mbean name so the default should be to use the remote port. Just the client Id (which default to the connection id will be used in the mbean name) when use of remote port is not allowed.,5205
Extract Method,Provide the ability for the RA to be given an existing ConnectionFactory If the RA could be given an existing ConnectionFactory the same connection factory could be reused for inbound and outbound messaging without requiring two different factories to be created.,5206
Extract Method,Enhance HTTP transport to support wire level Compression using GZip Provide a means of enabling HTTP compression using GZip for data sent and received over the HTTP and HTTPS transports. The is separate functionality form the Message level compression that can be used to compress only the Message body when configuring ActiveMQConnectionFactory.setUseCompression. The Http transport level compression would be enabled via a URI option such as:{noformat}http://localhost:8161?transport.useCompression=true{noformat},5207
Extract Method,STOMP 1.1 introduced the heartBeat header implemented by the inactivity monitor would be nice to have this option for stomp 1.0 Stomp 1.0 does not provide for an inactivity monitor. A client connect that stays idle will remain active on the broker indefinitely. With 1.1 the inactivity monitor has come into play in response to the heartBeat header. For 1.0 clients we need a way to indicate that there is a default heartBeat header so a broker readTimeout and no expectation of a writeTimeout.Providing a transport option for stomp like {{stomp://0.0.0.0:0?transport.defaultHeartBeat=50000}} would be nice. In the absence of a heartbeat header as in the stomp 1.0 case this default value would cause an InactivityMonitor with readCheck of 500 to be installed on each new broker stomp transport connection.Any client that remains inactive for more than 5 seconds will have their broker connection closed.,5208
Move Method,"Revert the Oracle jdbc adapter to a variant of the default jdbc adapter in place of the one supporting blobs The blob support does non atomic updates on a message add and is a little inefficient due to the need to insert and update the blob. With the latest ojdbc6.jar oracle drivers blobs can be used under the hood so the default jdbc adapter can work with oracle.Currently the following configuration will achieve this:{code}<persistenceAdapter><jdbcPersistenceAdapter dataSource=""#oracle-ds""><adapter><defaultJDBCAdapter><statements><statements longDataType=""NUMBER"" sequenceDataType=""NUMBER"" /></statements></defaultJDBCAdapter></adapter></jdbcPersistenceAdapter></persistenceAdapter>{code} where oracle-ds is a bean:{code}<bean id=""oracle-ds"" class=""org.apache.commons.dbcp.BasicDataSource"" destroy-method=""close""><property name=""driverClassName"" value=""oracle.jdbc.OracleDriver"" /><property name=""url"" value=""jdbc:oracle:thin:@localhost:1521:amq"" /><property name=""username"" value=""user"" /><property name=""password"" value=""pass"" /></bean>{code}This enhancement will make the Oracle adapter behave like this by default such that the following configuration will work{code}<jdbcPersistenceAdapter dataSource=""#oracle-ds"" />{code} and will not manipulate blobs directly.If blob support is necessary for backward compatibility for earlier drivers the blob adapter can be specified using the {{adapter}} element:{code}<persistenceAdapter><jdbcPersistenceAdapter dataSource=""#oracle-ds""><adapter><oracleBlobJDBCAdapter /></adapter></jdbcPersistenceAdapter></persistenceAdapter>{code}",5209
Extract Method,"Slave broker cannot be stopped in a JDBC Master/Slave configuration within OSGi A Blueprint container cannot be stopped while it is in the state ""Creating"" because both operations are synchronized in BlueprintContainerImpl.The impact is that a slave broker cannot be stopped. Fortunately before the broker itself is stopped first the OSGi services are unregistered which calls the configured OSGi unregistration listeners.This patch provides a class which is a OSGi service unregistration listener to allow to stop the database locker while it is blocked in the ""Creating"" state.",5210
Extract Method,"Failover transport: support priority urls In some use cases it's important to detect that ""local"" broker is available and force reconnecting to it.For example if we have url like{code}failover:(tcp://local:61616tcp://remote:61616)?priorityBackup=true{code}should try to backup ""local"" transport until it's ready and reconnect there when available.By default only the first url is considered ""priority"". If you want to further tune what urls are considered prioritized you can use something like{code}failover:(tcp://local1:61616tcp://local2:61616tcp://remote:61616)?priorityBackup=true&priorityURIs=tcp://local1:61616tcp://local2:61616{code}",5212
Inline Method,Dynamic Failover Sends Clients Resolved Host Names HelloWhen using dynamic failover the broker appears to resolve its transport connectors IP address to an available host which is then returned client and not the IP address. There are cases where it is desirable to have the IP address sent to the clients and not a resolved host.Having the broker return the exact address as it is shown in the transport configuration to dynamically configure clients is desirable.ThanksScott EShttp://fusesource.com,5213
Extract Method,Support doing non-blocking sends that uses an async callback that gets notified when the send has been received by the broker ,5214
Extract Method,add pluggable Policy which is fired in a background Timer to detect slow consumers for non-durable topics and kill them (maybe with a pre-warning of being slow a little before being killed) ,5215
Extract Method,useCompression on server side specially on network of brokers For WAN connections network of brokers needs to utilize bandwidth that's why if compression can be entered on server side this can be very usefull option.,5216
Extract Method,Output version number in started log line to be consistent AMQ logs when its starting and later when it has started. Only the 1st log line has the version number. It would be good to have the version number in the 2nd line as well.{code}[ main] BrokerService INFO ActiveMQ 5.6.0 JMS Message Broker (mybroker) is starting[ main] BrokerService INFO For help or more information please see: http://activemq.apache.org/[ JMX connector] ManagementContext INFO JMX consoles can connect to service:jmx:rmi:///jndi/rmi://localhost:1099/jmxrmi[ main] TransportServerThreadSupport INFO Listening for connections at: tcp://127.0.0.1:61616[ main] TransportConnector INFO Connector tcp://localhost:61616 Started[ main] BrokerService INFO ActiveMQ JMS Message Broker (mybroker ID:davsclaus.lan-54124-1345820752143-0:1) started{code}And for stopping the broker the version number is no longer included. That would be good to have to be consistent.{code}[ ActiveMQ ShutdownHook] BrokerService INFO ActiveMQ Message Broker (mybroker ID:davsclaus.lan-54124-1345820752143-0:1) is shutting down[cp://localhost/127.0.0.1:61616] FailoverTransport WARN Transport (tcp://127.0.0.1:61616) failed reason: java.io.EOFException attempting to automatically reconnect[cp://localhost/127.0.0.1:61616] FailoverTransport WARN Transport (tcp://127.0.0.1:61616) failed reason: java.io.EOFException attempting to automatically reconnect[cp://localhost/127.0.0.1:61616] FailoverTransport WARN Transport (tcp://127.0.0.1:61616) failed reason: java.io.EOFException attempting to automatically reconnect[cp://localhost/127.0.0.1:61616] FailoverTransport WARN Transport (tcp://127.0.0.1:61616) failed reason: java.io.EOFException attempting to automatically reconnect[ ActiveMQ ShutdownHook] TransportConnector INFO Connector tcp://localhost:61616 Stopped[cp://localhost/127.0.0.1:61616] FailoverTransport WARN Transport (tcp://127.0.0.1:61616) failed reason: java.io.EOFException attempting to automatically reconnect[ ActiveMQ ShutdownHook] BrokerService INFO ActiveMQ JMS Message Broker (mybroker ID:davsclaus.lan-54124-1345820752143-0:1) stopped{code},5219
Extract Method,Refactor IntrospectionSupport to avoid using java bean property editors Java bean property editors is slow and not thread safe to use. We at Apache Camel has removed its usage in the Camel type converter system.We should do the same in ActiveMQ. There is only a number of default converters needed that is fairly easy to implement by hand (eg String <-> Numbers Booleans etc.). This fixes some other issues with AMQ causing memory leaks in dynamic environments where you may run multiple brokers or hot deploy brokers.,5221
Extract Method,Make better use of commons-pool in activemq-pool Currently activemq-pool uses only a tiny portion of the functionality that's available in commons-pool opting instead to reinvent a lot of things that now exists there such as keyed object pools. We can refactor the current codebase to better use common-pool. This allows for easily adding features like enabling async checks for Connections that have idled out and removing them from the pool as well as adding more diagnostic methods to our API and using a well tested pooling backend instead of our own custom code.,5222
Rename Method,Job Scheduler Store Growth is Unrestricted When using scheduled delivery it is possible to grow the job scheduler store indefinitely. As no quota can be set on the size of this store a malfunctioning malicious or prodigious producer can easily consume all available storage with scheduled messages without any alerts being raised by the broker. If the operators do not have disk space monitoring in place outside of the broker the broker can become innoperable without warning.Provide a mechanism to set a usage quota for the job scheduler store. The mechanism should conform to the current resource quota model provided by SystemUsage as well as provide monitoring through JMX.I have attached a basic patch to add management enforcement and configurability to the size of the job scheduler data store. Any guidance on things I missed or did not account for would be greatly appreciated.While testing the size reporting in JMX I noticed that the he Kaha persistence adapter seems to calculate its size differently than the job scheduler store. It appears that the job scheduler store is reporting the size of the data files and index while the Kaha persistence adapter is only reporting the size of the data files. What is the reason for this difference? I noticed the difference because the broker was reporting a 33% usage of the job scheduler store (100MB limit) immediately on a clean broker startup.,5223
Extract Method,Use hawtbuf in activemq-client to make message properties and MapMessage body unmarshal more lazy Message properties and MapMessage body already use a lazy unmarshal strategy. We can use hawtbuf UTF8Buffer objects to unmarshal the bytes of a string properties or map value and only do the UTF-8 decode back to a String instance when needed. In some contrived test cases I was able to get an extra 100 msgs/sec through my consumers when they only read out a portion of the payload of large MapMessage instance.,5225
Rename Method,"PooledConnectionFactory should track Session checkouts and close associated resources When the user's code closes a Connection checked out from the pool I would expect activemq-pool to close Sessions MessageConsumers and MessageProducers that were created from it. Unfortunately activemq-pool only cleans up Sessions on Connection.close() when no one else is referencing the Connection (referenceCount == 0). This makes Sessions Consumers and Producers outlive the code that actually uses them thus leading to increased resource consumption and messages being trapped in prefetch buffers that are no longer monitored.Instead we should keep track of the Sessions that were created from each specific Connection checkout and close them when the borrowed Connection is closed.Otherwise we bump into situations like [SPR-10092|https://jira.springsource.org/browse/SPR-10092] when using Spring's DefaultMessageListenerContainer. In some cases DMLC ""forgets"" to explicitly close MessageConsumers and Sessions even though Connections are always closed but the pool doesn't take care of cleaning up associated sessions.",5226
Extract Method,JMX ObjectNames do not follow JMX Best practices to use a Hierarchical format The Current JMX ObjectNames result in a disjointed view from JConsole of the managed objects and services of the ActiveMQ Broker. By following best practices it will be easier to find and monitor all the endpoints associated with a Destination - for example.,5227
Extract Method,Messages with AMQ_SCHEDULED_DELAY do not respect transactions Currently delayed messages are delivered even if the session it was sent in is rolled back. According to http://activemq.2283324.n4.nabble.com/AMQ-SCHEDULED-DELAY-and-transactional-boundaries-td4658339.html this is because the message can be delivered far in the future and the transaction would take to long.I don't agree with that argument. The transaction can be short living. It is only the enqueuing of the delayed message in the broker that has to be part of the transaction. The delivery to the consumer is not part of the transaction anymore.e.g. consider the scenario in the following preudo code:while (application_runs)try{msg = session.Receive();session.SendDelayed(anotherMessage);if (random(5) != 0) throw exception;session.Commit();} catch { session.Rollback; }Currently a delayed message is sent for each retry. So we will get a lot more messages in the future as we would expect. When delayed messages would respect transactions just the successful ones would be enqueued. The other ones are rolledback with the transaction.,5228
Extract Method,"Broker-based redelivery plugin - support for maximumRedeliveries=""-1"" It'd be great if the RedeliveryPlugin would allow a policy configuration with maximumRedeliveries=""-1""",5229
Extract Method,Allow XAPooledConnectionFactory to be used from ee ( implement ObjectFactory [Queue|Topic]ConnectionFactory To easily bind a connection factory that is aware of the containers transaction manager(tm) the use of javax.naming ObjectFactory comes in handy.This allows the connection factory to create instances from a jndi lookup.Having the tm resolved from jndi at runtime makes sense to avoid wiring dependencies.Having the XAPooledConnectionFactory implement the ee type Queue and Topic connection factory interfaces makes such a factory useable.,5230
Extract Method,Expired Message check being done when its not really needed [Performance Issue] There are cases now where we are checking for expired Messages and DLQ'ing then and sending advisories when we don't really need to. One example is on destroy of a TempQueue we call purge in Queue which will end up removing all messages for the sake of freeing up their MemoryUsage but we end up processing expired Messages in this case which doesn't make a lot of sense since the destination is going away. More generally a do we really want to process expired on a Purge call?These expired Messages lead to advisories and DLQ'd Message that might not really need to be done.,5232
Inline Method,ActiveMQ should automatically restart if a Locker looses it's lock. Right now when a Locker looses its' lock the broker just stops. It should restart to retry acquiring the lock.,5233
Extract Method,Add EndpointCompleter functionality to ActiveMQ Camel component It will make it available for tools to autocomplete destinations names when creating routes.,5234
Rename Method,network connectors - new messageTTL and consumerTTL - split usage of networkTTL for mesh topology currently networkTTL in a networkConnector (default=1) means that a message can go one hop and demand (or info about a consumer) can go one hop.In a network (A<>B) messages and consumers can flow.In a linear network (A<>B<>C) networkTTL needs to be 2 for messages and consumers to flow two hops from A to C.In a mesh topology (A<>B<>C<A>) a networkTTL=1 for consumers makes sense because there is at most one hop. However for messages networkTTL > 1 is necessary if consumers need to hop around between brokers. Imagine a consumer on A which pulls messages to A from B then the consumer moves to C messages now need to hop again from A to C. This can repeat essentially messageTTL(networkTTL) needs to be infinite.With consumerTTL > 1 in a mesh managing demand for proxy (demand) consumers and proxy proxy consumers becomes very difficult.,5235
Extract Method,Provide a polling SlowConsumerPolicy that uses LastAck time on a sub The existing AbortSlowConsumer policy is event driven. It depends on a consumer slow event that is triggered when the prefetch is reached and there are subsequent dispatches.With prefetch=0|1 there still needs to be throughput to determine that the consumer is slow so one message can be pending if there are no new messages to sent to the destination.Providing an alternative implementation that will periodically poll consumers for their last ack time will be more deterministic. The slow advisory may never fire but the consumer will get aborted if it does not ack in a timely manner.if lastAckTime exceeds the max and there are dispatched messages it can be a candidate for removal.Optionally lastAckTime exceeding and no dispatched messages can be a way to remove idle consumers. Not sure if that is necessary.,5237
Extract Method,runtime configuration - allow selective application of changes to xml configuration without broker restart support on the fly configuration changes where appropriate.Via JMX it is possible to make changes but they don't persist.Via osgi we can restart the broker to pick up changes to xml configbut where it makes sense we should be able to apply changes on the fly.A first example would be the addition on a new network connector bythe addition of the relevant xml config (edit or copy over) that isin use by the broker.,5238
Extract Method,"runtime config - support addition of composite virtual destinations - forwardTo Add support for runtime additions of the form:{code}<destinationInterceptors><virtualDestinationInterceptor><virtualDestinations><compositeQueue name=""VirtualDestination.CompositeQueue""><forwardTo><queue physicalName=""VirtualDestination.QueueConsumer""/><topic physicalName=""VirtualDestination.TopicConsumer""/></forwardTo></compositeQueue></virtualDestinations></virtualDestinationInterceptor></destinationInterceptors>{code}currently composite dest runtime addition results in NPE:{code}Caused by: java.lang.NullPointerExceptionat org.apache.activemq.broker.region.virtual.CompositeDestinationFilter.send(CompositeDestinationFilter.java:53)at org.apache.activemq.broker.region.AbstractRegion.send(AbstractRegion.java:394)at org.apache.activemq.broker.region.RegionBroker.send(RegionBroker.java:442)at org.apache.activemq.broker.jmx.ManagedRegionBroker.send(ManagedRegionBroker.java:283)at org.apache.activemq.broker.BrokerFilter.send(BrokerFilter.java:147)at org.apache.activemq.broker.CompositeDestinationBroker.send(CompositeDestinationBroker.java:96)at org.apache.activemq.broker.TransactionBroker.send(TransactionBroker.java:307)at org.apache.activemq.broker.BrokerFilter.send(BrokerFilter.java:147)at org.apache.activemq.broker.BrokerFilter.send(BrokerFilter.java:147)at org.apache.activemq.broker.MutableBrokerFilter.send(MutableBrokerFilter.java:152)at org.apache.activemq.broker.TransportConnection.processMessage(TransportConnection.java:467)at org.apache.activemq.command.ActiveMQMessage.visit(ActiveMQMessage.java:751)at org.apache.activemq.broker.TransportConnection.service(TransportConnection.java:292)..{code}",5240
Rename Method,Added JMX metics for networks per destination ,5241
Extract Method,Add new mode to JMS Pool that allows for not caching producers The current JMS Pool creates a single anonymous producer instance for all requests to create a Producer. In some cases a user might want to have a separate producer instance created for each requestor. We will add a new option on the PooledConnectionFactory to have all PooledSessions create separate MessageProducers TopicPublishers and QueueSenders for each create call.,5242
Extract Method,"Improve performance of composite topic fanout and persistent asyncSend We have publishers publishing to a topic which has 5 topic -> queue routings and gets a max message rate attainable of ~833 messages/sec with each message around 5k in size.To test this i set up a JMS config with topic queues:TopicTopicRouted.1...TopicRouted.11Each topic has an increasing number of routings to queues and a client is set up to subscribe to all the queues.Rough message rates:routings messages/sec0 25001 14282 20003 14284 11115 833This occurs whether the broker config has producerFlowControl=""false"" set to true or false  and KahaDB disk synching is turned off. We also tried experimenting with concurrentStoreAndDispatch but that didn't seem to help. LevelDB didn't give any notable performance improvement either.We also have asyncSend enabled on the producer and have a requirement to use persistent messages. We have also experimented with sending messages in a transaction but that hasn't really helped.It seems like producer throughput rate across all queue destinations all connections and all publisher machines is limited by something on the broker through a mechanism which is not producer flow control. I think the prime suspect is still contention on the index.We did some test with Yourkit profiler.Profiler was attached to broker at startup allowed to run and then a topic publisher was started routing to 5 queues. Profiler statistics were reset the publisher allowed to run for 60 seconds and then profiling snapshot was taken. During that time ~9600 messages were logged as being sent for a rate of ~160/sec.This ties in roughly with the invocation counts recorded in the snapshot (i think) - ~43k calls. From what i can work out in the snapshot (filtering everything but org.apache.activemq.store.kahadb) For the 60 second sample period 24.8 seconds elapsed in org.apache.activemq.store.kahadb.KahaDbTransactionStore$1.removeAsyncMessage(ConnectionContext MessageAck).18.3 seconds elapsed in org.apache.activemq.store.kahadb.KahaDbTransactionStore$1.asyncAddQueueMessage(ConnectionContext Message boolean).From these a further large portion of the time is spent inside MessageDatabase:org.apache.activemq.store.kahadb.MessageDatabase.process(KahaRemoveMessageCommand Location) - 10 secs elapsedorg.apache.activemq.store.kahadb.MessageDatabase.process(KahaAddMessageCommand Location) - 8.5 secs elapsed.As both of these lock on indexLock.writeLock() and both take place on the NIO transport threads i think this accounts for at least some of the message throughput limits. As messages are added and removed from the index one by one regardless of sync type settings this adds a fair amount of overhead. While we're not synchronising on writes to disk we are performing work on the NIO worker thread which can block on locks and could account for the behaviour we've seen client side. To Reproduce:1. Install a broker and use the attached configuration.2. Use the 5.8.0 example ant script to consume from the queues TopicQueueRouted.1 - 5. eg:ant consumer -Durl=tcp://localhost:61616 -Dsubject=TopicQueueRouted.1 -Duser=admin -Dpassword=admin -Dmax=-13. Use the modified version of 5.8.0 example ant script (attached) to send messages to topics TopicRouted.1 - 5 eg:ant producer -Durl='tcp://localhost:61616?jms.useAsyncSend=true&wireFormat.tightEncodingEnabled=false&keepAlive=true&wireFormat.maxInactivityDuration=60000&socketBufferSize=32768' -Dsubject=TopicRouted.1 -Duser=admin -Dpassword=admin -Dmax=1 -Dtopic=true -DsleepTime=0 -Dmax=10000 -DmessageSize=5000This modified version of the script prints the number of messages per second and prints it to the console.",5243
Extract Method,Switch to using Proton's Event logic for detecting AMQP state changes We currently use a polling model to detect state changes in the proton engine when new data arrives. The recent update to Proton v0.7.0 allows us to switch to the new Event Collector model and remove the polling code. This change results in lower overhead when processing incoming AMQP frames and increases performance of the AMQP transport layer.,5245
Extract Method,Allow for changing logger levels via JMX Create a new MBean that is loaded if the Broker is running with Log4J as which would allow for changing the level of loggers via JMX to enable debug without needing access to the log4j.properties.,5246
Extract Method,"Queue; be able to pause/resume dispatch of message to all consumers It would be good to be able to pause/resume the dispatch of messages from a queue to the queues consumers.When the queue is ""paused"":- NO messages sent to the associate consumers- messages still to be enqueued on the queue- ability to be able to browse the queue- all the JMX counters for the queue to be available and correct.",5247
Extract Method,Add an in-memory JobSchedulerStore implementation For brokers that run with persistence disabled there currently no in-memory job scheduler store so an embedded broker without persistence can't have scheduled messages or do broker led redelivery without manually configuring in the normal JobSchedulerStore impl which requires a disk based store. We should add an in memory variant that is the default if persistence is disabled but scheduler support is enabled.,5248
Rename Method,MQTT clients using durable subscriptions on networked brokers received duplicates MQTT clients that create durable subscriptions and that operate in a network of Brokers can receive duplicate messages when they start failing back and forth between brokers in the network.We should investigate using Virtual Destinations under the covers instead of durable topics subscriptions as this is a known problem with the durable topic case. With Virtual Destinations the client would be subscribed to a Queue that would receive all messages sent to the target Topic.,5249
Extract Method,Destination should not have numerical suffix for single-dest perf tests The performance test module assumes that multiple destinations will be put under load and thereby assigns a numerical suffix to the destination name specified on the command line. Thus a producer/consumer configured to topic://foo will actually send/receive from topic://foo.0. This is annoying as when load testing a particular broker setup (such as with composite destinations) you need to be explicit about which destination is being targeted and it is not always possible to tweak the broker configuration.I propose that for single-destination tests no numerical suffix is added. Patch incoming.,5252
Extract Method,Update disk based limits periodically At the moment we set store and temp limits at broker startup based on the configuration and available space. It's possible that other artefacts such as logs can reduce available disk space so that our limits does not have effect. It'd be good to periodically check for the usable space left and adjust limits accordingly.,5253
Rename Method,Add means to dynamically allocate port number for integration testing using maven plugin Port numbers for connectors can be dynamically allocated using special port number 0 but there is currently no way for integration clients to determine the correct port number.Registering the connector URI's as maven project properties makes it easy to use the dynamically-allocated port numbers making integration tests safer to run on a shared server by eliminating the possibility of port conflicts.,5254
Extract Method,Add support for the BrokerView MBean to get the up-time in milliseconds Currently one can only get the broker's up-time as a formatted string via JMX. I need to be able to get the up-time in milliseconds as we're using DataDog as our monitoring tool (which doesn't understand the current up-time formatted string). The up-time formatted string will remain as-is.,5255
Extract Method,upgrade to karaf 2.4.1 matching camel seems more stable in the itests w.r.t to hangs at shutdown.,5256
Extract Method,Make some activemq jar executable and able to send/receive messages It would be nice to have basic verification/example tool builded directly in activemq-client.jar so that folks can do something like {code}java -jar lib/activemq-client-xxx.jar producerjava -jar lib/activemq-client-xxx.jar consumer{code},5257
Rename Method,Allow advisory messages to traverse a broker network Currently the filter applied to forwarding consumers is very restrictive. It will suppress all advisory messages. But only two types of advisory are used by the network bridge.Allowing the propagation of selective advisories like new connection advisories is handy for monitoring at the application level.,5258
Extract Method,When the JDBC database fails log all the next exceptions so we see the root cause of the database failure. ,5259
Rename Method,Time in queue statistics handy It would be very keen if the JMX console exposed queue statistics such as average length of time in queue for current messages longest/shortest time in queue for current messages average time in queue for serviced messages (as opposed to those waiting) longestlshortest time for same and a list of messages in the queue with when they were posted and how long they have been waiting.,5260
Extract Method,Add an option to virtual topic selector cache to enforce only a single selector at a given time At the moment the virtualTopicSelectorCache is pretty bare bones. we should allow configuration of the persist period the file location and some operational insight via JMX. We should also allow to configure the cache to a single number of selectors to enforce automatic clean-up for non-used selectors (and avoid large pileups of messages) and to ensure consistent usage across the queues that are used in the virtual topic.,5261
Rename Method,"Logging of ""Database ... is locked"" should be done on level DEBUG The SharedFileLocker tries to acquire a lock on the activemq lock file. Everytime it can not lock it outputs the logging message below at INFO level. On the slave it will try this forever till the master is down.So I propose we only log on DEBUG level so the messages do not fill up a log with a global default info log level.2015-04-07 12:35:36522 | INFO | Database .../activemq/data/lock is locked... waiting 10 seconds for the database to be unlocked. Reason: java.io.IOException: File '.../activemq/data/lock' could not be locked. | org.apache.activemq.store.SharedFileLocker | main",5262
Rename Method,Add the ability to get Message Size from a Message Store Currently the {{MessageStore}} interface supports getting a count for messages ready to deliver using the {{getMessageCount}} method. It would also be very useful to be able to retrieve the message sizes for those counts as well for keeping track of metrics.I've created a pull request to address this that adds a {{getMessageSize}} method that focuses specifically on KahaDB and the Memory store. The KahaDB store uses the same strategy as the existing {{getMessageCount}} method which is to iterate over the index and total up the size of the messages. There are unit tests to show the size calculation and a unit test that shows a store based on version 5 working with the new version (the index is rebuilt)One extra issue is that the size was not being serialized to the index (it was not included in the marshaller) so that required making a slight change and adding a new marshaller for {{Location}} to store the size in the location index of the store. Without this change the size computation would not work when the broker was restarted since the size was not serialized.Note that I wasn't sure the best way to handle the new marshaller and version compatibilities so I incremented the KahaDB version from 5 to 6. If an old version of the index is loaded the index should be detected as corrupted and be rebuilt with the new format. If there is a better way to handle this upgrade let me know and the patch can certainly be updated.,5263
Rename Method,AMQP: Add support for heartbeats and inactivity monitoring. After we update to Proton-J 0.9.1 we will be able to take advantage if the idle processing added in that release to send empty keep alive frames in order to keep idle connections active and detect dropped connections.,5264
Extract Method,ActiveMQSslConnectionFactory hardcodes the KeyStore type The issue is present in earlier versions but only 5.10.x and up are maintained right now. At the very minimum we should use KeyStore.getDefaultType() but even that is not sufficient as one may use a different keystore type (such as pkcs12 bks keychain on osx etc) and it should be configurable. The default should be getDefaultType as defined in java.security (which is by default jks so there shouldn't be any impact on current users).I am testing a patch should be able to commit it in a day or two.,5265
Rename Method,AMQP: Allow delivery transformer to fallback to lower level transformer when transformation fails If a client sends an AMQP that cannot be transformed using the configured transformer the broker shouldn't drop the message instead it should attempt to fall-back to a less aggressive transformer. An example would be a broker configured to use the JMS transformer and the incoming message contains a body consisting of a DescribedType. The JMS Transformer would fail as there is no direct way to map that into a JMS message type. We could in this case fallback to the Native transformer and still process the message. An OpenWire client for instance would just receive a BytesMessage while other AMQP clients would get the message in the form it was sent.This allows the message to round-trip for instance from AMQP -> OpenWire -> OpenWire -> AMQP (Broker network bridge) without losing its original payload or message properties.,5266
Rename Method,AMQP: Return a more complete Source when client looks up an existing durable subscription When a client is looking up an existing durable subscription to resubscribe we need to return a Source instance that contains as much of the original information used to create the durable sub. Things like noLocal flag and selector used should be returned to the client so that it can validate its request against the subscription that it is attempting to make and fail or otherwise respond if the old one does not match its expectations.,5267
Move Method,AMQP: Support transactions that span multiple session for a single TXN For some clients the ability to have a single TXN that spans multiple sessions is desired so that the TX Coordinator link can be opened in its own session and still allow for links in other sessions to send and receive inside the TX that the coordinator has declared. At the moment if a client does this it will appear to work in some cases but can leak some memory and any transacted sends from a session not associated with the coordinator link will not be rolled back a redelivered correctly.,5268
Extract Method,Support a single port for all wire protocols Both Apollo and Artemis support the ability to use a single port for all protocols and to have automatic detection for the protocol being used. It would be nice to be able to support at least a subset of this feature in the 5.x broker as well.Ideally we should at least be able to detect OpenWire MQTT STOMP and AMQP over a TCP SSL and NIO transport. Websockets and HTTP would be a bonus but could be more difficult to implement depending on how this could work with Jetty so that would take some investigation.This is especially useful in environments where having to open up several new ports can be difficult because of firewall and security restrictions.,5269
Rename Method,Improve performance of virtual topic fanout Virtual topics provide a nice alternative to durable subs. Each durable sub is modeled as a separate queue.There are performance implications however because a message has to be sent to each of the (fanout) queues. For a durable subs there is a single message in the journal and just index updates for each sub.To improve performance there are three ways to improve the comparison between virtual topics and durable subs.# avoid the disk sync associated with enqueue# do parallel enqueues to get the store batching writes# introduce message references in the journal to reduce the disk ioFor 1 introducing a transaction (either client side or automatically broker side) ensures there is a single disk sync on commit.For 2 using an executor to do sends in parallel allows the journal to batch as seen in AMQ-5077For 3 the implementation is a lot more involved; for recovery there needs to be a journal write per destination and reading the journal will require two reads because of the indirection. Tracking gc needs to be handled to ensure the referenced entry is maintained. In short this is a lot of work and will only be visible for large (> 8k) messages where the cost of a large v small journal write is noticeable. The messageId dataLocator provides an entry point for this work but considering that 1 & 2 combined can give a 3x improvement I don't think it is worth the effort (and added complexity) at the moment.,5271
Extract Method,Add remove(messageId) jmx operation to offline durable subscription Mirroring the queue remove jmx operaton have a jmx operation to remove a message from an offline durable subscription. Essentially force an ack for that subscription.Usage: browse to find the required messagId string then invoke remove(messageId),5272
Extract Method,Improve disk based limit configuration In AMQ-5393 a configuration option was added to update disk based limits periodically if the disk space shrank. It would be useful to improve this to allow disk based limits to also regrow in size to a maximum if disk space becomes free. Also it would be useful to be able to specify the limits as a percentage of the partition size instead of just an absolute value.,5273
Extract Method,AMQP: Add support for sending scheduled message using message annotations Add support for reading scheduled message instructions from specific Message Annotations that are mapped into values that work with the built-in broker scheduler feature.||Annotation Name||Description|||x-opt-delivery-time|Analogous to the JMS 2.0 delivery time message property. Value is set in millisecondssince the Unix Epoch.||x-opt-delivery-delay|Time in Milliseconds to wait before dispatching the message.||x-opt-delivery-repeat|Number of time to reschedule a message sent with a fixed delay.||x-opt-delivery-period|The time in ms to wait between successive repeats of a scheduled message.||x-opt-delivery-cron|A CronTab entry that controls how a message is scheduled.|,5274
Extract Method,Improve nio transport scalability NIO transport uses unbounded thread pool executor to handle read operation. Under large number of connections and load this could lead to large number of threads and eventually OOM errors. Which is the exact problem that nio transport is supposed to solve. Some work has been done in [AMQ-5480] to make this configurable but there's still more work to make it more robust. Creating a fixed thread pool with a queue in front gives much better results in my tests.Additionally the same thread pool is used for accepting connections ([AMQ-5269]). This can lead to the broker not being able to accept new connections under the load. I got much better results when experimenting with implementing acceptor logic directly and handling it in the same thread (without reintroducing the old problem). With these two improvements in place the broker accept and handle the number of connections up to the system limits.,5276
Extract Method,Max Frame Size Error exception shows incorrect values at times If the value is below the size of 1 MB the error message reads 0 MB instead of scaling down to bytes or KBs,5277
Extract Method,add XAConnectionFactory implementation ,5278
Extract Method,Allow advisory consumer prefetch configuration for network consumers In most places the prefetch value for advisory consumers is configured separately than the prefetch value for non-advisory consumers because by default advisory consumers are configured to optimize acknowledgements. However while the NetworkBridgeConfiguration allows setting prefetchSize it does not allow configuring advisory prefetch size. It would be useful to be able to configure the prefetch size separately for advisory consumers as well as the percentage of prefetch used for determining when to send back an ack (currently set to 75%),5279
Rename Method,KahaDB does journal recovery for last append in error in normal restart case On a normal restart - the journal is replayed from the last append location in error. Reporting some unnecessary info logging of the form{code}INFO | Recovering from the journal @1:503INFO | Recovery replayed 1 operations from the journal in 0.0 seconds.{code}Recovery is only required when the last append location is different from the recovery location.,5280
Inline Method,Add an option to time out connection attempts when blocked in ensureConnectionInfoSent In some rare cases client side stack traces show client can be stuck in ensureConnectionInfoSent wanting forever for a response from the broker. We will add an optional timeout to allow this operation to fail (defaults off).,5282
Extract Method,Add a command to synchronize durable subscriptions over a network bridge When dynamicallyIncludedDestinations are used for a NetworkConnector durable subscriptions are tracked (added/removed) in the connector. This allows the broker to create network subscriptions automatically on a remote broker to match the demand that is created and then to destroy the network subscriptions when no longer needed.The problem with this is that if the bridge is restarted or if the brokers are restarted the information about the durable subscriptions is lost. For example if the bridge is stopped and a local durable subscription is added this new durable will not be added to the remote conduit network conduit subscription on reconnect. Or if a local durable subscription is removed while the bridge is offline on reconnect the remote durable sub will not be cleaned up even if there are no more matching durables.To fix this we need to add a new OpenWire command to support the syncing of durable subscriptions when a bridge is restarted. The goal of this command is to re-add missing subscriptions and to clean up no longer needed subscriptions on reconnect.Note that this new sync option will only apply when dynamicOnly is false and conduitSubscriptions is true. (both of these are enabled by default),5283
Extract Method,Add methods for sending messages to topics or queues To simplify testing the EmbeddedActiveMQBroker (in activemq-junit) should have methods to send messages to topics and queues.,5286
Rename Method,Implement JMX destination query API In an environment when there thousands of destinations on the broker current way of exposing all MBeans and looking into them in the tools does not scale. We need to implement an API that can be used by tools to filter sort and page destinations in this scenario.,5287
Move Method,Durable sync over a network bridge should sync forced durable subscriptions The syncDurableSubs flag on a network bridge will cause durable subscriptions to be synced up on connection. However this only applies to real durable subscriptions. There's another option on a network bridge added in AMQ-6383 that allows forcing subscriptions over a bridge to always been durable. We should make sure that normal topic subscriptions are synced properly and that virtual consumer subscriptions are also synced properly if the the consumers are part of a destination that is configured to force durables.,5288
Extract Method,Support SSL configuration using JNDI Add ability to configure SSL parameters using a jndi.properties file. The current ApacheMQInitialContextFactory does not support configuring connection factories of type ActiveMQSslConnectionFactory.,5289
Extract Method,Update to Proton-J 0.16.0 Update to proton-j 0.16.0 and add support for answering senders who request link capability for delivery delay.,5290
Move Method,Update of AMQ C++ client Attached is a new update of the C++ client the zip-file contains the full source since the update is a major overhaul.,5291
Extract Method,Add new implementations of the writeUTF8 and readUTF8 methods Provide implementation based on Apache Harmony code and remove some code duplications,5292
Rename Method,several functions spelled wrong impairs code readability In the OpenWire marshalling code / scripts for Java s/Unmarsal/Unmarshal/.In OpenWireFormat.java WireFormatNegotiator.java and OpenWireFormatFactory.java s/negociat/negotiate/.Nitpicks yes but the cause of some wasted time looking around for functions that didn't exist.,5293
Extract Method,Startup performance improvement when log contains prepared transactions. I have a KahaDB that's performing a recovery on each startup. Digging deeper into it I've found that the issue is that the db.log contains prepared transactions. The MessageDatabase will discard those entries in memory however it does not remove transaction info from those messages (I believe that's by design). So on each restart the broker will find those entries and again discard them in memory. If I access the broker via JMX I can go to the prepared XAs and execute a clear on them one by one. When i restart my broker i don't have a recovery attempted again. Performing a remove operation for each message can be very time consuming so i'd like to introduce an optional parameter to allow all prepared XAs to be removed on recovery. Please see my forth coming patch with unit test.,5295
Extract Method,DestinationMap access inside Abstract Region readwrite lock does not need sync Using multiple virtual topic publishers there is unnecessary serialisation via the destination map. the read write lock introduced in AMQ-3454 is sufficient to guard access in this case and reads should operate in parallel. with many thousand destinations lookup can be expensive and the serialisation becomes apparent.,5297
Rename Method,Add support for TLS hostname verification Add support to the transport on both server and client side to configure if hostname verification is enabled or disabled,5298
Rename Method,"Suppress (optionally) warn logging of EOF or Reset exceptions when remote socket is closed when a load-balancer or health check pings any transport connector endpoint to verify that the broker is listening on a port; using socket.open/close any subsequent read failure is treated as an error and logged as a WARN. This makes sense in general b/c it is indicative of a rogue client. However when it is the norm ie: from a health check then the logs get filled with these worrying messages that are in fact expected. For the somtp transport where there is no protocol close method we already suppress EOF and connection reset exceptions. This improvement would make that the default behaviour for all tcp transports and allow it to be enabled when required via configuration: {code:java} < transportConnector warnOnRemoteClose=""true"" ..>{code}",5299
Rename Method,ActiveMQ reads lots of index pages upon startup (after a graceful or ungraceful shutdown) Hi. We noticed that ActiveMQ reads lots of pages in the index file when is starting up to recover the destinations statistics: [https://github.com/apache/activemq/blob/master/activemq-kahadb-store/src/main/java/org/apache/activemq/store/kahadb/KahaDBStore.java#L819] Nowadays in order to do that activemq traverse the storedDestination.locationIndex to get the messageCount and totalMessageSize of each destination. For destinations with lots of messages this process can take a while making the startup process take long time. In a case of a master-slave broker this prevent the broker to fast failover and does not meet what is stated on [http://activemq.apache.org/shared-file-system-master-slave.html.] {quote}If you have a SAN or shared file system it can be used to provide _high availability_ such that if a broker is killed another broker can take over immediately.  {quote} One solution for this is keep track of the destination statistics summary in the index file and doing so we dont need to read all the locationIndex on the start up. The code change proposed is backward compatible but need a bump on the kahadb version. If this information is not in the index the broker will fall back to the current implementation which means that the first time people upgrade to the new version it will still have to read the locationIndex but subsequent restarts will be fast. This change should have a negligible performance impact during normal activemq operation as this change introduce a few more bytes of data to the index and this information will be on checkpoints. Also this new information is synchronized with the locationIndex as they are update at the same transaction.,5300
Extract Method,Publish BrokerService as OSGi service If a local broker is started in karaf then it might come up later than user code that connects to it. This leads to unnecessary errors. If we publish the BrokerService as an OSGi service then the user can depend on this service and delay its startup until the service is present.,5301
Extract Method,Be nice to have a DestinationView.browse that could accept a message selector string It would be nice if the DestinationView MBean could implement a browse command that could accept a message selector to filter the messages that could be browsed.,5302
Extract Method,MapMessage to support nested Map objects to create a typesafe hierarchial message such as used on RV etc ,5303
Extract Method,support spool to disk for non-persistent topic consumers Rather than just blocking when RAM is full we could have a high-water mark where we start spooling messages to disk if there is not sufficient RAM to hold the messages.The good thing about this approch is that it avoids blocking the producers when RAM is full; the downside is that once spooling starts the producer will be slowed down to the speed of the disk spooling (as due to RAM exhaustion under steady state the producer will have to wait for the message to be spooled to disk so that it can evict it from RAM so that it can send the next message).Though the journal is quite fast so the slow down shouldn't be too many orders of magnitude (and is better than making things appear to 'lock up' while we wait for the slowest consumer to acknowledge more messages).,5304
Rename Method,allow asynchronous dispatch to consumers in the broker for non-durable topics We typically use the current thread in the broker to dispatch to all the available non-durable consumers for performance - as this hugely reduces the context switching and increases performance. However (see AMQ-688) sometimes this can cause one dead consumer to block a producer.Some folks may want to switch this strategy to use slower asynchronous dispatch with a thread pool to reduce the risk of blocking a producer at the expensive of lower performance,5305
Rename Method,AMQ-376 LDAP based authorization support Patch kindly added by ngcutura - discussion thread...http://www.nabble.com/LDAP-Authorization-tf1851705.html#a5344494,5306
Extract Method,Add support for prefetchSize = 0 This feature would enable to support following test case:2 servers are processing 3 submitted jobs with following processing times 10 min 1 min 1 min. This sequence should finish in 10 minutes as one service will pick up the 10 minutes job meanwhile the other one should manage the two 1 minute jobs. Since I cannot set prefetchSize=0 one of the 1 minute jobs is sitting in prefetch buffer and the jobs are processed in 11 minutes instead of 10.This is simplification of the real scenario where I have about 30 consumers submitting jobs to 20 consumers through AMQ 4.0.1. I have following problems:ï Messages are sitting in prefetch buffer are not available to processors which results in a lot of idle time.ï Order of processing is random. For some reason Job # 20 is processed after Job # 1500. Since senders are synchronously blocked this can result in time-outs.ï Some requests are real-time i.e. there is a user waiting so the system cannot wait so AMQ-850 does not fix this issue.,5308
Rename Method,Allow MessageEvictionStrategy to evict more than one MessageReference in evictMessage(LinkedList message) method For slow consumers every time a single message is added to a TopicSubscription where the pending message limit is reached a new call to evictMessage is made. To allow for more flexible and efficient means of evicting messages it would be nice to be able to evict multiple messages in one call to evictMessage. This allows new MessageEvictionStrategy implementations to evict based on age of messages (eg. evict all messages in the pending message list that are older than x ms) duplicate messages (evict all messages that are redundant based on newer messages currently in the pending message list) etc. As a single call to the evictMessage method may have the opportunity to reduce the size of the pending message list by more than one it means that the next message added to the TopicSubscription may not need to have to call the evictMessage again.,5310
Extract Method,allow messages to be copied moved or deleted using a JMS selector ,5311
Extract Method,ActiveMQ support for SSL authentication and authorization This patch adds new Transports Brokers and Plugins needed for authentication and authorization based on SSL certificates.It also adds a few unit tests for the mentioned classes.The new (or heavily modified) SslTransport SslTransportServer and SslTransportFactory classes allow for access to the underlying socket's need and want client auth settings. If a certificate is found it is set as the transportContext of the created connection.The JaasCertificateAuthenticationBroker uses the new CertificateLoginModule to authenticate certificates (this class is abstract to allow for different backends for certificate authentication a concrete class is TextFileCertificateLoginModule).JaasCertificateAuthenticationBroker also sets the security context's user name to that provided for the certificate by the login module. This allows for authorization using the existing authorization broker.,5312
Extract Method,Two TCP connection requirement for bidirectional message flow ... We noticed the following during our testing ....When a broker A establishes connection to broker B the message flow is unidirectional from A to B.This is a an issue for us: For example consider brokers associated with business critical services X and Y. There are many secondary services that either monitor/feed off of the messages coming from them.A FOO service would like to process messages going from X to Y. So in FOO's broker configuration we add X's name. However messages are not going to flow from X to FOO till X initiates a connection to FOO. It may not be desirable/possible to change business critical brokers' configuration for usage scenarios like this.TCP is bidirectional and asymmetry at connection establishment should not be translated to the higher level network connector. Is there a fundamental need/justification for this design that I may not be aware of ? Otherwise I would like to explore other design options.ThanksRegards- Sridhar Komandur,5314
Extract Method,Improved error reporting for SSL and transports. Changed Username for SSL cert to the DN The attached patch provides better error reporting for transport errors by reporting the host which was trying to connect.It also provides a toString for SslTransport to distinguish it from the regular tcp transport and includes some improved javadoc for the new SSL client certificate authentication feature.Additionally it changes the username reported through the JMSXUserId field to be the full distinguished name rather than the username mapped in the users properties file.,5315
Extract Method,add support for stomp+ssl ,5316
Extract Method,Fix sonar reported static code issues phase 1 ,5317
Inline Method,Reduce number of visiting metastore for job scheduler For KYLIN-3470 introduced cache for jobs' metadata it's also can be used in job scheduler to reduce the pressure on metastore,5318
Extract Method,Make calcite extras props available in JDBC Driver Like #KYLIN-3475  calcite can be configured in the server by `kylin.properties` while the JDBC Driver is closer with the real query sql generation and if calcite can be configured by jdbc client there would be more flexable to various situation. Like quoting as mysql or use mysql-specified SQL grammer.,5319
Extract Method,Fix potential thread-safe problem in ResourceTool Class org.apache.kylin.common.persistence.ResourceTool are called by other methods not only by command lines so it got this potential thread-safe problem regarding the static variable pathsSkipChildrenCheck if multiple threads are trying to modify this variable. !Screen Shot 2018-07-16 at 2.36.39 PM.png!,5322
Extract Method,Improve the cube building process when using global dictionary By current cubing process if the global dictionary is very large since the raw data records are unsorted it's hard to encode raw values into ids for the input of bitmap due to frequent swap of the dictionary slices. We need a refined process. The idea is as follows: # for each source data block there will be a mapper generating the distinct values & sort them # encode the sorted distinct values and generate a shrunken dict for each source data block. # when building base cuboid use the shrunken dict for each source data block for encoding.,5323
Extract Method,Fact distinct columns in Spark ,5324
Extract Method,For single column queries only dictionaries are enough A common use case for BI tools is as follows: # Firstly extract all of the values of a dimension column # Then select part of the values as filter condition. Previously query for the first step requires to hit all of the segments' cuboid data which may not be efficient especially when the segments occupy many regions. To use dictionary rather than cuboid data to answer this kind of queries will reduce the cost of many rpcs to hbase. Sample queries are as follows: {code} select A from T group by A {code} {code} select distinct A from T {code} {code} select max(A) from T {code},5325
Rename Method,Support prepare statement in Kylin server side Kylin use calcite as sql engine when a sql comes to Kylin server it requires to be parsed optimized code gen and then query Kylin's cube storage the previous 3 steps often take 50-150 ms to complete(depends on the complexity of the sql). If we support to cache the parsed result in Kylin server the 3 steps will be saved. The idea is to cache calcite's PreparedStatement object and related OLAPContexts in the server side when the prepare request comes with the same sql reuse the PreparedStatement to do the execution. Since the PreparedStatement is not thread safe so I planned to use ObjectPool to cache the PreparedStatement.(use apache commons-pool lib),5326
Move Method,Merge dictionary and statistics on Yarn Currently merge dictionary and statistics step is in kylin`s jvm  which causes a great burden on kylin. we should move this step on yarn.,5327
Extract Method,Improve cube size estimation for TOPN COUNT DISTINCT Currently Kylin has poor cube size estimation for TOPN COUNT DISTINCT. We should improve it then we can get a reasonable split num when cube building. ,5328
Rename Method,User interface for hybrid model Hybrid model is useful for model change. While now there is no entry for it from GUI this makes many users don't see such feature.,5329
Extract Method,Kill spark app when cube job was discarded Currently when we discard spark job the spark job will still running and when we restart JobServer the SparkExecutable will submit a new spark job. we should handle spark job as mr job.,5330
Extract Method,Performance improvement in FactDistinctColumnsMapper Currently FactDistinctColumnsMapper writes every cell to mapper output. In spite of mapper side Combiner we could do better de-dup using available mapper memory. The situation becomes worse after KYLIN-3370 because not only dictionary columns now it is every dimension column get written as mapper output. Suggest * For non-dictionary dimension column only write min/max value to mapper output.  ,5331
Extract Method,Support Kafka table join with Hive tables At this moment if the data source is Kafka only 1 table allowed in the data model. In some cases joining the kafka stream with hive lookup tables is expected,5332
Extract Method,"Enhance HQL of materializing views It is hard to maintain code of creating HQL such as: (""INSERT OVERWRITE TABLE "" + viewName + "" SELECT * FROM "" + tableName + "";\n"") it is liable to miss"";"" thus add an UT to prevent this situation.",5333
Extract Method,Add unit test for StorageCleanupJob ,5334
Extract Method,Add manager for user. ,5335
Extract Method,Add manager for project ACL. AclRecord need to be cached.,5336
Extract Method,Add a hook that can customer made test_case_data ,5337
Extract Method,Improve CI coverage The current CI aka BuildCubeWithEngine only test merge on the MR engine. The merge operation is not tested on Spark engine. Need to improve the test coverage.,5338
Extract Method,ResourceStore should add a API that can recursively list path. ,5339
Rename Method,Only clean necessary cache for CubeMigrationCLI Currently we simply clear ALL cache in CubeMigrationCLI. which will make a few of queries slower in prod env when we have many tables models cubes and migrate cube often.So we could only clean necessary cache for CubeMigrationCLI.,5340
Extract Method,"Support Kafka JSON message whose property name includes ""_"" So far Kylin doesn't support JSON message which has property name with ""_"" because that would be conflict with Kylin's logic. For example the JSON message is : {code} { ""user"" : { ""first_name"" : ""Tom"" ""age"" : ""20"" } } {code} When map this topic to a table the ""first_name"" is mapped to ""user_first_name""; When Kylin parse the message it separates by ""_"" and then try to find ""user"" -> ""first"" -> ""name""; as there is no ""first"" property an error is reported.",5342
Inline Method,Remove getKey/Value setKey/Value from Kylin's Pair. Pair has no semantic about key/value. And when serializing/deserializing Pair will both has first/second and key/value.,5343
Extract Method,Tolerate broken job metadata caused by executable ClassNotFoundException As Kylin evolves many executable class was renamed or deprecated. These lead to broken job metadata and ClassNotFoundException can be thrown from many places in the code. Better to tolerate these error metadata once for all.,5344
Extract Method,Refine Email Template for notification by freemarker ,5345
Rename Method,Metadata broadcast should only retry failed node In commit https://github.com/apache/kylin/commit/ecc01458c4ad361aaf863505884d51474a8fec9d Kylin starts to retry failed metadata sync event however it re-posts the failed event to all nodes. The retry SHOULD only apply to the failed node only.,5346
Extract Method,"Enable job retry for configurable exceptions In our production environment we always get some certain exceptions from Hadoop or HBase like ""org.apache.kylin.job.exception.NoEnoughReplicationException"" ""java.util.ConcurrentModificationException"" which results in job failure. While these exceptions can be handled by retry actually. So it will be much more convenient if we are able to make job retry on some configurable exceptions.",5350
Rename Method,Enlarge the reducer number for hyperloglog statistics calculation at step FactDistinctColumnsJob Currently only one reducer is assigned for hll stats calculation which may become the bottleneck for slow down this step. Since the stats for different cuboids will not influence each other it's better to divide the cuboid set into several and assign a reduce for each subset.The strategy of this patch is to assign 100 cuboids into a subset. And there's a upper limit of reducers for hll stats calculation. Currently it's 50.,5352
Rename Method,Use multiple threads to calculate HyperLogLogPlusCounter in FactDistinctColumnsMapper ,5354
Rename Method,Enable 'kylin.source.hive.flat-table-storage-format' for flat table storage format Flat table storage format is currently hard-coded as SEQUENCEFILE in the core-job/src/main/java/org/apache/kylin/job/JoinedFlatTable.java That prevents using Impala as a SQL engine while using beeline CLI (via custom JDBC URL) as Impala cannot write sequence files. Adding a parameter to kylin.properties to override the default setting would address the issue. Removing a hard-coded value for storage format might be good idea in and on itself.,5355
Rename Method,Support SQL Server as data source [KYLIN-1351|https://issues.apache.org/jira/browse/KYLIN-1351] has added Vertica as data source. Base on the work of KYLIN-1351 I'd like to enable SQL Server as data source of kylin.,5356
Extract Method,"Build the dict for UHC column with MR KYLIN-2217 has built dict for normal column with MR but the UHC column still build dict in JobServer. Like KYLIN-2217 we also could use MR build dict for UHC column. which could thoroughly release the memory pressure and improve job concurrent for JobServer as well as speed up multi UHC columns procedure.The MR input is the output of ""Extract Fact Table Distinct Columns"" the MR output is the UHC column dict. Because it is very hard build global dict with multi reducers I use one reducer handle one UHC column and allocate enough memory to the reducer. According to my test 8G memory is enough.",5359
Inline Method,Refactor CuboidScheduler to be extensible To allow other implementations like KYLIN-2727,5360
Rename Method,Move concept Table under Project Move concept Table under Project such that reloading table in one project won't affect cubes in another project.,5361
Extract Method,Log pushdown query as a kind of BadQuery User will want to know the past pushdown queries and possibly create model/cube to enhance their speed.,5363
Rename Method,Inspect available memory during MR job Sometimes it's hard to figure out the reason of OOM during MR job especially when Kylin runs on different environments.Hence it is helpful to have memory info during steps of setup/cleanup.,5365
Extract Method,"Pushdown non ""select"" query ",5366
Rename Method,New metric framework based on dropwizard With https://issues.apache.org/jira/browse/KYLIN-2721.We are plan to release a new metric framework. New metric is different hadoop metric and based on dropwizard . which has the following advantage:* Well-defined metric model for frequently-needed metrics (ie JVM metrics)* Well-defined measurements for all metrics (ie max mean stddev mean_rate etc)* Built-in pluggable reporting frameworks like JMX Console Log JSON We refactored QueryMetric with new metrics notice the exposed JMX MBeans have changed a little bit.A new tool called perflog is also introduced. Perflog traces call duration time and current active calls by recording them to metric system.Some snapshots of the new JMX MBeans can be seen in attachments,5367
Rename Method,Make HBase 1.x the default of master ,5368
Rename Method,Refine Spark Cubing to reduce serialization's overhead In Spark Cubing a lot of variables defined in driver and used in closures which cause extra serialization's overhead.Meanwhile remove the method of reading KylinConfig from HDFS.,5370
Extract Method,"Make default precision and scale in DataType (for hive) configurable currently these values are hard coded:{code:java}// FIXME 256 for unknown string precisionif ((name.equals(""char"") || name.equals(""varchar"")) && precision == -1) {precision = 256; // to save memory at frontend e.g. tableau will// allocate memory according to thisif (name.equals(""char"")) {precision -= 1; //at most 255 according to https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types#LanguageManualTypes-CharcharChar}}// FIXME (194) for unknown decimal precisionif ((name.equals(""decimal"") || name.equals(""numeric"")) && precision == -1) {precision = 19;scale = 4;}{code}",5371
Rename Method,Support RDBMS as data source From v2.0 Kylin's plug-in architecture makes it possible to have multiple data sources cube engines and storages. Some users ever aksed that whether Kylin support source data feeded from RDBMS like Oracle MySQL now it is possible to do that. Some tools like Apache Sqoop can easily export data from RDBMS to HDFS that would help Kylin get the data and then build that into cubes.,5373
Extract Method,Keep UUID in metadata constant When users reset or restore the metadata we should keep the UUID unchanged. Thus we can ensure the UUID as the only identifier of a metadata table. However those operations that read but not alter metadata should be allowed to copy UUID from metadata. To achieve this we add the resource filed `/UUID` into the excludes list of copyR and resetR method in ResourceTool.java which makes operations attending to copy or reset fields skip UUID. But for those read operations for example operations relating to backup we provide a overriding copy method that permit getting UUID from the metastable. Resolved. Please refer to:commit 071f3b92caccf56ed70c15147da32a9ef2538bffAuthor: auphyroc99 <454530524@qq.com>Date: Thu Jun 22 17:10:58 2017 +0800KYLIN-2676 Allow backup operation copying UUIDcommit b6cdab0eefc81dca79ec8eea0740de566a6cdb7dAuthor: auphyroc99 <454530524@qq.com>Date: Thu Jun 22 17:10:58 2017 +0800KYLIN-2676 Allow backup operation copying UUIDcommit 66dc5418ce9860103a67398256ee11a7e428d4a0Author: auphyroc99 <454530524@qq.com>Date: Fri Jun 16 15:32:33 2017 +0800KYLIN-2676 Add UUID into excludes listcommit 25861c3732ef5312debada5d7fba111df0ef7694Author: auphyroc99 <454530524@qq.com>Date: Fri Jun 16 15:32:33 2017 +0800KYLIN-2676 Add UUID into excludes list,5375
Rename Method,Route unsupported query back to query its source directly When Kylin cannot support a query due to lack of prepared mode and cube the query can be routed back to executing on its original source. It may integrate with Hive or SparkSQL or Drill or anything if so desired.There must be an interface defined first to allow plug-in of any ad-hoc query engine.This is a carry-on from KYLIN-742,5376
Extract Method,"Speed up prepared query execution BI tools use prepared query for function probing kylin should not execute such queries in standard way because it is too costly.It's still worth mentioning standard ""prepare-bindparameter-execute"" way of PreparedStatement is still not supported. By now kylin only support Prepared Statements WITHOUT parameters.",5377
Extract Method,"Refactor KylinConfig so that all the default configurations are hidden in kylin-defaults.properties Currently we ship a conf/kylin.properties file with a lot of configuration overrides. This is not a standard approach compared with other projects like hadoop or spark.It's better to have a kylin-defaults.properties file to hide all the default configurations users will only have to override necessary configurations in a blank kylin.properties.After the refactor a config might be override by the following precedence:1. KV in kylin.properties.override which is more of a ""secret feature"" never documented.2. KV in kylin.properties users are suggested to override configs here3. KV in kylin-defaults.properties readonly to users4. KV in KylinConfigBase readonly to usersThe refactor will be backward compatible",5378
Extract Method,Project level query authorization As we introduced ad-hoc queries in https://issues.apache.org/jira/browse/KYLIN-2515 we'll need to adjust query authorization as follows:Query authorization is encouraged to be set as project level. If someone is assigned READ permission on project then he has access to query all tables in the project regardless thru adhoc or cubesIf a user has READ permission on cubes but no READ permission on project. He can only issue queries only if the query can be satisfied by those cubes he has READ permission.,5380
Extract Method,"Prune cuboids by capping number of dimensions the scene like this: I have 20+ dimensions However the query will only use at most 5 dimensions in all dimensions so cuboid that contains 5+ dimensions(except base cuboid) is useless. I think we can add a configuration in cube which limit the max dimensions that cuboid includes. What's more we can config which level(number of dimension) need to calculate. in above scene we only calculate leve 12345. and skip level 5+ ============================= The dimension capping is turned on by adding dim_cap property in aggregation_groups definition. For example the following aggregation group sets the dimension cap to 3. All cuboids containing more than 3 dimensions are skipped in this aggregation group. {code:none} ""aggregation_groups"" : [ { ""includes"" : [ ""PART_DT"" ""META_CATEG_NAME"" ""CATEG_LVL2_NAME"" ""CATEG_LVL3_NAME"" ""LEAF_CATEG_ID"" ""LSTG_FORMAT_NAME"" ""LSTG_SITE_ID"" ""OPS_USER_ID"" ""OPS_REGION"" ""BUYER_ACCOUNT.ACCOUNT_BUYER_LEVEL"" ""SELLER_ACCOUNT.ACCOUNT_SELLER_LEVEL"" ""BUYER_ACCOUNT.ACCOUNT_COUNTRY"" ""SELLER_ACCOUNT.ACCOUNT_COUNTRY"" ""BUYER_COUNTRY.NAME"" ""SELLER_COUNTRY.NAME"" ] ""select_rule"" : { ""hierarchy_dims"" : [ [ ""META_CATEG_NAME"" ""CATEG_LVL2_NAME"" ""CATEG_LVL3_NAME"" ""LEAF_CATEG_ID"" ] ] ""mandatory_dims"" : [ ""PART_DT"" ] ""joint_dims"" : [ [ ""BUYER_ACCOUNT.ACCOUNT_COUNTRY"" ""BUYER_COUNTRY.NAME"" ] [ ""SELLER_ACCOUNT.ACCOUNT_COUNTRY"" ""SELLER_COUNTRY.NAME"" ] [ ""BUYER_ACCOUNT.ACCOUNT_BUYER_LEVEL"" ""SELLER_ACCOUNT.ACCOUNT_SELLER_LEVEL"" ] [ ""LSTG_FORMAT_NAME"" ""LSTG_SITE_ID"" ] [ ""OPS_USER_ID"" ""OPS_REGION"" ] ] ""dim_cap"" : 3 } } ] {code}",5381
Extract Method,Push 'having' filter down to storage We know push filter down to storage is good and have done that for 'where' filter. Is it possible to push 'having' filter down to storage as well?,5383
Extract Method,Correct reporting of HBase errors Whenever HBase error occurs metadata access fails user must see a clear error message saying it is HBase failing not Kylin.,5384
Extract Method,ResourceStore to support simple rollback Will be useful when updating multiple resources in an all-or-nothing fashion.,5385
Rename Method,Refactor DistributedLock The current {{DistributedLock}} could use some improvement:- A {{lockClient}} is unnecessarily required.- The {{watchPath}} is actually an on-unlock listener and the current name failed to make it clear.- Could add a blocking version of {{lockPath}} and that will ease use cases like KYLIN-2557 and {{GlobalDictionaryBuilder.lock()}}- Should add more javadoc on the interface,5387
Extract Method,Code refactor move data source statement to query module ,5388
Rename Method,"Number2BytesConverter could tolerate malformed numbers Some malformed numbers like ""0100"" ""100.1200"" currently does not work with Number Dictionary. And could be improved.",5390
Rename Method,Smooth upgrade to 2.0.0 from older metadata ,5391
Extract Method,Improve the sampling performance of FactDistinctColumns step The method putRowKeyToHLL() in FactDistinctColumnsMapper can be very slow when sampling rate is high. After carefully profiling we believe that it's performance can be improved by modifying it's hash method. At the same time we also found an algorithm that can estimate the row nums of each cuboid accurately with a lower sampling rate. I will share more test results and details of the algorithm once after this issue is done.,5392
Rename Method,add a configuration knob to disable spilling of aggregation cache Kylin's aggregation operator can spill intermediate results to disk when its estimated memory usage exceeds some threshold (kylin.query.coprocessor.mem.gb to be specific). While it's a useful feature in general to prevent RegionServer from OOM there are times when aborting this kind of memory-hungry query immediately is a more suitable choice to users.To accommodate this requirement I suggest adding a new configuration named -*kylin.storage.hbase.coprocessor-spill-enabled*- +*kylin.storage.partition.aggr-spill-enabled*+. The default value would be true which will keep the same behavior as before. If changed to false query that uses more aggregation memory than threshold will fail immediately.,5396
Rename Method,Report coprocessor error information back to client When query aborts in coprocessor the current error message (list below) doesn't carry any concrete reason. User has to check regionserver's log in order to figure out what's happening which is a tedious work and not always possible in a cloud environment. {noformat}< sub-thread for Query 4fb68974-de70-4f6e-a2ee-7048202e51a7 GTScanRequest 4d65f9bf>The coprocessor thread stopped itself due to scan timeout or scan threshold(check region server log) failing current query...{noformat}It would be better to report error message to client.,5398
Rename Method,Distinguish UHC columns from normal columns in KYLIN-2217 In current implement Kylin disable the whole feature of KYLIN-2217 when it founds that kylin.engine.mr.uhc-reducer-count>1. Actually we can distinguish UHC dictionaries from normal dictionaries and only leave the UHC dictionaries for job node to build.,5401
Rename Method,Create a branch for v1.5 with HBase 1.1 API Create a new branch for Kylin v1.5 compile with HBase v1.1 API.,5402
Rename Method,Improve resource utilization for DistributedScheduler Currently in DistributedScheduler we lock segment in JobService which will make the job of segment only schedule in jobServer that the job submitted and could not fully utilize the threadPool resource of all jobServers.For example we have two jobServer and the max concurrent jobs is 10 if we continuously submit 20 jobs to jobServer1 there will be only 10 jobs running at the same time not 20 and will no job running in jobServer2.,5404
Extract Method,"CuboidReducer has too many ""if (aggrMask[i])"" checks ",5405
Inline Method,Refine Table load/unload error message There is no exception handling in TableController so most of exceptions will not be found in kylin.log but kylin.out. The TableController should provide more useful messages and be stable when exception happens.,5406
Rename Method,"Reduce the size of metadata uploaded to distributed cache Currently each MR job uploads all the metadata belonging to a cube to distributed cache. When the total size of metadata increases the submission time (""MapReduce Waiting"" at Monitor UI) also increases and could become a significant problem.We could actually optimize the amount of metadata uploaded according to the type of job for example* CuboidJob only needs dictionary of the building segment* CubeHFileJob doesn't need any dictionary",5407
Extract Method,HyperLogLog codec performance improvement We have a cube with more than ten distinct count measure and use hll15 store the value we found it is too slow of HyperLogLogPlusCounter there are three methods will called frequentlly: merge/writeRegisters/readRegisters.I found in kylin-1.5.x add a parameter 'singleBucket' to store the only one bucket which can optimize base cuboid.However in other step of cuboid building it will slow down. I has modify the code to speed up the speed of three operation.,5408
Rename Method,Refactor CI merge with_slr and without_slr cubes Try to reduce CI time also prepare for snowflake cube in CI.,5409
Move Method,"Load Kafka client configuration from properties files The latest kafka source hardcode the connection properties(such as ""timeout.ms"" in source files it could be better if could refactor these properties into files.",5410
Extract Method,Refactor CI blend view cubes into the rest ,5411
Inline Method,"minor improvements on limit 1. deprecate kylin.query.max-limit-pushdown because there's already storage scan threshold. Any limit is ""good""2. simply enable limit logic and other minor refactors",5412
Rename Method,"Display reasonable exception message if could not find kafka dependency for streaming build Kafka is optional dependency for Kylin install. But is mandatory for streaming build. Currently if no KAFKA_HOME exported the build will show ""Error"" but without any more detail message. It's not convenient for new user.",5414
Extract Method,Snowflake schema support ,5415
Move Method,redesign the way to decide layer cubing reducer count currently the sizing algorithm does not leverage CubeStatsReader,5416
Extract Method,Reducers build dictionaries locally In KYLIN-1851 we reduce the peek memory usage of the dictionary-building procedure by splitting a single Trie tree structure to Trie forest. But there still exist a bottleneck that all the dictionaries are built in Kylin client. In this issue we want to use multi reducers to build different dictionaries locally and concurrentlywhich can further reduce the peek memory usage as well as speed up the dictionary-building procedure.,5417
Rename Method,Setup naming convention for kylin properties ,5419
Rename Method,Cannot support columns with same name under different table currently we implicitly assume all columns in the model have unique names for example in row key and aggregation group we use column name (without tablename) to identify each column,5420
Rename Method,"Enhance TableExt metadata metadata ""table_ext"" is not very elegant and difficult to manage. Here will enrich it.",5421
Rename Method,Mapper/Reducer cleanup() exception handling Or it could override the real exception happened in mapper() or reducer(),5424
Move Method,Move the partition offset calculation before submitting job ,5428
Extract Method,Scalable streaming cubing We try to achieve:1. Scale streaming cubing workload on a computation cluster e.g. YARN2. Support Kafka as a formal data source3. Guarantee no data loss reading from Kafka even records are not strictly ordered by time,5431
Move Method,"Add API to init the start-point (of each parition) for streaming cube Just like a normal cube need a ""partition start date"" on creation (which will be used as the start point of the first segment) a streaming cube also need a starting point of the topic; but it will be a collection of offsets (for all partitions). Otherwise Kylin has to build from the topic's very begining which may not be expected.",5433
Rename Method,TopN counter merge performance improvement Observed the reduce phase of cube build is slow when there is TopN counter. There should be room for performance improvement.,5434
Extract Method,Hive mr job use overrided MR job configuration by cube properties Currently user can also apply this property at cube level.Cube config kylin.job.mr.config.override.mapreduce.job.queuename=YOUR_QUEUE.But Hive job is invalid.,5435
Extract Method,Add API to check and fill segment holes The segments in a cube should be in sequence and no overlap and no hole (or say gap). But there might be holes (especially for streaming case in some exceptional case). Need API to report such holes and trigger the filling automatically.,5438
Extract Method,lookup table support count(distinct column) Now for dimension column on fact table we can write sql like 'select count(distinct columnName)...' but on lookup table it's not supported.need to add this.,5442
Extract Method,Send mail notification when runtime exception throws during build/merge cube Currently mail notification is only sent in the onExecuteFinished() method but no notification will be sent when RuntimeException throws that may cause user miss some important job build failure especially for some automation merge jobs. Sometimes job state update fails(the hbase metastore is unavailable in a short time) it will make the job always look like in a running state but actually it is failed should send mail to notify user.,5443
Inline Method,Make the creating intermediate hive table steps configurable (two options) In 1.5.3 for KYLIN-1677 Kylin changed the steps when pull data from hive as: 1) count the source table 2) create intermediate flat table with redistribution (reducer number is determined by step1)This works good for many cases but may not benefit for case like a view (which joins several tables) as the fact table.So it is better to keep the other option: 1) create intermediate flat table 2) count the intermediate table 3) redistribute it based on the output of step 2)Plan to make this configurable (system level & cube level).,5444
Rename Method,Allow bigint(long) as a partition date column In many cases hive tables may use long / bigint as date column. Currently Kylin only support 'yyyy-MM-dd' or 'yyyyMMdd' as date format. This jira is to support long / bigint format.,5445
Rename Method,"WebUI for GlobalDictionary Global Dictionary is introduced since v1.5.3. However there's no Web UI to config with Global Dict it's not convenience for users. The use case can be found in examples/test_case_data/localmeta/cube_desc/test_kylin_cube_without_slr_left_join_desc.json the ""dictionaries"" arrays. One Global Dict config may contains three elements ""column"" the column to generate dict ""builder"" the builder to build dict and ""reuse"" the column to be reused.",5447
Extract Method,Region server metrics: replace int type for long type for scanned row count ,5449
Move Method,Support Grouping Funtions Usually we used 'group by dim1 dim2' to fetch the metrics with every value of dims and we also want the summary metrics with the dim2 rolled up.This case could be resolved by union two query like:{code}select dim1 dim2 metric from table group by dim1 dim2union allselect dim1 'ALL' metric from table group by dim1{code}Now with the expression cube/rollup/grouping sets in calcite we can make the query sql more simple and more clearly:{code}select dim1 case(grouping(dim2) when 1 then 'ALL' else dim2 end) metricfrom tablegroup by grouping sets((dim1 dim2) (dim1)){code},5450
Extract Method,GlobalDictionary may corrupt when server suddenly crash Global Dictionary store data on hdfs directly and overwrite directly when data file updated. If the server crashed suddenly during writing file the data file may be corrupt and can't be recovered.To resolve this problem copy the data file into a tmp directory and copy back after the file is updated successfully. I'll post a patch later with this solution.,5451
Extract Method,More stable and functional precise count distinct implements after KYLIN-1186 After KYLIN-1186 we've gained the ability to count distinct Int type columns precisely.However the implements of KYLIN-1186 is not stable especially in 2.x-staging branch.The reason is that the measure's maxlength is used to allocate memory in 2.x version and the BitmapMeasure is hardcoded to 8MB in KYLIN-1186 causing OOM when cube building.To resolve this problem we have introduce precision on the bitmap measure such as bitmap(100) bitmap(10000) bitmap(1000000) meaning the measure could accept 100/10000/1M cardinality at most. This solution should be fine considering the reality if the count value over 1000000 the hyperloglog measure which produce approx. result should be acceptable.,5452
Extract Method,Make job engine scheduler configurable Today the job engine scheduler is a simple implementation; need abstract it to decouple the implementation,5453
Rename Method,Use KylinConfig inside coprocessor ,5454
Extract Method,"Improve performance of MRv2 engine by making each mapper handles a configured number of records In the current version of MRv2 build engine each mapper handles one block of the flat hive table (stored in sequence file). This has two major problems:# It's difficult for user to control the parallelism of mappers for each cube.User can change ""dfs.block.size"" in kylin_hive_conf.xml however it's a global configuration and cannot be override using ""override_kylin_properties"" introduced in [KYLIN-1534|https://issues.apache.org/jira/browse/KYLIN-1534].# May encounter mapper execution skew due to a skew distribution of each block's records number.This is a more severe problem since FactDistinctColumn and InMemCubing step of MRv2 is very cpu intensive in map task. To give you a sense of how bad it is one of our cube's FactDistinctColumnStep takes ~100min in total with average mapper time only 11min. This is because there exists several skewed map tasks which handled 10x records than average map task. And the InMemCubing steps failed because the skewed mapper tasks hit ""mapred.task.timeout"".To avoid skew to happen *we'd better make each mapper handles a configurable number of records instead of handles a sequence file block.* The way we achieved this is to add a `RedistributeFlatHiveTableStep` right after ""FlatHiveTableStep"".Here's what RedistributeFlatHiveTableStep do:1. we run a {{select count(1) from intermediate_table}} to determine the `input_rowcount` of this build2. we run a {{insert overwrite table intermediate_table select * from intermediate_table distribute by rand()}} to evenly distribute records to reducers.The number of reducers is specified as ""input_rowcount / mapper_input_rows"" where `mapper_input_rows` is a new parameter for user to specify how many records each mapper should handle. Since each reducer will write out its records into one file we're guaranteed that after RedistributeFlatHiveTableStep each sequence file of FlatHiveTable contains around mapper_input_rows. And since the followed up job's mapper handles one block of each sequence file they won't handle more than mapper_input_rows.The added RedistributeFlatHiveTableStep usually takes a small amount of time compared to other steps but the benefit it brings is remarkable. Here's what performance improvement we saw:|| cube || FactDistinctColumn before || RedistributeFlatHiveTableStep || FactDistinctColumn after||| case#1 | 51.78min | 8.40min | 13.06min || case#2 | 95.65min | 2.46min | 26.37min |And since mapper_input_rows is a kylin configuration user can override it for each cube.",5455
Inline Method,"Specify region cut size in cubedesc and leave the RealizationCapacity in model as a hint currently the cube region's size is determined in model's capacity(RealizationCapacity) it will enforce different realizations under the same model having uniformed region size. This is not wanted feature because cube builder might need to try out different region size to tune performance. I'm suggesting to add a field called ""region_size"" (float type 1.0 means 1G per region) in cube desc to explicitly indicate the cube's region size. The field ""region_size"" will not be included in cubedesc's signature calculation.when the region_size is not specified (this is the case for legacy cubes) the hint in model's capacity(RealizationCapacity) will be used to determine region size",5456
Rename Method,Support Hive View as Lookup Table If we use a view as a lookup table the cube building job fails when executing the 3rd step (Build Dimension Dictionary) with this log:java.io.IOException: java.lang.NullPointerExceptionat org.apache.kylin.dict.lookup.HiveTable.getSignature(HiveTable.java:72)at org.apache.kylin.dict.DictionaryManager.buildDictionary(DictionaryManager.java:202)at org.apache.kylin.cube.CubeManager.buildDictionary(CubeManager.java:166)at org.apache.kylin.cube.cli.DictionaryGeneratorCLI.processSegment(DictionaryGeneratorCLI.java:52)at org.apache.kylin.cube.cli.DictionaryGeneratorCLI.processSegment(DictionaryGeneratorCLI.java:41)at org.apache.kylin.job.hadoop.dict.CreateDictionaryJob.run(CreateDictionaryJob.java:52)at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)at org.apache.kylin.job.common.HadoopShellExecutable.doWork(HadoopShellExecutable.java:62)at org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:107)at org.apache.kylin.job.execution.DefaultChainedExecutable.doWork(DefaultChainedExecutable.java:51)at org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:107)at org.apache.kylin.job.impl.threadpool.DefaultScheduler$JobRunner.run(DefaultScheduler.java:130)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.NullPointerExceptionresult code:2,5457
Rename Method,Bring more information in diagnosis tool ,5459
Rename Method,Improve performance of converting data to hfile Supposed that we got 100GB data after cuboid building and with setting that 10GB per region. For now 10 split keys was calculated and 10 region created 10 reducer used in ‘convert to hfile’ step. With optimization we could calculate 100 (or more) split keys and use all them in ‘covert to file’ step but sampled 10 keys in them to create regions. The result is still 10 region created but 100 reducer used in ‘convert to file’ step. Of course the hfile created is also 100 and load 10 files per region. That’s should be fine doesn’t affect the query performance dramatically.,5460
Rename Method,"Count distinct on any dimension should work even not a predefined measure Currently count distinct on a dimension does not work:{""sql"":""select DATE'2015-07-18'  count(distinct country) as uniquecountry from pc_session e INNER JOIN pc_cal c ON e.part_date = c.cal_dt WHERE (part_date BETWEEN DATE'2015-07-18' AND DATE'2015-08-19') """"offset"":0""limit"":10""acceptPartial"":true""project"":""tracking""}""exception"": ""Can't find any realization. Please confirm with providers. SQL digest: fact table DEFAULT.PC_SESSIONgroup by []filter on [DEFAULT.PC_SESSION.PART_DATE]with aggregates[FunctionDesc [expression=COUNT_DISTINCT parameter=ParameterDesc [type=column value=COUNTRY] returnType=null]].while executing SQL: ""select DATE'2015-07-18'  count(distinct country) as uniquecountry from pc_session e INNER JOIN pc_cal c ON e.part_date = c.cal_dt WHERE (part_date BETWEEN DATE'2015-07-18' AND DATE'2015-08-19') LIMIT 10""""",5461
Extract Method,Persist some recent bad query ,5464
Move Method,Tool to dump information for diagnosis ,5465
Extract Method,Print version information with kylin.sh ,5466
Rename Method,When cube is not empty only signature consistent cube desc updates are allowed Currently when user update a cube desc he will be warned that if the update causes inconsistency existing segments might get purged. However users never knows ahead whether or not the update is consistent or not in other words users are not sure what are the effects if they change a specific cube desc field.For cube desc updates if the current cube is empty we'll radically allow desc updates even if it will change the signature. On the other side if the cube has existing segments we'll check signature consistency before allowing the update. A failure in signature consistency will return error message and no update will be performed.this will lead to a less-confusing interactivity with the user.,5467
Extract Method,"Support Hive client Beeline Some user ever mentioned that in their environment the ""hive"" shell client isn't available only Beeline is allowed; If Kylin can support using Beeline that would be a nice feature for him.",5468
Extract Method,Upgrade calcite version to 1.6 calcite is going to release 1.6 we'll upgrade to it as it contains bug fixes that blocks us,5469
Extract Method,query storage v2 enable parallel cube visiting Currently if a cube has multiple segments the endpoint coprocessor invocations for each segments are executed sequentially. We'll try to parallize it to see how it contributes to performance,5470
Extract Method,Check Kryo performance when spilling aggregation cache As shown by Mahone Kryo has performance issues at deserialization.Need to double check how that impact to aggregation cache spill and merge.,5471
Extract Method,Support Custom Aggregation Types Currently Kylin supports 6 basic aggregation measure functions:Min/Max/Sum/Count/Avg/DistinctCountBut there are also many other cases require to support more complicate measure expression for example:COUNT(CASE WHEN so.ft = 'fv' THEN soi.sc ELSE NULL END) or Sum(if...)Or even more complicated measures like TopN and RawRecordsTo open this JIRA to tracking further implementation,5473
Extract Method,Spill to disk when AggregationCache need too many memory In class GTAggregateScanner.AggregationCache we hard coded 10GB as max limit of AggregationCache. If exceeded exception will be thrown.We can leverage disk to ensure stable.,5474
Inline Method,FactDistinctColumnsJob to support high cardinality columns In FactDistinctColumnsJob's combiner and reducer it uses a HashSet to remove the duplicated values; But if a column's cardinality is very big say > 10 Million it may reports OutOfMemory error;It should be enhanced to support such case.,5479
Rename Method,Distinguish fast build mode and complete build mode currently BuildCubeWithEngineTest is responsible for building 4 test cubes for other test cases to use. we intend to build the cubes by stages(use incremental cube building and cube merging this meaning multiple rounds of MR jobs) so that we can make sure to cover both job engine's cube building and merging functionsOn the other hand sometimes when we're working on other modules we don't care how a cube is built we only need a queryable cube(like when we're developing query engine). This is when fast mode would be help full. In BuildCubeWithEngineTest fast mode every test cube will be built with a single segment this means only a single round of MR is required for each cube this will make BuildCubeWithEngineTest much faster.,5481
Rename Method,Support dictionary of cardinality over 10 millions Many use cases involve columns with high cardinality over 10 millions. Dictionary should be supported on these columns assuming mem is sufficient.,5482
Extract Method,"Allow ""YYYYMMDD"" as a date partition column Many hive tables use the format ""YYYYMMDD"" as an effective date column. However Kylin currently require the date format to be ""YYYY-MM-DD"". Support ""YYYYMMDD"" right away will save user's effort in many cases.",5484
Extract Method,Optimize the memory footprint for TopN counter The TopN implementation in StreamSummary is not perfect for Kylin as Kylin use a Double typed counter; In a big data scenario the possibility of multiple key having the same double counter is very low so the data structure may not be memory economic;,5485
Rename Method,ADD Streaming UI since we will enable streaming feature we also need to enable streaming module UI for user to use on web.,5486
Extract Method,Approximate TopN supported by Cube SpaceSaving (TopN algorithm) code could copy from https://github.com/addthis/stream-lib/blob/master/src/main/java/com/clearspring/analytics/stream/StreamSummary.javaWe don’t need the whole stream-lib but just one (or two) classes is enough. Make sure you give credit to stream-lib in class comment.In order to run SpaceSaving in parallel the TopN has to be merged using http://arxiv.org/pdf/1401.0702.pdf. No existing impl as I searched we have to implement ourselves.CheersYangFrom: Li Yang Sent: 2015_8_7_ 12:43To: DL-eBay-KylinSubject: Distributed TopN papersThe basic algorithm[1] https://icmi.cs.ucsb.edu/research/tech_reports/reports/2005-23.pdfIts application in distributed system[2] http://www.cs.utah.edu/~jeffp/papers/merge-summ-TODS.pdf[3] http://www.crm.umontreal.ca/pub/Rapports/3300-3399/3322.pdfCheersYang,5487
Inline Method,Support HBase in a separate cluster Currently Kylin assumes that HBase is deployed in the same cluster where Hive tables resides which is not necessarily the case.We should support Kylin to write cubes to HBase in another cluster.,5490
Rename Method,Allow user to configure the region split size for cube Kylin hardcoded the region split size in RangeKeyDistributionReducer.java; It will use 10G 20G and 100G as the split for a Small/Medium/Large cube; Besides it set a limit on max region count to 500; Hardcode is not good; we should externalize these values to config file.,5491
Rename Method,Growing dictionary in streaming is potentially memory consuming for slowly increasing dictionaries ,5495
Rename Method,"Add ""retention_range"" attribute for cube instance and automatically drop the oldest segment when exceeds retention Sometimes user want to only keep a certain range data in cube for example only 1 month; Kylin should allow define such a ""retention_range"" at cube instance level; When a new segment is built check and drop the oldest segment from the head if the retention is exceeded;",5496
Move Method,kylin performance insight [dashboard] enable front end dashboard show kylin performance data,5497
Extract Method,Hybrid model for multiple realizations/cubes Scenario: users have a cube already built with the history data; Now they create a new cube which adding more dimensions; Due to some reason user expects both the history cube and current cube can be scanned to get a full result set;A hybrid realization will be introduced which is composed by multiple cubes and these cubes have no overlap/gap on the partition date; Then the hybrid will delegate the query to all cubes and then merge the results;,5498
Extract Method,Upgrade Calcite to 1.3.0 ,5499
Extract Method,script for fill streaming gap automatically ,5501
Move Method,backport coprocessor improvement in 0.8 to 0.7 ,5502
Extract Method,improve performance of job query ,5503
Extract Method,Allow gap in cube segments for streaming case In current implementation Kylin doesn't allow gap in cube segments: all segments must be sequential no gap and no overlap;But in a streaming case when a segment was failed to build that should not block the building of the segments after that; Kylin should allow gap so that the failed segment can be fixed later without impacting on the on-going building;,5504
Rename Method,Streaming cubing allow multiple kafka clusters/topics ,5505
Extract Method,Migrate cube storage (query side) to use GridTable API ,5509
Extract Method,IGTStore implementation which use disk when memory runs short The main idea is use disk when memory runs short. Right now both cuboid and intermediate AggregationCache stays in memory. At least the cuboid can go to disk if necessary. So at minimal we only need one AggregationCache in memory at a time. And the biggest AggregationCache is about the size of base cuboid.Cuboid is stored in GTSimpleMemStore currently. Replace it with a flexible store that can switch between memory and disk. And you need to budget the total free memory to command the memory-to-disk switches smartly.,5511
Extract Method,Performance tuning for In-Mem cubing Got some performance data and we need improve the performance for cubing in mem:2015-04-09 18:31:15792 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Create base cuboid 81912015-04-09 18:31:19872 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 100000 records!2015-04-09 18:31:22442 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 200000 records!2015-04-09 18:31:24677 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 300000 records!2015-04-09 18:31:27146 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 400000 records!2015-04-09 18:31:29106 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 500000 records!2015-04-09 18:31:30899 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 600000 records!2015-04-09 18:31:32954 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 700000 records!2015-04-09 18:31:34940 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 800000 records!2015-04-09 18:31:37271 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 900000 records!2015-04-09 18:31:39576 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1000000 records!2015-04-09 18:31:42153 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1100000 records!2015-04-09 18:31:44334 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1200000 records!2015-04-09 18:31:46187 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1300000 records!2015-04-09 18:31:48076 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1400000 records!2015-04-09 18:31:50557 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1500000 records!2015-04-09 18:31:53111 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1600000 records!2015-04-09 18:31:56725 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1700000 records!2015-04-09 18:31:59761 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1800000 records!2015-04-09 18:32:04190 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1900000 records!2015-04-09 18:32:08442 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 2000000 records!2015-04-09 18:32:08804 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Totally handled 2010788 records!2015-04-09 18:32:12368 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Base cuboid has 305547 rows;2015-04-09 18:32:12368 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 8191 is built cache it to calculate children.2015-04-09 18:32:12377 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 6143 from parent 81912015-04-09 18:32:24372 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 6143 has rows: 2351152015-04-09 18:32:24372 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 6143 is built cache it to calculate children.2015-04-09 18:32:24373 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 2047 from parent 61432015-04-09 18:32:34832 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 2047 has rows: 2351152015-04-09 18:32:34832 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 2047 is built cache it to calculate children.2015-04-09 18:32:34833 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 1535 from parent 20472015-04-09 18:32:43139 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 1535 has rows: 2292362015-04-09 18:32:43139 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 1535 is built cache it to calculate children.2015-04-09 18:32:43140 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 511 from parent 15352015-04-09 18:32:51396 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 511 has rows: 2244802015-04-09 18:32:51397 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 511 is built cache it to calculate children.2015-04-09 18:32:51397 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 383 from parent 5112015-04-09 18:32:57896 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 383 has rows: 2053642015-04-09 18:32:57896 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 383 is built cache it to calculate children.2015-04-09 18:32:57897 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 127 from parent 3832015-04-09 18:33:04131 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 127 has rows: 1845142015-04-09 18:33:04131 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 127 is built cache it to calculate children.2015-04-09 18:33:04131 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 95 from parent 1272015-04-09 18:33:13391 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 95 has rows: 1399262015-04-09 18:33:13391 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 95 is built cache it to calculate children.2015-04-09 18:33:13391 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 31 from parent 952015-04-09 18:33:15882 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 31 has rows: 707022015-04-09 18:33:15882 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 31 is built cache it to calculate children.2015-04-09 18:33:15882 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 15 from parent 312015-04-09 18:33:17560 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 15 has rows: 361412015-04-09 18:33:17560 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 15 is built cache it to calculate children.2015-04-09 18:33:17560 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 14 from parent 152015-04-09 18:33:18815 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 14 has rows: 19442015-04-09 18:33:18815 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 14 is built cache it to calculate children.2015-04-09 18:33:18815 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 12 from parent 142015-04-09 18:33:18849 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 12 has rows: 2132015-04-09 18:33:18849 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 12 is built cache it to calculate children.2015-04-09 18:33:18850 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 8 from parent 122015-04-09 18:33:18851 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 8 has rows: 72015-04-09 18:33:18851 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 8 is built cache it to calculate children.2015-04-09 18:33:18852 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 8 children is completed; output itself now.2015-04-09 18:33:18853 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 12 children is completed; output itself now.2015-04-09 18:33:18889 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 14 children is completed; output itself now.2015-04-09 18:33:19118 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 15 children is completed; output itself now.2015-04-09 18:33:19827 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 30 from parent 312015-04-09 18:33:20457 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 30 has rows: 69422015-04-09 18:33:20457 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 30 is built cache it to calculate children.2015-04-09 18:33:20457 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 28 from parent 302015-04-09 18:33:20507 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 28 has rows: 8362015-04-09 18:33:20508 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 28 is built cache it to calculate children.2015-04-09 18:33:20508 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 24 from parent 282015-04-09 18:33:20513 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 24 has rows: 502015-04-09 18:33:20513 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 24 is built cache it to calculate children.2015-04-09 18:33:20513 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 16 from parent 242015-04-09 18:33:20514 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 16 has rows: 92015-04-09 18:33:20514 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 16 is built cache it to calculate children.2015-04-09 18:33:20515 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 16 children is completed; output itself now.2015-04-09 18:33:20515 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 24 children is completed; output itself now.2015-04-09 18:33:20515 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 28 children is completed; output itself now.2015-04-09 18:33:20519 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 30 children is completed; output itself now.2015-04-09 18:33:20549 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 31 children is completed; output itself now.2015-04-09 18:33:20872 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 79 from parent 952015-04-09 18:33:24827 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 79 has rows: 977322015-04-09 18:33:24827 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 79 is built cache it to calculate children.2015-04-09 18:33:24828 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 78 from parent 792015-04-09 18:33:25872 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 78 has rows: 113432015-04-09 18:33:25872 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 78 is built cache it to calculate children.2015-04-09 18:33:25872 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 76 from parent 782015-04-09 18:33:25961 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 76 has rows: 16202015-04-09 18:33:25961 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 76 is built cache it to calculate children.2015-04-09 18:33:25961 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 72 from parent 762015-04-09 18:33:25972 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 72 has rows: 1292015-04-09 18:33:25972 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 72 is built cache it to calculate children.2015-04-09 18:33:25973 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 64 from parent 722015-04-09 18:33:25974 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 64 has rows: 292015-04-09 18:33:25974 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 64 is built cache it to calculate children.2015-04-09 18:33:25975 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 64 children is completed; output itself now.2015-04-09 18:33:25975 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 72 children is completed; output itself now.2015-04-09 18:33:25975 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 76 children is completed; output itself now.2015-04-09 18:33:25982 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 78 children is completed; output itself now.2015-04-09 18:33:26033 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 79 children is completed; output itself now.2015-04-09 18:33:27730 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 94 from parent 952015-04-09 18:33:29563 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 94 has rows: 264972015-04-09 18:33:29563 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 94 is built cache it to calculate children.2015-04-09 18:33:29563 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 92 from parent 942015-04-09 18:33:29743 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 92 has rows: 41332015-04-09 18:33:29743 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 92 is built cache it to calculate children.2015-04-09 18:33:29743 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 88 from parent 922015-04-09 18:33:29771 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 88 has rows: 4832015-04-09 18:33:29771 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 88 is built cache it to calculate children.2015-04-09 18:33:29772 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 80 from parent 882015-04-09 18:33:29777 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 80 has rows: 1282015-04-09 18:33:29777 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 80 is built cache it to calculate children.2015-04-09 18:33:29777 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 80 children is completed; output itself now.2015-04-09 18:33:29778 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 88 children is completed; output itself now.2015-04-09 18:33:29780 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 92 children is completed; output itself now.2015-04-09 18:33:29799 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 94 children is completed; output itself now.2015-04-09 18:33:29912 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 95 children is completed; output itself now.2015-04-09 18:33:30844 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 111 from parent 1272015-04-09 18:33:36064 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 111 has rows: 1425382015-04-09 18:33:36064 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 111 is built cache it to calculate children.2015-04-09 18:33:36065 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 110 from parent 1112015-04-09 18:33:37519 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 110 has rows: 349832015-04-09 18:33:37520 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 110 is built cache it to calculate children.2015-04-09 18:33:37520 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 108 from parent 1102015-04-09 18:33:37842 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 108 has rows: 80762015-04-09 18:33:37842 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 108 is built cache it to calculate children.2015-04-09 18:33:37843 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 104 from parent 1082015-04-09 18:33:37915 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 104 has rows: 20402015-04-09 18:33:37915 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 104 is built cache it to calculate children.2015-04-09 18:33:37916 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 96 from parent 1042015-04-09 18:33:37942 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 96 has rows: 9162015-04-09 18:33:37942 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 96 is built cache it to calculate children.2015-04-09 18:33:37943 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 96 children is completed; output itself now.2015-04-09 18:33:37946 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 104 children is completed; output itself now.2015-04-09 18:33:37954 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 108 children is completed; output itself now.2015-04-09 18:33:37985 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 110 children is completed; output itself now.2015-04-09 18:33:38158 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 111 children is completed; output itself now.2015-04-09 18:33:40041 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 126 from parent 1272015-04-09 18:33:42976 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 126 has rows: 615212015-04-09 18:33:42976 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 126 is built cache it to calculate children.2015-04-09 18:33:42976 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 124 from parent 1262015-04-09 18:33:43633 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 124 has rows: 154542015-04-09 18:33:43633 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 124 is built cache it to calculate children.2015-04-09 18:33:43633 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 120 from parent 1242015-04-09 18:33:43755 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 120 has rows: 47112015-04-09 18:33:43755 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 120 is built cache it to calculate children.2015-04-09 18:33:43756 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 112 from parent 1202015-04-09 18:33:43803 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 112 has rows: 22532015-04-09 18:33:43803 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 112 is built cache it to calculate children.2015-04-09 18:33:43804 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 112 children is completed; output itself now.2015-04-09 18:33:43812 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 120 children is completed; output itself now.2015-04-09 18:33:43832 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 124 children is completed; output itself now.2015-04-09 18:33:43902 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 126 children is completed; output itself now.2015-04-09 18:33:44437 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 127 children is completed; output itself now.2015-04-09 18:33:46319 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 351 from parent 3832015-04-09 18:33:56565 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 351 has rows: 1714332015-04-09 18:33:56565 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 351 is built cache it to calculate children.2015-04-09 18:33:56565 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 287 from parent 3512015-04-09 18:34:01896 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 287 has rows: 1438772015-04-09 18:34:01896 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 287 is built cache it to calculate children.2015-04-09 18:34:01896 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 271 from parent 2872015-04-09 18:34:05170 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 271 has rows: 1017002015-04-09 18:34:05170 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 271 is built cache it to calculate children.2015-04-09 18:34:05171 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 270 from parent 2712015-04-09 18:34:05983 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 270 has rows: 128942015-04-09 18:34:05983 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 270 is built cache it to calculate children.2015-04-09 18:34:05984 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 268 from parent 2702015-04-09 18:34:06080 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 268 has rows: 18662015-04-09 18:34:06080 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 268 is built cache it to calculate children.2015-04-09 18:34:06081 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 264 from parent 2682015-04-09 18:34:06093 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 264 has rows: 1642015-04-09 18:34:06093 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 264 is built cache it to calculate children.2015-04-09 18:34:06093 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 256 from parent 2642015-04-09 18:34:06095 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 256 has rows: 392015-04-09 18:34:06095 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 256 is built cache it to calculate children.2015-04-09 18:34:06096 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 256 children is completed; output itself now.2015-04-09 18:34:06096 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 264 children is completed; output itself now.2015-04-09 18:34:06099 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 268 children is completed; output itself now.2015-04-09 18:34:06117 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 270 children is completed; output itself now.2015-04-09 18:34:06180 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 271 children is completed; output itself now.2015-04-09 18:34:07848 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 286 from parent 2872015-04-09 18:34:09051 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 286 has rows: 294662015-04-09 18:34:09052 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 286 is built cache it to calculate children.2015-04-09 18:34:09052 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 284 from parent 2862015-04-09 18:34:09215 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 284 has rows: 47822015-04-09 18:34:09215 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 284 is built cache it to calculate children.2015-04-09 18:34:09215 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 280 from parent 2842015-04-09 18:34:09244 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 280 has rows: 6262015-04-09 18:34:09244 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 280 is built cache it to calculate children.2015-04-09 18:34:09244 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 272 from parent 2802015-04-09 18:34:09250 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 272 has rows: 1722015-04-09 18:34:09250 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 272 is built cache it to calculate children.2015-04-09 18:34:09251 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 272 children is completed; output itself now.2015-04-09 18:34:09251 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 280 children is completed; output itself now.2015-04-09 18:34:09254 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 284 children is completed; output itself now.2015-04-09 18:34:09273 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 286 children is completed; output itself now.2015-04-09 18:34:09404 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 287 children is completed; output itself now.2015-04-09 18:34:10074 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 335 from parent 3512015-04-09 18:34:14470 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 335 has rows: 1279852015-04-09 18:34:14470 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 335 is built cache it to calculate children.2015-04-09 18:34:14470 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 334 from parent 3352015-04-09 18:34:15526 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 334 has rows: 227522015-04-09 18:34:15527 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 334 is built cache it to calculate children.2015-04-09 18:34:15527 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 332 from parent 3342015-04-09 18:34:15715 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 332 has rows: 37272015-04-09 18:34:15715 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 332 is built cache it to calculate children.2015-04-09 18:34:15715 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 328 from parent 3322015-04-09 18:34:15740 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 328 has rows: 4422015-04-09 18:34:15740 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 328 is built cache it to calculate children.2015-04-09 18:34:15740 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 320 from parent 3282015-04-09 18:34:15745 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 320 has rows: 1212015-04-09 18:34:15745 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 320 is built cache it to calculate children.2015-04-09 18:34:15745 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 320 children is completed; output itself now.2015-04-09 18:34:15746 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 328 children is completed; output itself now.2015-04-09 18:34:15748 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 332 children is completed; output itself now.2015-04-09 18:34:15764 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 334 children is completed; output itself now.2015-04-09 18:34:15896 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 335 children is completed; output itself now.2015-04-09 18:34:16630 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 350 from parent 3512015-04-09 18:34:19604 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 350 has rows: 466502015-04-09 18:34:19604 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 350 is built cache it to calculate children.2015-04-09 18:34:19604 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 348 from parent 3502015-04-09 18:34:20104 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 348 has rows: 87412015-04-09 18:34:20105 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 348 is built cache it to calculate children.2015-04-09 18:34:20105 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 344 from parent 3482015-04-09 18:34:20188 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 344 has rows: 14882015-04-09 18:34:20188 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 344 is built cache it to calculate children.2015-04-09 18:34:20188 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Calculating cuboid 336 from parent 3442015-04-09 18:34:20201 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 336 has rows: 4712015-04-09 18:34:20201 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 336 is built cache it to calculate children.2015-04-09 18:34:20201 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 336 children is completed; output itself now.2015-04-09 18:34:20202 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 344 children is completed; output itself now.2015-04-09 18:34:20207 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 348 children is completed; output itself now.2015-04-09 18:34:20238 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Cuboid 350 children is completed; output,5514
Extract Method,Implement fine grained cache for cube and ii ,5515
Extract Method,Export dictionary to map to speed repeated lookup in local-dict scenario ,5519
Extract Method,Create GridTable a data structure that abstracts vertical and horizontal partition of a table ,5520
Move Method,clean useless code and improve code coverage ,5525
Extract Method,Data model upgrade for legacy cube descs ,5527
Move Method,Make hive table cardinality calculation as a job Today the source table's cardinality calculation is a synced action which need be taken by the admin; The action will trigger a map-reduce job and blocks the user's action. If the hive table is small that is okay but if it is a big table the job may take tens of minutes to finish that is not user friendly.With the generic job flow engine be released the cardinality calculation should be able to converted to a backend job so user doesn't need be hold on UI.,5528
Extract Method,improve release selection for ranges currently ranges such as (1.0) do not work as it is incapable of locating the newest version < 1.0.Also ranges such as [1.0) result in RELEASE which may not be present for all artifacts.We should replace or augment RELEASE with a full listing of versions for an artifact in the repository. This would be repository metadata of the same fashion as the plugin prefix -> id mapping.,5529
Rename Method,"implement group/artifactID looked by plugin prefix We have a default rule which will be used as a fallback where theprefix ""idea"" is turned into ""maven-idea-plugin"". This will be retainedif no other information is found.Before applying the default rule some more metadata from the repositorywill be consulted. This will be stored at the group level and named""plugins.xml""./org/apache/maven/plugins/plugins.xml/org.apache.maven.plugins/plugins.xmlWhile this could potentially include version information as well it isworth being able to check these on a plugin-by-plugin basis and it alsofits with the potential RELEASE specifier on dependencies. This could bereconsidered in future versions.Format of the file for now is simple:< prefixes><groupId>org.apache.maven.plugins</groupId><plugins><plugin><prefix>idea</prefix><artifactId>maven-idea-plugin</artifactId></plugin>...</plugins>< /prefixes>This particular file will also be updated at release time for a plugin(though it should rarely be necessary as only new additions need to bepublished back).The list of group IDs to search will be configured from settings.xmlwith the default being just org.apache.maven.plugins. The process willbe to load the plugins.xml file from each of the configured groups thenbuild the map of prefixes to plugin group/artifactIDs. If there is aclash initially it should fail. We might allow using the firstdiscovered or some other resolution mechanism but would hope not to getthat situation as a goal representation might start taking on differentmeanings in different contexts.",5530
Extract Method,clean up of exception handling error reporting and logging we need to get rid of the traces for non-fatal errors and give something user friendly.The traces for fatal errors need to be more descriptive.,5531
Extract Method,Refactor release plugin to handle reactored build and release-pom.xml-SCM interactions. The release plugin should be refactored to:1/ handle injection/deletion of release-pom.xml from SCM HEAD/trunk before and after tagging...we're waiting on maven-scm to settle down a little before implementing this. See MNG-607 for more information.2/ handle reactored builds (rather builds where the POM has a <modules/> section which is NOT the same as using the '-r' tag). See MNG-521 for more information on this.,5532
Rename Method,<jdk></jdk> clause in the activation section has to provide more complex expressions. For now <jdk></jdk> provides only one operator '!' which means negation but it would be great if i can use '+' and ~ operator:< jdk>1.5+</jdk> <!-- this will be activated when the current JDK version is 1.5 or above (e.g. 1.6) -->< jdk>1.1 ~ 1.4</jdk> <!-- this will be activated when the current JDK version is between 1.1 and 1.4 -->< jdk>~ 1.3</jdk> <!-- this will be activated when the current JDK version is 1.3 or below -->< jdk>1.4 ~</jdk. <!-- the same with 1.5+ -->,5533
Extract Method,Support of EAR lifecycle The following patches integrates the ear plugin into a m2 lifecycle so that m2 package / install / deploy / etc goals could be used with an 'ear' packaging.First patch is to be applied on maven-core to register ear lifecycleSecond patch is to be applied on maven-plugins/maven-ear-plugin to add support of generateApplicationXml flag which states whether application.xml should be generated or not (default is true).This relates to MNG-563 where the original discussion took place.,5534
Extract Method,Allow to substitute custom artifact resolver Embedder should allow to substitute custom artifact resolver. The usecase is to be able to use projects that are not deployed to Maven repository but available in IDE (e.g. Eclipse workspace). This would also allow to resolve sources for projects that are not deployed to Maven repository.,5535
Rename Method,"Several small stylistic and spelling improvements to code and documentation The following can easily be squashed:{noformat}d99f9ef8c7ffe56966945d6f1b66f0280866ded5 Fix checkstyle error1c9362be4328713386bd23b01f9e2c87674cb952 Use static final values instead of literals52945a679ec8f3f571d77658eda2fce06d637aa7 Use proper spelling of ""e.g.""a26cc1b9636e19c28cd32ed1c844fec64dec55b6 Use the proper term for char U+002D (-) hyphen(-minus) instead of dash{noformat}",5536
Extract Method,Mojos need some way to indicate support of multithreading ,5537
Extract Method,Deprecate and replace incorrectly spelled public API According to [PR #101|https://github.com/apache/maven/pull/101] some of the public API methods are incorrectly spelled. Let's clean that up by deprecating and adding a correctly spelled version. The old version will simply call the new one.,5538
Extract Method,Add a getWagon(Repository) method to the WagonManager getWagon(String protocol) returns an unconfigured Wagon which is confusing deprecate it and add a new getWagon(Repository) that returns a configured Wagon,5539
Extract Method,"Printout version of last built module in reactor build MNG-6352 introduced printout of the version in a reactor build. If I build a multi-module project not just the parent has the version printout but also the last built module. {code:java} [INFO] ------------------------------------------------------------------------ [INFO] Reactor Summary: [INFO] [INFO] parent 4.0.0-SNAPSHOT ......................... SUCCESS [ 3.610 s] [INFO] parent-lib .................................... SUCCESS [ 0.492 s] [INFO] commons ....................................... SUCCESS [ 25.444 s] [INFO] loadbalancer-starter .......................... SUCCESS [ 21.198 s] [INFO] proxy-config-starter 4.0.0-SNAPSHOT ........... SUCCESS [ 7.496 s] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ {code} If I remove the ""proxy-config-starter"" module ""loadbalancer-starter"" got the version printout. Also this is not the order I configured the modules in the parent pom but I think this could be something on my side. {code:java} < modules> <module>commons</module> <module>loadbalancer-starter</module> <module>parent-lib</module> <module>proxy-config-starter</module> < /modules> {code}",5541
Extract Method,ANSI color logging for improved output visibility Is it possible for Maven to use ANSI color logging? IMO it would make Maven logs much easier to read and increase the visibility of items that the user want to see at any given point in time. I think Andrew Williams did some work under Plexus sandbox to enable color logging. There also a color logger available in Ant. http://ant.apache.org/manual/listeners.html#AnsiColorLogger,5542
Extract Method,implement version range support in the artifact collector http://docs.codehaus.org/display/MAVEN/Dependency+Mediation+and+Conflict+Resolution,5543
Inline Method,make aggregation feasible some plugins need to be able to aggregate eg the assembly plugin could take all the content based on its descriptor from the subprojects and include them under a base path. However at the moment it runs across all of the subprojects as well.Need to be able to have a goal turn off the execution of the reactor (essentially it will be in aggregation mode) but have all the projects available to it (possibly just collecting - see the other bug about reactor execution modes).,5545
Extract Method,"Set property containing the currently executing maven version. It would be helpful to have an easy way to access the current version of maven. This might be accomplished by setting a property like ""maven.version"" during startup that would be available to the pom and/or to plugins. This could be used to record what version of maven was used during the build to facilitate build reproducibility.",5546
Extract Method,process profiles before repositories are used When building projects that inherit a parent pom installed on remote repo that define the remote repo the parent pom cannot be downloaded.To resolve this we need to be able to configure a bootstrap repo definition in settings.xml.,5548
Extract Method,Improve output readability of our MavenTransferListener implementations The current output of Downloading/Downladed/Uploading/Uploaded transfer notification has some flaws:1. It does not scale numbers between 1 and 1000 with appropriate units2. It should use correct size ({{kB}} {{MB}} {{GB}} and time units ({{s}}) but doesn't. (see https://en.wikipedia.org/wiki/Binary_prefix and https://en.wikipedia.org/wiki/Metric_prefix)3. When Aether downloads in parallel (which applies for non-POM files) the progress interleaves due to race conditions to {{System.out}} and you do not know to which resource a progress belongs to.Let's use an improved version of MPIR {{DependenciesRenderer}}'s [{{FileDecimalFormat}}|https://github.com/apache/maven-plugins/blob/trunk/maven-project-info-reports-plugin/src/main/java/org/apache/maven/report/projectinfo/dependencies/renderer/DependenciesRenderer.java#L1583] for it.concrete examples:before{noformat}191/191 KB 27/48 KB 48/119 KB 80/87 KB 13/13 KB {noformat}after:{noformat}Progress (4): 500/800 B | 40/45 kB | 193 kB/315 kB | 1.3/9.0 MB | 12/30 MB{noformat}if total size is unavailable or the file has already been downloaded but not removed from the list the output will be:{noformat}Progress (4): 800 B | 40/45 kB | 193 kB | 9.0 MB | 12 MB{noformat}or in debug mode:{noformat}Progress (5): xml-apis-1.3.04.jar (<progress>) | maven-shared-utils-0.6.jar (<progress>) | xercesImpl-2.9.1.jar (<progress>) | commons-digester-1.6.jar (<progress>) | maven-reporting-impl-2.3.jar (<progress>){noformat}If the scale is between 1 and 10 one decimal place will be printed out. If it is between 10 and 1000+ it will be an integer.,5549
Extract Method,Improve the message when the version is missing from a dependency When using a dependencyManagement element it's not easy to tell which dependency element in the current POM actually has the problem because we get a message like the following:---Project ID: org.apache.geronimo.specs:j2eePOM Location: /home/jvanzyl/js/org/apache/geronimo/specs/trunk/j2ee/pom.xmlValidation Messages:[0] 'dependencies.dependency.version' is missing.[1] 'dependencies.dependency.version' is missing.[2] 'dependencies.dependency.version' is missing.[3] 'dependencies.dependency.version' is missing.[4] 'dependencies.dependency.version' is missing.---Would be nice if it also displayed the standard g:a so you can tell what it's referring to.,5550
Extract Method,Custom packaging types: configuring DefaultLifecycleMapping mojo executions Currently DefaultLifecycleMapping does not support mapping phases to goals with a custom configuration (see maven-core/src/main/resources/META-INF/plexus/default-bindings.xml). It is impossible to bind say an assembly plugin to 'package' phase within a custom packaging type since assembly plugin requires a meaningful configuration to be set.At my job we have a number of poms each serving a purpose of defining a lifecycle for a particular type of project (there's one for jar a couple for wars and several more for other types of deployable artifacts).Now that I somewhat understand maven's lifecycle It seems natural to convert such poms to custom packaging types leaving only a single parent with global config and pluginManagement. But it is currently impossible since we are using mostly standard plugins (only occasional dedicated ones) to configure projects' lifecycles.I did some digging around and put together a relatively straightforward change to maven-core: https://github.com/apache/maven/compare/master...atanasenko:mng-5805-lifecycle-mojo-config?w=1It both introduces support for specifying configuration and dependencies for mojo executions:{code:xml}< install><mojos><mojo><goal>org.apache.maven.plugins:maven-install-plugin:2.4:install</goal><configuration>...</configuration><dependencies>...</dependencies></mojo><mojo>...</mojo></mojos>< /install>{code}as well as retains support for existing mapping syntax:{code:xml}< install>org.apache.maven.plugins:maven-install-plugin:2.4:install ...</install>{code}I will put together some its (as well as make sure that existing are running ok) and create a pull request for both. Also there are a couple of changes that break API in org/apache/maven/lifecycle/Lifecycle.java and org/apache/maven/lifecycle/mapping/Lifecycle.java. How critical is it to mantain compatibility in those two?ITS: https://github.com/apache/maven-integration-testing/compare/master...atanasenko:mng-5805-lifecycle-mojo-config?w=1,5551
Move Method,Allow plugin implementors to choose how they want the configuration created for a particular MojoExecution Provide finer grained control over how a <configuration/> is processed before handing a final mojo configuration over the Configurator which takes the configuration and applies it to the Mojo. My specific use case is that I want to allow many mojos to be configured clearly by scoping the configuration for a Mojo by Mojo name.,5552
Extract Method,Use BeanConfigurator for configuration sub-elements Attached patch extends BeanConfigurator to allow picking individual configuration sub-elements. This is useful when embedding application like m2e only needs access to subset of original configuration. The change is an addition to brand new API introduced in 3.0 and is fully backwards compatible with (unlikely) existing clients so I wonder if this is okay to include this change in 3.0.1?,5553
Extract Method,Add --settings to be able to control the settings.xml file the m2 cli uses ,5554
Move Method,Allow a user to specify logging to a text file We should have an option to log to text files: mvn -l build.txt clean,5556
Extract Method,PluginDescriptor location enhancement I got the following errorjava.lang.IllegalStateException: Plugin descriptor ID incomplete: null:null:nullat org.apache.maven.plugin.descriptor.PluginDescriptor.getId(PluginDescriptor.java:112)at org.apache.maven.plugin.DefaultPluginManager.componentDiscovered(DefaultPluginManager.java:129)....I fixed the errormessage to include the location of the Plugin Descriptor plugin.xml file.To do that I had to modify PluginDescriptor to add a 'source' attribute; modify maven-core'sMavenPluginDiscoverer to not disregard the source parameter and pass it on to maven-plugin-descriptor'sPluginDescriptorBuilder (retaining backwards compatiblity) and set it there.Now the errormessage looks like:java.lang.IllegalStateException: Plugin descriptor ID incomplete: null:null:null in jar:file:/home/forge/.m2/repository/org/apache/maven/plugins/maven-war-plugin/1.0-SNAPSHOT/maven-war-plugin-1.0-20050405.162144-1.jar!/META-INF/maven/plugin.xmlat org.apache.maven.plugin.descriptor.PluginDescriptor.getId(PluginDescriptor.java:112)at org.apache.maven.plugin.DefaultPluginManager.componentDiscovered(DefaultPluginManager.java:130)which is a lot more helpful to me.I thought I'd share this with you hoping you'll find it useful enough to incorporate it.By the way this exception is triggered in a new NullpointerException (in DefaultPluginManager)warning about the PluginDescriptor's version being null; while constructing this message anotherexception is triggered so the original error doesn't get through at all.For the record I tried to enhance Plexus' ComponentSetDescriptor but found I could only accessthe source (parameter) in 2 out of 5 places without API modification so I abandoned this approachbut it might prove wise to include the source there rather than in the special case of maven's PluginManager.,5557
Extract Method,add input location tracking for site's reportPlugins injected by reports conversion working on MPH-160 there is no explanation for site's reportPlugins injected by [reports conversion to decoupled site plugin|https://maven.apache.org/ref/3.6.0/maven-model-builder/],5559
Extract Method,Access toolchains without maven-toolchain-plugin The original idea of toolchains was to have the same tool being used by different plugins within the same project.This seems like a good approach but there are several cases where you want more control over the tool to choose for instance:* the maven-compiler-plugin should use the lowest (preferably matching) jdk version to ensure proper bytecode for the classes* the maven-surefire-plugin might need a higher version due to requirements of the testing frameworks* some code-generators require a more recent JDK compared to the code they're actually producing.* the look-and-feel of javadoc has changed per JDK. If you like the latest you should be able to use it.* In case of the maven-invoker-plugin you should be able to test all combinations of JDK and Maven as runtime environment.,5560
Extract Method,Provide an exact pointer to documentation specific to each known exception that can occur Improve the exception handler so that a pointer can be provided to the user that gives a full explanation of the error that occurred.,5561
Extract Method,compile and package should be reactor-aware When compiling or packaging a project consisting of subprojectswhere one depends on another maven should first check the build environmentfor existing jars or classes (in target/ directories) from the dependent subprojectrather than always looking in the local and remote repositories for jar artifacts.Attached is a very simple test-case with 2 subprojects where one dependson the other.Only m2 install works; m2 compile and m2 package should also work.,5562
Extract Method,Parallel resolution of artifacts Artifacts should be resolved in parallel grouped by group id's to get around the lack of synchronization in the local repository. The patch does the following:* Use a ThreadPoolExecutor to parallelize artifact resolution but takes care not to resolve multiple artifacts from the same group id simultaneously. (requires Java 5)* Makes the http wagon the default instead of the poor performing http-clientDisadvantages: * Requires Java 5 but the backport jars could be substituted pretty easily* Breaks some plugins due to commons-logging being in the Maven uber jar (required by commons-httpclient) notably the apt plugin (maybe more should use the isolatedRealm setting?)* Screws up the progress monitor as multiple threads are updating itAdvantages:* Much faster when combined with the http wagon (WAGON-98). I was seeing 40% improvement on some test builds.,5563
Extract Method,setup lax parsing of repository poms and metadata we also need to slip in a piece of metadata for the metadata token but not publish it to the repo until 2.0 support is dropped,5564
Rename Method,built in notion of a mirror repository this probably relates to repository work as much as Maven work.Currently if users wish to use a mirror of ibiblio they specify that as an alternative remote repository. In m1 this causes a problem because if you add a new repository not equivalent to that in your project it is not in the build.properties.In m2 they are all added to the list so the mirror will be used first but the others may also be checked. This is better but still requires the user specify it in their configuration and it may be used on projects it is never needed for.It would be good if we could have a repository descriptor at the base of the repository that lists mirrors. On first use of the repository the user could be prompted to select any mirrors they'd like to use and it can be saved in their configuration for that repository ID.That way any project listing ibiblio as a repository could actually use say planetmirror instead (and fall back to ibiblio last).,5565
Inline Method,Provide line number information when there are errors processing a pom.xml When there is an error in a pom.xml Maven 2 does a reasonable job identifying the error but it doesn't provide the location of the error. For example I forget to include a <version> inside one of my dependencies:{noformat}bash-3.00$ mvn[INFO] Scanning for projects...[INFO] ----------------------------------------------------------------------------[ERROR] FATAL ERROR[INFO] ----------------------------------------------------------------------------[INFO] Error building POM (may not be this project's POM).Project ID: hivemind:hivemindPOM Location: c:\workspace\jakarta-hivemind\library\pom.xmlValidation Messages:[0] 'dependencies.dependency.version' is missing.Reason: Failed to validate POM[INFO] ----------------------------------------------------------------------------[INFO] Traceorg.apache.maven.reactor.MavenExecutionException: Failed to validate POMat org.apache.maven.DefaultMaven.getProjects(DefaultMaven.java:359)at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:276)at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:113)at org.apache.maven.cli.MavenCli.main(MavenCli.java:249)at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)at java.lang.reflect.Method.invoke(Method.java:585)at org.codehaus.classworlds.Launcher.launchEnhanced(Launcher.java:315)at org.codehaus.classworlds.Launcher.launch(Launcher.java:255)at org.codehaus.classworlds.Launcher.mainWithExitCode(Launcher.java:430)at org.codehaus.classworlds.Launcher.main(Launcher.java:375)Caused by: org.apache.maven.project.InvalidProjectModelException: Failed to validate POMat org.apache.maven.project.DefaultMavenProjectBuilder.processProjectLogic(DefaultMavenProjectBuilder.java:774)at org.apache.maven.project.DefaultMavenProjectBuilder.build(DefaultMavenProjectBuilder.java:624)at org.apache.maven.project.DefaultMavenProjectBuilder.buildFromSourceFile(DefaultMavenProjectBuilder.java:298)at org.apache.maven.project.DefaultMavenProjectBuilder.build(DefaultMavenProjectBuilder.java:276)at org.apache.maven.DefaultMaven.getProject(DefaultMaven.java:509)at org.apache.maven.DefaultMaven.collectProjects(DefaultMaven.java:441)at org.apache.maven.DefaultMaven.getProjects(DefaultMaven.java:345)... 11 more[INFO] ----------------------------------------------------------------------------[INFO] Total time: < 1 second[INFO] Finished at: Sat Dec 10 10:09:16 PST 2005[INFO] Final Memory: 1M/2M[INFO] ----------------------------------------------------------------------------{noformat}Maven should have identified the LINE NUMBER not just the file. I shouldn't have to pick my way though the document when Maven already has. Better yet it should display the invalid portion of the file and highlight the exact point of the error.I realize that it is controversial to add this issue as a bug and not an enhancement but I feel strongly about the absolute importance of developer feedback in any complex tool.,5566
Rename Method,"New resolution from local repository is very confusing I just discover the change introduced in Maven 3.x to try to improve the resolution mechanism and to avoid to use a local artifact which may not be available in its remote repository : https://cwiki.apache.org/confluence/display/MAVEN/Maven+3.x+Compatibility+Notes#Maven3.xCompatibilityNotes-ResolutionfromLocalRepositoryEven if the feature is interesting it has several problems :# When an artifact isn't accessible from its remote repository it isn't used by maven which replies a classical ""dependency not found error"". It is really annoying for a user with some Maven 2 skills which will have a look at his local repo will find the artifact and won't understand why Maven doesn't use it. At least the error reported by Maven should be clear that even if the dependency is available locally it isn't used because it's remote repository isn't available.# This behavior cannot be configured to be only a warning for example. It is really annoying because it doesn't take care of some context and constraints we may have in a development team. Let's imagine that the remote artifact is really removed. Cool Maven broke the build to warn us. But it brakes the build of all the team whereas perhaps only one of them may try to solve the issue (and it can be a long resolution). Thus having the ability to configure if this control is blocker or warning may allow the team to configure it as blocker on the CI server and as warning on the development environment.# This behavior may introduce some bad practices for example when we are using a staging feature on a repository manager. In our case my teams have a dedicated profile to activate a staging repository when we are validating a release. I recommend to not have this profile always activated but to do it only on-demand to avoid them to DL staging stuffs they don't need. With this new feature they need for all builds they run to activate this staging profile while binaries are stored in it. When you have to do it 20 times per day minimum let's imagine what the developer does : It adds it as an alwaysActive profile and then forget to remove it when the release is ended.For all these reason I would like we improve this feature to make it more usable and before all bet understandable for ours users.",5568
Inline Method,Hide drive-relative paths from plugins Take this nice path: /tmp (note the leading slash). This is an absolute path on Unix derivates but a drive-relative path on a Windows box. Drive-relative paths are resolved by prepending the drive of the current directory not the entire current directory.This notation of a drive-relative path is not well-known even among Windows users and in particular not easy going for cross-platform Java tools. A Java developer usually assumes that a path is either (directory-)relative or absolute.To avoid unnecessary complications as seen on our own multi-OS CI grid (cf. dev@ thread [CI Grid Windows and Paths|http://www.nabble.com/CI-Grid%2C-Windows-and-Paths-to21153292.html]) and as reported by users (e.g. MECLIPSE-404) the core should not only align directory-relative paths but also resolve drive-relative paths.,5569
Extract Method,ProjectBuilder.build(FileProjectBuildingRequest) returns null project if dependency version info is missing ProjectBuilder.build(FileProjectBuildingRequest) returns null MavenProject instance for the following pom.xml even when using ModelBuildingRequest.VALIDATION_LEVEL_MINIMAL. Expected/desired behaviour is to return MavenProject instance populated with all good/resolved dependencies and information about all bad/missing dependencies in MavenExecutionResult.getExceptions and/or MavenExecutionResult.getDependencyResolutionResult.{noformat} < project><modelVersion>4.0.0</modelVersion><groupId>xxx</groupId><artifactId>m01</artifactId><version>0.0.1-SNAPSHOT</version><dependencies><dependency><groupId>junit</groupId><artifactId>junit</artifactId></dependency></dependencies>< /project>{noformat} Original m2e bugreport https://bugs.eclipse.org/bugs/show_bug.cgi?id=343568,5573
Extract Method,reimplement parallel artifacts download the current maven3 trunk (3.0-alpha-3) doesn't contains a nice feature which exists in 2.x : parallel download artifacts.This made really faster starting with an empty repo.,5575
Extract Method,adding a method in AbstractMavenReport to obtain newSink() Actually when extending AbstractMavenReport I can get a Sink for write only one page.For report I need to create some pages.I like to have a new method called newSink(FileWriter) to write some other pages.Or could we have a SinkFactory object injected in AbstractMavenReport ?Note I need a Sink with the current site skin.Thanks--Olivier,5576
Inline Method,allow exclusion of certain dependencies from inclusion in an archive this has been requested for WAR but it should apply to all archives that include dependencies.,5578
Extract Method,maven lifecycle participant Attached patch introduces AbstractMavenLifecycleParticipant that allows maven core extensions participate in maven lifecycle. My immediate use case is injection of OSGi dependencies from Tycho but I believe the same approach can be applied to other problems like for example reworked reporting plugins discussed for 3.0 (this however will need new callback methods).,5579
Extract Method,inherited URLs should always have artifact ID when a URL (SCM web etc) is inherited it should always have the artifactId appended to it. This is a default to avoid needing to respecify - however if your path structure doesn't match the artifactId convention you should respecify it.This should happen regardless of the original structure of the URL - there wasn't really an instance where inheriting a URL that doesn't change makes sense.,5580
Extract Method,"reactor failed project behaviour reactor should have ""ignoreFailures"" like m1 but smarted.failure setting:- fail build on first subproject failure- process all but overall fail if there were any failures- process all report success regardless (eg we were running site and want to continue with something else next)Additionally if the second two options are in play a project should fail if one of its dependencies failed (eg if maven-artifact fails to build so does maven-core but maven-model does not)",5581
Rename Method,"Allow configuring the deployment of a timedstamped artifact when using SNAPSHOT in the version This is essentially the same issue as MPARTIFACT-59 but for the Maven 2 code base.When the current version contains SNAPSHOT every deploy creates a SNAPSHOT artifact and an artifact with a date/time. This causes a LOT of artifacts being created when using a continuous build. Would like to see this ""feature"" configurable so I can turn it off at the very least or have it happen only when I want to or on a scheduled basis.On a related note I think it would be good for the date/timestamped version to still contain the word SNAPSHOT. This way it's very easy to determine whether a project currently has dependencies on snapshot (unreleased) versions. I can parse for the date/time format as well but the common SNAPSHOT keyword would be easier.",5582
Rename Method,Provide before/after callbacks for project and mojo execution As a build extension developer I would like to be able to receive before/after callback events for project and mojo executions. For project-level events I need MavenSession and ManveProject (obviously) as well as calculated project execution plain. For mojo executions I need MavenSession ManveProject MojoExecution and Mojo instances. The idea is to allow extensions observe and participate project build as a whole not as set of independent mojo executions.,5583
Move Method,Move detection of plugin-execution id-collisions into the project validator rather than the DefaultMavenProjectBuilder. ,5584
Extract Method,Check for artifacts in POM-defined repositories before checking the central repository In our Maven 1.0.2 build environment we went to the trouble of building a replica of the ibiblio repository on a local machine because we would have builds hang for minutes if ibiblio decided to hiccup while the build was trying to update the snapshots of our current projects.It would be nice if Maven 2 would look for dependencies in the local repositories (those defined in the project and parent POMs) before looking in the central repository. This would allow locally-produced artifacts to be found faster (especially if the internet connection is dial-up) and would probably also reduce the load on the ibiblio servers.,5587
Extract Method,Support selection of wagon implementation for a particular protocol We have competing bug lists in the different implementations of the HTTP wagon so it'd be nice to provide a default (lightweight) wagon implementation then allow users who need it to specify the httpclient-driven implementation via Maven configuration.This is implemented for Maven 2.2.1 just needs integration tests.,5588
Rename Method,"Modify maven-toolchain to look in ${maven.home}/conf/toolchains.xml and in ${user.home}/.m2/toolchains.xml Actually we can only specify the toolchains.xml in {{$\{user.home}/.m2/toolchains.xml.}}However like for the settings.xml it would be very convenient to specify a default toolchains.xml in {{$\{maven.home}/conf/toolchains.xml}}The idea is : If there is NO {{$\{user.home}/.m2/toolchains.xml}} then uses {{$\{maven.home}/conf/toolchains.xml}} otherwise NONE defined.Merging both would also be good but not necessary.The change is very simple. Edit the file*maven-toolchain\src\main\java\org\apache\maven\toolchain\DefaultToolchainManager.java*and replace {code:java}private PersistedToolchains readToolchainSettings() throws MisconfiguredToolchainException {File tch = new File(System.getProperty(""user.home"") "".m2/toolchains.xml"");if (tch.exists()) {MavenToolchainsXpp3Reader reader = new MavenToolchainsXpp3Reader();...{code}by {code:java}private PersistedToolchains readToolchainSettings() throws MisconfiguredToolchainException {File tch = null;tch = new File(System.getProperty(""user.home"") "".m2/toolchains.xml"");if (tch == null || !tch.exists()) {tch = new File(System.getProperty(""maven.home"") ""conf/toolchains.xml"");}if (tch.exists()) {MavenToolchainsXpp3Reader reader = new MavenToolchainsXpp3Reader();...{code}I did that on my local environment by compiling this 2.0.11-SNAPSHOT class and integrating it in my maven-2.0.9-uber.jar and it works perfectly.",5589
Extract Method,simplify mojo qdox specification now that these are mapped to fields the name type and default value can be derived from the field definition instead simplifying the rest.validator should no longer be necessary.The other parameter attributes (required expression) could be specified on the field themselves. Description can be the javadoc,5590
Extract Method,Make Like Reactor Mode Add a commandline option to enable maven to expand the reactor scope to find projects that are dependenciesof the projects currently in the reactor and add them.Currently only the current project and child projects are included in the reactor search. I'm proposingto add a commandline switch that lets maven check parent directories to find the root of the project treeand then do a normal reactor scan only adding projects that would normally not be added if they're neededas dependencies of the projects that would normally be built.Here's a sample project tree:* root** p1*** c1 (depends on p2)** p2 (depends on c2)** p3*** c2And a sample algorithm:- When building c1 the reactor would contain [c1].- Maven would check p1 then root etc using the <parent> tags (without the versions!)to see if the project is still in the current reactor.- It would then create a second list of projects (reactor2) containing ALL projects using the newly discovered root: [root p1 c2 p2].- remove all projects from reactor2 contained in reactor: reactor2 = [root p1 p2]- resolve all direct dependencies for all projects in reactor in reactor2 and add them to reactor taking versions into account: reactor = [p2 c1]- repeat previous step until all projects have their dependencies resolved from reactor 2. first iteration would yield reactor = [c2 p2 c1]next iteration would stop since c1 doesn't have any dependencies present in reactor2.This would ensure that when some local project's sources have changed they'll be incorporatedin the build regardless of where you build. So you don't have to do a reactor build each time you change morethan 1 project and you don't have to remember which projects you changed and build them in the correct orderyourself manually.,5591
Move Method,Introduce password encryption to the trunk ,5592
Extract Method,Poor ProjectBuilder.build performance for projects with unresolvable extension plugins Need to extend PluginArtifactsCache to also cache extension plugin artifact resolution errors otherwise DefaultProjectBuildingHelper keeps failing to resolve the same plugin again and again.,5593
Rename Method,"project-specific default jvm options and command line parameters Some of the projects builds I work on require special jvm options like minimal -Xmx value and specific command line parameters like --builder. Currently I have to manually configure these every time run the build which is rather annoying and error prone. This manual configuration also makes it harder for new or external developers to build the projects and many simply give up trying after ""mvn package"" does not work from the first try.This enhancement request proposes to introduce two new optional configuration files .mvn/jvm.config and .mvn/maven.config located at the base directory of project source tree. If present these files will provide default jvm and maven options. Because these files are part of the project source tree they will be present in all project checkouts and will be automatically used every time the project is build.",5594
Rename Method,Update slf4j and simplify its color integration Update dependences for maven build Slf4j 1.7.22 -> 1.7.25 ([SLF4J-394|https://jira.qos.ch/browse/SLF4J-394] and [SLF4J-395|https://jira.qos.ch/browse/SLF4J-395] for [SLF4J-389|https://jira.qos.ch/browse/SLF4J-389]) With slf4j update we can simplify maven-slf4j-provider implementation given [SLF4J-394|https://jira.qos.ch/browse/SLF4J-394] ,5597
Extract Method,"add optional flag for dependencies ""optional"" should be equivalent to ""compile"" but not passed on in transitive deps. This will allow us to repair dependencies like dom4j and avoid a lot of <exclusions/>",5598
Extract Method,Consider layout for mirror selection Extensions like Tycho employ custom repo layouts to access P2 or OBR repos. When it comes to mirroring it's desirable to use different mirrors for the normal Maven repos and those OSGi repos. Neverthess users should still be able to use wildcards for easy mirror maintenance but a wildcard matches any repo regardless of its layout/type. So we should enrich the settings model to allow the specification of a layout for the mirror itself that can be considered when selecting a mirror for a specific repository.,5599
Move Method,Provide a way to customize lifecycle mapping logic Default lifecycle mapping logic uses <phase> plugin execution configuration element to map to lifecycle instance. For custom lifecycles I would like to be able to provide custom logic. Specific usecase is to be able to define custom lifecycle which only runs specific phase from the default lifecycle and nothing else.,5601
Extract Method,improve version resolution logging a couple of things missing:- log when a range is restricted due to encountering the same dep with a different version- log when a version is selected from a range- later log conflict resolutionalso too verbose:junit:junit:jar:3.8.1 (removed - nearer found: 3.8.1),5602
Extract Method,Allow class realm manager delegates to alter public part of Maven core realm As part of the fix for MNG-4747 a new class realm was introduced that represents the public part of the Maven core. Life for integrators that need to contribute additional classes like Polyglot Maven or M2Eclipse would be easier if this realm is also subject to the class realm delegates. This would allow to inject the custom types at a central place rather than injecting them into each and every plugin realm.,5603
Extract Method,improve checksum handling presently missing or invalid checksums are just warned on. sha1 and md5 are always uploaded.- Need to pick one or the other for a repository (might choose to upload both though probably not worth testing both on the way down)- Need a way to specify a master override (command line) to turn off strict checking of things like this and anything else (eg skip bad transitive deps) **- Might want to retry on failure?** had a discussion about this and it seems better to do this than to allow it to be specified in the project to encourage those that have bad metadata to fix it. Need to still reassess the impact on the user but we are in a position to make sure repo1 is verified and its integrity is maintained.,5604
Extract Method,[perf] enhance code applying best practices There are some parts in myfaces that with some small changes we can reduce the ammount of objects created and make iteration faster.,5605
Extract Method,Allow view state to be rendered in the beginning of the form As discussed on the user list Leonardo Uribe asked me to create a ticket for this feature. The idea is to allow the view state to be rendered in the beginning of the form to avoid a ViewExpiredException in case of a postback on a page that isn't completely loaded yet.Details: http://markmail.org/search/?q=view%20list%3Aorg.apache.myfaces.users#query:view%20list%3Aorg.apache.myfaces.users%20order%3Adate-backward+page:1+mid:uqp2l6y2iwlmwbso+state:resultsAs a workaround I'm using PrimeFaces deferred loading. It hides the form components while the page isn't fully loaded. http://www.primefaces.org/showcase/ui/outputPanel.jsfThanks,5606
Extract Method,"Improvements in State Saving Algorithm Following the improvements done in:MYFACES-3117 Current server state saving implementation prevents multi-window usageWhich included also:MYFACES-3134 Move all code related to state caching into one placeMYFACES-3137 Align ResponseStateManager implementation with the specMYFACES-3138 Simplify ResponseStateManager implementation codeIt is possible to improve this part a lot more. The are some parts of the code that works fine but are too old and are not very well understood. I think it is reasonable to review this part in deep. Note this could suppose some changes in ServerSideStateCacheImpl and other related classes.Our PSS algorithm is very good but these changes are more related to the logic involved in save/restore the state for both modes client side and server side state saving. It is possible to imagine a ""mixed"" strategy between client side and server side state saving that could reduce the session size and in that way achieve an even better scalability. Also it is possible to imagine way to secure the token sent in the view when server side state saving is used using a random number allowing to disable encryption in such cases.",5607
Extract Method,"JSR-252 Issue #54: Added new extension elements to the Faces XML schema. Added new extension elements to the Faces XML schema. Please see Section 1.1 ""XML Schema Definition"".Also see https://javaserverfaces-spec-public.dev.java.net/issues/show_bug.cgi?id=54",5608
Inline Method,Remove Servlet 2.5 compatibility hacks ,5609
Extract Method,[GAE] Add param to select jar files to be scanned for .faces-config.xml or .taglib.xml or annotations In MyFaces users list it has been reported that Spring and JSF are slow in GAE when they start because it is necessary to scan the classpath for .faces-config.xml or .taglib.xml or annotations.It is possible to put myfaces jars and other jsf related jars outside /WEB-INF/lib folder but in GAE we can skip this limitation so it becomes more effective to indicate the jar files to scan in a web config param:org.apache.myfaces.GAE_JSF_JAR_FILESMyFaces jars does not need to be scanned so with this param it is possible to set:<context-param><param-name>org.apache.myfaces.GAE_JSF_JAR_FILES</param-name><param-value>none</param-value></context-param>And skip scanning at all (/WEB-INF/classes will still be scanned).With this changes startup in GAE will be quick. Note this is not necessary in other application containers because initialization occur when the application is deployed and just take some seconds.Anyway I consider it is worth to do it.,5610
Extract Method,"Performance improvement in HtmlRenderKitImpl we did some profiling in our project and found out that HtmlRenderKitImpl creates some amount of transient object garbage when getRenderer is called:Self 8005 000 792 0 2894448 J:org/apache/myfaces/renderkit/html/HtmlRenderKitImpl.getRenderer(Ljava/lang/String;Ljava/lang/String;)Ljavax/faces/render/Renderer;Child 24015 000 469 0 1714064 J:java/lang/StringBuffer.append(Ljava/lang/String;)Ljava/lang/StringBuffer;The above values were recorded with just 2 request to a page with many components - 2.8MB of transient objects were created by 8005 calls to getRenderer.I assume that this is due to the ""keying"" currenlty implemented which always creates a concatinated string. I guess using a Map<String Map<String Renderer>> doubleMap could improve the performance here since string creation for keying would not be nessary.Might also touch 1.2 and 2.0.",5611
Extract Method,[perf] minimize FacesContext.getCurrentInstance() calls part II It is still possible to minimize even more the number of calls to FacesContext.getCurrentInstance() just doing some small changes in some methods.For example Application.createComponent(String componentType) has an internal call to FacesContext.getCurrentInstance() but the variant Application.createComponent(FacesContext context String componentType String rendererType) does not because it can be retrieved from the parameter. Since rendererType can be null according to JSF spec the result of both methods is equivalent so it is possible to use the second variant and save one call per component that does not have rendererType.Other important place is when EL evaluation occur (FacesCompositeELResolver) FacesContext.getCurrentInstance() is always called but it is preferred to get the FacesContext from the ELContext first because a get over a map of 1 or 2 elements will be faster than a ThreadLocal lookup which is proportional to the number of threads running.,5612
Extract Method,Lookup ExpressionFactory also if SUPPORT_JSP_AND_FACES_EL is false Currently if SUPPORT_JSP_AND_FACES_EL is set to false MyFaces can't initialize without setting org.apache.myfaces.EXPRESSION_FACTORY. We can simply define a list of known ExpressionFactory implementations so that the org.apache.myfaces.EXPRESSION_FACTORY is just optional.,5613
Rename Method,SearchExpression API There is a proposal from PrimeFaces guys to include a search expression api to locate components in cases like f:ajax execute/render attributes h:message for attribute and others.The idea comes from here:http://blog.primefaces.org/?p=2740The idea is support a syntax like this in the attributes:< clientId>:<id>< id>:<id>:<id>@<keyword>:<id>id:@<keyword>@<keyword>(<param1>)@<keyword>(<param1><param2>)There is a patch for this I have been working over the last weeks but still require more tests and update the components in MyFaces Core 2.3 to use the new API.,5614
Inline Method,"[core] Improve PSS algorithm when a dynamic view (use of c:if or ui:include src=#{...}) is used Implement change according to mail sent about it to dev list under the name:[core] Improve PSS algorithm when a dynamic view (use of c:if or ui:include src=#{...}) is usedIn the last months I have been working on a solution to improve Partial State Saving (PSS) performance in cases where the view is updated dynamically by effect of a facelet tags like:- <c:if ...>- <c:when>- <ui:include src=""#{...}"">- <ui:decorate template=""#{...}"">In simple words any use of the previous tags in a page causes all components inside them to be saved and restored fully. The side effect is the overall state gets bigger. With the introduction of PSS in JSF 2.0 instead save an array of properties a key/value pairs are used so usually this effect is difficult to notice but it is relevant specially when <ui:include src=""#{...}""> is used to update content dynamically. It is quite simple to find examples with a search engine on the internet.I'll explain in detail what's going on.Let's see what happen when c:if is used:< c:if test=""#{condition}""><h:outputText value=""Some text""/>< /c:if>The first time the view is rendered if the condition is false the component is not added but later in a postback if the condition changes from false to true the component is added. Here the algorithm have two options:1. Ignore it.2. Mark the component or branch to be restored fully.Most of the time ignore (1) is ok but in some complex cases the state synch is lost because test=""#{condition}"" is evaluated every time the view is restored with different results. The users usually have reported this as ""state get lost"" or ClassCastException problems. To deal with such cases a special mode was added in MyFaces to implement (2) with a web config param called org.apache.myfaces.REFRESH_TRANSIENT_BUILD_ON_PSS.But what happen if the algorithm save c:if ""condition"" the first time the view is rendered? With that PSS algorithm will always restore the initial view as expected. Recently in 2.0.10 / 2.1.4 this improvement (MYFACES-3329) was added so it is no longer necessary to enable the web config param. Great! But note this does not solve the ""state gets bigger"" problem.Now consider what happen if c:if ""condition"" is saved every time it change (before render response). If the condition is false and changes to true the initial state will now be restored including thecomponent so if it is called markInitialState() over the component and then the delta is saved the state size will be smaller and finally it will be saved more efficently because the initial state is the one who gets bigger instead the part that is saved as delta.This solution can be applied to <c:if ...> <c:when> <ui:include src=""#{...}""> and <ui:decorate template=""#{...}""> which is enough because <c:forEach> can be replaced with <h:dataTable rowStatePreserved=true ...> or a similar component like the ones available in Tomahawk or any other variant. It is interesting to note the solution also fix the problem when <h:dataTable rowStatePreserved=true ...> is used inside a dynamic part.Fortunately the spec doesn't say anything about how markInitialState() is called and let it as an implementation detail. Also javax.faces.IS_BUILDING_INITIAL_STATE description is so general that even with the change there is no need to change the javadoc. After considering all history behind PSS algorithm it seems reasonable to activate markInitialState() call and set javax.faces.IS_BUILDING_INITIAL_STATE to true when a dynamic update in a component tree is done by a facelet tag and deactivate it as soon as the code process the content.At the end applications using the previous tags will have a really huge improvement into its state. But anyway since it is a ""extension"" of the initial intention of the flag I consider desirable to mention it. It is difficult to measure the impact because it depends of the view structure itself but it sounds like a very promising change.Suggestions opinions and what you do want to say about this proposed change is welcome. If no objections I'll commit the proposed change soon.",5615
Rename Method,Improve the plugin mechanism forgot to add the version,5616
Extract Method,[perf] do not check for duplicate ids when saving view on production stage see discussion here: http://www.mail-archive.com/dev@myfaces.apache.org/msg52995.html,5617
Rename Method,[perf] Additional performance improvements Some performance improvements in : 1) ApplicationImpl.java 2) ServletExternalContextImpl.java 3) HtmlResponseWriterImpl.java 4) HTMLEncoder.java we also discussed on the mailing list changing encodeURIAtributte to encodeUriAttribute to fix the typo in the method name so I'll do that here as well. 5) ResourceValidationUtils.java The following changes were made: - Skip calling ConcurrentHashMap.containsKey since we will call get afterward if containsKey == true. - Stop using Boolean for variables that don't have a null meaning. If null == false then just use boolean with a default of false. - Don't call String.length() constantly for String variables that aren't re-assigned. - Change conditional order to avoid calling validateResourceName unless the other conditions are true,5618
Extract Method,enable 'standard' checkstyle checks in myfaces-core We currently only have the 'minimal' checks enabled in core which actually only checks the correct license headers.We should go for the 'standard' checkstyle rules even if this would take some time to fix (found 1111 errors only in the first module).,5619
Extract Method,Utility method in ClassUtils to not throw exception A new utility method called simpleClassForName(String type boolean logException) can be added to the ClassUtils to not throw exception if the boolean is false. This method would be nice to have for our Websphere WebConfigProvider implementation.A patch has been provided,5620
Rename Method,Support for valueChangeListener method without ValueChangeEvent parameter A ValueChangeListener method can now also take no arguments (see javadoc for MethodExpressionValueChangeListener for details),5621
Extract Method,catch Throwable errors when using ErrorPageWriter (myfaces error handling) One possible enhancement to the myfaces error handling capability is catch Throwable errors when using myfaces error handling.This should be done taking into account what the spec says about it:- Call the execute() method of the saved Lifecycle instance passing theFacesContext instance for this request as a parameter. If the execute()method throws a FacesException re-throw it as a ServletException withthe FacesException as the root cause.Call the render() method of the saved Lifecycle instance passing theFacesContext instance for this request as a parameter. If the render() methodthrows a FacesException re-throwThe idea is catch and rethrow non Exception classes like errors (extends from Throwable or Error classes directly). If Myfaces error handling is used use it to show the error page with the info taking into account that not all info could be available.The idea is do this on FacesServlet:try {_lifecycle.execute(facesContext);if (!handleQueuedExceptions(facesContext)){_lifecycle.render(facesContext);}}catch (Exception e){handleLifecycleException(facesContext e);}catch (Throwable e){//Handle Error and Throwable error cases (out-of-memory-errors ....).handleLifecycleThrowable(facesContext e);} finally{facesContext.release();}Please note that any change should not break old functionality.,5622
Extract Method,[perf] use grails StreamCharBuffer instead FastWriter in HtmlResponseWriterImpl Looking for a solution to replace the default FastWriter implementation with one that allocate multiple blocks on demand and in this way reduce the memory used to render a page I founded one cool implementation in:org.codehaus.groovy.grails.web.util.StreamCharBufferhttp://grails.org/doc/latest/api/org/codehaus/groovy/grails/web/util/StreamCharBuffer.htmlThis file has Apache v2.0 license so we can include it into MyFaces and use it into our HtmlResponseWriterImpl as internal buffer. I did some performance tests and I notice the impact in speed is minimal or non existent but in memory it is good enough because HtmlResponseWriterImpl usually is cloned on ajax requests and FastWriter always initialize a buffer no matter if it is used or not and if the page is big the buffer grows without unnecessary copy operations.,5623
Extract Method,Provide an interface to override how to find spi interfaces This is the last step to solve MYFACES-2944 and MYFACES-2945 problem related to OSGi and SPI. Now it is possible to call ServiceLoaderFinderFactory.setServiceLoaderFinder(ExternalContext ectx ServiceLoaderFinder slp)or ServiceLoaderFinderFactory.setServiceLoaderFinder(ServletContext ctx ServiceLoaderFinder slp)Just before initialization to set a ServiceLoaderFinder that will be used later to locate SPI interfaces. In this way it is possible to provide a code that looks SPI interfaces using OSGi bundles.,5624
Rename Method,Google App Engine Support for Myfaces 2 Google App Engine Support for Myfaces 2,5626
Extract Method,"[perf] Cache EL expressions using an indirection for ui:param and user tag attributes I have been trying for some time to find new ways to improve the code insideMyFaces. Working in MYFACES-3811 (fix c:forEach) I have realized that the wayhow VariableMapper works allows us to cache EL expressions in those places wherewe have thought EL caching was not possible. This fact is important because it changes the way how we have been thinking aroundview pooling technique (See MYFACES-3664 for details). If all ValueExpression/MethodExpression instances in a view can be considered ""static"" orin other words it does not change each time the view is built or refreshed we canbe sure that with a plain visitTree call it is possible to ""reset"" any view andreuse it safely even in cases like when ui:param is used or user facelet tags.If all components in a view support pooling (hard/soft reset using saveState method) any view using those components can be poolable. First of all let's remember how VariableMapper works. Basically it is just a mapwith var names as keys and ValueExpression as values. When a EL expression iscreated the variables that are on the context VariableMapper and are usedto solve the expression are copied and stored into an inner VariableMapper ofthe created EL expression. For example if we have this:< c:set var=""item"" value=""Hello""/>< c:set var=""item2"" value=""#{item}""/>< c:set var=""item3"" value=""#{item2}""/>the EL expression for item2 will have an inner VariableMapper with an ELexpression pointing to ""Hello"".Now we need to remember the problematic cases for EL caching:1. Use combinations of c:set and c:if<c:if test=""#{condition}""><c:set var=""item"" value=""Hello""/><c:if><h:outputText value=""#{item}""/>This case is unlikely but most of all it can be refactored very easily to avoidthe c:if and move the condition to the c:set EL Expression. It is common to foundthis technique in old JSP pages. But it is clear with JSF this kind of logicshould reside in a managed bean. So at the end it is not a big deal. Anyway There is a mode called ""strict"" that disable EL caching for the whole page if c:set is found.2. Use of ui:param<ui:decorate template=""uiparamcache1_1.xhtml""></ui:decorate><ui:decorate template=""uiparamcache1_1.xhtml""><ui:param name=""param1"" value=""ALFA""/></ui:decorate>The first time the template is called it has no params so all expressions arecached inside the inner template. But once we call the same template again thosecached expressions are now invalid and needs to be recalculated again. The hackdone with ""alwaysRecompile"" mode recompiles the facelet but takes into accountthe known parameters for the template. In this way the EL expressions that areaffected by the param are not cached.3. Use of facelet user tags<user:usertagtest1 var1=""ALFA"" id=""comp1""></user:usertagtest1><user:usertagtest1 var2=""BETA"" id=""comp2""></user:usertagtest1><user:usertagtest1 var1=""GAMMA"" var2=""OMEGA"" id=""comp3""></user:usertagtest1>This is quite the same to the case with ui:param but in this case affect facelet tag attributes. 4. An expression uses a variable resolved through VariableMapperThis is unlikely because there are no standard tags using this strategy but it ispossible to create a facelet tag that uses a VariableMapper wrapper. This is not something we should worry about.In MYFACES-3811 (fix c:forEach) there is a part where a wrapper (IteratedValueExpression or MappedValueExpression) is required to hold the associated item and inject it into the VariableMapper. This is indeed a good ideabecause it shows that we can just put a wrapper inside VariableMapper and things willkeep working. If we can substitute the ValueExpression associated with a var with something else wecan avoid the propagation effect that makes EL caching fail in 2 and 3. The trick is use an unique id associated with the facelet tag and put the real EL expression ina central point like FaceletState object which is stored in UIViewRoot. The resultingstructure can be generated over and over if PSS is enabled and if is disable it needsto be saved with the state. If the component tree changes dynamically the generatedstructure will change too.The final effect will be that 100% of the EL Expressions managed by facelets using ""alwaysRecompile"" mode will be cacheable which will be a great improvement. It alsoremoves one of the biggest disadvantages we had for include view pooling techniqueinto MyFaces 2.2.x.",5627
Extract Method,"Enabled exception handling like in Facelets for the rest of the MyFaces LifeCycle By express permission of Jacob Hookom for dual licensing one class out of Facelets as ASL I've added the Facelets-Error-Handling code to the rest of the MyFaces LifeCycle. Jacob has an ICLA on file so paperwork has been done before.To tweak this behaviour two new context-parameters have been added (add them to your web.xml if you need to use this tweaking):private static final String ERROR_HANDLING_PARAMETER = ""org.apache.myfaces.ERROR_HANDLING"" - if you want to disable the behaviour completelyprivate static final String ERROR_HANDLER_PARAMETER = ""org.apache.myfaces.ERROR_HANDLER"" - if you want to choose a different class for handling the exceptionEnjoy!regardsMartin=================see our mail-discussion attached as a referencenot a problem-- i think it'd be a cool solution for JSF 1.2 is to grab the ELResolver and walk through the Features-- so even custom ELResolvers would be able to output custom variables....-- JacobOn 7/23/07 Martin Marinschek <martin.marinschek@gmail.com> wrote:Hi Jacobis it ok if I use your error-handling page also for errors in the full life-cycle of MyFaces? I just love this page and I would want to see it for other exceptions than render-exceptions as well...I'd surely add you as an author - we'd definitely need to put the ASL-license on top though.regardsMartin",5628
Rename Method,TODO 65: Partial View Lifecycle Add a partial view lifecycle as described in 13.4 Partial View Traversal of the EDR2,5629
Extract Method,Reload of faces-config-files if changes are detected This will check regularly for changes in the web.xml file and reloads it if needed. The time interval can be set by means of a context parameter (default is 2 seconds right now).,5631
Extract Method,Add ServiceLoader code to our Chainloading init code We have a set of internal events which go beyound of what JSF 2.x can deliver to use those we have for now to add a context param to our web.xml. Now JDK6 has a service faclity which we should use optionally if present to load the plugins as services.The commits under this issue will enable this facilty if present.,5632
Extract Method,Add annotation processing logic JSF 2.0 specifies the use of the following annotations for Managed Bean configuration:- ManagedBean- ManagedBeans- ManagedProperty- RequestScoped- SessionScoped- ViewScoped- ApplicationScoped- NoneScopedThe annotations are already there it now needs processing logic.I'm on it.,5633
Extract Method,Make create AjaxBehavior accessible in AjaxHandler I'm currently trying to create a custom ajax tag that is based on f:ajax but supports an additional attribute. For this I wanted to create a new tag handler that extends AjaxHandler.What I found out is that it is VERY hard to do so. The creation of the AjaxBehavior is buried inside applyAttachedObject(). The only reasonable way I found to set an additional value expression on the created AjaxBehavior is to pass in a wrapped FacesContext/Application from the derived class.It would be so much easier if for instance creating the behavior would be done in a protected method. Then the behavior would be accessible in derived classes.,5635
Rename Method,[perf] UIForm#invokeOnComponent with prependId=true In case when invokeOnComponent is used and the form has prependId=true we can early skip the whole component tree if the baseClientId != form clientId. Will also check other components UIData already contains this enhancement,5636
Extract Method,[perf] do not calculate facelets view mappings and context suffixes if not necessary Doing some perf tests I notice some methods in DefaultViewHandlerSupport and DefaultRestoreViewSupport are called multiple times but always return the same value over and over because they are based on initialization params which does not change over application lifetime.The solution is do some small changes over these two classes passing FacesContext as constructor and calculating those values only the first time those classes are created.,5637
Rename Method,[perf] use shared StringBuilder instance method javax.faces.component.UIComponentBase._getSharedStringBuilder(FacesContext) already provide this. Add same method to public API and use request-shared StringBuilder instance in renderers too.,5639
Rename Method,Make those add*** methods public in WebXml In the Geronimo integration work we have an internal structure for the parsed web.xml file and we hope to use that instance to fill in the org.apache.myfaces.shared.webapp.webxml.WebXml so that myfaces does not need to parse the web.xml file again But those add*** method are package scope. Is it possible to make those methods public I did not see it will break anyting.Thanks,5640
Inline Method,Minor performance improvements for org.apache.myfaces.el org.apache.myfaces.context and org.apache.myfaces.util Fixed most of the findbugs errors in the performance category and apply many pmd optimization rulesMade many of the classes final.A method argument that is never assigned can be declared final. A local variable assigned only once can be declared final,5641
Extract Method,JSR-252 Unified EL: Implement sections 5.5 5.6 5.7 and 5.8 of the JSF 1.2 spec There was no specific EG issue number for this so this Jira task will serve as a catch-all for integration with the new Unified EL provided in JSP 2.1.,5642
Move Method,javax.faces.convert - refactor common behaviour + DateTimeConverter changes All available converters look very similar. Extract the common behavior in base class.Also DateTimeConverter can be migrated to work with type safe enums for style and type properties.There are comments in source like //TODO: validate timeStyle. According to java doc of DateTimeConverter on sun there should not have validation. The validation of these will be performed when asString/asObject methods are called.,5644
Extract Method,"Fix PSS algorithm to ensure c:if and ui:include src=""#{...}"" related use cases will work without rely on org.apache.myfaces.REFRESH_TRANSIENT_BUILD_ON_PSS_PRESERVE_STATE There is a well known issue related to use c:if and ui:include src=""#{...}"". It has been described in different ways for a long time. For example see:MYFACES-3271 ui:include tag handler evaluates its bindings to soon during JSF lifecycleThere is a lot of related issues but to keep things simple PSS algorithm is based on the following conditions:1. Each time the view is build by first time or when it is restored the same initial state will be retrieved.2. It is possible to calculate a delta state based on that initial state.In practice there are some use cases that breaks that:a. c:if tagb. c:choose c:when and c:otherwisec. ui:include src=""#{...}"" (usually known as ""dynamic include"")d. ui:decorate template=""#{...}""e. c:forEach The reason is they change the component tree dynamically. For example a c:if tag could be bound to a ValueExpression that in one moment could be true and then false so the ""initial state"" calculated by PSS algorithm through a vdl.buildView() will be different each time breaking condition 1.It is true there exists the workaround with org.apache.myfaces.REFRESH_TRANSIENT_BUILD_ON_PSS_PRESERVE_STATE which mark some parts of the tree to be restored fully so it does not matter the initial state at the end just we restore such parts fully and everything works as facelets 1.1.x. But even so it is a workaround and we need a better solution to that.In practice points a. b. c. and d. are relevant. The case e. related to c:forEach is not because the iterated data has usually a life longer than the view (session conversation or view scope) so we can exclude this case. The solution to this problem is store the result of the evaluated expressions within the view. It is the only that will work as good as org.apache.myfaces.REFRESH_TRANSIENT_BUILD_ON_PSS_PRESERVE_STATE. To do that we need to generate an unique id per tag and make components and tags inside them generate unique tags too but the same ids should be generated each time the view is build no matter the conditions. For example:< h:outputText value=""-A-""/>< c:if test=""#{condition}"">< h:outputText value=""-B-""/>< /c:if>< h:outputText value=""-C-""/>-C- should have the same id each time no matter what happen inside c:if. Since PSS algorithm uses clientIds to store the ""delta"" state of the components ensure that is CRITICAL otherwise the state will get mixed or lost.Additionally we must restore that ""facelet state"" before the view is built to use it when the initial state is derived.",5645
Extract Method,Implement ResourceHandler.markResourceRendered(...) and ResourceHandler.isResourceRendered(...) Implement ResourceHandler.markResourceRendered(...) and ResourceHandler.isResourceRendered(...) as described in the spec.The current implementation is just move the code from ResourceUtils that uses a simple map but it is relevant to see how this feature works with dynamic resources loaded from ajax requests.,5646
Rename Method,Implement CDI changes for JSF 2.3 The idea is implement the following annotations:ApplicationMapFlowMapHeaderMapHeaderValuesMapInitParameterMapRequestCookieMapRequestMapRequestParameterMapRequestParameterValuesMapSessionMapViewMapThe tricky part here is some objects are managed by JSF and others by CDI.,5647
Move Method,Make a way to get the FacesConfig from a provider Currently MyFaces startup listener will parse the all the faces configuration files and sort them on each startup time and it will be better to do it once in the deployment time and get those data structure instances from a provider. One possible way is to make those FacesConfig class serializable.,5648
Extract Method,Make method of determinine app context in FactoryFinder pluggable As discussed on dev list.... geronimo would like to explicitly mark component boundaries rather than relying on the TCCL changing.,5649
Extract Method,[perf] [concurrency] check correct logger creation <BLOCKED>java.util.logging.Logger.getLogger(String)javax.faces.component.UIViewRoot.<init>()problem is that has non-static logger and logger is requested in every view init. For a class (static) method the monitor associated with the Class object for the method's class is used. Check myfaces code base for this usage.,5650
Extract Method,"CSP: nonce attribute on script tags will be ignored on ajax updates simple CSP case: - add a static nonce via phaselistener/servlerfilter in the headers - add the the static nonce to a script tag this works fine for a GET request or non-ajax POST but our ajax engine just ignores the nonce attribute on scripts and following error occurs in the browser: Content Security Policy: Die Einstellungen der Seite haben das Laden einer Ressource auf inline blockiert (""script-src""). There will probably other tickets in the future but thats the first basic case which must be supported. There are of course other problems like onclick handlers in the DOM or the eval node in the partial-response. Similar to: https://github.com/jquery/jquery/issues/3541",5651
Inline Method,cdi support for converters and validators with<context-param><param-name>org.apache.myfaces.CDI_MANAGED_CONVERTERS_ENABLED</param-name><param-value>true</param-value></context-param>and<context-param><param-name>org.apache.myfaces.CDI_MANAGED_VALIDATORS_ENABLED</param-name><param-value>true</param-value></context-param>it should be possible to enable cdi support for converters/validators.we need the config because it was postponed for the spec.,5652
Extract Method,Update Model didn't throw an exception if an exception occurred but adds a message for display to the user According to all good sense update model should throw an exception and not show a message in the message list if setting the value of the model-bean throws an exception itself. Hopefully this won't make any problems with the TCK if it does we should challenge it.regardsMartin,5653
Inline Method,[perf] avoid FormInfo instances ,5654
Rename Method,"Create new module for JUnit Mock Testing using MyFaces Core MyFaces Test and CDI This issue is the next step in the work started in MYFACES-3376 ""Create abstract test classes that runs MyFaces Core as in a container"".",5655
Move Method,create shared-public module Create shared-public module as discussed on dev list. See discussion:http://markmail.org/message/ujqdvipurs6zzju5?q=[DISCUSS]+how+to+get+rid+of+tons+of+duplicated+code,5656
Extract Method,Implement Bean Validation Implement BeanValidator class corresponding tags/configs and logic related to Bean Validation.,5657
Move Method,Remove CompositeComponentResourceTagHandler.ATTACHED_OBJECT_HADLERS_KEY from Attributes Map Another state saving improvement is to remove the attached object handlers from the component attributes map. Instead a map that holds the attached object handlers indexed by the component reference will be created in CompositeComponentResourceTagHandler.,5658
Rename Method,"Allow ELResolvers filtering related to topichttp://www.mail-archive.com/dev@myfaces.apache.org/msg49177.htmlProblem: how to disable ELResolver smartly? Adding a context-param for each is anoverkill.But we have https://cwiki.apache.org/MYFACES/elresolver-ordering.html incodebase already. I propose to add new feature ""ELResolver filtering""and new context-param:< context-param><param-name>org.apache.myfaces.EL_RESOLVER_PREDICATE</param-name><param-value>org.foo.bazz.ELResolverPredicate</param-value>< /context-param>Filter is simple instance of org.apache.commons.collections.Predicate.For application where no ManagedBean(Resolver) is used or no Flash usercan simply return false from Predicate.evaluate and ELResolver won't beinstalled.See mail thread here: http://www.mail-archive.com/dev@myfaces.apache.org/msg52082.html",5659
Rename Method,Ajax behavior and renderer need improvements and new implementations There was no delta state saving in our existing ajax behavior and the renderer has not been implemented,5662
Extract Method,MyFaces performance improvements for production Several fixes to enhance startup memory footprint and runtime performance taking advantage of ProjectStage.-lazy loading of validators converters behaviorscomponents - can have a substantial impact on startup footprint in applications with multiple or very large widget libraries.Turn off some updating of resources for ProjectStage=Production by default (can always override using javax.faces.FACELETS_REFRESH_PERIOD)-change default facelets refresh interval to -1 when projectStage is production. This by itself gains a 60% improvement in throughput.-disable reloading of web.xml and faces-config after the first load. -store a map to cache Class to listenerFor and resourceDependency annotations when in production. ,5663
Rename Method,Implement f:websocket and related api Implement f:websocket proposal as described in the latest javadoc.,5664
Move Method,Remove @ResolveComponent it was a prototype for the new search expressionsi think we should create a patch and create a spec issuecurrently there is no need to leave it in MyFaces we have to discuss some solutions first on spec level,5665
Move Method,Remove < EL 2.2 compatibility hacks as we also removed servlet 2.5 compatibility ,5667
Extract Method,[perf] cache FacesContext at ClientBehaviorBase level Just use the same hack done in UIComponentBase to avoid calls to FacesContext.getCurrentInstance(),5668
Rename Method,"Add alwaysRecompile mode for EL Expression Cache Mode In MYFACES-3160 EL Expression Cache Mode was introduced but soon it was seen aproblem found on MYFACES-3169 (ui:param and c:set implementations does not work as expected).There are two problems that limit the scope where EL Expression Cache can be used:1. Facelets user tags cannot cache EL Expressions.2. Inclusions using ui:param must always contains the same number of parameters.To understand the reasons it is worth to remember this example:a.xhtml< ui:composition template=""c.xhtml""><ui:param name=""var1"" value=""value1""/>< /ui:composition>b.xhtml< ui:composition template=""c.xhtml""><ui:param name=""var1"" value=""value1""/><ui:param name=""var2"" value=""value2""/>< /ui:composition>c.xhtml< ui:composition><h:outputText value=""#{var1}/><h:outputText value=""#{var2}/>< /ui:composition>When facelet c.xhtml is constructed from a.xhtml ""var2"" is not recognized asa parameter so all EL expressions inside c.xhtml holding refereces to ""var2""will be cached. Later facelet c.xhtml is reused from b.xhtml but since some EL expressions are cached the passed value in ""var2"" is not taken into account and the error arise.In this point it is good to remember that ui:include or ui:decorate or user tags are build view time tags so they are executed only when the view isbuilt. Parameters or attributes passed by ui:param or as user tag attributesfollows the same principle they are calculated on build view time throughVariableMapper and the evaluation is stored inside the EL Expression. Thismeans all EL Expressions holding references to these variables cannot becached and needs to be generated each time the view is built.There is no way to know beforehand which references are affected becausein a template or an user tag there is no declaration of the parameters orattributes. But from user point of view that's good because in this contexta declaration of the parameters is just not necessary.The problem is ui:param and user tags are very useful features widely used.A solution to this problem will improve performance in those cases.I have been thinking for a long time how to solve this trying different strategies. Use some kind of concurrency algorithm inside TagAttributeImpldoes not work because it is too expensive or use a central storage for cache the expressions by the cost involved in the comparisons.The objective of cache EL expressions inside facelets abstract syntax tree (AST) is minimize the calculations required to get a valid expression. ELimplementations has already an internal map that cache that informationbut that code usually has synchronized blocks or similar things. In thatsense the idea is rely on that storage in those EL expressions where there is no choice and they need to be recreated.After doing many experiments in this part I came up with a solution whichinvolves the following points:1. Associate to a facelet the parameters that were considered as passed through ui:param or as a user tag attribute. If in some point of timewe know for example c.xhtml uses var1 just consider it as c.xhtml(var1).2. Use DefaultVariableMapper to track the parameters that are passed throughui:param or as a user tag attribute. When the EL expression is created ifit uses at least one parameter mark the expression as not cacheable.3. Override FaceletCache implementation and force a recompilation of a facelet if a new parameter is detected that was not considered the first time the template was created.4. A facelet stored in the cache can be used if and only if all the parameters used for the template where considered when it was compiled atfirst time.In the example proposed when facelet c.xhtml is constructed from a.xhtmlwe say that c.xhtml was built with var1 as a known parameter or c.xhtml(var1). when we try to reuse facelet c.xhtml from b.xhtml we discoverthat var2 is also a parameter but since the cached facelet is c.xhtml(var1)the algorithm discard the facelet and create a new one but taking intoaccount var2 too so the new facelet becomes c.xhtml(var1var2). If thereis a call to c.xhtml with no params it is considered that c.xhtml(var1var2)can be used in that case.The final effect is just some extra compilations of the same facelet atstartup but in the medium/long term the information we need is calculated and associated with the facelet url. Nice!. Facelet is very fast doing thoseextra compilation steps and the final effect over performance really pays off. We could even set this mode as default.The only disadvantage of this strategy is the current contract of FaceletCacheis insuficient. As it has been described in MYFACES-3705 there are implementation details inside MyFaces Core and in our facelets implementationthat needs to be exposed in a proper way. We need to create a customAbstractFaceletCache and specify how to implement it.",5669
Rename Method,Use the same key in server side state saving for ajax requests The current code for server side state saving creates one key per request to store the view state. This is ok but it is not necessary for ajax requests. The reason why is not necessary is because you can never go back to a page when using ajax. If you are on page A and the current request is an ajax request and it returns to the same page and the view is the same that the one that has been restored the key or the token sent does not need to change what changes is the internal state of the view. From the client side the page is the same. We can take advantage of this fact and just update the state stored in SerializedViewCollection for the view. The challenge here is detect when this strategy is applicable. For example what happen if there is an ajax redirect? It looks is a good idea for implement in 2.2 because it avoids to store unnecessary information into session and optimize the use of each view slot.,5670
Extract Method,Handle JSP-Exceptions in MyFaces Error-Handling as well JSP-Exceptions hadn't been handled specificially - now they are so the stack-trace is not cut off at JSP-Exceptions.,5671
Extract Method,"Improved ViewState handling I improved the performance of the view state writing in MyFaces 1.2.1 quite a bit with the following 2 changes:1) The algorithm for replacing the form state marker in JspViewHandlerImpl is much faster now (improved indexOf buffered writing StringBuilder)2) The view state can now be rendered with javascript. If this feature is enabled (via context parameter ""org.apache.myfaces.VIEWSTATE_JAVASCRIPT"") the view state hidden inputs in the forms are rendered with empty value attributes. The actual viewstate is then filled in on the client with a rendered javascript function that simply iterates over all forms.",5672
Move Method,"[perf] Improve html Renderers There are some improvements we can do for our html Renderers:- Complete the optimization started in MYFACES-3237 reducing call to getAttributes().get().- Replace StringBuffer with StringBuilder.- Optimize client behaviors like with MYFACES-3237 too.- ValueExpressions that return empty String ("""") for a passthrough property should not be rendered because it is not necessary.- Create some new optimized methods for render properties but let the old api intact for now.",5673
Extract Method,"Implement h:selectOneRadio ""group"" (distributed radio button) Implement h:selectOneRadio ""group"" (distributed radio button)",5674
Extract Method,"[perf] minimize ExternalContext.getInitParameter invocations in myfaces API init-param from webapp context cannot change - it is sufficient read it once in constructor (ApplicationImpl for example)New problem (new in 2.1) is ""great"" javax.faces.HONOR_CURRENT_COMPONENT_ATTRIBUTES param for pushComponentToEL and popComponentFromEL - at least we can cache it in component (because push and pop are called more then ones during one lifecycle) and maybe cache it's value also in org.apache.myfaces.context.servlet.ServletExternalContextImplBase.getInitParameter(String)I have about ~ 90 000 invocations of getInitPaparameter in my test case in one request/response and that is not cheap.",5676
Rename Method,Use Java8 instead of commons e.g. Base64 - will check other used partsCollections:* EmptyIterator (/)* LRUMap (/)* CollectionUtils.filter (/)* Predicate (/)Coded: (/)* Hex (Could be replaced by javax/xml/bind/DatatypeConverter) (/)* DecoderException (/)* Base64 (/)Digester:* Digester (x)BeanUtils:* BeanUtils (x)* PropertyUtils (x),5677
Extract Method,"[perf] Use lazy init for HashMap/HastSet where possible goal: make component tree creation fast as possible and avoid unnecessary instancessome components instantialize attributes direct in field or in constructor. In many cases it is not necessary because those HashMap/Set instances are not used in current request/response - it depends on use case.Example: UIViewRoot.listenerSuccessMap: lazy init is very suitable because many view have no phase listener.Check all components: candidates are ""smarter"" components like UIDate or UIInput",5679
Extract Method,cleanup of UIInput looking at shouldValidateEmptyFields() I see this code:...ExternalContext extCtx = context.getExternalContext();String validateEmptyFields = (String)extCtx.getInitParameter(VALIDATE_EMPTY_FIELDS_PARAM_NAME);if (validateEmptyFields == null){validateEmptyFields = (String)extCtx.getApplicationMap().get(VALIDATE_EMPTY_FIELDS_PARAM_NAME);}....=> Should be cached on application_map instead of always parsing the web.xml (getInitParam())-Actually I don't see a put to stored it on the applcaitonMap;Inside of the validate() I see similar code:String contextParam =context.getExternalContext().getInitParameter(EMPTY_VALUES_AS_NULL_PARAM_NAME);=> not cachedSimilar in the _ExternalSpecifications clazz no cache maintained here,5680
Rename Method,Allow ConvertCharacterSet to accept expression language This issue arose from a user on the mailing list. It demonstrates the need to be able to use expression language to set the incoming (and potentially outgoing) character sets:I'm looking to process many files into common formats. The source files are coming in various character sets mime types and new line terminators.My thinking for a data flow was along these lines:GetFile (from many sub directories) -> ExecuteStreamCommand (file -i) ->ConvertCharacterSet (from previous command to utf8) ->ReplaceText (to change any \r\n into \n) ->PutFile (into a directory structure based on values found in the original file path and filename)Additional steps would be added for archiving a copy of the original converting xml files etc.Attempting to process these with Nifi leaves me confused as to how to process within the tool. If I want to ConvertCharacterSet I have to know the input type. I setup a ExecuteStreamCommand to file -i ${absolute.path:append(${filename})} which returned the expected values. I don't see a way to turn these results into input for the processor which doesn't accept expression language for that field.I also considered ConvertCSVToAvro as an interim step but notice the same issue. Any suggestions what this dataflow should look like?,5681
Inline Method,DFM should be allowed to inspect/interact with FlowFiles on a Connection User should be able to see the attributes as well as download the content for flowfiles in the top of the active queue.Additional tickets have been created and linked for searching removing and uploading flowfiles in a given queue.,5682
Extract Method,Add option to ExecuteStreamCommand to put value of execution to an attribute This issue arose from a user on the mailing list. It demonstrates the need to be able to put the output of ExecuteStreamCommand to an attribute:I'm looking to process many files into common formats. The source files are coming in various character sets mime types and new line terminators.My thinking for a data flow was along these lines:GetFile (from many sub directories) -> ExecuteStreamCommand (file -i) ->ConvertCharacterSet (from previous command to utf8) ->ReplaceText (to change any \r\n into \n) ->PutFile (into a directory structure based on values found in the original file path and filename)Additional steps would be added for archiving a copy of the original converting xml files etc.Attempting to process these with Nifi leaves me confused as to how to process within the tool. If I want to ConvertCharacterSet I have to know the input type. I setup a ExecuteStreamCommand to file -i ${absolute.path:append(${filename})} which returned the expected values. I don't see a way to turn these results into input for the processor which doesn't accept expression language for that field.I also considered ConvertCSVToAvro as an interim step but notice the same issue. Any suggestions what this dataflow should look like?Bryan Bende's response:One problem with the above flow is that ExecuteStreamCommand will replace the contents of the FlowFile with the results of the command so the FlowFIle will have the encoding value and no longer have the original content.,5683
Extract Method,Add multipart upload support to PutS3Object A new `PutS3ObjectMultipart` processor using the AWS S3 API to upload files larger than those supported by `PutS3Object` which has a [5GB limit|http://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html] limit.To support S3 compatible endpoints this will also add an `Endpoint Override URL` property to `AbstractAWSProcessor` to set the service [endpoint|http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/AmazonWebServiceClient.html#setEndpoint(java.lang.String)] to override the endpoint URL normally selected based on the the Amazon region.,5684
Extract Method,Allow user to configure Batch Size for site-to-site Currently there is no way for a user to specify the batch size that Site-to-Site will use. The framework decides this for you. However if we want to use the List/Fetch Pattern it will be helpful to specify a small batch size so that a small number of things that are listed are still well distributed across the cluster.,5685
Extract Method,Kerberos based authentication Add support for Kerberos based authentication.,5687
Extract Method,Enhance AWS S3 fetch to access bucket across accounts The AWS S3 Fetch Object component does not allow access to bucket across accounts. AWS S3 Fetch Object with can be enhanced to provide this functionality by using assume role session/credentials,5688
Move Method,Expose the Content-Type used for Invoke/PostHTTP InvokeHTTP and PostHTTP make use of the mime.type attribute. While this is in documentation it is not as apparent to the user and kind of applied behind the scenes. What would be nice is to have a property that brings this to the forefront in the processor configuration and shows that this is mapped to the EL ${mime.type}.,5689
Rename Method,Splunk Processors To continue improving NiFi's ability to collect logs a good integration point would be to have a processor that could listen for data from a Splunk forwarder (https://docs.splunk.com/Splexicon:Universalforwarder). Being able to push log messages to Splunk would also be useful.Splunk provides an SDK that may be helpful:https://github.com/splunk/splunk-sdk-java,5690
Extract Method,Develop Bootstrap module We need a module that can parse a configuration file in order to determine runtime parameters to run the Java executable for NiFi.This should include JVM args such as agentlib Xmx Xms etc. as well as indicating where the NiFi properties file can be found and what the Java command should be (java vs. $JAVA_HOME/bin/java etc.),5691
Rename Method,Refactor lifecycle code for Processors and other components Similar lifecycle handling improvements that went in as part of the NIFI-1164 for ControllerServices could/should be applied to other components (e..g Processors).The improvements may also help to address NIFI-78 (may be without killing the thread).,5692
Extract Method,Add multipart request support to ListenHTTP Processor The current ListenHTTP processor does not seem to support multipart requests that are encoded with multipart/form-data. When a multipart request is received the ListenHTTPServlet just copies the Request InputStream to the FlowFiles content which leaves the form encoding wrapper in the content and in turn makes the file invalid. Specifically we want to be able to support file uploads in a multipart request. See this thread in the mailing list for more info: http://mail-archives.apache.org/mod_mbox/nifi-users/201602.mbox/%3C6DE9CEEF-2A37-480F-8D3C-5028C590FD9E%40acesinc.net%3E,5694
Extract Method,Allow user to specify file filter regex when unpacking zip/tar archives There are times where you may want to only extract a portion of an archive such as a specific folder or perhaps a specific file. Similar to how the {{GetFile}} processor works we should provide a property ({{File Filter}}) which by default extracts all files. The user can modify this property to extract only files they wish to process downstream. ,5695
Extract Method,"Add ability to query DB table using ""last value"" from column It would be a useful feature for a processor to be able to query from a database table for only those records that have are added/available since the last time the query was executed. For example if the processor could keep the max value of a timestamp column for the last result set returned the next time the query was issued it could use that timestamp value and column to only return records whose timestamps were greater than ""the last time"".This shouldn't be limited to timestamps of course; it would be useful for any strictly-increasing value (like primary key ID) but would be up to the user to select the table and column. The processor would simply keep the state for the specified column's max value.Proposed is a ""QueryDBTable"" processor which would have a properties to specify the table name and the column in the table for which to keep state information about the last maximum value retrieved. Subsequent queries of the table use this value to filter for rows whose value for the specified column are greater than the ""last maximum value"". Upon each successful query the ""last maximum value"" is updated.",5696
Extract Method,New processor to update attributes with state This idea was sparked by a thread on the user list and should allow basic data science:I expect that in the future I’ll need something a little more sophisticated but for now my problem is very simple:I want to be able to trigger an alert (only once) when an attribute in an incoming stream for instance goes over a predefined threshold. The Processor should then trigger (only once again) another trigger when the signal goes back to normal (below threshold). Basically a RouteByAttribute but with memory.Thanks Claudio------------------------------------------------Hello ClaudioYour use-case actually could leverage a couple of recently added features to create a really cool open-source processor. The two key features that were added are State Management and the ability to reference processor specific variables in expression language. You can take a look at RouteText to see both in action. By utilizing both you can create a processor that is configured with multiple Expression language expressions. There would be dynamic properties which would accept expression language and then store the evaluated value via state management. Then there would be a routing property (that supports expression language) that could simply add an attribute to the flowfile with the evaluated value which would allow it to be used by flowing processors for routing.This would allow you to do your use-case where you store the value for the incoming stream and route differently once you go over a threshold. It could even allow more complex use-cases. One instance I believe would be possible is to have a running average and standard deviation and route data to different locations based on it's standard deviation.You can think of this like an UpdateAttribute with the ability to store and calculate variables using expression language.Joe,5697
Extract Method,ListHDFS to retaining wrong state in zookeeper The expected state that should be retained by listHDFS processor is the last modified timestamp. The processor instead is retaining the filename and path of every file it lists. This results in excessive disk usage by zookeeper to retain this filename based state.,5698
Extract Method,Create PutUDP Processor Create a processor to send each FlowFile as a datagram to a UDP server.Processor must provide properties to allow the user to configure the destination host address and port to send to.,5699
Move Method,Add support for ORC format From the Hive/ORC wiki (https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC): The Optimized Row Columnar (ORC) file format provides a highly efficient way to store Hive data ... Using ORC files improves performance when Hive is reading writing and processing data.As users are interested in NiFi integrations with Hive (NIFI-981 NIFI-1193 etc.) NiFi should be able to support ORC file format to enable users to efficiently store flow files for use by Hive.,5700
Extract Method,"Extend QueryDatabaseTable to support arbitrary queries The QueryDatabaseTable is able to observe a configured database table for new rows and yield these into the flowfile. The model of an rdbms however is often (if not always) normalized so you would need to join various tables in order to ""flatten"" the data into useful events for a processing pipeline as can be build with nifi or various tools within the hadoop ecosystem.The request is to extend the processor to specify an arbitrary sql query instead of specifying the table name + columns.In addition (this may be another issue?) it is desired to limit the number of rows returned per run. Not just because of bandwidth issue's from the nifi pipeline onwards but mainly because huge databases may not be able to return so many records within a reasonable time.",5701
Extract Method,Update HTTP processors to allow a proxy authentication strategy If trying to use the HTTP processors from behind a proxy server the following error occurs:Received error HTTP_ERROR: HTTP/1.1 407 Proxy Authentication Required. Will attempt to reconnectThe library NiFi uses for HTTP processors has the ability to set a proxy authentication strategy but currently doesn't use it.,5702
Extract Method,Simplify revision management that happens in StandardNiFiServiceFacade Currently StandardNiFiServiceFacade has a huge amount of repetitive code to ensure that changes to the flow occur always with the appropriate revision and that the revision is updated appropriately.However this is very tedious and prone to copy/paste errors and requires updating the code in many places if something changes.With NiFi 1.0 depending on Java 8 we can make use of Java's Lambda functions in order to dramatically simplify the code.,5703
Extract Method,Create a FetchHBase Processor We should provide a processor to fetch a row from HBase. The processor should support receiving an incoming FlowFile and taking the row id to fetch from an attribute on the incoming and should also be able to fetch a static row id.,5704
Move Method,"Create General MQTT Processors MQTT[1] is a great ""Internet of Things"" (IoT) connectivity protocol that implementing processors for would allow NiFi to continue expanding into the IoT domain. A prime opportunity would be to use in conjunction with Apache NiFi - MiNiFi.[1] http://mqtt.org/",5705
Extract Method,Add nifi-env script The nifi-env script should be created for use by nifi startup status and shutdown scripts to set env variables like JAVA_HOME location of log dir location of pid dir etc. This makes it easier to manage these locations in a single script. For example currently the path to java is set in bootstrap.conf however if your system does not have JAVA_HOME set or java in path the nifi start command fails (so the user has to explicitly set JAVA_HOME or PATH before invoking nifi start).,5706
Rename Method,"Allow encrypted passwords in configuration files Storing passwords in plaintext in configuration files is not a security best practice. While file access can be restricted through OS permissions these configuration files can be accidentally checked into source control shared or deployed to multiple instances etc. NiFi should allow a deployer to provide an encrypted password in the configuration file to minimize exposure of the passwords. On application start-up NiFi should decrypt the passwords in memory. NiFi should also include a utility to encrypt the raw passwords (and optionally populate the configuration files and provide additional metadata in the configuration files). I am aware this simply shifts the responsibility/delegation of trust from the passwords in the properties file to a new location on the same system but mitigating the visibility of the raw passwords in the properties file can be one step in a defense in depth approach and is often mandated by security policies within organizations using NiFi. The key used for encryption should not be hard-coded into the application source code nor should it be universally consistent. The key could be determined by reading static information from the deployed system and feeding it to a key derivation function based on a cryptographically-secure hash function such as PBKDF2 bcrypt or scrypt. However this does introduce upgrade system migration and portability issues. These challenges will have to be kept in consideration when determining the key derivation process. Manual key entry is a possibility and then the master key would only be present in memory but this prevents automatic reboot on loss of power or other recovery scenario. This must be backward-compatible to allow systems with plaintext passwords to continue operating. Options for achieving this are to only attempt to decrypt passwords when a sibling property is present or to match a specific format. For these examples I have used the following default values:{code}password: thisIsABadPasswordkey: 0123456789ABCDEFFEDCBA98765432100123456789ABCDEFFEDCBA9876543210iv: 0123456789ABCDEFFEDCBA9876543210algorithm: AES/CBC 256-bit{code}**Note: These values should not be used in production systems -- the key and IV are common test values and an AEAD cipher is preferable to provide cipher text integrity assurances however OpenSSL does not support the use of AEAD ciphers for command-line encryption at this time**Example 1: *here the sibling property indicates the password is encrypted and with which implementation; the absence of the property would default to a raw password*{code}hw12203:/Users/alopresto/Workspace/scratch/encrypted-passwords (master) alopresto__ 0s @ 16:25:56 $ echo ""thisIsABadPassword"" > password.txthw12203:/Users/alopresto/Workspace/scratch/encrypted-passwords (master) alopresto__ 0s @ 16:26:47 $ ossl aes-256-cbc -e -nosalt -p -K 0123456789ABCDEFFEDCBA98765432100123456789ABCDEFFEDCBA9876543210 -iv 0123456789ABCDEFFEDCBA9876543210 -a -in password.txt -out password.enckey=0123456789ABCDEFFEDCBA98765432100123456789ABCDEFFEDCBA9876543210iv =0123456789ABCDEFFEDCBA9876543210hw12203:/Users/alopresto/Workspace/scratch/encrypted-passwords (master) alopresto__ 0s @ 16:27:09 $ xxd password.enc0000000: 5643 5856 6146 6250 4158 364f 5743 7646 VCXVaFbPAX6OWCvF0000010: 6963 6b76 4a63 7744 3854 6b67 3731 4c76 ickvJcwD8Tkg71Lv0000020: 4d38 6d32 7952 4776 5739 413d 0a M8m2yRGvW9A=.hw12203:/Users/alopresto/Workspace/scratch/encrypted-passwords (master) alopresto__ 0s @ 16:27:16 $ more password.encVCXVaFbPAX6OWCvFickvJcwD8Tkg71LvM8m2yRGvW9A=hw12203:/Users/alopresto/Workspace/scratch/encrypted-passwords (master) alopresto__ 0s @ 16:27:55 ${code}In {{nifi.properties}}: {code}nifi.security.keystorePasswd=VCXVaFbPAX6OWCvFickvJcwD8Tkg71LvM8m2yRGvW9A=nifi.security.keystorePasswd.encrypted=AES-CBC-256{code}Example 2: *here the encrypted password has a header tag indicating both that it is encrypted and the algorithm used*{code:java}@Testpublic void testShouldDecryptPassword() throws Exception {// ArrangeKeyedCipherProvider cipherProvider = new AESKeyedCipherProvider()final String PLAINTEXT = ""thisIsABadPassword""logger.info(""Expected: ${Hex.encodeHexString(PLAINTEXT.bytes)}"")final byte[] IV = Hex.decodeHex(""0123456789ABCDEFFEDCBA9876543210"" as char[])final byte[] LOCAL_KEY = Hex.decodeHex(""0123456789ABCDEFFEDCBA9876543210"" * 2 as char[])// Generated via openssl enc -afinal String CIPHER_TEXT = ""VCXVaFbPAX6OWCvFickvJcwD8Tkg71LvM8m2yRGvW9A=""byte[] cipherBytes = Base64.decoder.decode(CIPHER_TEXT)SecretKey localKey = new SecretKeySpec(LOCAL_KEY ""AES"")EncryptionMethod encryptionMethod = EncryptionMethod.AES_CBClogger.info(""Using algorithm: ${encryptionMethod.getAlgorithm()}"")logger.info(""Cipher text: \$nifipw\$${CIPHER_TEXT} ${cipherBytes.length + 8}"")// ActCipher cipher = cipherProvider.getCipher(encryptionMethod localKey IV false)byte[] recoveredBytes = cipher.doFinal(cipherBytes)// OpenSSL adds a newline character during encryptionString recovered = new String(recoveredBytes ""UTF-8"").trim()logger.info(""Recovered: ${recovered} ${Hex.encodeHexString(recoveredBytes)}"")// Assertassert PLAINTEXT.equals(recovered)}{code}In {{nifi.properties}}: {code}nifi.security.keystorePasswd=$nifipw$VCXVaFbPAX6OWCvFickvJcwD8Tkg71LvM8m2yRGvW9A={code}Ideally NiFi would use a pluggable implementation architecture to allow users to integrate with a variety of secret management services. There are both commercial and open source solutions including CyberArk Enterprise Password Vault [1] Hashicorp Vault [2] and Square Keywhiz [3]. In the future this could also be extended to Hardware Security Modules (HSM) like SafeNet Luna [4] and Amazon CloudHSM [5]. [1] http://www.cyberark.com/products/privileged-account-security-solution/enterprise-password-vault/[2] https://www.vaultproject.io/[3] https://square.github.io/keywhiz/[4] http://www.safenet-inc.com/data-encryption/hardware-security-modules-hsms/luna-hsms-key-management/luna-sa-network-hsm/[5] https://aws.amazon.com/cloudhsm/",5707
Inline Method,Add support for Azure Blob Storage and Table Storage It would be useful to have an Azure equivalent of the current S3 capability. Azure also provides a Table storage mechanism providing simple key value storage. Since the Azure SDKs are Apache Licensed this should be reasonably straightforward. A first cut is available as an addition to the existing azure bundle.,5708
Extract Method,Auto-adjust template layouts upon placement on the canvas Positions of components in templates created by pre-1.0.0 versions of NiFi need to be positionally rescaled due to the UI changes described in NIFI-1799. New templates created by NiFi as of version 1.0.0 will need to include a template encoding version that gets updated as the template DTO changes.,5709
Extract Method,"Support Custom Properties in Expression Language Add a property in ""nifi.properties"" config file to allows users to specify a list of custom properties files (containing data such as environmental specific values or sensitive values etc.). The key/value pairs should be loaded upon NIFI startup and availbale to processors for use in expression languages. Optimally this will lay the groundwork for a UI driven Variable Registry.",5710
Extract Method,Enhance JoltTransformJSON processor to support custom transforms Jolt supports additional custom transforms via fully-qualified Java classnames. Would like to provide the ability to support custom transformation (via drop in jars) for the Jolt Transform processor.,5711
Inline Method,Support processor implementation across several programming languages Should also support Clojure as per discussion and request on mailing list http://mail-archives.apache.org/mod_mbox/nifi-dev/201506.mbox/%3CCAMpSqch4GK1gnw6M1u8tH6AN8e_miXZN5SNkAeMjBujXYGqJiw%40mail.gmail.com%3EUPDATE: The ScriptEngine for Clojure is not being maintained and is not currently available via Maven Central or a public repository. Recommend adding Clojure as a separate Improvement Jira case.UPDATE: Scala support proved too buggy using plain script engine. Needs much further analysis/review/testing before inclusion.,5712
Extract Method,Cache compiled XSLT in TransformXml TransformXml appears to be recompiling the XSLT on every onTrigger event which is slow. It should cache the compiled stylesheets.,5713
Extract Method,Improve RouteText performance with pre-compilation of RegEx in certain cases When using RegEx matches for the RouteText processor (and possibly other processors) the RegEx gets recompiled every time the processor works. The RegEx could be precompiled / cached under certain conditions in order to improve the performance of the processorSee email from Mark Payne:Re #2: The regular expression is compiled every time. This is done though because the Regex allows the ExpressionLanguage to be used so the Regex could actually be different for each FlowFile. That being said it could certainly beimproved by either (a) pre-compiling in the case that no Expression Language is used and/or (b) cache up to say 10Regex'es once they are compiled.,5714
Extract Method,Mock Framework needs to allow TestRunner.run indicate whether or not to call @OnScheduled methods Currently each time TestRunner#run is called all methods in the Processor with an @OnScheduled annotation get called. This cannot be avoided. This can cause a lot of problems in some cases though. Need to be able call TestRunner#run without it invoking @OnScheduled methods.,5716
Extract Method,Add support for LDAPS in authentication provider [~mcgilman] [~alopresto] please add thoughts if you have them.I propose we add support for LDAPS despite StartTLS being the now preferred approach. This offers more flexibility for use with many of the long standing LDAP environments out there.,5717
Extract Method,Update usage of deprecated methods ,5719
Extract Method,SSLContextService should support specifying Key Password Currently the SSLContextService supports only a keystore password and assumes that the key password is the same. We should support setting a separate password for the key itself.,5721
Inline Method,TestRunner should not expose a VariableRegistry construct Currently the TestRunners class provides a mechanism for creating a TestRunner that takes in a Variable Registry. The concept of providing variables to the Test Runner is very valuable. However the notion of the VariableRegistry is an internal implementation detail that is getting exposed creating a leaky abstraction.We should remove the override of the createTestRunner() method that takes the Variable Registry and instead expose a set of methods of TestRunner:void setVariable(String name String value)String getVariable(String name)void setVariables(Map<String String> variables)Map<String String> getVariables()Additionally as-is if no VariableRegistry is provided the default is to use a Variable Registry that provides System and Environment variables. This should be avoided as it encourages unit tests to depend on environment-specific settings.,5722
Rename Method,Allow for configuration of Controller Services and Reporting Tasks in UI Currently Controller Services and Reporting Tasks are specified in configuration files where NiFi is installed. We need to make this configuration more accessible by moving into the UI and not requiring a restart of the application.,5723
Extract Method,QueryDatabaseTable should support splitting the ResultSet into multiple flow files A new Property that defaults to disabled should allow for ResultSets to be split up into multiple output flow files.,5728
Extract Method,"Add attributes to track where a flow file came from when receiving over site-to-site With MiNiFi starting be used to send data to a central NiFi it would be helpful if information about the sending host and port was added to each flow file received over site-to-site. Currently this information is available and used to generate the transit URI in the RECEIVE event but this information isn't available to downstream processors that might want to make routing decisions.For reference:https://github.com/apache/nifi/blob/e23b2356172e128086585fe2c425523c3628d0e7/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-site-to-site/src/main/java/org/apache/nifi/remote/protocol/AbstractFlowFileServerProtocol.java#L452A possible approach might be to add two attributes to each flow file something like ""remote.host"" and ""remote.address"" where remote.host has only the sending hostname and remote.address has the sending host and port.",5729
Extract Method,JDBC-to-Avro processors handle BigDecimals as Strings The original SQL processors implemented BigDecimal values as Strings for Avro as the version of Avro it used (1.7.6) did not support DECIMAL type.As of Avro 1.7.7 (AVRO-1402) this type is supported and so the SQL -/HiveQL- processors should be updated to handle BigDecimals correctly if possible.UPDATED: This JIRA improved only ExecuteSQL and QueryDatabaseTable processors. SelectHiveQL is removed from target. Hive tables can be queried by ExecuteSQL/QueryDatabaseTable once NIFI-3093 is resolved (and logical types will also be supported with those processors).,5730
Extract Method,Allow PublishJMS processor to create TextMessages Create a new configuration option for PublishJMS that allows the processor to be configured to emit instances of TextMessages as well as BytesMessage.,5731
Extract Method,"ListS3 improvements: ""Use versions"" and ""Commit mode"" Our team needs to be able to list individual versions in S3. We also ran into a use case where a bucket with many objects (over 1 million in our case) seemed to cause ListS3 to run forever. The S3 list command finished in a few minutes but we believe it was taking a very long time for NiFi to commit all the flow files at once.To handle this use case we added a Commit Mode property to ListS3 that allows you specify that you want to commit ""Per page"" vs. ""Once"". This has proven to correctly emit the flow files as the S3 paging progresses.We also implemented support for S3 List Versions which includes the ""s3.version"" and ""s3.isLatest"" attributes if applicable. The ""s3.version"" attribute can in turn be used in the FetchS3 processor.",5732
Rename Method,Add JMS properties to FlowFile attributes on receive in ConsumeJMS ConsumeJMS currently adds JMS headers to the FlowFile attributes when it receives a message but ignores any JMS properties coming through. It should be reading both the headers and properties and merging them into the FlowFile attributes.,5734
Rename Method,The great typo cleanup ,5735
Extract Method,tls-toolkit standalone should allow DN specification When running the tls-toolkit in standalone mode users should be able to specify a DN for the nifi instance,5736
Rename Method,NiFi Site-To-Site with port forwarding It would be useful to be able to use port forwarding with NiFi Site-To-Site. This would allow NiFi to appear externally to be listening on a privileged port without having been granted elevated permissions.For example an administrator could configure iptables to forward traffic from port 443 to port 9443. Then users could use NiFi at port 443. This provides more flexibility as far as firewall configuration is concerned.The above scenario causes problems with Site-To-Site though because in a clustered scenario the nodes will still advertise themselves with port 9443. This would prevent a Site-To-Site client from being able to talk to them from outside the firewall.We need a way (probably a nifi property) to tell NiFi to listen on one port (9443) and advertise another (443) for Site-To-Site purposes to enable this usecase.,5737
Rename Method,"In TLS-Toolkit allow users to specify separate input and output locations for client configuration settings Currently when using the tls-toolkit to generate client certificate artifacts (keystore/truststore etc) users have the option to provide the location of a configuration file that will provide the information necessary to create those items (using the ""F"" argument). Another option can be used to allow toolkit to write out all of the settings generated back to the indicated input file (using the ""f"" argument). For scenarios where users may want to pipe in input using stdin vs referring to a file on disk this is not optimal since toolkit will attempt to write out to stdin causing an error.To prevent this error proposing to have the ""f"" argument also support an output location that is separate from the location provided with the ""F"" argument.",5738
Move Method,Add expression language support for JoltTransformJson processor Provide the ability to incorporate expression language within a Jolt Specification so that certain operations such as dynamic creation of key/value pairs can be achieved.,5740
Extract Method,Encrypted configuration migrator should be able to update sensitive properties key and migrate flow.xml.gz In order to allow changing of nifi.sensitive.props.key and updating of the flow.xml.gz the ConfigEncryptionTool should be able to accept a new value for that field and update encrypted values in the flow.xml.gz appropriately.,5741
Rename Method,Refactor base class from MergeContent The binning logic in MergeContent is extremely useful and could be pulled out into an abstract superclass. This would allow other processors to perform binning logic without being tied to a merged flow file. For example a processor may want to submit a batch request to a database like Solr or to a REST endpoint. The vast majority of the code in MergeContent would remain in the concrete class but there are several points such as the BinManager usage that could be abstracted for easy extensibility.,5742
Inline Method,"Consider deprecating org.apache.nifi.stream.io.ByteArrayOutputStream in favor of java.io.ByteArrayOutputStream I believe the ""efficiency"" statements need to be revisited. For example preliminary testing shows that there are no performance difference between using the one provided with java.io.",5744
Extract Method,"Remove Jasypt library The [Jasypt|http://www.jasypt.org/index.html] library is used internally by NiFi for String encryption operations (specifically password-based encryption (PBE) in {{EncryptContent}} and sensitive processor property protection). I feel there are a number of reasons to remove this library from NiFi and provide centralized symmetric encryption operations using Java cryptographic primitives (and BouncyCastle features where necessary). * The library was last updated February 25 2014. For comparison BouncyCastle has been [updated 5 times|https://www.bouncycastle.org/releasenotes.html] since then* {{StandardPBEStringEncryptor}} the high-level class wrapped by NiFi's {{StringEncryptor}} is final. This makes it and features relying on it difficult to test in isolation* Jasypt encapsulates many decisions about {{Cipher}} configuration specifically salt-generation strategy. This can be a valuable feature for pluggable libraries but is less than ideal when dealing with encryption and key derivation which are in constant struggle with evolving attacks and improving hardware. There are hard-coded constants which are not compatible with better decisions available now (i.e. requiring custom implementations of the {{SaltGenerator}} interface to provide new derivations). The existence of these values was opaque to NiFi and led to serious compatibility issues [NIFI-1259] [NIFI-1257] [NIFI-1242] [NIFI-1463] [NIFI-1465] [NIFI-3024]* {{StringEncryptor}} the NiFi class wrapping {{StandardPBEStringEncryptor}} is also final and does not expose methods to instantiate it with only the relevant values (i.e. {{algorithm}} {{provider}} and {{password}}) but rather requires an entire {{NiFiProperties}} instance. * {{StringEncryptor.createEncryptor()}} performs an unnecessary ""validation check"" on instantiation which was one cause of reported issues where a secure node/cluster blocks on startup on VMs due to lack of entropy in {{/dev/random}}* The use of custom salts with PBE means that the internal {{Cipher}} object must be re-created and initialized and the key re-derived from the password on every decryption call. Symmetric keyed encryption with a strong KDF (order of magnitude higher iterations of a stronger algorithm) and unique initialization vector (IV) values would be substantially more resistant to brute force attacks and yet more performant at scale. I have already implemented backwards-compatible code to perform the actions of symmetric key encryption using keys derived from passwords in both the {{ConfigEncryptionTool}} and {{OpenSSLPKCS5CipherProvider}} and {{NiFiLegacyCipherProvider}} classes which empirical tests confirm are compatible with the Jasypt output. Additional research on some underlying/related issues:* [Why does Java allow AES-256 bit encryption on systems without JCE unlimited strength policies if using PBE?|https://security.stackexchange.com/questions/107321/why-does-java-allow-aes-256-bit-encryption-on-systems-without-jce-unlimited-stre]* [How To Decrypt OpenSSL-encrypted Data In Apache NiFi|https://community.hortonworks.com/articles/5319/how-to-decrypt-openssl-encrypted-data-in-apache-ni.html]* [dev@nifi.apache.org ""Passwords in EncryptContent""|https://lists.apache.org/thread.html/b93ced98eff6a77dd0a2a2f0b5785ef42a3b02de2cee5c17607a8c49@%3Cdev.nifi.apache.org%3E]",5745
Extract Method,"Add ability to wait for N signals to Wait/Notify processors The recently added Wait and Notify processors allow a flow file to be held at the Wait processor until a signal is received in the Notify processor. It would be nice to be able to wait for N signals before releasing.One way this could be done is to have a property like ""Signal Count"" on the Wait processor and then count the keys in the cache starting with some pattern and release when the # of keys equals the signal count.This would require the ability to get all the keys from the cache or at least get all keys matching a pattern: https://issues.apache.org/jira/browse/NIFI-3214",5747
Rename Method,"Allow PublishAMQP to use NiFi expression language Enable the use of NiFi expression language for the PublishAMQP processors Routing Key value to allow it to be better used within the NiFi workflows.PublishAMQP fields to enable:""Routing Key""",5748
Rename Method,AttributesToJson performance improvements AttributesToJson does a lot of work in every onTrigger() that it doesn't need to.A lot of the attributes map logic can be done at schedule time so that it doesn't need to be done on every trigger.Also all of the properties gotten from the process context can be fetched in onSchedule(),5749
Extract Method,"TLS Toolkit - add the possibility to define a SAN in issued certificates To ease the deployment of a load balancer in front of NiFi it would be nice to allow users to define a SAN in certificates issued by the CA.To load balance the access to the UI or even with a ListenHTTP processor both will cause errors with a ""Host mismatch"" kind of error because of different fqdn between nodes certificate and LB certificate. This is also discussed here: http://stackoverflow.com/questions/40035356/nifi-load-balancer",5750
Rename Method,MoveHDFS Processor Today the PutHDFS Processor merely places a file into HDFS from NiFi. There are times when we may want to move Files/Directories around on HDFS as part of a workflow. This could be after the PutHDFS processor has placed the file or from some other trigger. Initially we are targeting to take a flow file attribute of an absolute HDFS path and be able to move it to a target HDFS Path using the Filesystem RENAME API.,5751
Extract Method,Add nifi.flow.configuration.archive.max.count property Currently we can limit the number of flow.xml.gz archive files by:* total archive size (nifi.flow.configuration.archive.max.storage)* archive file age (nifi.flow.configuration.archive.max.time)In addition to these conditions to manage old archives there's a demand that simply limiting number of archive files regardless time or size constraint.https://lists.apache.org/thread.html/4d2d9cec46ee896318a5492bf020f60c28396e2850c077dad40d45d2@%3Cusers.nifi.apache.org%3EWe can provide that by adding new property 'nifi.flow.configuration.archive.max.count' so that If specified only N latest config files can be archived.Make those properties optional and process in following order:- If max.count is specified any archive other than the latest (N-1) is removed- If max.time is specified any archive that is older than max.time is removed- If max.storage is specified old archives are deleted while total size is greater than the configuration- Create new archive keep the latest archive regardless of above limitationsTo illustrate how flow.xml archiving works here are simulations with the updated logic where the size of flow.xml keeps increasing:h3. CASE-1archive.max.storage=10MBarchive.max.count = 5||Time || flow.xml || archives || archive total |||t1 | f1 5MB | f1 | 5MB||t2 | f2 5MB | f1 f2 | 10MB||t3 | f3 5MB | f2 f3 | 10MB||t4 | f4 10MB | f4 | 10MB||t5 | f5 15MB | f5 | 15MB||t6 | f6 20MB | f6 | 20MB||t7 | f7 25MB | t7 | 25MB|* t3: The oldest f1 is removed because f1 + f2 + f3 > 10MB.* t5: Even if flow.xml size exceeds max.storage the latest archive iscreated. f4 is removed because f4 + f5 > 10MB. WAR message is logged because f5 is greater than 10MB.In this case NiFi will keep logging WAR messageindicating archive storage size is exceeding limit from t5.After t5 NiFi will only keep the latest flow.xml.h3. CASE-2If at least 5 archives need to be kept no matter what then setblank max.storage and max.time.archive.max.storage=archive.max.time=archive.max.count = 5 // Only limit archives by count|Time || flow.xml || archives || archive total |||t1 | f1 5MB | f1 | 5MB||t2 | f2 5MB | f1 f2 | 10MB||t3 | f3 5MB | f1 f2 f3 | 15MB||t4 | f4 10MB | f1 f2 f3 f4 | 25MB||t5 | f5 15MB | f1 f2 f3 f4 f5 | 40MB||t6 | f6 20MB | f2 f3 f4 f5 f6 | 55MB||t7 | f7 25MB | f3 f4 f5 f6 (f7) | 50MB (75MB)||t8 | f8 30MB | f3 f4 f5 f6 | 50MB|* From t6 oldest archive is removed to keep number of archives <= 5* At t7 if the disk has only 60MB space f7 won't be archived. Andafter this point archive mechanism stop working (Trying to create newarchive but keep getting exception: no space left on device).In either case above once flow.xml has grown to that size some humanintervention would be needed,5752
Extract Method,"Support batch update in Notify processor NIFI-3216 added ability to wait for N signals. It supports waiting for N fragments split by SplitXXXX processors. However since Notify processor has to increase count one by one by calling expensive replace cache operation over network it doesn't provide a practical performance when user configured a flow looks like below as N glow:{code}Split--> (original) -> Wait for N--> (split) -> Do something -> Notify 1{code}This JIRA improves Notify processor by:- Add ""Signal Buffer Count"" to specify max number of flow files that can be buffered and update cache at once- Add ""Signal Counter Delta"" to specify delta grater than 1 EL supported""Signal Buffer Count"" would be useful in a flow like this:{code}Split--> (original) -> Wait for N--> (split) -> Filter or something -> Notify M{code}Buffer incoming M flow files and perform cache replace operation once.So does ""Signal Counter Delta"":{code}Split--> (original) -> Wait for N--> (split) -> Filter or something -> Merge M -> PutXXX -> Notify M{code}Specify 'M' via Attribute Expression Language.",5753
Extract Method,"Add multipart request support to HandleHttpRequest Processor Currently HandleHttpRequest outputs a single FlowFile containing all multipart values as following:{code}--------------------------ef07e8bf36c274d3Content-Disposition: form-data; name=""p1""v1--------------------------ef07e8bf36c274d3Content-Disposition: form-data; name=""p2""v2--------------------------ef07e8bf36c274d3--{code}Many users requested adding upload files support to NiFi.In order for HandleHttpRequest to support multipart data we need to add followings (this is based on a brief researching and can be more complex or simple):We need to use HttpServletRequest#getParts() as written in this stackoverflow thread:http://stackoverflow.com/questions/3337056/convenient-way-to-parse-incoming-multipart-form-data-parameters-in-a-servletAlso we probably need a custom MultiPartInputStreamParser implementation. Because Jetty's default implementation writes input data to temporary directory on file system instead we'd like NiFi to write those into output FlowFiles content in streaming fashion.And we need request size validation checks threshold for those validation should be passed via javax.servlet.MultipartConfigElement.Finally we have to do something with HandleHttpResponse processor.Once HandleHttpRequest processor start splitting incoming request into multiple output FlowFiles we need to wait for every fragment to be processed then execute HandleHttpRequest.I think Wait/Notify processors (available from next version) will be helpful here.",5754
Extract Method,GenerateTableFetch Should Allow for Right Boundary When using GenerateTableFetch it places no right hand boundary on pages of data. This can lead to issues when the statement says to get the next 1000 records greater then a specific key but records were added to the table between the time the processor executed and when the SQL is being executed. As a result it pulls in records that did not exist when the processor was run. On the next execution of the processor these records will be pulled in a second time.Example:Partition Size = 1000First run (no state): Count(*)=4700 and MAX(ID)=4700.5 FlowFiles are generated the last one will say to fetch 1000 not 700. (But I don't think this is really a bug just an observation).5 Flow Files are now in queue to be executed by ExecuteSQL. Before the 5th file can execute 400 new rows are added to the table. When the final SQL statement is executed 300 extra records with higher ID values will also be pulled into NiFi.Second run (state: ID=4700). Count(*) ID>4700 = 400 and MAX(ID)=5100.1 Flow File is generated but includes 300 records already pulled into NiFI.The solution is to have an optional property that will let users use the new MAX(ID) as a right boundary when generating queries.,5755
Extract Method,TLS Toolkit - define SAN in standalone mode Following NIFI-3331 it would be useful to have the same option (add Subject Alternative Names in certificates) when using the TLS toolkit in standalone mode.,5756
Extract Method,"Let M FlowFilews pass through once N signals arrive If Wait processor can:""Let M flow files pass through once N notify signals arrived for key K""we can support more variety type of use-cases. Currently it only support ""Let 1 flow file pass through once N notify signals arrived for key K""h3. How it works? SimulationFor example let's say there are 50 incoming flow files at the beginning f1 to f50.N=3 M=100It can be read as ""Wait processor is allowed to convert 3 signals to get 100 pass tickets.""1. There's no signal for K all flow files are waiting2. Notify sends a signal. K( N=1 ) doesn't meet Wait condition Wait processor is still waiting3. Notify sends another two signals. Now K( N=3 ) matches Wait condition4. Wait processor starts consuming flow files f1 to f50 update K( N=3 M=50) where M denotes remaining number of flow files those can go through5. Another 30 flow files arrive Wait processor consumes f51 to f80 update K( N=0 M=20)6. Another 30 flow files arrive Wait processor consumes f81 to f100. K is now K( N=0 M=0 ). Since all N and M is used Wait processor removes K. f101 to f110 are waiting for signals same state as #1.h4. Alternative path after 67a. If Notify sends additional signals then f101 to f110 can go through7b. If Notify doesn't send any more signal then f101 to f110 will be routed to expiredh4. Alternative path after 56a. If Notify sends additional signal at this point K would be K( N=1 M=20). Wait processor can process 20 flow files because it still has M=20.6b. If Notify sends additional three signals K would be K(N=3 M=20). Wait processor consumes 20 flow files and when 21th flow file comes it immediately convert N to M meaning consume N(3) to create M(100) pass then K(N=0 M=100)Additionally we can let user configure M=0. Meaning Wait can release any number of incoming flow files as long as N meets the condition.With this Notify +1 can behave as if it opens a GATE and Notify –1 will close it.h4. Another possible use-case 'Limit data flow rate at cluster wide'It's more complex than just supporting GATE open/close state. However if we support M flow files to go through it can also provide rate limit across cluster.Example use case NiFi A push data via S2S to NiFi B and want to limit 100 flow files per 5 min.On NiFi A:Notify part of flow: GenerateFlowFile(5 min on primary) -> Notify(K N=+1)Wait part of flow: Some ingested data -> Wait(K N=1 M=100)Since Wait/Notify state is managed globally via DistributedCache we can limit throughput cluster wide.If use case requires to limit rate exactly then they can design Notify part as:GenerateFlowFile(5 min on primary) -> Notify(K N=0) -> Notify(K N=+1)It avoids N to be added up when there's no traffic.",5757
Extract Method,Fetcher to parse and follow Nth degree outlinks Fetcher improvements to parse and follow outlinks up to a specified depth. The number of outlinks to follow can be decreased by depth using a divisor. This patch introduces three new configuration directives:{code}< property><name>fetcher.follow.outlinks.depth</name><value>-1</value><description>(EXPERT)When fetcher.parse is true and this value is greater than 0 the fetcher will extract outlinksand follow until the desired depth is reached. A value of 1 means all generated pages are fetched and their first degreeoutlinks are fetched and parsed too. Be careful this feature is in itself agnostic of the state of the CrawlDB and does notknow about already fetched pages. A setting larger than 2 will most likely fetch home pages twice in the same fetch cycle.It is highly recommended to set db.ignore.external.links to true to restrict the outlink follower to URL's within the samedomain. When disabled (false) the feature is likely to follow duplicates even when depth=1.A value of -1 of 0 disables this feature.</description>< /property>< property><name>fetcher.follow.outlinks.num.links</name><value>4</value><description>(EXPERT)The number of outlinks to follow when fetcher.follow.outlinks.depth is enabled. Be careful this can multiplythe total number of pages to fetch. This works with fetcher.follow.outlinks.depth.divisor by default settings the followed outlinksat depth 1 is 8 not 4.</description>< /property>< property><name>fetcher.follow.outlinks.depth.divisor</name><value>2</value><description>(EXPERT)The divisor of fetcher.follow.outlinks.num.links per fetcher.follow.outlinks.depth. This decreases the numberof outlinks to follow by increasing depth. The formula used is: outlinks = floor(divisor / depth * num.links). This preventsexponential growth of the fetch list.</description>< /property>{code}Please do not use this unless you know what you're doing. This feature does not consider the state of the CrawlDB nor does it consider generator settings such as limiting the number of pages per (domain|host|ip) queue. It is not polite to use this feature with high settings as it can fetch many pages from the same domain including duplicates.Also this feature will _not_ work if fetcher.parse is disabled. With parsing enabled you might want to consider not to store downloaded content.,5758
Extract Method,"Indexer-elastic plugin should use Elasticsearch BulkProcessor and BackoffPolicy Elasticsearch's API (since at least v2.0) includes the {{BulkProcessor}} which automatically handles flushing bulk requests given a max doc count and/or max bulk size. It also now (I believe since 2.2.0) offers a {{BackoffPolicy}} option allowing the BulkProcessor/Client to retry bulk requests when the Elasticsearch cluster is saturated. Using the {{BulkProcessor}} was originally suggested [here|https://issues.apache.org/jira/browse/NUTCH-1527?focusedCommentId=13666616&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13666616].Refactoring the {{indexer-elastic}} plugin to use the {{BulkProcessor}} will greatly simplify the existing plugin at the cost of slightly less debug logging. Additionally it will allow the plugin to handle cluster saturation gracefully (rather than raising a RuntimeException and killing the reduce task) by using a configurable ""exponential back-off policy"".https://www.elastic.co/guide/en/elasticsearch/client/java-api/2.3/java-docs-bulk-processor.html",5759
Extract Method,Add solr5 solrcloud indexer support Nutch cannot index to Solr5. Also proper SolrCloud support is missing.,5760
Move Method,Refactor LinkDb and LinkDbMerger to reuse code LinkDb.Merger.reduce and LinkDb.reduce works the same way. Refactor Nutch so that we can use the same code for both.,5761
Extract Method,Change HtmlParseFilter 's to return ParseResult object instead of Parse object The current implementation of HtmlParseFilters.java doesn't allow a filter to add parse objects to the ParseResult object.A change to the HtmlParseFilter is needed which allows the filter to return ParseResult . and ofcourse a change to HtmlParseFilters .,5762
Rename Method,"Add alias capability in parse-plugins.xml file that allows mimeType->extensionId mapping Jerome and I have been talking about an idea to address the current issue raised by Stefan G. about having a mapping of mimeType->list of pluginIds rather than mimeType->list of extensionIds in the parse-plugins.xml file. We've come up with the following proposed update that would seemingly fix this problem.We propose to have the concept of ""aliases"" in the parse-plugins.xml file defined at the end of the file something lie:<parse-plugins>....<mimeType name=""text/html""><plugin id=""parse-html""/></mimeType>.....<aliases><alias name=""parse-html""extension-point=""org.apache.nutch.parse.html.HtmlParser""/>....<alias name=""parse-html2"" extension-point=""my.other.html.Parser""/>....</aliases>< /parse-plugins>What do you guys think? This approach would be flexible enough to allow the mapping of extensionIds to mimeTypes but without impacting the current ""pluginId"" concept.Comments welcome.",5763
Extract Method,Remove deprecated hadoop api calls (FS) There are quite a lot of calls to deprecated hadoop api functionality. Following patch will take care of fs related ones.,5764
Extract Method,Simple admin API to fetch status and stop the service REST API needs a simple info / stats service and the ability to shutdown the server.,5765
Extract Method,Configure minimum throughput for fetcher Large fetches can contain a lot of url's for the same domain. These can be very slow to crawl due to politeness from robots.txt e.g. 10s per url. If all other url's have been fetched these queue's can stall the entire fetcher 60 url's can then take 10 minutes or even more. This can usually be dealt with using the time bomb but the time bomb value is hard to determine.This patch adds a fetcher.throughput.threshold setting meaning the minimum number of pages per second before the fetcher gives up. It doesn't use the global number of pages / running time but records the actual pages processed in the previous second. This value is compared with the configured threshold.Besides the check the fetcher's status is also updated with the actual number of pages per second and bytes per second.,5766
Rename Method,Upgrade all instances of commons logging to slf4j (with log4j backend) Whilst working on another issue I noticed that some classes still import and use commons logging for example HttpBase.java{code}import java.util.*;// Commons Logging importsimport org.apache.commons.logging.Log;import org.apache.commons.logging.LogFactory;// Nutch importsimport org.apache.nutch.crawl.CrawlDatum;{code}At this stage I am unsure how many (if any others) still import and reply upon commons logging however they should be upgraded to slf4j for branch-1.4.,5768
Inline Method,Port mime type framework to use Tika mime detection framework With Tika (http://incubator.apache.org/tika/) nearing a stable 0.1 release candidate I think it would be a good time to patch Nutch to use Tika's mime detection system (an improvement over the existing Nutch one written primarily by Jerome). Tika's mime system is based on the mime system from Freedesktop.org and includes several improvements over the existing Nutch mime system such as:1. reliable XML-based content detection (a clear issue plaguing Nutch for some time now) ability to delineate between RSS XML ATOM etc.2. mime magic pattern matching including support for multiple patterns3. glob pattern matches (ability to support > 1)I'll get together a patch and then attach it to the list once it's relatively stable.,5769
Extract Method,Better cmd line parsing for NutchServer We can't currently stop a running server without killing the job via pid or something similar.A simple switch should be added to permit this.All is needs to do is call NutchServer#stop which will check to see if there are running tasks... if not then gracefully shut down the server instance.,5770
Extract Method,"Variable generate.max.count and fetcher.server.delay In some cases we need to use host specific characteristics in determining crawl speed and bulk sizes because with our (Openindex) settings we can just recrawl host with up to 800k urls.This patch solves the problem by introducing the HostDB to the Generator and providing powerful Jexl expressions. Check these two expressions added to the Generator:{code}-Dgenerate.max.count.expr='if (unfetched + fetched > 800000) {return (conf.getInt(""fetcher.timelimit.mins"" 12) * 60) / ((pct95._rs_ + 500) / 1000) * conf.getInt(""fetcher.threads.per.queue"" 1)} else {return conf.getDouble(""generate.max.count"" 300);}'-Dgenerate.fetch.delay.expr='if (unfetched + fetched > 800000) {return (pct95._rs_ + 500);} else {return conf.getDouble(""fetcher.server.delay"" 1000)}'{code}For each large host: select as many records as possible that are possible to fetch based on number of threads 95th percentile response time of the fetch limit. Or: queueMaxCount = (timelimit / resonsetime) * numThreads.The second expression just follows up to that settings the crawlDelay of the fetch queue.",5771
Extract Method,Scoring API: extension point scoring filters and an OPIC plugin This patch refactors all places where Nutch manipulates page scores into a plugin-based API. Using this API it's possible to implement different scoring algorithms. It is also much easier to understand how scoring works.Multiple scoring plugins can be run in sequence in a manner similar to URLFilters.Included is also an OPICScoringFilter plugin which contains the current implementation of the scoring algorithm. Together with the scoring API it provides a fully backward-compatible scoring.,5772
Rename Method,Reduce dependency of Nutch on config files Currently many components in Nutch rely on reading their configuration from files. These files need to be on the classpath (or packed into a job jar). This is inconvenient if you want to manage configuration via API e.g. when embedding Nutch or running many jobs with slightly different configurations.This issue tracks the improvement to make various components read their config directly from Configuration properties.,5773
Extract Method,Parser to add paragraph line breaks (initially reported with patch/pull-request by Vipul Behl see [#190|https://github.com/apache/nutch/pull/190])The parser (parse-tika and parse-html) could be improved to add line breaks between paragraphs instead of writing the whole document into a single line.,5774
Extract Method,IndexingFilterChecker to optionally follow N redirects As mentioned in NUTCH-2194 we sometimes use it as a backend for a web application. If so it should at least be able to follow N redirects.,5775
Extract Method,Port NUTCH-1467 and NUTCH-1561 to 2.x NUTCH-1467 and NUTCH-1561 which include improvements to plugins parse-metatags and index-metadata should be ported from 1.x to 2.x.,5776
Extract Method,"New configuration for CommonCrawlDataDumper tool Hi all you can find in attachment a new patch including support for new options for {{CommonCrawlDataDumper}}.In particultar new options are passed to {{CommonCrawlFormat}} object (which provides methods to create JSON output) using a configuration object ({{CommonCrawlConfig}}).In particular in this patch {{CommonCrawlDataDumper}} provides support for the following options:* {{-SimpleDataFormat}}: enables timestamps in GMT epoche (milliseconds) format.* {{-epochFilename}}: files extracted will be organized in a reversed-DNS tree based on the FQDN of the webpage followed by a SHA1 hash of the complete URL. Scraped data will be stored in these directories as individual GMT-timestamped files using ""epoche time (in milliseconds)"" plus file extension.* {{-jsonArray}}: organizes both request and response headers into a JSON array instead of using a JSON sub-object.*{{-reverseKey}}: enables to use the same layout as described for -epochFilename option with underscore in place of directory separators.You can use the options above in addition to the options already supported as described in the [Nutch wiki|https://wiki.apache.org/nutch/CommonCrawlDataDumper] page.This patch starts from [NUTCH-1974|https://issues.apache.org/jira/browse/NUTCH-1974].Thanks [~chrismattmann] and [~annieburgess] for supporting me on this work.",5778
Extract Method,Injector not to filter and normalize existing URLs in CrawlDb With NUTCH-1712 the behavior of the Injector has changed in case new URLs are added to an existing CrawlDb:- before only injected URLs were filtered and normalized- now filters and normalizers are applied to all URLs including those already in the CrawlDbThe default should be as before not to filter existing URLs. Filtering and normalizing may take long for large CrawlDbs and/or complex URL filters. If URL filter or normalizer rules are not changed there is no need to apply them anew every time new URLs are added. Of course injected URLs should be filtered and normalized by default.,5779
Extract Method,Support non-default FileSystem If a path (input or output) does not belong to the configured default FileSystem various Nutch tools may raise an exception like{noformat}Exception in ... java.lang.IllegalArgumentException: Wrong FS: s3a://... expected: hdfs://...{noformat}This is fixed by getting a reference to the FileSystem from the Path object{noformat}FileSystem fs = path.getFileSystem(getConf());{noformat}instead of{noformat}FileSystem fs = FileSystem.get(getConf());{noformat}A given path (e.g. {{s3a://...}}) may not belong to the default file system ({{hdfs://}} or {{file://}} in local mode) and simple checks such as {{fs.exists(path)}} then will fail. Cf. [FileSystem.checkPath(path)|https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/fs/FileSystem.html#checkPath(org.apache.hadoop.fs.Path)] and [FileSystem.get(conf)|https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/fs/FileSystem.html#get(org.apache.hadoop.conf.Configuration)] vs. [FileSystem.get(URIconf)|https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/fs/FileSystem.html#get(java.net.URI%20org.apache.hadoop.conf.Configuration)] which is called by [Path.getFileSystem(conf)|https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/fs/Path.html#getFileSystem%28org.apache.hadoop.conf.Configuration%29]. Note that the FileSystem for input and output may be different e.g. read from HDFS and write to S3.,5780
Extract Method,Allow for per-host configurable protocol plugin Introduces new configuration file for mapping protocol plugins to hostnames. {code} # This file defines a hostname to protocol plugin mapping. Each line takes a # host name followed by a tab followed by the ID of the protocol plugin. You # can find the ID in the protocol plugin's plugin.xml file. # # <hostname>\t<plugin_id>\n # nutch.apache.org org.apache.nutch.protocol.httpclient.Http # tika.apache.org org.apache.nutch.protocol.http.Http #{code} ,5781
Rename Method,Document webpage.avsc and host.avsc We can easily document Avro schema files as defined in the current Avro specificationhttp://avro.apache.org/docs/current/spec.html#schema_complexWe should document both of the above files to provide more meaningful comments in generated source.,5782
Rename Method,"Enhance ParserFactory plugin selection policy The ParserFactory choose the Parser plugin to use based on the content-types and path-suffix defined in the parsers plugin.xml file.The selection policy is as follow:Content type has priority: the first plugin found whose ""contentType"" attribute matches the beginning of the content's type is used. If none match then the first whose ""pathSuffix"" attribute matches the end of the url's path is used.If neither of these match then the first plugin whose ""pathSuffix"" is the empty string is used.This policy has a lot of problems when no matching is found because a random parser is used (and there is a lot of chance this parser can't handle the content).On the other hand the content-type associated to a parser plugin is specified in the plugin.xml of each plugin (this is the value used by the ParserFactory) AND the code of each parser checks itself in its code if the content-type is ok (it uses an hard-coded content-type value and not uses the value specified in the plugin.xml => possibility of missmatches between content-type hard-coded and content-type delcared in plugin.xml).A complete list of problems and discussion aout this point is available in:* http://www.mail-archive.com/nutch-user%40lucene.apache.org/msg00744.html* http://www.mail-archive.com/nutch-dev%40lucene.apache.org/msg00789.html",5783
Extract Method,"Run IndexingFilterChecker as simple Telnet server We have used a customized IndexingFilterChecker running as server to be able to quickly test/check pages from web applications. I'll add this feature back by letting IndexingFilterChecker run optionally as a simple server.Run it with:{code}export NUTCH_HEAPSIZE=25 ; bin/nutch indexchecker -normalize -dumpText -followRedirects -listen 1234{code}Then perform a request over TCP:{code}echo ""http://apache.org/"" | nc localhost 1234{code}",5784
Extract Method,SolrIndexer to write to multiple servers. SolrUtils should return an array of SolrServers and read the SolrUrl as a comma delimited list of URL's using Configuration.getString(). SolrWriter should be able to handle this list of SolrServers.This is useful if you want to send documents to multiple servers if no replication is available or if you want to send documents to multiple NOCs.edit:This does not replace NUTCH-1377 but complements it. With NUTCH-1377 this issue allows you to index to multiple SolrCloud clusters at the same time.,5785
Extract Method,Any23 Plugin: Add Content-Type filtering It should be possible to filter based on a document's Content-Type when using Any23 extractors.,5786
Rename Method,Add protocol-htmlunit HtmlUnit is opposed to other Javascript enabled headless browsers a portable library and should therefore be better suited for very large scale crawls. This issue is an attempt to implement protocol-htmlunit.,5787
Extract Method,code dedup fetcher queue redirects 20 lines of duplicated code in Fetcher when a new FetchItem is created for a redirect and queued.,5788
Extract Method,"improve usability of parse-metatags and index-metadata Usually the plugins parse-metatags and index-metadata are used in combination: the former ""extracts"" meta tags the latter adds the extracted tags as fields to the index. Configuration of the two plugins differs which causes pitfalls and reduces the usability (see example config):* the property ""metatags.names"" of parse-metatags uses ';' as separator instead of '' used by index-metadata* meta tags have to be lowercased in index-metadata{code}< property><name>metatags.names</name><value>DC.creator;DCTERMS.bibliographicCitation</value>< /property>< property><name>index.parse.md</name><value>metatag.dc.creatormetatag.dcterms.bibliographiccitation</value>< /property>{code}",5789
Extract Method,Update jakarta poi jars to the most relevant version Update jakarta poi jars to the most relevant version closes bug NUTCH-591.,5790
Move Method,Support HTTP auth for Solr communication At the moment we cannot send data directly to a public HTTP auth protected Solr instance. I've a WIP that passes a configured HTTPClient object to CommonsHttpSolrServer it works. This issue should add this ability to indexing dedup and clean and be configured from some configuration file.Enable Solr HTTP auth communication by setting the following parameters in your nutch-site config:* solr.auth=true* solr.auth.username=USERNAME* solr.auth.password=PASSWORD,5791
Extract Method,Update to the latest selenium and add code to use chrome and firefox headless mode with the remote web driver * Selenium needs to be updated * missing remote web driver for chrome  * necessity to add headless mode for both remote WebDriverBase Firefox & Chrome * use case with Selenium grid using docker (1 hub docker container several nodes in different docker containers Nutch in another docker container streaming to Apache Solr in docker container that is at least 4 different docker containers),5792
Extract Method,Allow to overwrite CrawlDatum's with injected entries Injector's reducer does not permit overwriting existing CrawlDatum entries. It is however useful to optionally overwrite so users can reset metadata manually.,5793
Extract Method,Multi Language Support Add multi-lingual support in Nutch as described in http://wiki.apache.org/nutch/MultiLingualSupportThe document analysis part is actually implemented and two analysis plugins (fr and de) are provided for testing (not deployed by default).The query analysis part is missing for a complete multi-lingual support.,5794
Extract Method,Indexer to filter and normalize URL's Indexers should be able to normalize URL's. This is useful when a new normalizer is applied to the entire CrawlDB. Without it some or all records in a segment cannot be indexed at all.,5795
Extract Method,Support for sitemap processing by hostname Add support to sitemap processor for processing just hostnames. Similar to the mapper eating sitemap URL's but then with BaseRobotRules finding the sitemap URL's itself. Will upload patch soon.,5796
Extract Method,Pass additional SolrParams when indexing to Solr This is a simple improvement of the SolrIndexer. It adds the ability to pass additional Solr parameters that are applied to each UpdateRequest. This is useful when you have to pass parameters specific to a particular indexing run which are not in Solr invariants for the update handler and modifying the Solr configuration for each different indexing run is inconvenient.,5797
Extract Method,Delegate language identification to Tika In 2.0 the language identification is delegated to Tika and is done as part of the parsing step (and not during the indexing as done currently).The patch attached is a backport from trunk which implements this and adds a new parameter to determine the strategy to use{code:xml} < property><name>lang.extraction.policy</name><value>detectidentify</value><description>This determines when the plugin uses detection andstatistical identification mechanisms. The order in which thedetect and identify are written will determine the extractionpolicy. Default case (detectidentify) means the plugin willfirst try to extract language info from page headers and metadataif this is not successful it will try using tika languageidentification. Possible values are:detectidentifydetectidentifyidentifydetect</description>< /property>{code},5798
Extract Method,Index checker server to optionally keep client connection open As the title says: for easier testing without having to start up the indexchecker JVM every time.{code}bin/nutch org.apache.nutch.indexer.IndexingFiltersChecker -normalize -followRedirects -keepClientCnxOpen -listen 5000{code}Just telnet to it an send URL's with line feed to get output fast.,5799
Rename Method,Scoring filter should distribute score to all outlinks at once Currently ScoringFilter.distributeScoreToOutlink as its name implies takes only a single outlink and works on that. I would suggest that we change it to distributeScoreToOutlink_s_ so that it would take all the outlinks of a page at once. This has several advantages:1) A ScoringFilter plugin returns a single adjust datum to set its score instead of returning several.2) A ScoringFilter plugin can change the score of the original page (via adjust datum) even if there are no outlinks. This is useful if you have a ScoringFilter plugin that say scores pages based on content instead of outlinks.3) Since the ScoringFilter plugin recieves all outlinks at once it can make better decisions on how to distribute the score. For example right now it is not possible to create a plugin that always distributes exactly a page's 'cash' to outlinks(that is if a page has score 5 it will always distribute exactly 5 points to its outlinks no matter what the internal/external factors are) if internal / external score factors are not 1.,5800
Rename Method,Redirected urls should be handled more cleanly (more like an outlink url) This is specifically for Nutch2.x. Handling a redirects url like an outlink is much more cleaner because this makes it more simple to trace how new urls are added to the webpage database. Instant fetching of redirects won't work but this is a small price to pay. (Note that this currently does not work at all because the http.max.redirect property has no effect). Will be attaching a patch in the upcoming days.,5801
Move Method,support solr authentication in nutch 2.x can solr authentication in nutch 2.x like 1.x,5802
Extract Method,Move statistical language identification from indexing to parsing step The statistical identification of language is currently done part in the indexing step whereas the detection based on HTTP header and HTML code is done during the parsing.We could keep the same logic i.e. do the statistical detection only if nothing has been found with the previous methods but as part of the parsing. This would be useful for ParseFilters which need the language information or to use with ScoringFilters e.g. to focus the crawl on a set of languages.Since the statistical models have been ported to Tika we should probably rely on them instead of maintaining our own.Any thoughts on this?,5803
Extract Method,Fetcher to guarantee delay for same host/domain/ip independent of http/https protocol  Fetcher uses a combination of protocol and host/domain/ip as ID for fetch item queues see [FetchItem.java|https://github.com/apache/nutch/blob/2b93a66/src/java/org/apache/nutch/fetcher/FetchItem.java#L101]. This inhibits a guaranteed delay in case both http:// and https:// URLs are fetched from the same host/domain/ip e.g. here with a large delay of 30 sec.: {noformat} 2018-07-23 14:54:39834 INFO fetcher.FetcherThread - FetcherThread 24 fetching http://nutch.apache.org/ (queue crawl delay=30000ms) 2018-07-23 14:54:39846 INFO fetcher.FetcherThread - FetcherThread 23 fetching https://nutch.apache.org/ (queue crawl delay=30000ms) {noformat},5804
Extract Method,Ability to index raw content Some use-cases require Nutch to actually write the raw content a configured indexing back-end. Since Content is never read a plugin is out of the question and therefore we need to force IndexJob to process Content as well.,5805
Extract Method,Enhance Searcher interface Current Searcher interface is too limited for many purposes:Hits search(Query query int numHits String dedupField String sortFieldboolean reverse) throws IOException;It would be nice that we had an interface that allowed adding different features without changing the interface. I am proposing that we deprecate the current search method and introduce something like:Hits search(Query query Metadata context) throws IOException;Also at the same time we should enhance the QueryFilter interface to look something like:BooleanQuery filter(Query input BooleanQuery translation Metadata context)throws QueryException;I would like to hear your comments before proceeding with a patch.,5806
Extract Method,support for Crawl-delay in Robots.txt Nutch need support for Crawl-delay defined in robots.txt it is not a standard but a de-facto standard.See:http://help.yahoo.com/help/us/ysearch/slurp/slurp-03.htmlWebmasters start blocking nutch since we do not support it.,5807
Extract Method,"Redirection handling: YahooSlurp's algorithm After reading Yahoo's algorithm (then one Andrzej linked to:http://help.yahoo.com/l/nz/yahooxtra/search/webcrawler/slurp-11.html )in the redirect/alias handling discussion I had a bit of a sparetime so I implemented it.Note that the patch I am attaching is for the 'choosing' algorithm described inYahoo's help page. It makes no attempt to handle aliases in any way. (See http://www.nabble.com/Redirects-and-alias-handling-%28LONG%29-tf4270371.html#a12154362 for the discussion about alias handling).E.ggenerate ""http://www.milliyet.com.tr/""fetch ""http:/www.milliyet.com.tr/"" which redirects to""http://www.milliyet.com.tr/2007/08/29/index.html?ver=39"".Update second page's datum's metadata to indicate that""http://www.milliyet.com.tr/"" is the representative form.Updatedb invertlinks etc...While indexing second page change its ""url"" field to""http://www.milliyet.com.tr/"".",5808
Inline Method,NTLM Basic and Digest Authentication schemes for web/proxy server Added basic digest and NTLM authentication schemes to protocol-httpclient. The authentication schemes can be configured for proxy server as well as web servers of a domain. HTTP authentication can take place over HTTP/1.0 HTTP/1.1 and HTTPS.The authentication guide can be found here: [http://wiki.apache.org/nutch/HttpAuthenticationSchemes].,5809
Extract Method,allow parsers to return multiple Parse object this will speed up the rss parser allow Parser#parse to return a Map<StringParse>. This way the RSS parser can return multiple parse objects that will all be indexed separately. Advantage: no need to fetch all feed-items separately.see the discussion at http://www.nabble.com/RSS-fecter-and-index-individul-how-can-i-realize-this-function-tf3146271.html,5810
Rename Method,Nutch should delegate compression to Hadoop Some data structures within nutch (such as Content ParseText) handle their own compression. We should delegate all compressions to Hadoop. Also nutch should respect io.seqfile.compression.type setting. Currently even if io.seqfile.compression.type is BLOCK or RECORD nutch overrides it for some structures and sets it to NONE (However IMO ParseText should always be compressed as RECORD because of performance reasons).,5811
Inline Method,Upgrade Tika to version 1.7 Hi Folks. Nutch currently uses version 1.6 of Tika. There were no significant API changes between 1.6 and 1.7. So this should be a one line update.,5812
Extract Method,Upgrade nutch to use released apache-tika-0.1-incubating This patch will upgrade Nutch to use the released tika-0.1-incubating jar containing stable APIs and code as opposed to the -dev version of the jar file that's currently in place in SVN.,5813
Extract Method,"workflow action allow user auto retry Workflow action only allows transient error retry currently. User often wants to control retry in each action level such as define custom retry count for each action. For a FAILED action the possible reason could be startData or endData not set or EL exception. The potential problem worth to retry is when Oozie not able to get running job with a hadoop id. For a ERROR action most of errors come from job application error such as failed to parse action conf buffer overflow in ssh executor or file not existed in fs action executor.The solution is to define 0.3 workflow schema with new attributes in action level to get user defined retry and to add default Oozie conf for system level max user-retry. EX:workflow.xml< workflow-app xmlns=""uri:oozie:workflow:0.3"" name=""test-wf"">< action name=""a"" retry-max=""2"" retry-interval=""1"">< /action>oozie-default.xml<!-- Workflow Action Automatic Retry -->< property><name>oozie.service.LiteWorkflowStoreService.user.retry.max</name><value>3</value><description>Automatic retry max count for workflow action is 3 in default.</description></property><property>< name>oozie.service.LiteWorkflowStoreService.user.retry.inteval</name><value>10</value><description>Automatic retry interval for workflow action is in minutes and the default value is 10 minutes.</description></property><property>< name>oozie.service.LiteWorkflowStoreService.user.retry.error.code</name><value>JA017</value><description>Automatic retry interval for workflow action is handled for these specified error code.</description></property><property>< name>oozie.service.LiteWorkflowStoreService.user.retry.error.code.ext</name><value> </value><description>Automatic retry interval for workflow action is handled for these specified extra error code.</description>< /property>",5814
Extract Method,"Add EL function to allow date ranges to be used for dataset ranges Dataset ranges are currently specified with EL functions such as {{${coord:current(int n)\}}} which is basically returns the nominal datetime for the nth dataset instance relative to the coordinator action creation (materialization) time (in other words it specifies a multiple of the dataset frequency). It would be useful to have a new function that lets users specify a date range offset instead of a frequency range offset. A new function {{${coord:offset(int n String timeUnit)\}}} would be similar to the {{${coord:current(int n)\}}} function except that the offset would be based on the TimeUnit (i.e. ""MINUTE"" ""HOUR"" ""DAY"" ""MONTH"" or ""YEAR"") instead of the frequency. For example if the frequency was 1 day then the following would all be equivalent:{{${coord:current(1)\}}}{{${coord:offset(1 ""DAY"")\}}}{{${coord:offset(24 ""HOUR"")\}}}{{${coord:offset(1440 ""MINUTE"")\}}}When specifying dataset instances the resolved value of {{${coord:offset(int n String timeUnit)\}}} would have to line up with an offset of a multiple of the frequency when used in an {{<instance>}} element.However when used in {{<start-instance>}} and {{<end-instance>}} the function would automatically resolve the range of instances to match the offset of a multiple of the frequencythat would fall between the {{<start-instance>}} and {{<end-instance>}}. For example if the frequency is 1 hour and the {{<start-instance>}} is {{${coord:offset(-90 ""MINUTE"")\}}} (-1.5 hours).then the {{<start-instance>}} would be effectively equivalent to {{${coord:offset(-60 ""MINUTE"")\}}} as we are dealing with a range.",5815
Rename Method,"Improve forkjoin validation to allow same errorTo transitions It seems common that users will have the ""error to"" transition from every action go to the same action node (e.g. email action) which then goes to the kill node instead of just going to the kill node directly. When this is done in action nodes within a forkjoin path the forkjoin validation doesn't allow it. We should improve the forkjoin validation code to allow the same ""error to"" transition as long as it eventually leads to a kill node.",5816
Extract Method,Optimize latest and future EL resolution in case of start-instance and end-instance <start-instance>${coord:latest(-23)}</start-instance>< end-instance>${coord:latest(0)}</end-instance>Assuming all 24 instances are available Oozie makes 300(1+2+3+..24) calls instead of 24 calls. This leads to some very high CPU usage when number of latest instances is high.,5817
Rename Method,Add a dryrun option for workflows Add a dryrun option for Workflows that would do all of the validation/parsing/checking/etc that normally happens when you submit a workflow but without actually submitting it.,5818
Extract Method,"Create example using AltKerberosAuthenticationHandler HADOOP-9054 adds AltKerberosAuthenticationHandler which allows non-browsers to use Kerberos authentication while allowing browsers to use some alternative authentication (to be implemented by the subclass). This is particularly useful for users of Oozie who want to use Kerberos for the Oozie client but allow access to the web UI using some other means of authentication such as LDAP. To encourage this we should create an example implementation of AltKerberosAuthenticationHandler and a login server example to work with it. This example isn't designed to be secure but to make it easier for users to integrate their own authentication systems with Oozie. There are two main components:(1) ExampleAltAuthenticationHanlder extends the AltKerberosAuthenticationHandler: The AltKerberosAuthenticationHandler deals with determining if the user-agent is a browser or not and with falling back to KerberosAuthenticationHandler so all the ExampleAltAuthenticationHandler has to do is create the AuthenticationToken when it sees that the user has a cookie named ""oozie.web.login.auth"" in their browser (the value of the cookie is the username). (2) The login server example: This is where the ExampleAltAuthenticationHandler will redirect unauthenticated users to. It has two implementations one is a very basic servlet (LoginServlet) that provides a form to get the username and password and checks if they are equal (e.g. user=foo pass=foo) and writes a cookie named ""oozie.web.login.auth"" with the username if so. The second implementation (LDAPLoginServlet) checks the username and password against an LDAP server before writing the cookie. The flow of all of this would be the user goes to the Oozie web UI in their browser the ExampleAltAuthenticator determines that they are not authenticated so redirects them to the login server example which authenticates the user writes the cookie and redirects them back to the web UI where the ExampleAltAuthenticationHandler sees from the cookie that they should now authenticated. From a non-browser such as the Oozie client the ExampleAltAuthenticationHandler would fall back to the KerberosAuthenticationHandler. More detailed information is in the documentation in the patch. ExampleAltAuthenticationHandler LoginServlet and LDAPLoginServlet are part of a new login module that builds oozie-login.war and oozie-login.jar when the loginServerExample maven profile is activated (much like how the workflow generator is built). The oozie-login.war can be deployed in the same tomcat as Oozie or somewhere else. Because ExampleAltAuthenticationHandler depends on AltKerberosAuthenticationHandler which isn't in the current Hadoop release we can temporarily include a copy of it and create a JIRA to delete it later.",5819
Extract Method,"Print a more helpful message when ProxyUserService is configured wrong If you don't properly configure {{oozie.service.ProxyUserService.proxyuser.#USER#.hosts}} and {{oozie.service.ProxyUserService.proxyuser.#USER#.groups}} and then try to use the ProxyUserService you can get an exception like this:{noformat}2013-01-14 13:46:25482 ERROR V1JobsServlet:536 - USER[-] GROUP[-] TOKEN[-] APP[-] JOB[-] ACTION[-] URL[GET http://localhost:11000/oozie/v1/jobs?doAs=foo] error proxyUser cannot be nulljava.lang.IllegalArgumentException: proxyUser cannot be nullat org.apache.oozie.util.ParamChecker.notEmpty(ParamChecker.java:68)at org.apache.oozie.service.ProxyUserService.validate(ProxyUserService.java:131)at org.apache.oozie.servlet.JsonRestServlet.getUser(JsonRestServlet.java:553)at org.apache.oozie.servlet.JsonRestServlet.service(JsonRestServlet.java:278)at javax.servlet.http.HttpServlet.service(HttpServlet.java:717)at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)at org.apache.oozie.servlet.AuthFilter$2.doFilter(AuthFilter.java:126)at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:372)at org.apache.oozie.servlet.AuthFilter.doFilter(AuthFilter.java:131)at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)at org.apache.oozie.servlet.HostnameFilter.doFilter(HostnameFilter.java:67)at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:861)at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:606)at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)at java.lang.Thread.run(Thread.java:680){noformat}It would be more helpful to the user if it gave more information such as ""proxyUser cannot be null please make sure that oozie.service.ProxyUserService.proxyuser.#USER#.hosts and oozie.service.ProxyUserService.proxyuser.#USER#.groups are configured correctly""",5820
Extract Method,Optimize current EL resolution in case of start-instance and end-instance <start-instance>${coord:current(-23)}< /start-instance>< end-instance>${coord:current(0)}< /end-instance>Assuming all 24 instances are available Oozie makes 300(1+2+3+..24) calls instead of 24 calls. This leads to some very high CPU usage when number of current instances is high.OOZIE-1073 fixes similar issue for latest and future,5821
Extract Method,Fix logging issues - latency accurate job ids coord Job UI to show job logs For exampleWARN CallableQueueService:542 - USER[-] GROUP[-] queue if full ignoring queuing for [(org.apache.oozie.command.wf.SignalXCommand@5c819194)(org.apache.oozie.command.wf.ActionEndXCommand@3e7cbafe)(org.apache.oozie.command.wf.SignalXCommand@6ed899be)(org.apache.oozie.command.wf.ActionEndXCommand@4e55c1cc)(org.apache.oozie.command.wf.ActionStartXCommand@60266041)(org.apache.oozie.command.wf.ActionStartXCommand@77797cb7)(org.apache.oozie.command.wf.ActionStartXCommand@48eb0fa8)(org.apache.oozie.command.wf.ActionStartXCommand@405103fe)(org.apache.oozie.command.wf.ActionStartXCommand@6dd39af)(org.apache.oozie.command.wf.ActionStartXCommand@25f613ae)]is an illustration of log messages being unclear. This one prints out the reference address information for the command instead of any meaningful info such as job-id and user whom this command pertains to.The log messages around callable commands can be fixed in a few other places too.,5822
Extract Method,Make sure HA works with HCat and SLA notifications We need to make sure HA works with HCat integration and SLA notifications. Both have in-memory datastructures and HA will impact them.,5823
Inline Method,[DB optimization] revisit eagerLoadState at places ,5824
Rename Method,Workflow performance optimizations Creating a combo JIRA for small performance optimizations.1. changing from asynchronous action start to a synchronous one to overcome the undue delay in transitioning from ::start:: control node to the actual first node owing to a loaded queue. This delay has been observed to be close to 30 min at times in stress conditions.,5825
Extract Method,Cleanup database before every test While investigating a flakey test ({{org.apache.oozie.sla.TestSLAJobEventListener.testOnJobEvent}}) I realized that some of the flakey SLA tests that I've seen lately are the same issue: The database has some leftover stuff from a previous test that its not expecting. Normally this is easy to fix because we can simply call {{cleanUpDBTables()}}. However {{cleanUpDBTables}} requires some of the {{Services}} to be running so you have to call it after starting {{Services}}; but some of the failures were occurring during Services initialization (specifically when {{SLAService}} initializes the {{SLACalculatorMemory}} which tries to load some data from the database which may be incomplete (e.g. SLA registration for a job that doesn't exist)). So in this case we can't call {{cleanUpDBTables()}} before or after starting {{Services}}.This brings the larger issue that we should be cleaning up the database before every test anyway to make sure that the tests are truly independent and to prevent harmful leaking (just like we did a while back with the {{Services}}). I think we should have {{XTestCase.setup()}} call {{cleanUpDBTables()}} so that every test automatically it (and handle the {{Services}} dependency appropriately).,5826
Extract Method,Oozie timers are not biased Oozie timers are not biased that is the statistical metrics they expose are over the run-time of the Oozie server instead of a window of time. This makes them not very useful especially after the server has been running for a while (codehale has very efficient and easy to use biased histograms that can be used instead).,5827
Move Method,Change hadoop-1 profile to use 1.2.1 We should change the hadoop-1 profile to use 1.2.1 instead of 1.1.1.,5828
Rename Method,use pom properties rather than specific version numbers in the pom files of hbaselibs hcataloglibs sharelib etc version numbers (hbase hive hcatalog sqoop etc) are hard coded in the pom files.,5829
Rename Method,"Use Curator leader latch instead of checking the order of Oozie servers We currently have a few tasks (e.g. Purging old jobs) that we only want to do in one Oozie server. We currently simply check which Oozie server is first in ZooKeeper's list of servers (i.e. the order they connected). We haven't seen any problems with this but it might be a good idea to replace this with Curator's leader-latch which sounds more robust. The leader path should probably be something like ""/services/leader"".Make sure errors and edge cases are handled properly including what happens when the leader dies without unregistering etc.http://curator.apache.org/curator-recipes/leader-latch.html",5830
Extract Method,Support getting ATS delegation tokens for tez jobs Need a new server side option to enable getting delegation tokens from ATS (Application Timeline Server - generic history server for YARN Apps) for tez jobs. Putting yarn.timeline-service.enabled=true in yarn-site.xml will enable it for mapreduce jobs as well which is not needed. There are fixes being done in future releases of YARN and TEZ to not fail but just warn and proceed if they could not talk to ATS server. Since that is not available now and ATS is not stable yet in Hadoop 2.6 it is preferable to use it only for Tez jobs and not impact mapreduce jobs as jobs will fail if ATS token could not be fetched.,5832
Extract Method,"oozie validate' command should be moved server-side The {{oozie validate}} command runs an XML validator against a workflow coordinator or bundle XML file to check that it's valid with any of the XSD schema files we have.Currently this is implemented in the Oozie CLI ({{OozieCLI.validateCommand(...)}} which has some downsides:# It's only available to OozieCLI users; anyone using the REST API can't use it# It's currently hardcoded to the specific XSD files we ship with Oozie## Whenever we add a new schema we have to also manually update this which is easy to forget## Users can't validate custom schemas that the Oozie server would acceptWe should move this to the Oozie server perhaps at a new ""validate"" endpoint. It should be able to accept a local file path (the current behavior) and perhaps also an HDFS file while we're at it. For the local XML file it can just be uploaded as part of the REST call.Also the description for the command needs to be updated to mention that it also handles coordinators and bundles.",5833
Extract Method,Use Hadoop's CredentialProvider for passwords in oozie-site We have a few passwords in oozie-site:- {{oozie.email.smtp.password}}- {{oozie.service.JPAService.jdbc.password}}It would be good if we supported Hadoop's {{CredentialProvider}} so that the passwords can be specified in an external encrypted file. The file can be prepared as described [here|http://hadoop.apache.org/docs/r2.7.0/hadoop-project-dist/hadoop-common/CommandsManual.html#credential] in the Hadoop docs.,5835
Rename Method,Allow table drop in hcat prepare The hcat prepare only allows to drop partitions. It would be nice to also allow dropping of table depending on the URL. The current format of the URL is{code}hcat://[metastore server]:[port]/[database name]/[table name]/[partkey1]=[value];[partkey2]=[value]{code}where at least one partition must be provided otherwise the prepare step fails with the fololwing exception.{code}Starting the execution of prepare actionsCreating HCatClient for user=ehsan.haq (auth:SIMPLE) and server=thrift://datavault-prod-app2.internal.machines:9083Prepare execution in the Launcher Mapper has failedFailing Oozie Launcher Main class [org.apache.oozie.action.hadoop.SqoopMain] exception invoking main() Error trying to drop hcat://datavault-prod-app2.internal.machines:9083/test_rdbms_import_2015110600/testorg.apache.oozie.action.hadoop.LauncherException: Error trying to drop hcat://datavault-prod-app2.internal.machines:9083/test_rdbms_import_2015110600/testat org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:178)at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)at java.security.AccessController.doPrivileged(Native Method)at javax.security.auth.Subject.doAs(Subject.java:415)at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)Caused by: org.apache.oozie.action.hadoop.LauncherException: Error trying to drop hcat://datavault-prod-app2.internal.machines:9083/test_rdbms_import_2015110600/testat org.apache.oozie.action.hadoop.HCatLauncherURIHandler.delete(HCatLauncherURIHandler.java:64)at org.apache.oozie.action.hadoop.PrepareActionsDriver.execute(PrepareActionsDriver.java:89)at org.apache.oozie.action.hadoop.PrepareActionsDriver.doOperations(PrepareActionsDriver.java:67)at org.apache.oozie.action.hadoop.LauncherMapper.executePrepare(LauncherMapper.java:446)at org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:174)... 8 moreCaused by: java.net.URISyntaxException: URI path is not in expected format: hcat://datavault-prod-app2.internal.machines:9083/test_rdbms_import_2015110600/testat org.apache.oozie.util.HCatURI.parse(HCatURI.java:66)at org.apache.oozie.util.HCatURI.<init>(HCatURI.java:52)at org.apache.oozie.util.HCatURI.<init>(HCatURI.java:48)at org.apache.oozie.action.hadoop.HCatLauncherURIHandler.delete(HCatLauncherURIHandler.java:52)... 12 moreOozie Launcher failed finishing Hadoop job gracefully{code}h3. SuggestionIn the URL if the partition is not provided then it should delete the entire table.,5836
Extract Method,Add BCC to oozie email action The Oozie e-mail action support To: and CC:A request has been made to add support for BCC: as well.,5837
Extract Method,SortBy filter for ordering the jobs query results Currently jobs query results are order by job creation time. It is good to have a filter option for ordering the results by lastModifiedTime or createdTime.,5838
Extract Method,Read properties files in action configurations Current logic of acton configuration only READ xml files within Oozie action directory but for pig and other applications properties files are supported and may be default configuration file format. To simplify the logic of reusing these properties files when using Oozie with them we add properties file reading in action conf.,5839
Extract Method,Oozie Coordinator EL Functions to get first day of the week/month Some users are demanding functions to give first day of week and first day of month. It would help them in aggregation jobs accumulating data from first day of the month/week to the intended date (current date). Currently we have no way to define such start-instance with any existing EL function.,5840
Extract Method,"Queue dump command message is confusing when the queue is empty If the callable queue is empty and you run the queue dump command the message says:{noformat}# oozie admin queuedump[Server Queue Dump]:Queue dump is null!******************************************[Server Uniqueness Map Dump]:Uniqueness dump is null!{noformat}This message makes it sound like a bad thing especially the exclamation mark. We should change the message to something more neutral and helpful like ""The queue is empty"" or something like that.",5841
Rename Method,Deprecate Instrumentation in favor of Metrics OOZIE-1817 added the option to use DropWizard Metrics instead of our homegrown Instrumentation. We left the Instrumentation as the default for compatibility; in Oozie 5 we should drop Instrumentation and only have Metrics. We can also use this opportunity to clean up the code and interface for Metrics which currently has to conform to Instrumentation for pluggability. ---- Update: for 5.0.0 we only deprecate {{InstrumentationService}} and make {{MetricsInstrumentationService}} the default.,5842
Rename Method,Extend HTTPS configuration settings for embedded Jetty Regarding HTTPS settings currently Oozie only support {{oozie.https.include.protocols}} and {{oozie.https.exclude.cipher.suites}} (introduced by OOZIE-2666).However Jetty SslContextFactory supports the following configurations:* excludeProtocols* includeProtocols* excludeCipherSuites* includeCipherSuitesTo have more control over employed protocols and cipher suites we should extend current implementation to allow users to configure {{excludeProtocols}} and {{includeCipherSuites}}. Sensible defaults are also needed.,5843
Extract Method,Make all Oozie JUnit tests pass on dist_test We have Oozie JUnit test cases above 2000 pieces right now. To run all the JUnit tests before a patch submission is an overkill since these take more than two hours.As a workaround we can use the [*dist_test*|https://github.com/cloudera/dist_test] framework that allows for parallel test execution - the load is distributed across possibly hundreds of cloud engine slaves. So the JUnit tests run time will equal to the longest JUnit test run that is in the order of 10-ish minutes for the whole test suite across all the Oozie components.Nevertheless some test cases that try to read files with relative access (e.g. {{src/test/resources/oozie-site.xml}}) or get system properties that are not present on cloud slaves will certainly fail. This improvement addresses that exact issue by making those tests pass also on the cloud.,5844
Move Method,"Implement new mechanism to specify ShareLibs for workflow actions OOZIE-2687 introduces the {{launcher}} element for workflows:{code}< launcher><memory>1024</memory><vcores>1</vcores><java-opts>-Dsome.property=true -XX:+RandomJVMSwitch</java-opts><env>key=value</env><queue>root.oozie</queue><sharelib>sparkhive</sharelib></launcher>{code}The purpose of this ticket is to discuss and implement new mechanism for handling ShareLib. {{addActionShareLib}} in {{JavaActionExecutor}} should adjusted. Regarding ""precedence order"":if global and an action level {{launcher}} and {{configuration}} (e.g. {{oozie.action.sharelib.for.#ACTIONTYPE#}}) tries to override the sharelib the following should apply:{quote} config properties defined in an action's <configuration> have priority over an action's <job-xml> which has priority over the global section's <configuration> and <job-xml> which has priority over the action defaults in oozie-site and so on.{quote}Here we have multiple choices how to handle sharelib:- Alternative 1: override sharelib in a way that is consistent with current way of handling Oozie configuration settings.- Alternative 2: make sharelib additive-- For example if there is a global {{launcher}} with {{sharelib}} element in a workflow that includes multiple ShareLibs (e.g. AB) and {{oozie.action.sharelib.for.#ACTIONTYPE#}} is also specified for an action's configuration (e.g. CD) then we take the union of the specified entities (ABCD would be included). -- It's inconsistent with everything else",5845
Extract Method,Add option to allow list of users to access system config Currently we allow access to system config for all users. Though we mask any sensitive information it is unnecessary to show it to all users. Can restrict the access to admins but services like Hue use the server side config values to display UI accordingly (For eg: Server Timezone whether SLA is configured what kind of credentials are supported etc). So it would be good to support restricting it to admins and a list of users.,5846
Extract Method,Instrument SLACalculatorMemory When there are lots of {{WorkflowJobBean}} and {{CoordinatorJobBean}} instances that have to be followed up on creating {{SLASummaryBean}} instances following can occur: * we set {{oozie.sla.service.SLAService.capacity}} to a sane value like {{10000}} to preserve heap consumption * {{SLACalculatorMemory#addRegistration()}} and {{SLACalculatorMemory#updateRegistration}} would: ** either emit {{TRACE}} level logs like {{SLA Registration Event - Job:}} showing the add / update of {{SLARegistrationBean}} was successful ** or emit {{ERROR}} level logs like {{SLACalculator memory capacity reached. Cannot add or update new SLA Registration entry for job}} showing the add / update of {{SLARegistrationBean}} was not successful Since sometimes stale or already processed {{SLAEvent}} entries from {{SLACalculatorMemory#slaMap}} get removed it's pretty hard to say what is its the actual size - that is whether the next add or update command will succeed We need an {{Instrumentation.Counter}} instance that gets incremented when there is an {{SLACalculatorMemory#slaMap#put()}} with a new entry added and gets decremented when there happens a {{SLACalculatorMemory#slaMap#remove()}} with an existing entry removed. This counter will be automatically present within REST interface and Oozie client.,5847
Rename Method,Enable definition of admin users using oozie-site.xml Currently the list of admin users is defined in the {{adminusers.txt}} file hard coded to the Oozie config dir. For a more streamlined solution we could define the list of admin users via {{oozie-site.xml}} by introducing the following configuration which receives the comma separated values of the users that are admins. {{oozie.service.AuthorizationService.admin.users}},5848
Rename Method,"[fluent-job] Create error handler ACTION only if needed The Shell and MultipleShellActions example of the Fluent Job API generates multiple actions with the same name ({{email-on-error}}) which gives {{E0705}} error code. For MultipleShellActions the generated XML: {noformat} Workflow job definition generated from API jar: < ?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?> < workflow:workflow-app xmlns:email=""uri:oozie:email-action:0.2"" xmlns:workflow=""uri:oozie:workflow:1.0"" xmlns:shell=""uri:oozie:shell-action:1.0"" name=""shell-example""> <workflow:start to=""parent""/> <workflow:kill name=""kill""> <workflow:message>Action failed error message[${wf:errorMessage(wf:lastErrorNode())}]</workflow:message> </workflow:kill> <workflow:action name=""email-on-error""> <email:email> <email:to>somebody@apache.org</email:to> <email:subject>Workflow error</email:subject> <email:body>Shell action failed error message[${wf:errorMessage(wf:lastErrorNode())}]</email:body> </email:email> <workflow:ok to=""kill""/> <workflow:error to=""kill""/> </workflow:action> <workflow:action name=""parent""> ... </workflow:action> <workflow:decision name=""decision1""> ... </workflow:decision> <workflow:action name=""email-on-error""> ... </workflow:action> <workflow:action name=""happy-path-0""> ... </workflow:action> <workflow:decision name=""decision2""> ... </workflow:action> ... {noformat} The error message: {noformat} bin/oozie job -oozie http://localhost:11000/oozie -runjar fluenttest.jar -config job.properties -verbose ... Error: E0705 : E0705: Nnode already defined node [email-on-error] {noformat} The Shell example also creates an XML with multiple {{email-on-error}} actions.",5849
Extract Method,[examples] [action] Fix Git example. PrepareActionsHandler should support XML namespace prefixes The git action example is not working it gives E0701 error code: {noformat} $ bin/oozie job -oozie http://localhost:11000/oozie -config examples/src/main/apps/git/job.properties -run -DnameNode=hdfs://localhost:9000 -D jobTracker=localhost:8032 Error: E0701 : E0701: XML schema error cvc-complex-type.2.4.c: The matching wildcard is strict but no declaration can be found for element 'git'. {noformat},5850
Extract Method,"[core] Coordinator action's status is SUBMITTED after E1003 error If I try to run a coordinator job which gives an {{E1003}} error code the coordinator's status is not changed to {{FAILED}}. I was using the following {{coordinator.xml}} {noformat} < coordinator-app name=""cron-coord"" frequency=""0/10 * * * *"" start=""${start}"" end=""${end}"" timezone=""UTC"" xmlns=""uri:oozie:coordinator:0.2""> <action> <workflow> <app-path>${workflowAppUri}</app-path> <configuration> <property> <name>resourceManager</name> <value>${resourceManager}</value> </property> <property> <name>nameNode</name> <value>${nameNode}</value> </property> <property> <name>queueName</name> <value>${queueName}</value> </property> <property> <name>user.name</name> <value>admin</value> </property> </configuration> </workflow> </action> < /coordinator-app> {noformat} The status of the coordinator job is {{RUNNING}} {noformat} $ oozie job -oozie http://localhost:11000/oozie -info 0000000-181105104843399-oozie-andr-C Job ID : 0000000-181105104843399-oozie-andr-C ------------------------------------------------------------------------------------------------------------------------------------ Job Name : cron-coord App Path : hdfs://localhost:9000/user/andrassalamon/examples/apps/cron-schedule Status : RUNNING Start Time : 2010-01-01 00:00 GMT End Time : 2010-01-01 01:00 GMT Pause Time : - Concurrency : 1 ------------------------------------------------------------------------------------------------------------------------------------ ID Status Ext ID Err Code Created Nominal Time 0000000-181105104843399-oozie-andr-C@1 SUBMITTED - - 2018-11-05 09:54 GMT 2010-01-01 00:00 GMT ------------------------------------------------------------------------------------------------------------------------------------ 0000000-181105104843399-oozie-andr-C@2 READY - - 2018-11-05 09:54 GMT 2010-01-01 00:10 GMT ------------------------------------------------------------------------------------------------------------------------------------ 0000000-181105104843399-oozie-andr-C@3 READY - - 2018-11-05 09:54 GMT 2010-01-01 00:20 GMT ------------------------------------------------------------------------------------------------------------------------------------ 0000000-181105104843399-oozie-andr-C@4 READY - - 2018-11-05 09:54 GMT 2010-01-01 00:30 GMT ------------------------------------------------------------------------------------------------------------------------------------ 0000000-181105104843399-oozie-andr-C@5 READY - - 2018-11-05 09:54 GMT 2010-01-01 00:40 GMT ------------------------------------------------------------------------------------------------------------------------------------ 0000000-181105104843399-oozie-andr-C@6 READY - - 2018-11-05 09:54 GMT 2010-01-01 00:50 GMT ------------------------------------------------------------------------------------------------------------------------------------ {noformat} The status of the first coordinator action is {{SUBMITTED}} {noformat} $ ./distro/target/oozie-5.2.0-SNAPSHOT-distro/oozie-5.2.0-SNAPSHOT/bin/oozie job -oozie http://localhost:11000/oozie -info 0000000-181105104843399-oozie-andr-C@1 ID : 0000000-181105104843399-oozie-andr-C@1 ------------------------------------------------------------------------------------------------------------------------------------ Action Number : 1 Console URL : - Error Code : - Error Message : - External ID : - External Status : - Job ID : 0000000-181105104843399-oozie-andr-C Tracker URI : - Created : 2018-11-05 09:54 GMT Nominal Time : 2010-01-01 00:00 GMT Status : SUBMITTED Last Modified : 2018-11-05 09:54 GMT First Missing Dependency : - ------------------------------------------------------------------------------------------------------------------------------------ {noformat} The log contains the {{E1003}} error message: {noformat} $ oozie job -oozie http://localhost:11000/oozie -log 0000000-181105104843399-oozie-andr-C ... 2018-11-05 11:04:57837 ERROR CoordActionStartXCommand:517 - SERVER[SalamonAndras-MBP15.local] USER[-] GROUP[-] TOKEN[-] APP[-] JOB[0000000-181105104843399-oozie-andr-C] ACTION[0000000-181105104843399-oozie-andr-C@1] XException org.apache.oozie.command.CommandException: E1003: Invalid coordinator application attributes user.name=admin at org.apache.oozie.command.coord.CoordActionStartXCommand.mergeConfig(CoordActionStartXCommand.java:180) at org.apache.oozie.command.coord.CoordActionStartXCommand.execute(CoordActionStartXCommand.java:197) at org.apache.oozie.command.coord.CoordActionStartXCommand.execute(CoordActionStartXCommand.java:63) at org.apache.oozie.command.XCommand.call(XCommand.java:291) at org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:363) at org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:292) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at org.apache.oozie.service.CallableQueueService$CallableWrapper.run(CallableQueueService.java:210) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) ... {noformat} We should change the status to {{FAILED}} in this case.",5851
Extract Method,"SSH action shows empty error Message and Error code Currently when an SSH action fails the only message that is returned is the Status. Neither the {{error Message}} nor {{Error code}} fields are filled. This makes reporting on the causes of SSH Action failures via Oozie highly impractical: the only meaningful bit of information there is on a failed SSH Action is the Status. The Status is filled based on the presence (or lack of) the {{.error file}} that is produced in case the user submitted script returns with any other value than 0. {noformat} SshActionExecutor#getActionStatus ... String outFile = getRemoteFileName(context action ""error"" false true); String checkErrorCmd = SSH_COMMAND_BASE + action.getTrackerUri() + "" ls "" + outFile; int retVal = getReturnValue(checkErrorCmd); ... {noformat}   User requirement is to provide some more detailed information on the success/failure of the user-submitted script. That could be at a minimum the return value optionally the last ~1K of the stderr that is drained. This information could then be communicated via {{errorMessage}} and {{ErrorCode}}",5852
Extract Method,"[FS Action] Refactor and optimize FsActionExecutor.java decision making part When I read FsActionExecutor.java I found a not good code in this class.  When judging which logic to use based on commands should use ""switch/case"" replace ""if/else"": # “if/else” make the code hard to read # “if/else” make the code hard to extend # “if/else” has low  efficience So I suggest using “switch/case” instead.",5853
Rename Method,"Oozie to allow drill down to hadoop job's details High-level Requirements:-----------------------------------Since Oozie is designed as the gateway to grid we need to support WS API for most common hadoop commands through Oozie. User doesn't want to go to multiple system to get the required data. Based on these we propose to implement the following requirements into Oozie.R1: Oozie will provide WS endpoints to get hadoop job details (including job counters).R2: It will support both types of hadoop jobs : MR job created for MR action MR jobs created as part of pig script.R3: In addition for pig action oozie will provide a way to query the pig stats.Proposed design:----------------------D1: Oozie will store the *summary* jobcounter /pigstats into oozie DB. The items in the summary stats will be determined by oozie to limit the size. Howeverthe commonly used stats will be include into the summary. It is important to note that summary information will be collected *after* the job finished.D2: If the user asks for *details* hadoop job stats  the user needs to query using different WS API. In this query a user will specify *a* hadoop job id. Oozie will directly query the hadoop JT/RM/HS. Since it is an external call with undetermined response time Oozie will provide only one hadoop job id per-request to avoid the timeout in WS call. Caveats: If hadoop is down or the job is not in JT/RM/History Server Oozie will fail to collect the details. D3: For pig Oozie will store the pig-generated hadoop ids in it DB and will expose that to user throw the ""verbose"" query.D4: Oozie will need to collect those summary pig stats and corresponding job counters and store it in Oozie DB. PigStats has a way of getting job counter for each hadoop job that it submits. We could use that API to collect summary counters for pig-created jobs.D5: The complete/detail pigstats will be stored into Pig Launcher Mapper as job counter. So that if a user wants to get the detail pig stats we could get it from the LM directly.Open questions:----------------------* What should be in the summary counters/stats? * What is the max size of stats?Advanced planning: <Not in the scope of this task but might required for design to support later>--------------------------* Some users are asking to query the job stats when the job is RUNNING. They need it to decide for subsequent job submissions.* By the above design  user could use D2 to get the counter when MR action is running.* However for pig it is not that straight forward. Because Pig submits the jobs during execution. But the new PigRunner provide a listener concept where user can get the notifications such as when a new MR job submitted and its ID.* By using this Oozie could get the running hadoop job id instantly. In future user might want this to query using D2.",5854
Extract Method,add default action configs per cluster Similar to hadoop configs per cluster Oozie should support default action configurations per cluster.This default config per action per cluster mechanism would act as defaults for the action configuration section enabling things like:* defining a special queue for launcher jobs* setting properties required to tune pig/hive etcIt should load/work in identical way as the hadoop configurations but instead using just a 'host:name' key it would use a composed key 'host:name + action-type',5856
Rename Method,add support for multiple/configurable sharelibs for each action type Currently there is a fixed sharelib per action type. I.e.:{code}/share/lib/mapreduce-streaming/pig/hive/sqoop/{code}It many situations it would be desirable to support multiple versions of sharelib per component have a system default and allow users to override the default for a specific version. I.e.:{code}/share/lib/mapreduce-streaming/pig-0_8/ (default)pig-0_9/sqoop/{code},5857
Extract Method,Support for choosing timezone in Oozie UI Add the ability to choose a different timezone (e.g. PST) in the Oozie web UI and command line. This would only be a superficial change that only affects the web UI and command line output; it wouldn't change any of the actual processing logs etc.,5858
Extract Method,"Clarify and improve Oozie logging configuration and streaming Oozie's logging configuration has a number of ""restrictions"" which are not listed anywhere and aren't all obvious. There are also some improvements to the streaming log code that can be made to simplify it and make it more robust. Additionally having the ability to automatically delete old log files can help prevent problems from having too many files in the same directory.",5859
Extract Method,"Add Name Node job-xml and configuration Elements to FS action Adding a name node Element to the FS action will allow users to shorten FS actions and avoid specifying the name node (hdfs://HOST:PORT) multiple times. We can also add job-xml and configuration elements to allow users to set properties when the FS instance is created. e.g.{code}< action name=""fs-node""><fs><name-node>hdfs://host:port</name-node><job-xml>fs-info.xml</job-xml><configuration><property><name>some.property</name><value>some.value</value></property></configuration><mkdir path=""/user/${wf:user()}/output-data/1""/><mkdir path=""/user/${wf:user()}/output-data/2""/> </fs><ok to=""end""/><error to=""fail""/>< /action>{code}This can then also leverage the global section from OOZIE-874 automatically.",5860
Extract Method,Remove some duplicated code in FsActionExecutor OOZIE-913 adds some code to FsActionExecutor to parse <job-xml> and <configuration> elements that is very similar to some code in JavaActionExecutor. With some minor refactoring the code in JavaActionExecutor can be used for both.,5861
Extract Method,Add formal Parameters to bundle XML Just like OOZIE-239 but add formal parameters to Bundle jobs too. There's also a minor bug in OOZIE-239 that will prevent Oozie from printing a warning when a user submits a coordinator job using schema >= 0.4 without a <parameters> section.,5862
Extract Method,Refactor the DefaultTokenContextGenerator to make it easier to create a sub-class Refactor the DefaultTokenContextGenerator to make it easier to create a sub-class. Basically we need to change members visibility to protected and create a method that creates the default context in a list so subclasses can use this method without array-list conversions.,5863
Extract Method,Add L-BFGS parameter estimation training to maxent Add support for the L-BFGS algorithm to train a maxent classifier.,5865
Inline Method,Parallel computing the objective function and its gradient for MAXENT_QN Although the current L-BFGS trainer runs in a sequential manner Maxent's objective function (i.e. the negative log-likelihood function) and its gradient can be computed in parallel. This JIRA will focus on improving the training time of MAXENT_QN.,5866
Extract Method,Extend Morfologik Addon Extends Morfologik Addon functionalities by:- Adding a Tag Dictionary implementation- Adding support to build Morfologik binary dictionaries from CLI- Adding a POSTaggerFactory extension to load a Morfologik binary dictionary embedded in a POS Tagger model bundle,5867
Extract Method,Skip POSTag dictionary validation during runtime The POS Tagger tool validates a POS Dictionary by checking if the POS tags in the dictionary are known by the model. Depending on the dictionary size it can take several seconds to validate it. We should have a mechanism to validate it only once during model creation.,5870
Extract Method,Add convenience methods to load models from Path Add a convenience methods to load models from a Path. Some of our users need that and their code will look slightly nicer if they can directly instantiate a model from a Path.,5871
Extract Method,"Improve OSGi support for OpenNLP extensions We have very basic OSGi support currently. We simply export all the packages we have and don't use any other OSGi features. This works well for anything we do expect the places where we try to access classes by class name e.g. to load custom factories via Class.forName(...). Most users will just be happy with that.Such calls do not work in an OSGi environment because the class we try to load is not on ""our"" class path.In OSGi this is done via services and we need to use them if we are running in an OSGI environment.Anyway OpenNLP needs to work with and without OSGi.I suggest that we make OSGi an optional dependency and write code which can detect if the OSGi classes are there or not.To instantiate a user class we would need to do something like this:- Try to load via Class.forName(...)- If cannot be found check if running in an OSGi environment- If so try to get an OSGi service which provides an instance to the user class",5873
Extract Method,Add cross validation cmd line tool for the name finder The cmd line interface should have a cross validation tool for the name finder.,5874
Extract Method,Create a detailed FMeasure results listener Create a evaluation listener that would output detailed FMeasure for samples that uses typed span. For example it lets the user know individual precision and recall for person organization date in a NameFinder model.,5877
Move Method,Evaluators should allow tools to register a report interface OPENNLP-220 introduced the -misclassified argument that enables evaluators to print misclassified items while using the command line evaluators. We should expand it to allow any other tool that uses evaluators to register an interface to get that information.,5878
Extract Method,Add train method to TokenNameFinder that takes TrainingParameters generatorDescriptor and resourceMap. Can't train TokenNameFinder using the params argument and the generatorDescriptor and resourceMap.,5881
Extract Method,Refactoring the Command Line Parameter interfaces Refactoring the Command Line Parameter interfaces as described at https://cwiki.apache.org/OPENNLP/command-line-parameter-interfaces.html,5882
Extract Method,Add additional context support to POS Tagger Some applications would benefit from having additional context support in POS Tagger. For example I could improve the model accuracy by using output from Name Finder.The change would be to include a field String[][] additionalContext to the POSSample and modify the code to allow the POS Tagger use it during training and runtime.,5883
Extract Method,Detailed evaluator output CLI evaluation tools (Evaluator and CrossValidator) should optionally print details of false positives and negatives and wrong tags. Will add the optional argument -printerrors to the CLI tools.,5885
Extract Method,"Add MERGE_BOTH option to Detokenizer An MERGE_BOTH option would be useful to train using some corpus. For example in a Portuguese corpus we have:... devolva - me o livro .... (give the book back to me)We need to detokenize it as ""devolva-me o livro"". Configure ""-"" token as MERGE_BOTH in the detokenizer dictionary would be helpful.",5886
Extract Method,Add test data verificatin for OntoNotes4 eval tests It should be verified before the test is run if the data has the expected checksum.,5887
Extract Method,Add multi threading support to GIS training The GIS training is famous for taking quite some time to finish. Now days CPUs have many cores. The training algorithm should be updated to use multiple CPU cores to perform the training.There are various approaches to solve this tasks we will document them in the wiki anddiscuss them on the mailing list.,5888
Extract Method,Refactor cross validation and training code to always use the new Training Parameters object For backward compatibility OpenNLP still needs to support the direct passing of the iterations and cutoff params to many methods in code which is related to training a component. The iterations and cutoff of parameters should always be wrapped in a Training Parameters object and then the new methods should be called.,5891
Extract Method,Models should also have constructors which accept URL and File objects Add a URL and a File constructor to all models.Further information can be found in this thread:http://mail-archives.apache.org/mod_mbox/opennlp-dev/201204.mbox/%3C4F9A5E8C.4000104%40gmail.com%3E,5893
Move Method,SentenceDetector should support new line as and end of sentence char The Sentence Detector should have support to consider new line chars as the end of a sentence. This will probably require special handling in the training code to assume that there is an new line char if any other eos is missing.,5894
Extract Method,Create a Factory to customize the Tokenizer ,5895
Extract Method,"Change GeneratorFactory class to allow registering custom feature generators I am trying to setup the OpenNLP NameFinder in a project with an XML feature generator descriptor and some non-standard features. The XML descriptor has support for custom feature generators:< generators><cache><generators>...<custom class=""com.example.MyFeatureGenerator""/></cache>< /generators>However it is not possible to pass parameters to the custom feature generator and registering new feature generators is currently not possible due to access restrictions in the opennlp.tools.util.featuregen.GeneratorFactory class. Lifting some access restrictions (private to public) would solve the issue.See also: http://stackoverflow.com/questions/19375053/using-custom-feature-generators-with-parameters-in-opennlp",5897
Extract Method,POS Tagger context generator should use feature generation classes As part of the name finder refactoring a number of reusable feature generator classes have been created. The POS Tagger should use these classes and drop custom feature generation code as much as possible.,5898
Extract Method,Add a language detection component Many of the components in OpenNLP are sensitive to the input language. It would be nice if OpenNLP would have a component to detect the language of an input text.Two commonly used solutions today are:Apache Tikas Language IdentifierLanguage Detection from Shuyo Nakatani,5899
Rename Method,Add a cli tool for the doccat evaluation support There should be a command line tool which can be used to evaluate the document categorizer modelon a test file.,5900
Extract Method,Evaluator CLI tools should use the Parameters interface Some CLI evaluation tools are not using the Parameters interface to describe arguments. It is easier to set optional parameters if the tool is using the interface.,5901
Extract Method,Add letsmt format support ,5903
Rename Method,facilitating the specialization of POSDictionary The train method in POSTaggerME receives in input a POSDictionary. This makes the implementation of custom dictionaries painful.I suggest to replace the POSDictionary input as a TagDictionary.Another improvement may also be the declaration of POSDictionary fields as protected to help the extension of this class.,5904
Extract Method,Add util method to POS Tagger which can build the ngram dictionary The POS Tagger has support for an ngram dictionary the code which creates the ngram dictionary inside POSTaggerTrainer should be refactored and moved to a util method. Additionally the cmd line interface should be extended to train with the ngram dictionary.,5905
Extract Method,Add support to influence all available machine learning settings during training of all our components There are quite some settings which influence the training of a classification model. Currenlty it is difficult to control these settings and many can only be changed through re-compiling. To improve the situation a user should be able to pass an object which contains all the machine learning settings to the various train methods and the command line interface should be extended to accept a properties file which contains all the machine learning settings.The settings also contain the training algorithm which enables us to use either maxent or perceptron in all our components.The solution which will be implemented was discussed in this thread:http://mail-archives.apache.org/mod_mbox/incubator-opennlp-dev/201105.mbox/%3C4DD284DA.1000900@gmail.com%3E,5907
Extract Method,Add extra information to DocumentSample Often a document has additional information fields such as title sender date key words. We should add field to the DocumentSample where to store this information and change the API in such a way that users could implement feature generators using this information.,5908
Rename Method,Remove deprecated iteration and cutoff params The deprecated iterations and cutoff parameters have been replaced by a properties file (or TrainingParameters object) which can contain all necessary parameters for a certain machine learning implementation.Remove all deprecated API which is still using them. Also remove the parameters from the command line interface.,5910
Extract Method,Refactor the GIS trainer integration The GIS code was never reshaped to fit properly into the new Training API. There are a couple of issues e.g. not using parameters which should be fixed.TODO: Update this description and list the changes,5911
Extract Method,Unify code to sum up input context features The code to sum up input features in the mal package is duplicated and should be unified in a util method.,5913
Move Method,probabilistic lemmatizer Current SimpleLemmatizer is dictionary-based. A probabilistic lemmatizer works better for unknown words and can be combined with dictionaries.The method we will implement here is based on: Grzegorz Chrupa_a. 2008. Towards a Machine-Learning Architecture for Lexical Functional Grammar Parsing. PhD dissertation Dublin City University. http://grzegorz.chrupala.me/papers/phd-single.pdf,5915
Extract Method,make EOS character set configurable Currently the EOS symbols to be used by the sentence detector cannot be configured (at the moment a user would have to make changes in opennlp.tools.sentdetect.lang.FactorySince it is important to use the same EOS symbols during training and during testing/prediction the EOS symbols should be stored with the model's properties,5917
Rename Method,Fix remaining issue in L-BFGS parameter estimation to get it stable Enhance the L-BFGS parameter estimation code to at least perform as well as the GIS training. The work on this issue should bring the implementation to a level where the experimental flag can be removed.For remaining problems see this issue: OPENNLP-338.,5918
Extract Method,Add perceptron sequence training support to the name finder The name finder should also support the perceptron sequence training.,5920
Extract Method,"Use Object values in TrainingParameters instead of String When I worked on OPENNLP-1032 I realized that TrainingParameters manages parameters as Map<StringString>. So users have to set their int parameters like this:{code}trainParam.put(""name"" ""100"");{code}but it should look like this:{code}trainParam.put(""name"" 100);{code}",5922
Rename Method,Use stupid backoff by default in NGramLanguageModel {{NGramLanguageModel}} is already using [Stupid Backoff|http://www.aclweb.org/anthology/D07-1090.pdf] discounting when it contains more than 1M ngrams.However since the not very good performance of Laplace smoothing for smaller models it'd be better to simply use Stupid Backoff in all cases.,5925
Move Method,Name Finder Sequence validator should be configurable via API The name finder uses a sequence validator to only create valid name sequences. A user might want to further restrict the possible sequences for this purpose it is necessary to pass in a custom sequence validator.The Name Finder ME constructor should be extended to accept a custom user defined sequence validator and should make the current sequence validator public.,5926
Rename Method,Add Concatenate Stream method for Collections of streams Minor change to opennlp.tools.util.ObjectStreamUtls. First change the signature of the createObjectStream(final ObjectStream<T>... streams) to concatenateObjectStream(final ObjectStream<T>... streams) and add a method concatenateObjectStream(final Collection<ObjectStream<T>> streams)The reason behind this is that I often pull data from multiple files whereas it is possible to create an array of ObjectStreams it is easier to work with Lists. Also the name of the method is clearer. It concatenates a list/array of ObjectStreams as opposed the the createObjectStream(final Collection<T> collection) which makes an obectstream of items in the collection.,5927
Extract Method,Error messages for command line arguments introduced into command line tools Command line tools do not report error messages resorting always to showing help. This is a bit confusing. Attached patch improves this.,5928
Extract Method,Chunker should output chunks also as Spans The chunker currently takes a string array as input and outputs a tag for each input string.The interface should be extended in a way that it can output an array of Spans instead whereeach Span contains the type and the begin/end offset in the input array. Like the name finderdoes. Like its done by ChunkSample.getPhrasesAsSpanList().,5930
Rename Method,Add support for custom feature generator configuration embedded in the model package Add support for custom feature generator configuration embedded in the model package.The configuration of the feature generators for the name finder component can be quite complex and the configuration mustbe always done twice once for training and once for tagging. Doing it twice at two different points in time makesthe feature generation very error prone. Small mistakes lead to a drop in detection performance which mightbe difficult to notice. To solve this issue add the configuration to the model then it must only be specified during training andcan be loaded from the model during tagging.Another advantage is that custom feature generation is difficult to use otherwise because the integrationcode must deal itself with setting up the feature generators. In some cases the user even does not have controlover the code or does not want to change it e.g. in the UIMA wrappers.The same logic should be used for the POS Tagger and Chunker.The issues is migrated from SourceForge:https://sourceforge.net/tracker/?func=detail&aid=1941380&group_id=3368&atid=353368,5932
Extract Method,Remove pmap indirection via int mapping Currently the hot loop of the classifiers use a mapping from String to int to Context this can be reduced to a direct mapping from String to Context which increases performances from 2 % to 10 % for maxent depending on the component.,5933
Extract Method,Restore the abbreviation dictionary support in SentenceDetector Today the abbreviation dictionary features of SentenceDetector are only usable though the API. We should add mechanism to allow training with an abbreviation dictionary from command line and also add the dictionary to the model as we do with POS Tagger.,5934
Extract Method,Optimize XML Parser configuration ,5935
Extract Method,Add validate() which checks validity of parameters in the process of the framework When I worked on OPENNLP-1039 I saw the client codes throw IllegalArgumentException when isValid() returns false but I think such kind of methods should throw the Exception by themselves and the timing of use should be controlled by the framework.So it should look like:{code}public abstract class AbstractTrainer {@Depracatedpublic boolean isValid() { ... }// if the subclass overrides this it should call super.validate();public void validate() throws IllegalArgumentException {// default implementation here}// this is the controller of the flow of training...public final void train() {// initializing init();// validating parametersvalidate();}}{code},5936
Inline Method,Remove deprecated GIS class ,5937
Rename Method,Organize imports according to new order It would be nice to do this for the code base and enforce it via checkstyle. We can tell people to make sure their IDE is configured correctly and everything will be fine.,5938
Extract Method,Brat Document Parser should support name type filters Brat Document Parser fails if there is a span overlap. Sometimes we are interested in only some types. In that case we could ignore the overlapping that are not of our interest. ,5939
Rename Method,Extend eval tests to run more ml algorithms ,5940
Extract Method,Add abbreviation dictionary support to Tokenizer The Tokenizer component can take advantage of using an abbreviation dictionary in context generator.Although it modifies the default tokenizer context generator it won't break compatibility with old models because the features would be applied only if the dictionary is present.,5941
Move Method,Create an eval test for the lemmatizer There should be two evaluation tests for the lemmatizer:- Train on the Spanish UD corpus and measure the accuracy- Load a 1.8.0 model and test on the leipzig corpus,5942
Extract Method,Write a test for the POSDictionary to test the case sensitive/insensitive flag The test should be part of the existing POSDictionaryTest class.And should test the following:- Loading an xml dict with the case sensitive flag works in both cases- When writing the dictionary the tag is serialized correctly,5943
Extract Method,Refactor ChunkSample class The class needs some improvements:1. Internally it works with Lists but all methods outputs Arrays it is always making Lists to Arrays conversions. Better to work with arrays directly.2. Create a static method to create spans of phrase chunks 3. Add javadoc,5944
Inline Method,Replace references to deprecated NameFinderME.train() Replace references to deprecated NameFinderME.train() and remove the deprecated method.,5945
Extract Method,Make the sequence codec in the name finder configurable The name finder should have the ability to change the sequence codec. The name finder uses by default IOB2. It should be possible to use other codecs such as BILOU as well.,5946
Rename Method,Improve resource loading for custom feature generators Currently the feature generators are matched in the TokenNameFinderTool which is part of the cmd line interface. To improve this the logic should be moved to the GeneratorFactory.,5948
Extract Method,Add training format support for the brat format The brat rapid annotation tool defines its own format to store annotations. It would be nice to have format support to directly train OpenNLP on the brat format.More information about the tool can be found here:http://brat.nlplab.org/,5949
Extract Method,support for skewed outer join Similarly to skewed inner join skewed outer join will help to scale in the presense of join keys that don't fit into memory,5951
Extract Method,"Optimize nested distinct/sort to use secondary key If nested foreach plan contains sort/distinct it is possible to use hadoop secondary sort instead of SortedDataBag and DistinctDataBag to optimize the query. Eg1:A = load 'mydata';B = group A by $0;C = foreach B {D = order A by $1;generate group D;}store C into 'myresult';We can specify a secondary sort on A.$1 and drop ""order A by $1"".Eg2:A = load 'mydata';B = group A by $0;C = foreach B {D = A.$1;E = distinct D;generate group E;}store C into 'myresult';We can specify a secondary sort key on A.$1 and simplify ""D=A.$1; E=distinct D"" to a special version of distinct which does not do the sorting.",5952
Extract Method,[zebra] Zebra Performance Optimizations Many in-core performance optimization opportunities exist in zebra such as removal of redundant precautionary checks use of better collection types to reduce levels of indirection to the memory objects changing of input splits in ascending sizes to descending sizes. Observed improvements of wall clock time of some PIG LOAD queries are around 10%.,5953
Rename Method,"[zebra] Provide streaming support in Zebra. Hadoop streaming is very popular among Hadoop users. The main attraction is the simplicity of use. A user can write the application logic in any language and process large amounts of data using Hadoop framework. As more people start to use Zebra to store their data we expect users would like to run Hadoop streaming scripts to easily process Zebra tables. The following lists a simple example of using Hadoop streaming to access Zebra data. It loads data from foo table using Zebra's TableInputFormat and then writes the data into output using default TextOutputFormat. $ hadoop jar hadoop-streaming.jar -D mapred.reduce.tasks=0 -input foo -output output -mapper 'cat' -inputformat org.apache.hadoop.zebra.mapred.TableInputFormat More detailed Zebra uses Pig DefaultTuple implementation of Tuple for its records. Currently when Zebra's TableInputFormat is used for input the user script sees each line containing "" key_if_any\tTuple.toString() "". We plan to generate CSV format representation of our Pig tuples. To this end we plan to do the following: 1) Derive a sub class ZupleTuple from pig's DefaultTuple class and override its toString() method to present the data into CSV format. 2) On Zebra side the tuple factory should be changed to create ZebraTuple objects instead of DefaultTuple objects. Note that we can only support streaming on the input side - ability to use streaming to read data from Zebra tables. For the output side the streaming support is not feasible since the streaming mapper or reducer only emits ""Text\tText"" the output collector has no way of knowing how to convert this to (BytesWritableTuple).",5954
Rename Method,[zebra] Use of Hadoop 2.0 APIs Currently Zebra is still using already deprecated Hadoop 1.8 APIs. Need to upgrade to its 2.0 APIs.,5955
Extract Method,[zebra] performance improvements Current input split generation is row-based split on individual TFiles. This leaves undesired fact that even for TFiles smaller than one block one split is still generated for each. Consequently there will be many mappers and many waves needed to handle the many small TFiles generated by as many mappers/reducers that wrote the data. This issue can be addressed by generating input splits that can include multiple TFiles. For sorted tables key distribution generation by table which is used to generated proper input splits includes key distributions from column groups even they are not in projection. This incurs extra cost to perform unnecessary computations and more inappropriately creates unreasonable results on input split generations; For unsorted tables when row split is generated on a union of tables the FileSplits are generated for each table and then lumped together to form the final list of splits to Map/Reduce. This has a undesirable fact that number of splits is subject to the number of tables in the table union and not just controlled by the number of splits used by the Map/Reduce framework; The input split's goal size is calculated on all column groups even if some of them are not in projection; For input splits of multiple files in one column group all files are opened at startup. This is unnecessary and takes unnecessarily resources from start to end. The files should be opened when needed and closed when not; ,5956
Extract Method,"Pig script runs half way after which it reports syntax error I have a Pig script which is structured in the following way{code}register cp.jardataset = load '/data/dataset/' using PigStorage('\u0001') as (col1 col2 col3 col4 col5);filtered_dataset = filter dataset by (col1 == 1);proj_filtered_dataset = foreach filtered_dataset generate col2 col3;rmf $output1;store proj_filtered_dataset into '$output1' using PigStorage();second_stream = foreach filtered_dataset generate col2 col4 col5;group_second_stream = group second_stream by col4;output2 = foreach group_second_stream {a = second_stream.col2b = distinct second_stream.col5;c = order b by $0;generate 1 as key group as keyword MYUDF(c 100) as finalcalc;}rmf $output2;--syntax error herestore output2 to '$output2' using PigStorage();{code}I run this script using the Multi-query option it runs successfully till the first store but later fails with a syntax error. The usage of HDFS option ""rmf"" causes the first store to execute. The only option the I have is to run an explain before running his script grunt> explain -script myscript.pig -out explain.outor moving the rmf statements to the top of the scriptHere are some questions:a) Can we have an option to do something like ""checkscript"" instead of explain to get the same syntax error? In this way I can ensure that I do not run for 3-4 hours before encountering a syntax errorb) Can pig not figure out a way to re-order the rmf statements since all the store directories are variablesThanksViraj",5957
Extract Method,[zebra] Support of locally sorted input splits Current Zebra supports sorted or unsorted input splits on sorted table or sorted table unions. The sorted input splits are based upon key ranges which do not overlap. And the splits are basically globally sorted in that they are locally sorted and their key ranges do not overlap.The biggest problem of the key-range splits are performance hits suffered if data skew is present particularly if a key range contains a duplicate key solely which makes the data trunk of the duplicate keys virtually unsplittable regardless how many mappers are available: it just has to be processed by a single mapper.On the other hand there are scenarios when the globally sorted splits are a over-kill and only locally sorted splits are good enough. Examples are the use of Zebra sorted tables as the probe table in a map-side merge inner join.,5958
Extract Method,Pig should exclude hadoop conf in local mode Currently the behavior for hadoop conf look up is:* in local mode if there is hadoop conf bail out; if there is no hadoop conf launch local mode* in hadoop mode if there is hadoop conf use this conf to launch Pig; if no still launch without warning but many functionality will go wrongWe should bring it to a more intuitive way which is:* in local mode always launch Pig in local mode* in hadoop mode if there is hadoop conf use this conf to launch Pig; if no bail out with a meaningful message,5959
Extract Method,[Zebra] Avoid making unnecessary name node calls for writes in Zebra Currently table and column group level meta data is extracted from job configuration object and written onto HDFS disk within checkOutputSpec(). Later on writers at back end will open these files to access the meta data for doing writes. This puts extra load to name node since all writers need to make name node calls to open files. We propose the following approach to this problem:For writers at back end they extract meta information from job configuration object directly rather than making name node calls and going to HDFS disk to fetch the information.,5960
Extract Method,"[Zebra] No type check when we write to the basic table In Zebra we do not have any type check when writing to a basic table. Say we have a schema: ""f1:int f2:string""however we can write a tuple (""abc"" 123) without any problem which is definitely not desirable.To overcome this problem we decide to perform certain amount of type checking in Zebra - We check the first row only for each writer.This only serves as a sanity check purpose in cases where users screw up specifying the output schema. We do NOT perform a rigorous type checking for all rows for apparently performance concerns.",5961
Rename Method,Need a way for Pig to take an alternative property file Currently Pig read the first ever pig.properties in CLASSPATH. Pig has a default pig.properties and if user have a different pig.properties there will be a conflict since we can only read one. There are couple of ways to solve it:1. Give a command line option for user to pass an additional property file2. Change the name for default pig.properties to pig-default.properties and user can give a pig.properties to override3. Further can we consider to use pig-default.xml/pig-site.xml which seems to be more natural for hadoop community. If so we shall provide backward compatibility to also read pig.properties pig-cluster-hadoop-site.xml.,5962
Rename Method,Implement Pig counter to track number of rows for each input files A MR job generated by Pig not only can have multiple outputs (in the case of multiquery) but also can have multiple inputs (in the case of join or cogroup). In both cases the existing Hadoop counters (e.g. MAP_INPUT_RECORDS REDUCE_OUTPUT_RECORDS) can not be used to count the number of records in the given input or output. PIG-1299 addressed the case of multiple outputs. We need to add new counters for jobs with multiple inputs.,5963
Rename Method,Allow casting relations to scalars This jira is to implement a simplified version of the functionality described in https://issues.apache.org/jira/browse/PIG-801.The proposal is to allow casting relations to scalar types in foreach.Example:A = load 'data' as (x y z);B = group A all;C = foreach B generate COUNT(A);.....X = ....Y = foreach X generate $1/(long) C;Couple of additional comments:(1) You can only cast relations including a single value or an error will be reported(2) Name resolution is needed since relation X might have field named C in which case that field takes precedence.(3) Y will look for C closest to it.Implementation thoughts:The idea is to store C into a file and then convert it into scalar via a UDF. I believe we already have a UDF that Ben Reed contributed for this purpose. Most of the work would be to update the logical plan to(1) Store C(2) convert the cast to the UDF,5964
Move Method,Consider clean up backend code Prior to 0.7 Pig had its own local execution mode in addition to hadoop map reduce execution mode. To support these two different execution modes Pig implemented an abstraction layer with a set of interfaces and abstract classes. Pig 0.7 replaced the local mode with hadoop local mode and made this abstraction layer redundant.Our goal is to remove those extra code. But we need also keep code backward compatible since some interfaces are exposed by top-level API.So we propose the first steps:* Deprecate methods on FileLocalizer that have DataStorage as parameter.* Remove ExecPhysicalOperator ExecPhysicalPlan ExecScopedLogicalOperator ExecutionEngine and util/ExecTools from org.apache.pig.backend.executionengine package.,5965
Rename Method,multi file input format for loaders We frequently run in the situation where Pig needs to deal with small files in the input. In this case a separate map is created for each file which could be very inefficient. It would be greate to have an umbrella input format that can take multiple files and use them in a single split. We would like to see this working with different data formats if possible.There are already a couple of input formats doing similar thing: MultifileInputFormat as well as CombinedInputFormat; howevere neither works with ne Hadoop 20 API. We at least want to do a feasibility study for Pig 0.8.0.,5967
Rename Method,Optimize scalar to consolidate the part file Current scalar implementation will write a scalar file onto dfs. When Pig need the scalar it will open the dfs file directly. Each scalar file contains more than one part file though it contains only one record. This puts a huge load to namenode. We should consolidate part file before open it. Another optional step is put the consolicated file into distributed cache. This further bring down the load of namenode.,5968
Extract Method,Suggest to allow PigServer can register pig script from InputStream Currently Pig only allow users to register script from file. Although it satisfy most people's requirements sometimes people hope to build pig script dynamically using code then they need to create temp file for the script they build. So here I suggest to allow PigServer be able to register pig script from InputStream.InputStream is a more general type than File pig script can been from file (FileInputStream)or from in-memory (ByteArrayInputStream) even it can been from remote machines (SocketInputStream)Here's a blog which explains why using InputStream is better than using File in interface http://java.dzone.com/articles/using-files-your-interfaces-0So I suggest to add the following 4 methods in PigServer:{code}public void registerScript(InputStream in) throws IOExceptionpublic void registerScript(InputStream in Map<StringString> params) throws IOExceptionpublic void registerScript(InputStream in List<String> paramsFiles) throws IOExceptionpublic void registerScript(InputStream in Map<StringString> paramsList<String> paramsFiles) throws IOException {code},5972
Extract Method,"support project-range expression. (was: There needs to be a way in foreach to indicate ""and all the rest of the fields"" ) A common use case we see in Pig is people have many columns in their data and they only want to operate on a few of them. Consider for example if before storing data with ten columns the user wants to perform a cast on one column:{code}...Z = foreach Y generate (int)firstcol secondcol thridcol forthcol fifthcol sixthcol seventhcol eigthcol ninethcol tenthcol;store Z into 'output';{code}Obviously this only gets worse as the user has more columns. Ideally the above could be transformed to something like:{code}...Z = foreach Y generate (int)firstcol ""and all the rest"";store Z into 'output'{code}",5973
Extract Method,Update Pig parser so that function arguments can contain newline characters We want to add this feature so that users can put long function argument strings in multiple lines. PIG-1748 depends on this.,5974
Extract Method,Clean up duplicated code in Physical Operators A lot of the getNext() implementations in PhysicalOperators is copy-pasted with only the method signatures and casts changing. Shorter code leads to less bugs and is easier to read.,5975
Extract Method,Add macro expansion to Pig Latin As production Pig scripts grow longer and longer Pig Latin has a need to integrate standard programming techniques of separation and code sharing offered by functions and modules. A proposal of adding macro expansion to Pig Latin is posted here: http://wiki.apache.org/pig/TuringCompletePigBelow is a brief summary of the proposed syntax (and examples):* Macro Definition The existing DEFINE keyword will be expanded to allow definitions of Pig macros. *Syntax*{code}define <name> (<params>) returns <aliases> {<Pig Latin fragment>};{code}*Example*{code}define my_macro(A sortkey) returns C {B = filter $A by my_filter(*);$C = order B by $sortkey;}{code}* Macro Expansion *Syntax*{code}< aliases> = <macro name> (<params>);{code}*Example:* Use above macro in a Pig script:{code}X = load 'foo' as (user address phone);Y = my_macro(X user);store Y into 'bar';{code}This script is expanded into the following Pig Latin statements: {code}X = load 'foo' as (user address phone);macro_my_macro_B_1 = filter X by my_filter(*);Y = order macro_my_macro_B_1 by user;store Y into 'bar';{code}*Notes*1. Any alias in the macro which isn't visible from outside will be prefixed with macro name and suffixed with instance id to avoid namespace collision. 2. Macro expansion is not a complete replacement for function calls. Recursive expansions are not supported. * Macro Import The new IMPORT keyword can be used to add macros defined in another Pig Latin file.*Syntax*{code}import <Pig Latin file name>;{code}*Example*{code}import my_macro.pig;{code}*Note:* All macro names are in the global namespace.,5976
Extract Method,"Javascript support for Pig embedding and UDFs in scripting languages The attached patch proposes a javascript implementation for Pig embedding and UDFs in scripting languages.It is similar to the Jython implementation and uses Rhino provided in the JDK.some differences:- output schema is provided by: <functionName>.outSchema=""<schema>"" as javascript does not have annotations or decorators but functions are first class objects- tuples are converted to objects using the input schema (the other way around using the output schema)The attached patch is not final yet. In particular it lacks unit tests.See test/org/apache/pig/test/data/tc.js for the ""transitive closure"" exampleSee the following JIRAs for more context:https://issues.apache.org/jira/browse/PIG-928https://issues.apache.org/jira/browse/PIG-1479",5977
Extract Method,"ability to turn off the write ahead log for pig's HBaseStorage Added an option to allow a caller of HBaseStorage to turn off the WriteAheadLog feature while doing bulk loads into hbase.From the performance tuning wikipage: http://wiki.apache.org/hadoop/PerformanceTuning""To speed up the inserts in a non critical job (like an import job) you can use Put.writeToWAL(false) to bypass writing to the write ahead log.""We've tested this on HBase 0.20.6 and it helps dramatically. The -noWAL options is passed in just like other options for hbase storage:STORE myalias INTO 'MyTable' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage('mycolumnfamily:field1 mycolumnfamily:field2''-noWAL');This would be my first patch so please educate me with any steps I need to do.",5978
Rename Method,"""0"" value seen in PigStat's map/reduce runtime even when the job is successful Pig runtime calls JobClient.getMapTaskReports(jobId) and JobClient.getReduceTaskReports(jobId) to get statistics about numbers of maps/reducers as well as max/min/avg time of these tasks. But from time to time these calls return empty lists. When that happens pig is reports 0 values for the stats. The jobtracker keeps the stats information only for a limited duration based on the configuration parameters mapred.jobtracker.completeuserjobs.maximum and mapred.job.tracker.retiredjobs.cache.size. Since pig collects the stats after jobs have finished running it is possible that the stats for the initial jobs are no longer available. To have better chances of getting the stats it should be collected as soon as the job is over. ",5979
Extract Method,Typed map for Pig Currently Pig map type is untyped which means map value is always of bytearray(ie. unknown) type. In PIG-1277 we allow unknown type to be a shuffle key which somewhat relieve the problem. However typed map is still beneficial in that:1. User can make semantic use of the map value type. Currently user need to explicitly cast map value which is ugly2. Though PIG-1277 allow unknown type be a shuffle key the performance suffers. We don't have a raw comparator for the unknown type instead we need to instantiate the value object and invoke its comparatorHere is proposed syntax for typed map:map[type]Typed map can be used in place of untyped map could occur. For example:a = load '1.txt' as(map[int]);b = foreach a generate (map[(i:int)])a0; - - Map value is tupleb = stream a through `cat` as (m:map[{(i:intj:chararray)}]); - - Map value is bagMapLookup a typed map will result datatype of map value.a = load '1.txt' as(map[int]);b = foreach a generate $0#'key';Schema for b:b: {int}The behavior of untyped map will remain the same.,5980
Extract Method,Need a special interface for Penny (Inspector Gadget) The proposed Penny tool needs access to Pig's new logical plan in order to inject code into the the dataflow. Once it has modified the plan it needs to then be able to hand back that modified plan and have Pig execute it.As we don't want to open this functionality up to general users the proposal is to do this by subclasses PigServer with a new class that is marked as LimitedPrivate for Penny only. This class will provide calls to parse a Pig Latin script and return a logical plan and one to take a logical plan and execute it.,5981
Extract Method,support project-range as udf argument With changes in PIG-1693 project-range ('..') is supported in all use cases where '*' (project-star) is supported except as udf argument. To be consistent with usage of project-star project-range should be supported as udf argument as well.,5982
Extract Method,HBaseStorage constructor syntax is error prone Using {{HBaseStorage}} like so seems like a reasonable thing to do but it will yield unexpected results:{code}STORE result INTO 'hbase://foo' USINGorg.apache.pig.backend.hadoop.hbase.HBaseStorage('info:first_name info:last_name');{code}The problem us that a column named {{info:first_name}} will be created with the trailing comma included. I've had numerous developers get tripped up on this issue since everywhere else in Pig variables are separated by commas so I propose we fix it.I propose we trim leading/trailing commas from column names but I'm open to other ideas.Also should we accept column names that are comman-delimited without spaces?,5983
Extract Method,Allow macro to return void Pig macro is allowed to not have output alias. But this property isn't clear from macro definition and macro invocation (macro inline). Here we propose to make it clear:1. If a macro doesn't output any alias it must specify void as return value. For example:{code} define mymacro(...) returns void {... ...};{code}2. If a macro doesn't output any alias it must be invoked without return value. For example to invoke above macro just specify:{code}mymacro(...);{code}3. Any non-void return alias in the macro definition must exist in the macro body and be prefixed with $. For example:{code} define mymacro(...) returns B {... ...$B = filter ...;};{code},5984
Extract Method,Bundle registered jars via distributed cache Currently registered jars get collapsed into a single job megajar that gets submitted to Hadoop.A better pattern would be to take advantage of the distributed cache.,5986
Extract Method,"Allow registering multiple jars from DFS via single statement Pig currently allows users to register jars from local and remote filesystems but only one jar can be specified at a time. It would be great to be able to say something along the lines of ""register hdfs://user/me/lib/*lucene*.jar"" and get all the jars registered in one go.",5987
Extract Method,Improve nested cross to stream one relation PIG-1916 added nested cross support for PIG. One optimization is instead of materialize all bags before producing result we can stream one of the input to save on memory.,5988
Extract Method,"Add more append support to DataByteArray I was recently writing a UDF to use a DataByteArray similar to CONCAT and thought it would be convenient if DBA supported more append options similar to Appendable and also that you can string them together if you wish as in dba.append(""foo"").append(""bar"").",5989
Extract Method,Allow any compression codec to be specified in AvroStorage Currently it's only possible to specify deflate - it would be useful to be able to specify Snappy which Avro now supports.,5990
Rename Method,Set default number of reducers for S3N filesystem Currently pig only estimates default reducers based on input file size for the HDFS and local file systems. This patch adds support for the S3N file system as well.,5991
Extract Method,Support more efficient Tuples when schemas are known Pig Tuples have significant overhead due to the fact that all the fields are Objects.When a Tuple only contains primitive fields (ints longs etc) it's possible to avoid this overhead which would result in significant memory savings.,5992
Rename Method,Speed up TestBuiltin On our build TestBuiltin takes over 4 minutes. No reason for that.,5993
Extract Method,Make reducer estimator plugable I'd like to refactor the logic contained in this method into a pluggable interface:{noformat}static int JobControlCompiler.estimateNumberOfReducers(Configuration conf List<POLoad> lds);{noformat},5994
Extract Method,Make the Pig unit faciliities more generalizable and update javadocs This ticket has two goals for Pig unit:1) Pig unit has a really nice method assertOutput(String inputAlias String[] inputValues String outputAlias String[] expectedOutputValues). That method lets you override an input alias variable with a hardcoded list of values. That way the script doesn't actually have to read that input variable from hdfs or cassandra. Then it runs the script and checks the specified output alias variable against the expected set of values. It's a really nice way to test your entire pig script with a single method call but only IF your script has exactly 1 input and 1 output. If you want to test more complicated scripts you have to jump through some hoops in order to override more input variables. But it would be fairly easy to change PigUnit so that it can override any number of inputs and check any number of outputs and do so easily. That's basically the change that I put into the base testing class I wrote. But it would be better to push that into PigUnit itself and it's something that could easily be done in an afternoon.2) Update javadocs for the pig unit test classes to make them more readable.,5995
Extract Method,pretty print schema currently 'describe' dumps the schema in one line. If you have a long or complicated schema it is pretty much impossible to figure out how the schema looks or what the fileds are.will provide an example below.,5996
Extract Method,"Add function to read schema from outout of Schema.toString() I want to toString() schemas and send them to the backend via UDFContext. At the moment this requires writing your own toString() method that Utils.getSchemaFromString() can read. Making a readable schema for the backend would be an improvement.I spoke with Thejas who believes this is a bug. The workaround for the moment is for example:String schemaString = inputSchema.toString().substring(1 inputSchema.toString().length() - 1);// Set the input schema for processingUDFContext context = UDFContext.getUDFContext();Properties udfProp = context.getUDFProperties(this.getClass());udfProp.setProperty(""horton.json.udf.schema"" schemaString);...schema = Utils.getSchemaFromString(strSchema);",5997
Extract Method,Lazily register bags with SpillableMemoryManager Currently all Spillable DataBags get registered by the BagFactory at the moment of creation. In practice a lot of these bags will not get large enough to be worth spilling; we can avoid a lot of memory overhead and cheapen the process of finding a bag to spill when we do need it by allowing Bags themselves to register when they grow to some respectable threshold.Related JIRAs: PIG-2917 PIG-2918,5998
Extract Method,"HBaseStorage filter optimizations Our HBase pal/guru Gary Helmling was kind enough to do a code review of HBaseStorage. He suggested some good filter optimizations:* when using the ""lt*"" and ""gt*"" options set the start/stop rows on the Scan instance at least in addition to the RowFilters. Without this you're doing a full table scan regardless of the RowFilters.* when selecting specific columns or entire families to return it would be more efficient to set the family + columns on the Scan object (addFamily() addColumn()) instead of using a FilterList. I'm not familiar with the family:prefix handling you mention but that would still seem to require filters. But if that's not being used it would be better to avoid the FilterList for columns. At minimum we should probably call Scan.addFamily() with the distinct families so we can skip entire column families that are not being used. In the case of a table with 4 CFs if say only 1 is being used this could be a big gain.",5999
Extract Method,"Trigger POPartialAgg compaction under GC pressure If partial aggregation is turned on in pig 10 and 11 20% (by default) of the available heap can be consumed by the POPartialAgg operator. This can cause memory issues for jobs that use all or nearly all of the heap already.If we make POPartialAgg ""spillable"" (trigger compaction when memory reduction is required) we would be much nicer to high-memory jobs.",6000
Move Method,Check the size of a relation before adding it to distributed cache in Replicated join Right now if someone makes a mistake and put the large relation last Pig will copy a huge file into distributed cache and it will take a long time before the job eventually fails. It would be better to check before copying the relation that it is of reasonable size.< 1 GB ?,6001
Extract Method,Make PigStorage.readField() protected for the cases when we need to extend PigStorage just to override readField. Currently we need to copy/paste several private fields and all getNextI've changed readField from private to protected and added a new method: protected void addToCurrentTuple(DataByteArray data),6002
Extract Method,Change HBaseStorage to permit overriding pushProjection In some cases it's useful to subclass {{HBaseStorage}} and override the logic in {{pushProjection}}. To do that we need to create the following protected methods:{noformat}protected void setColumnInfoList(List<ColumnInfo> columnInfoList);protected void storeProjectedFieldNames(RequiredFieldList requiredFieldList) throws FrontendException;{noformat},6003
Extract Method,Decouple PigServer.executeBatch() from compilation of batch executeBatch() currently does parsing and building of LogicalPlan in addition to the actual execution. It will be beneficial to separate out parsing/building from execution - that will allow us to get a handle on load/store and other operators before execution of batch. Useful for folks using PigServer API.,6004
Rename Method,Giving CSVExcelStorage an option to handle header rows Adds an argument to CSVExcelStorage to skip the header row when loading. This works properly with multiple small files each with a header being combined into one split or a large file with a single header being split into multiple splits.Also fixes a few bugs with CSVExcelStorage including PIG-2470 and a bug involving quoted fields at the end of a line not escaping properly.,6005
Extract Method,"PigTest.assertOutput doesn't allow non-default delimiter {{PigTest.assertInput(String aliasInput String[] input String alias String[] expected)}} assumes that the default delimiter is used (i.e. tab char) in input data.{code:title=TestPig.java}override(aliasInput String.format(""%s = LOAD '%s' AS %s;"" aliasInput destination sb.toString()));{code}But it will be useful to be able to use a non-default delimiter. For example here is an email from the user mailing list:http://search-hadoop.com/m/Pxcfq1TrnIb/PigUnit+test+for+script+with+non-default+PigStorage+delimiter&subj=PigUnit+test+for+script+with+non+default+PigStorage+delimiter",6006
Rename Method,Refactor physical operators to remove methods parameters that are always null The physical operators are sometimes overly complex. I'm trying to cleanup some unnecessary code.in particular there is an array of getNext(*T* v) where the value v does not seem to have any importance and is just used to pick the correct method.I have started a refactoring for a more readable getNext*T*().,6007
Extract Method,AVRO: Support user specified schema on load It would be useful for users to be able to explicitly specify the schema to use for reading avro input. This allows users to exactly specify how to resolve inputs with multiple schemas rather than depending on the guessing done when 'multiple_schemas' is set.,6008
Extract Method,Return more information for parsing related exceptions. There are a number of places in the Pig code where information useful for debugging parsing problems is being buried. This patch tries to expose that information by passing it up all the way to the Main run method and attaching it to the PigStats object.,6009
Inline Method,Reduce threadlocal conf access in backend for each record Noticed few things while browsing code1) DefaultTuple has a protected boolean isNull = false; which is never used. Removing this gives ~3-5% improvement for big jobs2) Config checking with ThreadLocal conf is repeatedly done for each record. For eg: createDataBag in POCombinerPackage. But initialized only for first time in other places like POPackage POJoinPackage etc.,6010
Extract Method,Pig should use hadoop local mode for small jobs Pig should use hadoop local mode for small jobs - few mappers few reducers and few mb of data.,6011
Move Method,ORC support for Pig Adding LoadFunc and StoreFunc for ORC.,6013
Extract Method,support adding archives to the distributed cache Support adding archives to the distributed cache by calling DistributedCacheFile.addCacheArchive instead of addCacheFile in JobControlCompiler.setupDistributedCache for common archive endings: .zip .tgz .tar.gz .tar which are the extensions checked in downloadCacheObject in org.apache.hadoop.filecache.TrackerDistributedCacheManager,6014
Extract Method,"Direct HDFS access for small jobs (fetch) With this patch I'd like to add the possibility to directly read data from HDFS instead of launching MR jobs in case of simple (map-only) tasks. Hive already has this feature (fetch). This patch shares some similarities with the local mode of Pig 0.6. Here fetching kicks off when the following holds for a script:* it contains only LIMIT FILTER UNION (if no split is generated) STREAM (nested) FOREACH with expression operators custom UDFs..etc* no scalar aliases* no SampleLoader* single leaf job* DUMP (no STORE)The feature is enabled by default and can be toggled with:* -N or -no_fetch * set opt.fetch true/false; There's no STORE support because I wanted to make it explicit that this ""optimization"" is for launching small/simple scripts during development rather than querying and filtering large number of rows on the client machine. However a threshold could be given on the input size (an estimation) to determine whether to prefer fetch over MR jobs similar to what Hive's '{{hive.fetch.task.conversion.threshold}}' does. (through Pig's LoadMetadata#getStatistic ?)",6015
Rename Method,"Move FileLocalizer.setR() calls to unit tests Currently temporary paths are generated by FileLocalizer using Random.nextInt(). To provide strong randomness MapReduceLauncher resets the Random object every time when compiling physical plan to MR plan:{code}MRCompiler comp = new MRCompiler(php pc); comp.randomizeFileLocalizer(); // This in turn calls FileLocalizer.setR(new Random()).{code}Besides there are a couple of places calling FileLocalizer.setR() (e.g. MRCompiler) with some random seed.I think-# Randomizing Random seed is unnecessary if we switch to UUID.# Setting Random objects in code like this is error-prone because it can be easily broken by having or missing a FileLocalizer.setR() somewhere else. See an example [here|http://search-hadoop.com/m/2nxTzQXfHw1].So I propose that we remove all this ""randomizing Random seed"" code and use UUID instead in temporary paths.For unit tests that compare the results against gold files we should still allow to set Random seed through FileLocalizer.setR(). But this method will be annotated as ""VisibleForTesting"" to ensure it is not used nowhere else other than in unit tests.Regarding the existing gold files they can be easily regenerated by TestMRCompiler as follows-{code}FileOutputStream fos = new FileOutputStream(expectedFile + ""_new"");PrintWriter pw = new PrintWriter(fos);pw.write(compiledPlan);{code}I assume there won't be any kind of regressions due to this change. But please let me know if I am wrong.",6016
Move Method,Change TaskContext to abstract class PIG-3860 introduced the generic TaskContext for different execution modes. One suggestion from Rohini is to change TaskContext to an abstract class so that we can avoid instanceof calls and putting it in shims layer.,6017
Rename Method,Add LoadCaster to EvalFunc(UDF) this ticket was very close to http://stackoverflow.com/questions/8828839/how-can-correct-data-types-on-apache-pig-be-enforced.To reproduce the issue first we have an UDF to cast map to bag code almost like(http://stackoverflow.com/questions/12476929/group-key-value-of-map-in-pig?answertab=votes#tab-top){code:title=test.pig}$ cat test.pigregister polisan/maptobag.jar;define MAPTOBAG maptobag.MAPTOBAG();A = load 'polisan/input1.txt' using PigStorage(' ') as (id:chararray kv:[]);B = foreach A generate id MAPTOBAG(kv) as to_bag;C = foreach B generate id flatten(to_bag) as (key:chararray value:chararray);D = group C by (id key);E = foreach D generate group MIN(C.value);dump E;{code}{code:title=polisan/input1.pig}1 [x#1y#ab]1 [x#2y#cd]{code}then run the pig I got exception as following:{noformat}2014-05-15 19:44:52944 [Thread-2] WARN org.apache.hadoop.mapred.LocalJobRunner - job_local_0001org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing (Name: D: Local Rearrange[tuple]{tuple}(false) - scope-42 Operator Key: scope-42): org.apache.pig.backend.executionengine.ExecException: ERROR 2106: Error while computing min in Initialat org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:289)at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNextTuple(POLocalRearrange.java:263)at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:282)at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:277)at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:1)at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 2106: Error while computing min in Initialat org.apache.pig.builtin.StringMin$Initial.exec(StringMin.java:81)at org.apache.pig.builtin.StringMin$Initial.exec(StringMin.java:1)at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:352)at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNextTuple(POUserFunc.java:391)at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:334)at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:378)at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:298)at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:281)... 8 moreCaused by: java.lang.ClassCastException: org.apache.pig.data.DataByteArray cannot be cast to java.lang.Stringat org.apache.pig.builtin.StringMin$Initial.exec(StringMin.java:73)... 15 more{noformat},6018
Extract Method,group all performance garbage collection and incremental aggregation I have a PIG statement similar to:summary = foreach (group data ALL) generate COUNT(data.col1) SUM(data.col2) SUM(data.col2) Moments(col3) Moments(data.col4)There are a couple of hundred columns.I set the following:SET pig.exec.mapPartAgg true;SET pig.exec.mapPartAgg.minReduction 3;SET pig.cachedbag.memusage 0.05;I found that when I ran this on a JVM with insufficient memory the process eventually timed out because of an infinite garbage collection loop.The problem was invariant to the memusage setting.I solved the problem by making changes to:org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperator.POPartialAgg.javaRather than reading in 10000 records to establish an estimate of the reduction I make an estimate after reading in enough tuples to fill pig.cachedbag.memusage percent of Runtime.getRuntime().maxMemory().I also made a change to guarantee at least one record allowed in second tier storage. In the current implementation if the reduction is very high 1000:1 space in second tier storage is zero.With these changes I can summarize large data sets with small JVMs. I also find that setting pig.cachedbag.memusage to a small number such as 0.05 results in much better garbage collection performance without reducing throughput. I suppose tuning GC would also solve a problem with excessive garbage collection.The performance is sweet.,6019
Move Method,New logical optimizer rule: ConstantCalculator Pig used to have a LogicExpressionSimplifier to simplify expression which also calculates constant expression. The optimizer rule is buggy and we disable it by default in PIG-2316.However we do need this feature especially in partition/predicate push down since both does not deal with complex constant expression we'd like to replace the expression with constant before the actual push down. Yes user may manually do the calculation and rewrite the query but even rewrite is sometimes not possible. Consider the case user want to push a datetime predicate user have to write a ToDate udf since Pig does not have datetime constant.In this Jira I provide a new rule: ConstantCalculator which is much simpler and much less error prone to replace LogicExpressionSimplifier.,6020
Rename Method,Add pattern matching to PluckTuple PluckTuple is useful when cleaning up long prefixes in a lengthy Pig script. Currently the udf filters out fields only with exact match but it would be useful if it could filter based on regex/wildcard.,6022
Extract Method,Pig's register command should support automatic fetching of jars from repo. Currently Pig's register command takes a local path to a dependency jar . This clutters the local file-system as users may forget to remove this jar later.It would be nice if Pig supported a Gradle like notation to download the jar from a repository.Ex: At the top of the Pig script a user could addregister '<group>:<module>:<version>'; It should be backward compatible and should support a local file path if so desired.RB: https://reviews.apache.org/r/31662/,6023
Extract Method,Pig should support reading log4j.properties file from classpath as well Currently we can specify a log4j.properties file to Pig command line like so{noformat}pig -4 /path/to/log4j.properties{noformat}It will be quite helpful if it read the log4j.properties from the classpath as well.For instance in the same command we can specify...{noformat}pig -4 log4j.properties{noformat}And Pig can first try to read it as a file and failing that could try reading it from the classpath similar to how log4j handles the property - log4j.configuration.We needed this as one of your projects contained the log4j.properties file embedded inside the project jarI've attached a simple patch with a testcase to that effect.,6024
Extract Method,Improve auto-parallelism for tez Tez auto-parallelism currently has some limitation:1. ShuffledVertexManager only decrease parallelism not increase2. Pig currently exaggerate parallelism at frontend ShuffledVertexManager might get initial parallelism way large than actual that would be costlyInstead of that we can gradually adjust initial vertex parallelism at runtime once upstream vertexes finishes.,6025
Extract Method,Serialize relevant part of the udfcontext per vertex to reduce payload size What HCatLoader/HCatStorer puts in UDFContext is huge and if there are multiple of them in the pig script the size of data sent to Tez AM is huge and also the size of data that Tez AM sends to tasks is huge causing RPC limit exceeded and OOM issues respectively. If Pig serializes only part of the udfcontext that is required for each vertex it will save a lot. HCat folks are also looking up at cleaning what goes into the conf (it ends up serializing whole job conf not just hive-site.xml) and moving out the common part to be shared by all hcat loaders and stores. Also looking at other options for faster and compact serialization. Will create separate jiras for that. Will use PIG-4653 to cleanup all other pig config other than udfcontext.,6027
Inline Method,Honor tez.staging-dir setting in tez-site.xml Want to have an independent setting for tez staging directory. Currently tez staging directory is set to pig temporary directory(pig.temp.dir).,6028
Extract Method,Autoparallelism should estimate less when there is combiner When there is a combiner it reduces records by a lot. Auto-parallelism should take that into account. Also currently we multiply by a factor of 10 if there is any FLATTEN. Users usually have FLATTEN(group) when the group by key is a compound key and it ends up estimating high. Only FLATTEN of bag should be considered.,6029
Rename Method,Do not turn off UnionOptimizer for unsupported storefuncs in case of no vertex groups We turn of UnionOptimizer for unsupported storefuncs as writing from two vertices may overwrite data. But in the case where there is only one unique union member we don't create vertex groups and merge the union operators into the Split vertex we can turn it on.,6030
Extract Method,POPartialAgg processing and spill improvements ,6031
Extract Method,Do not serialize PigContext in configuration to the backend It increases the conf sizes and in Tez it is really worse as we have multiple copies of the conf in a task - inputs outputs edge processor.,6032
Extract Method,Input of empty dir does not produce empty output part file in Tez ,6033
Rename Method,Provide option to disable DAG recovery Tez 0.7 has lot of issues with DAG recovery with auto parallelism causing hung dags in many cases as it was not writing auto parallelism decisions to recovery history. Rewrite was done in Tez 0.8 to handle that.Code was added to Tez to automatically disable recovery if there was auto parallelism so that it would benefit both Pig and Tez. It works fine and the second AM attempt fails with DAG cannot be recovered error when it sees there are vertices with auto parallelism. But problem is it is hard to see what the actual problem is for the users and is hard to debug as well as the whole UI state is rewritten with the partial recovery information.Doing the disabling of recovery in Pig itself by setting tez.dag.recovery.enabled=false will make it not go for the second attempt at all which will eventually fail. It also makes it easy to debug the original failure.,6034
Rename Method,Add a Bloom join In PIG-4925 added option to pass BloomFilter as a scalar to bloom function. But found that actually using it for big data which required huge vector size was very inefficient and led to OOM.I had initially calculated that it would take around 12MB bytearray for 100 million vectorsize (100000000 + 7) / 8 = 12500000 bytes) and that would be the scalar value broadcasted and would not take much space. But problem is 12MB was written out for every input record with BuildBloom$Initial before the aggregation happens and we arrive at the final BloomFilter vector. And with POPartialAgg it runs into OOM issues. If we added a bloom join implementation which can be combined with hash or skewed join it would boost performance for a lot of jobs. Bloom filter of the smaller tables can be sent to the bigger tables as scalar and data filtered before hash or skewed join is used.,6035
Extract Method,Add api getDisplayString to PigStats I am working on pig interpreter for zeppelin ( ZEPPELIN-335 ). For now I have to copy code in SimplePigStats#display and TezPigScriptStats#display as they are private it would be helpful if it provide api PigStats#getDisplayString.,6036
Extract Method,Does pig need a NATIVE keyword? Assume a user had a job that broke easily into three pieces. Further assume that pieces one and three were easily expressible in pig but that piece two needed to be written in map reduce for whatever reason (performance something that pig could not easily express legacy job that was too important to change etc.). Today the user would either have to use map reduce for the entire job or manually handle the stitching together of pig and map reduce jobs. What if instead pig provided a NATIVE keyword that would allow the script to pass off the data stream to the underlying system (in this case map reduce). The semantics of NATIVE would vary by underlying system. In the map reduce case we would assume that this indicated a collection of one or more fully contained map reduce jobs so that pig would store the data invoke the map reduce jobs and then read the resulting data to continue. It might look something like this:{code}A = load 'myfile';X = load 'myotherfile';B = group A by $0;C = foreach B generate group myudf(B);D = native (jar=mymr.jar infile=frompig outfile=topig);E = join D by $0 X by $0;...{code}This differs from streaming in that it allows the user to insert an arbitrary amount of native processing whereas streaming allows the insertion of one binary. It also differs in that for streaming data is piped directly into and out of the binary as part of the pig pipeline. Here the pipeline would be broken data written to disk and the native block invoked then data read back from disk.Another alternative is to say this is unnecessary because the user can do the coordination from java using the PIgServer interface to run pig and calling the map reduce job explicitly. The advantages of the native keyword are that the user need not be worried about coordination between the jobs pig will take care of it. Also the user can make use of existing java applications without being a java programmer.,6037
Rename Method,Support FLATTEN of maps I have come across users asking for this quite a few times. Don't see why we should not support it with FLATTEN instead of users having to write a UDF for that,6038
Extract Method,Fix DOT file parsing to enable DOT-based physical plan testing ,6039
Extract Method,PERFORMANCE: multi-query optimization Currently if your Pig script contains multiple stores and some shared computation Pig will execute several independent queries. For instance:A = load 'data' as (a b c);B = filter A by a > 5;store B into 'output1';C = group B by b;store C into 'output2';This script will result in map-only job that generated output1 followed by a map-reduce job that generated output2. As the resuld data is read parsed and filetered twice which is unnecessary and costly.,6040
Extract Method,Need to give user control of OutputFormat Pig currently allows users some control over InputFormat via the Slicer and Slice interfaces. It does not allow any control over OutputFormat and RecordWriter interfaces. It just allows the user to implement a storage function that controls how the data is serialized. For hadoop tables we will need to allow custom OutputFormats that prepare output information and objects needed by a Table store function.,6042
Rename Method,Autocompletion doesn't complete aliases Autocompletion only knows about keywords but in different contexts it would be nice if it completed aliases where an alias is expected.,6043
Extract Method,Add LIMIT as a statement that works in nested FOREACH I'd like to compute the top 10 results in each group.The natural way to express this in Pig would be:{code}A = load '...' using PigStorage() as (date: intcount: inturl: chararray);B = group A by ( date );C = foreach B {D = order A by count desc;E = limit D 10;generateFLATTEN(E);};dump C;{code}Yeah I could write a UDF / PiggyBank function to take the top n results. But since LIMIT already exists as a statement it seems like it should also work in the nested foreach context.Example workaround code.{code}C = foreach B {D = order A by count desc;E = util.TOP(D 10);generateFLATTEN(E);};dump C;{code},6044
Rename Method,Error reporting for failed MR jobs If we have multiple MR jobs to run and some of them fail the behavior of the system is to not stop on the first failure but to keep going. That way jobs that do not depend on the failed job might still succeed.The question is to how best report this scenario to a user. How do we tell which jobs failed and which didn't?One way could be to tie jobs to stores and report which store locations won't have data and which ones do.,6045
Extract Method,PERFORMANCE: Support skewed join in pig Fragmented replicated join has a few limitations:- One of the tables needs to be loaded into memory- Join is limited to two tablesSkewed join partitions the table and joins the records in the reduce phase. It computes a histogram of the key space to account for skewing in the input records. Further it adjusts the number of reducers depending on the key distribution.We need to implement the skewed join in pig.,6046
Extract Method,support conversion from numeric types to chararray ,6047
Move Method,Make import list configurable Currently it is hardwired in PigContext.,6048
Extract Method,use distributed cache for the replicated data set in FR join Currently the replicated file is read directly from DFS by all maps. If the number of the concurrent maps is huge we can overwhelm the NameNode with open calls.Using distributed cache will address the issue and might also give a performance boost since the file will be copied locally once and the reused by all tasks running on the same machine.The basic approach would be to use cacheArchive to place the file into the cache on the frontend and on the backend the tasks would need to refer to the data using path from the cache.Note that cacheArchive does not work in Hadoop local mode. (Not a problem for us right now as we don't use it.),6049
Rename Method,support cast of chararray to other simple types Pig should support casting of chararray to integerlongfloatdoublebytearray. If the conversion fails for reasons such as overflow cast should return null and log a warning.,6051
Rename Method,UDFs in scripting languages It should be possible to write UDFs in scripting languages such as python ruby etc. This frees users from needing to compile Java generate a jar etc. It also opens Pig to programmers who prefer scripting languages over Java.,6054
Extract Method,"Enable merge join in pig to work with loaders and store functions which can internally index sorted data Currently merge join implementation in pig includes construction of an index on sorted data and use of that index to seek into the ""right input"" to efficiently perform the join operation. Some loaders (notably the zebra loader) internally implement an index on sorted data and can perform this seek efficiently using their index. So the use of the index needs to be abstracted in such a way that when the loader supports indexing pig uses it (indirectly through the loader) and does not construct an index.",6055
Extract Method,Using Hadoop's optimized LineRecordReader for reading Tuples in PigStorage PigStorage's reading of Tuples ( lines ) can be optimized using Hadoop's {{LineRecordReader}}.This can help in following areas- Improving performance reading of Tuples (lines) in {{PigStorage}}- Any future improvements in line reading done in Hadoop's {{LineRecordReader}} is automatically carried over to PigIssues that are handled by this patch- BZip uses internal buffers and positioning for determining the number of bytes read. Hence buffering done by {{LineRecordReader}} has to be turned off- Current implementation of {{LocalSeekableInputStream}} does not implement {{available}} method. This method has to be implemented.,6056
Extract Method,"PERFORMANCE: Implement a map-side group operator to speed up processing of ordered data The general group by operation in Pig needs both mappers and reducers (the aggregation is done in reducers). This incurs disk writes/reads between mappers and reducers.However in the cases where the input data has the following properties1. The records with the same key are grouped together (such as the data is sorted by the keys).2. The records with the same key are in the same mapper input.the group by operation can be performed in the mappers only and thus remove the overhead of disk writes/reads.Alan proposed adding a hint to the group by clause like this one:{code}A = load 'input' using SomeLoader(...);B = group A by $0 using ""mapside"";C = foreach B generate ...{code}The proposed addition of using ""mapside"" to group will be a mapside group operator that collects all records for a given key into a buffer. When it sees a key change it will emit the key and bag for records it had buffered. It will assume that all keys for a given record are collected together and thus there is not need to buffer across keys. It is expected that ""SomeLoader"" will be implemented by data systems such as Zebra to ensure the data emitted by the loader satisfies the above properties (1) and (2).It will be the responsibility of the user (or the loader) to guarantee these properties (1) & (2) before invoking the mapside hint for the group by clause. The Pig runtime can't check for the errors in the input data.For the group by clauses with mapside hint Pig Latin will only support group by columns (including *) not group by expressions nor group all. ",6058
Inline Method,"[zebra] Zebra Column Group Access Control Access Control: when processes try to read from the column groups Zebra should be able to handle allowed vs. disallowed user/application accesses. The security is eventuallt granted by corresponding HDFS security of the data stored.Expected behavior when column group permissions are set:When user selects only columns that they do not have permissions to access Zebra should return error with message ""Error #: Permission denied for accessing column <column name or names> Access control applies to an entire column group so all columns in a column group have same permissions. ",6059
Extract Method,remove code duplication from PrivilegedAction handling The same code is repeated a couple of times for MiscUtil.getUGILoginUser() which can be refactored,6061
Rename Method,Optimize tag-download to include only tags that have policies For the calls to download tags from plugins Ranger Admin returns all the service-resources that have one or more tags associated. This can be optimized to include only service-resources that have tags for which policies exists.For example if tag-based policies exists for tags PII and PCI Ranger Admin should return service-resources that are associated with PII or PCI tags only; any service-resource that is not associated with either of these tags should be excluded. In addition to reducing the size of the tag-download this can improve policy-engine performance by not having to deal with tags that don't have policies.,6063
Rename Method,Ranger policies should support notion of OWNER user Components like HDFS have the notion of an owner for the resource being accessed. For such components it should be possible to setup Ranger policies to grant specific permissions for owners of accessed resources.Example usecase: users should have read/write/execute/delegate-admin privileges on files and directories they own under /home/. Ranger policy should be like:{noformat}{ path=/home/*; users=[ {OWNER} ]; permission=[ read write execute ]; isDelegateAdmin=true}{noformat},6064
Extract Method,Improve Ranger Usersync to sync AD/LDAP users and/or groups incrementally. During every sync cycle Ranger Usersync performs full LDAP/AD sync and computes the delta in-memory and updates ranger admin. Since usersync computes the delta (including group memberships) of all the users that are sync’d in memory for every sync cycle UserSync can take a lot of resources on the server it is running on. Enhance usersync to perform full sync only during startup and incremental or delta sync for the subsequent sync cycles. This way the delta computation of group memberships can be highly reduced and can increase usersync performance.,6065
Extract Method,"Support FILENAME and BASE_FILENAME tokens for HDFS plugin Use case:It should be possible to allow access only to files directly under the directory (say ""/data/oracle"")  and not to files under subdirectories of it (such as ""/data/oracle/PII""). This cannot be done using wildcards such as ""/data/oracle/*"" because it allows access recursively to all files under ""/data/oracle/"" including those under its subdirectories.As Ranger supports tokens (introduced by RANGER-698) a Ranger policy for (""/data/oracle/{FILENAME}) will meet this need if FILENAME token holds the value of file being accessed. Moreover a more specific policy such as ""/data/oracle/{BASE_FILENAME}.txt"" can match only files with "".txt"" extension where ""."" is used to demarcate extension from base filename.",6066
Extract Method,"Do some code improvement in Java method XTrxLogService.searchXTrxLogs In method searchXTrxLogs of class \security-admin\src\main\java\org\apache\ranger\service\XTrxLogService.javathere is some code can be improved:1.When search type is SearchField.SEARCH_TYPE.PARTIAL stringPredicate generated twice which first is redundant.{code}if(attr != null){stringPredicate = criteriaBuilder.equal(rootEntityType.get(attr) paramValue);if (searchField.getSearchType().equals(SearchField.SEARCH_TYPE.PARTIAL)) {String val = ""%"" + paramValue + ""%"";stringPredicate = criteriaBuilder.like(rootEntityType.get(attr) val);}predicate = criteriaBuilder.and(predicate stringPredicate);} {code}2.""datePredicate = criteriaBuilder.equal(...)"" is repeated the first is redundant and duplicated code.{code}if (searchField.getCustomCondition() == null) { datePredicate = criteriaBuilder.equal(rootEntityType.get(attr) fieldValue);if (searchField.getSearchType().equals(SearchField.SEARCH_TYPE.LESS_THAN)) {datePredicate = criteriaBuilder.lessThan(rootEntityType.get(attr) fieldValue);} else if (searchField.getSearchType().equals(SearchField.SEARCH_TYPE.LESS_EQUAL_THAN)) {datePredicate = criteriaBuilder.lessThanOrEqualTo(rootEntityType.get(attr) fieldValue);} else if (searchField.getSearchType().equals(SearchField.SEARCH_TYPE.GREATER_THAN)) {datePredicate = criteriaBuilder.greaterThan(rootEntityType.get(attr) fieldValue);} else if (searchField.getSearchType().equals(SearchField.SEARCH_TYPE.GREATER_EQUAL_THAN)) {datePredicate = criteriaBuilder.greaterThanOrEqualTo(rootEntityType.get(attr) fieldValue);} else {datePredicate = criteriaBuilder.equal(rootEntityType.get(attr) fieldValue);}predicate = criteriaBuilder.and(predicate datePredicate);}{code}3.The following code is repated in Java method XTrxLogService.searchXTrxLogsCount it can be refactored to a method. {code}for(String key : paramList.keySet()){...}{code}4.Do some other code improvement in Java method XTrxLogService.searchXTrxLogs and related code.Such as equal usage and local variable etc.5.XTrxLogService.searchXTrxLogs is used in Audit Admin WebPage please refer screenshot [AduitAdminSearch.jpg|https://issues.apache.org/jira/secure/attachment/12855809/AduitAdminSearch.jpg] for more detailso it can be tested easily.",6067
Extract Method,Refactor RangerPolicyEngineOptions RangerConfiguration is looked up too many times RangerPolicyEngineOptions has a lot of public fields which is written from various places from the code base which should be avoided. That object is configured from RangerConfiguration but in the middle of the plugin initialization code which makes this a bit more complex than it should be.Suggestions:* RangerConfiguration should be treated as an object not a static facade for a couple of config values* RangerPolicyEngineOptions should get his configuration from directly the RangerConfiguration in an explicit encapsulated way.,6068
Extract Method,Remove KeyProtector code in KMS The KMS service uses reflection to call on the com.sun.crypto.provider.KeyProtector class to encrypt/decrypt keys using a password. This causes problems with Java 9 and is generally unnecessary as we can just use the normal Java API to do this.,6069
Extract Method,Avoid Classloading default AtlasResourceMappers In AtlasResourceMapperUtil we classload both the default AtlasResourceMappers and then the custom ones. This is unnecessary - we should just instantiate the default ones and classload the custom ones.,6070
Extract Method,Ranger to support authorization and auditing for Apache Solr This JIRA will track the work to enable support in Apache Argus/Ranger for managing authorization policies for Apache Solr and auditing of user access through Solr. The model is similar to Argus/Ranger support for Apache Hive or Apache HBase,6071
Rename Method,Upgrade Ranger to support Apache Hadoop 3.0.0 This task is to upgrade Ranger to support Apache Hadoop 3.0.0. Here are some notes about the upgrade: a) The Hive plugin needs the Hadoop 3.0.0 jars to run the tests properly as Hive only supports the older Hadoop version so an exclusion and some additional 3.0.0 dependencies need to be added. b) The Storm plugin bundles the hadoop-auth jars in storm-core (although they really should be renamed here). Therefore we have no option but to package Storm with the Hadoop 2.7.x jars until such time that Storm upgrades the Hadoop dependency. This is an initial patch to get some feedback. If there is broad agreement on the upgrade I will test the distributions properly.,6072
Extract Method,The Ranger support the Kafka 1.0.0 Now the Ranger don't support the Kafka 1.0.0. We should support the Kafka 1.0.0.,6074
Rename Method,Auditing for Ranger Usersync operations During every sync cycle ranger usersync should audit some basic information like number of users number of groups that are sync'd for that cycle. Also provide details on sync source like the unix file or ldap with relevant configuration like ldap filters applied for that sync cycle ldap host url etc... Add a new tab in the ranger admin UI audits for usersync and show the above information.,6075
Rename Method,Policy effective dates to support time-bound and temporary authorization Currently Ranger policies have effectiveness period that is permanent i.e. once authored they can only be disabled or enabled. There are many use cases where such policies or even a policy condition needs to be time bound. For example certain financial information about earnings that is sensitive and restricted only until the earnings release date.  it would be great to have the ability to specify with each policy a time horizon when it is effective (i.e.) either be effective after a certain date and/or expire after a specific date or only valid within a certain time window and have Ranger check whether the policy is effective before evaluating in the policy engine. Therefore policy authoring can be simplified and does not require any subsequent action from the user basically making policy authoring a one time effort and users do not have to go back disable the policies once it is past the expiration date. This means that: # Ranger policy engine needs to be able to recognize the start and end times for policies  and enforce them based on period of validity specified by the user. # Active policies should be checked not only based on the resource user and environment context but also whether the policy is effective.,6076
Extract Method,Update Ranger Atlas Authorizer for the authorization model changes in Atlas Apache Atlas authorization model was updated in master branch in ATLAS-2459 for upcoming 1.0 release. This requires corresponding updates in Ranger Atlas Authorizer implementation.,6077
Inline Method,Good coding practices for KMS and unixauth Good coding practices for KMS and unixauth,6078
Extract Method,Update Ranger authorizer for Atlas for new method added in authorization interface (ATLAS-2765) Atlas authorizer interface was updated in ATLAS-2765 with addition of method scrubSearchResults(). Ranger authorizer for Atlas should be updated to implement this new method.,6079
Extract Method,Support multiple threads to build Trie and on-lookup post-setup for Trie nodes Time for building a Trie index for resources may become a bottleneck when dealing with large number of resources. It is desirable to build different non-overlapping parts of Trie structure using multiple threads to reduce overall build time if configured to do so. Also instead of building out all Trie nodes completely (that is by propagating wildcard evaluators all the way to all leaves of Trie tree) at the initialization time it is optimal to do so as a Trie-Node is accessed for the first time during resource lookup.,6080
Extract Method,Ranger to work with HA enabled WebHDFS with automatic failover Ranger to work with HA enabled WebHDFS with automatic failover.Speficially file look up in Ranger Admin should continue to work when namnode fails over.,6081
Extract Method,Filter/exclude multiple users in audit search Currently the audit search only allows to: * filter to one user's activity * exclude all 'service users' from every user's activity. If there were way to search for multiple users or exclude multiple users from the search list it would make debugging complex interactions simpler for example only look for actions for 'alice' and 'hive' and 'yarn',6082
Extract Method,Enhancements to support roles in Ranger policies Current Ranger policy model supports authorization/column-masking/row-filtering for users/user-groups based on various criteria like accessed-resource resource-classifications IP-address and custom conditions. Given the wide-spread use of role-based authorization in traditional enterprise applications (like RDBMS J2EE) it will be very useful for Ranger policy model to support 'roles' i.e. to be able to specify authorization/column-masking/row-filtering for roles as well - in addition to existing support for users and user-groups.,6083
Extract Method,Ranger spends 36% of CPU in ObjectMapper Ranger uses ObjectMapper to convert to/from JSON. In a profile of a workload (an Impala authorization test) I see that 36% of the ranger CPU is spent in these functions. 26% of total CPU is in the '_findRootDeserializer' method which gets cached if a type is deserialized multiple times. However the caching is only effective if the ObjectMapper is reused. JSONUtil appears to create a new ObjectMapper on every call which defeats the caching.,6084
Move Method,Improvement in setting cluster Name in RangerAccessRequest handling Clustername setting part during Policy engine instantiation.,6086
Move Method,Support for Incremental tag updates to improve performance Currently every change to any tag/service-resource/service-resource->tag mapping causes complete rebuilding of portions of policy-engine that map accessed resource to their tags. There are several disadvantages: 1. Compute time for rebuilding 2. Large traffic from ranger-admin to each of the plugins 3. Large load on JVM memory system because of frequent complete rebuilding of portions of policy-engine. It will be more optimal to communicate only the changes to tags and apply them to existing policy-engine.,6087
Extract Method,Export API to get zone unzone as well as tag based policies from Ranger. A separate policy export API needs to be created which would give all zone unzone as well as tag policies for a specified resource.Currently export policy gives zone or unzone policies only.,6089
Extract Method,Constant increase in memory usage by ranger-usersync process and eventually going out of memory When Usersync is configured with Unix as sync source noticed a is constant increase in memory usage and long running tests causing out of memory issue.,6090
Extract Method,Need changes for usersync(unix/ldap) to support HA without load balancer. Need changes in usersync(unix/ldap) to support HA without load balancer.,6091
Rename Method,Policy evaluation optimization: reorder policies and short-circuit evaluation The policy engine currently evaluates policies in the order received from the cache/REST API. To minimize the policy evaluation time it will be best to start with policies that are most likely to match for the access request. For example policies that deal with all-wildcard for resource names etc.This JIRA is to implement this optimization. There will be another JIRA to track more sophisticated optimization like use of caching usage pattern etc.,6096
Rename Method,Remove custom class loader used by ranger admin for resource lookup Remove custom class loader used by ranger admin for resource lookupRight now ranger admin resource lookup for hdfs hive and hbase are relying custom classloader org.apache.ranger.plugin.client.HadoopClassLoader. This requires us to define property name to site file (e.g. core-site.xml) mapping in resourcenamemap.properties.We should try to eliminate the need for the custom class loader and the need to maintain property name to site file mapping to make it easy to onboard new plugins.,6097
Rename Method,Implement reliable streaming audits to configurable destinations Currently for audit to HDFS Ranger writes to the file and then transfers the entire file to HDFS on regular interval. This adds additional write operation to the local disk.The proposal is to write a more intelligent audit writer audits are sent to destination in real time (or batches) and if the destination is down then write to local file. When the destination is available then first send the audit logs from the file system and after it is caught up resume real-time streaming.This design also need to address use cases where the destination is slower than the audit producer. In which case if the internal queue reaches a certain threshold then the audit will be written to local file till the destination till the in-memory queue is drained.The design should be generic enough to support any type of destination. By default the implementation for the following destinations should be provided:1. HDFS2. Solr3. Local File4. Log4J (with any supported appender)Additional good to have destinations are :1. RDBMS2. Kafka,6098
Rename Method,Provide REST API to change user role Provide REST API to change user role.,6100
Rename Method,Provide a way to clean-up old policy-engine and related resources. When updated service-policies are fetched by a plug-in from ranger-admin a new policy-engine is created to process authorization requests using the updated service-policies. A clean way to release critical resources held by the old policy-engine instance is needed for optimal use of system resources.,6101
Rename Method,"Use system supplied mechanism to get users and groups on unix The unix user sync currently reads /etc/passwd /etc/groups . This is often not a reflection of users and groups available on a system especially when nsswitch is configured (eg. sssd ldap etc).Secondly in some cases groups will contain user names that are not returned with ""getent passwd"" especially ""external users"" and it is required to add these using the group information.",6103
Extract Method,Policy engine API to find list of users/groups having access to a resource This JIRA is to enhance policy engine with addition of an API to find list of users/groups having a specific access to a resource. This API can be helpful for purposes like reporting.,6105
Rename Method,Support download csv in Reports page as enhancement +*LIst of changes / improvements*+# Along with download excel spreadsheet feature add support for download file as CSV format.# On Reports page for searched policies: downloaded policy file should have multiple policy items on separate row. ,6107
Extract Method,Enduser must provide an easy way to enable/disable visualization and sorting of USER attributes Currently with SYNCOPE-1005 we specified a way to define custom attributes sorting in user self create/update form.We should include this into a configurable section of AngularJS application configuration in which we can define also attributes to show/hide and their sorting strategy.Introduce also readonly flag on such configuration in order to allow enduser to edit or not a certain field. This configuration has not to be confused with schemas read-only flag that has another meaning like stated here https://syncope.apache.org/docs/reference-guide.html#schema.,6108
Rename Method,User Authentication using email In additional to existing user authentication using username an email will be used as the user for authenticating the subject. In the modern social networking sites mostly uses email as the user for authentication.,6109
Rename Method,Template mechanism for Enduser UI Provide a mechanism for defining templates (in terms of HTML CSS and image files) to simply the appearance customization of the Enduser UI.The the goal of the issue is to provide a way to:- avoid HTML code duplication and define reusable components.- define an HTML template mechanism which aim is to improve customizability of the enduser (enduser is meant to be customized and extended it is not a finite product but a proposal from which to start a new implementation).- exploit as much as possible code re-usability features provided by AngularJS (if possible).- if needed review actula CSS implementation in order to better fit the new template mechanism- do NOT compromise (or change) enduser functionalities at all! Buttons selects wizard and other components should preserve their role and function; the core logic should remain the same though enduser is open to discuss improvements also in that way ;)[This|https://code.angularjs.org/1.6.3/docs/guide/templates] could be a good starting point to understand how to use AngularJS tools to implement templating.BTW If you have proposals that does not involve AngularJS features they are well accepted and can be accepted anyway.Some of the features described there like directives have also been used in the Enduser to define some reusable components (e.g. dynamic-plain-attribute).,6110
Rename Method,Hide key when creating / editing Security Questions from Admin Console When creating or editing Security Questions from Admin Console the key is an auto-generated field of no interest.It is currently reported and not modifiable: it should be better hidden instead.,6111
Extract Method,SAML 2.0 Service Provider feature Provide the ability to perform SSO to Admin Console and Enduser UI via an external SAML 2.0 Identity Provider.,6113
Extract Method,Replace Activiti-based workflow adapter with Flowable Following the discussion in ML the idea is to upgrade the current workflow engine based on Activiti 5.X to one featuring Flowable 6.,6114
Extract Method,Remove final landing page after user create/update Remove final landing page after user successful create/update and restore old feedback panel to show success (visible in login page),6115
Extract Method,"Improve security of customization mechanism A smart and malicious user could ""hack"" angularjs frontend components and send info that is not allowed to create/edit.Solve this by checking info on server side against form customization JSON.",6116
Extract Method,Display or enable add button only to realms were CREATE is owned In the realms page the add button (for users groups and any objects) is always displayed even on realms for which the logged admin was not granted {{USER_CREATE}} {{GROUP_CREATE}} or the dynamic {{*_CREATE}} entitlements generated for the defined any object types.This can lead to confusion as fatal errors are now reported only when at he last step on create.The add button could be hidden or disabled for realms on which no create entitlements were granted to the logged admin.,6119
Extract Method,Extension: Elasticsearch-based search engine As outlined in SYNCOPE-1006 the current search engine is somehow fragile (being based on SQL views) and highly depends on the DBMS used as internal storage - with frequent issues for MySQL / MariaDB.[As suggested|https://issues.apache.org/jira/browse/SYNCOPE-1006?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15856313#comment-15856313] an idea could be to provide another optional search engine implementation which requires an Elasticsearch node.,6120
Rename Method,Downloaded file for binary attribute better naming Name of the downloaded file should be like: <key>_<schemaName>.<standard extension for that mimetype>,6122
Extract Method,Remove misleading getAttrMap and similar methods from TOs {{ConnObjectTO}} and the classes implementing {{AttributableTO}} provide methods like as {{getAttrMap()}} {{getPlainAttrMap()}} {{getDerAttrMap()}} and {{getVirAttrMap()}} with purpose of providing a read-only view of all attributes (of different types).Using such methods however can be costing (the resulting map is built for each invocation) and also confusing as one would expect that adding / removing entries would result into effective attribute changes.Such methods should be removed and substituted with more tailored replacements where needed.,6123
Rename Method,Support functions for internal JEXL engine JEXL can register objects or classes used as [function namespaces|https://commons.apache.org/proper/commons-jexl/reference/syntax.html#Functions].Since JEXL expressions are used everywhere such feature would enhance the capabilities to adapt to different use cases as the one intended to be supported by SYNCOPE-1116 for Realms' object link.,6124
Rename Method,"Third Party JWT SSO integration This task is to support SSO using third party JWT tokens.It involves two tasks:a) Create a new interface extending JwsSignatureVerifier to provide a method to resolve a JWT subject into Syncope username (known user).b) When processing a received token if the issuer is different from the known issuer (""jwtIssuer"" in security.properties) then instead of retrieving the default jwsSignatureVerifier implementation the authentication component will enable the ClassPathScanImplementationLookup to dynamically discover an implementation of the interface above.",6125
Extract Method,Realm-based authorization The current authentication / authorization model has some weaknesses as outlined at [1].In the same mail thread a refactoring proposal is shown for implementing a system-wide realm-based hierarchical security model.This will impact nearly every component and layer in the system so great care should be taken and extensive testing.Discussion on wiki: https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Realms[1] http://syncope-dev.1063484.n5.nabble.com/syncope-dev-Authorization-entitlements-td4830322.html,6129
Inline Method,Provide latest GIT commit hash alongside with version number The {{PlatformInfo}} object provides the version number of the running platform.Especially with SNAPSHOT versions however it is also useful to know which is the latest GIT commit included.,6130
Extract Method,Allow to update user data during approval Even though supported by the Core it is not currently possible for administrators once a given user is under approval to further edit such user before approval.This feature naturally requires that the underlying workflow definition allows to update users under approval.An integration [test case|https://github.com/apache/syncope/blob/2_0_X/fit/core-reference/src/test/java/org/apache/syncope/fit/core/UserWorkflowITCase.java#L214] implements this scenario.,6131
Extract Method,Password change on an external resource only Give option to change password only on one or more connected external resources; currently it's only possible to change password on Syncope core and then propagate accordingly to the connected external resources.,6133
Extract Method,"Support Groovy implementations in the Netbeans IDE plugin SYNCOPE-956 introduced the possibility to provide Groovy-based implementations. The Netbeans IDE plugin should be extended to support this. This issue basically starts from the work already done for SYNCOPE-808. This means that there is an already existing netbeans plugin for Syncope. For more info please see  [this|https://syncope.apache.org/docs/reference-guide.html#netbeans-ide-plugin]  and  [https://syncope.apache.org/docs/getting-started.html#netbeans-ide-plugin] Please install this plugin and let it work with your local Syncope installation in order to understand how it works and interacts with Syncope. The goal is to improve the current netbeans plugin to create and edit (also format) Groovy scripts used to extend default Syncope behavior. Refer to SYNCOPE-956 to have an idea of what I'm talking about. Basically this means that some default or ""open to implementation"" functionalities of Syncope Idm can be improved or developed by pluggin in groovy scripts. This mechanism allows to develop custom behaviors for Syncope based on specific project requirements. [Here|http://syncope.apache.org/docs/reference-guide.html#customization-core] is the list of customizable features. Since [Netbeans 9|https://cwiki.apache.org/confluence/display/NETBEANS/Apache+NetBeans+9.0+Beta] is near to be released develop this plugin referring to Netbeans 9 (currently beta).",6134
Extract Method,Search funcionality in Schemas I think it would be very useful in environments with several issues to be able to search schemas using it's attributes (key typeetc),6135
Extract Method,SyncDelta pre-processing ConnId's [SyncDelta|http://connid.tirasa.net/apidocs/1.4/org/identityconnectors/framework/common/objects/SyncDelta.html] encapsulates the information about retrieved objects during pull. The SyncDelta content can be seen then as the real starting point of the pull process of a certain entry. Currently such instances can be modified by several [PullActions|https://github.com/apache/syncope/blob/2_0_X/core/provisioning-api/src/main/java/org/apache/syncope/core/provisioning/api/pushpull/PullActions.java]' methods; such option was meant to allow flexibility when dealing with wicked use cases. For several scenarios however such kind of modification occurs too much later in the process: we need to extract and separate the SyncDelta pre-processing into a dedicated PullActions' method to be invoked at the very beginning.,6136
Extract Method,Prevent task execution request for running tasks As done for reports in SYNCOPE-102 discard task execution requests while the same task is currently running.,6137
Move Method,Remediation Errors during pull might arise for various reasons: for example values are not provided for all mandatory attributes or values are failing the configured validation. Currently if an entity (User Group or Any Object) is failing during a Pull Task execution it is simply reported as an error and logged as execution result. As a new feature administrators could be given the chance to perform _remediation_ on the failing entities in a similar fashion they do with approval forms.,6140
Rename Method,Use Remote Key during Pull to match internal entities Following SYNCOPE-1182 extend the same approach to internal entities matching during Pull e.g. do not use {{__UID__}} but rather the value for the attribute flagged as {{Remote Key}} in the Mapping.,6141
Rename Method,Resource: ignoreCase match Add a flag to ExternalResource to indicate whether match (during propagation and / or pull) should be performed as case-sensitive (as currently implemented) or not.,6142
Extract Method,Add UserRequestController.execute() that will execute the provided UserRequest and remove it on success This job is currently performed by the console.,6143
Rename Method,Add pagination for approvals forms If there are many approval tasks the console takes a long time to load all the forms. It's necessary to add pagination in the query.  ,6144
Rename Method,Password required for resource subscription Currently cleartext password is always required when subscribing to a new external resource.However in some cases (for example when passwords are stored with some symmetric algorithm) this can be avoided.For example it could be:Case 1: 2-way (a.k.a. symmetric) password cipher algorithm is configured in SyncopeUse decrypted password from SyncopeUser to subscribe new resource.Case 2: 1-way (a.k.a. hash or asymmetric) password cipher algorithm is configured in Syncope and no clear-text password is available (for example passed via UserMod or provided by a synchronizing resource)Provide on a resource-basis a mean to configure how new password should be generated:* constant* random password generation (compliant with resource password policy if present - see SYNCOPE-121)* provide custom Java classDiscussion thread: http://syncope-dev.1063484.n5.nabble.com/new-password-issue-td5589622.html,6145
Extract Method,"User requests With user requests users can initiate whichever request among the ones defined for example ""assign me a mobile phone"" or ""give me those groups on AD"" for them or on behalf of others; once initiated such requests can then follow their own path which might include one or more approval steps. There is also no limitation on the number of concurrent requests that an user can initiate. Unfortunately our current implementation is not able to properly implement the user requests as briefly outlined above; among other things the impossibility to handle more than an approval process at a time per user. Hence and a major refactoring is needed: # remove approvals features from the current Flowable user workflow adapter # define a new UserRequest entity which includes at least ## some triggering conditions ## a Flowable workflow definition possibly containing approval form(s) # adjust REST services Admin Console and Enduser UI to cope with the new User Request entity",6146
Extract Method,Make configurable resource check timeout Waiting for long timeouts during resource check can negatively affect the functionality of the console. Give the possibility to disable or configure check timeouts,6148
Rename Method,Reduce usage of Reflection to improve overall performance The source code - especially for core and extensions - is filled up with Reflection-intensive invocation which are supposed to negatively affect the overall performance; mostly: * ReflectionUtils * Spring's BeanUtils#copyProperties * ReflectionToStringBuilder * EqualsBuilder#reflectionEquals * HashCodeBuilder#reflectionHashCode,6149
Extract Method,Add un-claim capability for requests Add unclaim capability in order to revert claim of an User Request or generic create/update approval coming from user workflow. ,6150
Rename Method,remove user_search_null_attr view Remove user_search_null_attr view because affects performance negatively,6153
Extract Method,"Provide refresh button in task and report modal windows Task and report modal windows would benefit from a ""refresh executions"" button to get update information about things happened while modal windows were open.",6156
Extract Method,Provide a PropagationActions to maintain a conservative membership policy management If a user has a group assigned on LDAP or AD but the group is not managed by Syncope during the modification of the user Syncope removes the assignment. Give the possibility to provide a conservative management and assignment of groups to user on the resource when the group is not managed from Syncope. The groups already assigned to an user on LDAP or AD will not be removed if the group is not managed from Syncope.,6157
Extract Method,Manage creator lastmodifier and approvers information about each SyncopeUser bean Add and populate/manage the following SyncopeUser bean attributes:1. creator (who has created the user)2. lastModifier (who has performed the last modification to the user profile)3. approvers (every approver which approved some operation about the user),6158
Extract Method,Find Anys using FIQL: SQL improvements PR contains improvements when Anys are searched using FIQL queries: The resulted SQL query that finds any_to_keys can have a huge list with OR clauses for effective realms. This can be replaced to the IN clause. {code:sql} SELECT u.any_idsv.username FROM (SELECT DISTINCT any_id FROM user_search WHERE (realm_id=? AND any_id IN ( SELECT DISTINCT any_id FROM user_search WHERE lastChangeDate<=?))) uuser_search sv WHERE u.any_id=sv.any_id AND u.any_id IN (SELECT any_id FROM user_search WHERE realm_id IN (SELECT id AS realm_id FROM Realm WHERE id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=?)) ORDER BY sv.username ASC {code} When anys are searched by keys a lot of single sql queries are executed. This can be improved using single SQL query with In clause.,6159
Extract Method,Support SCIM REST API SCIM (System for Cross-domain Identity Management) is the open API for managing identities.The specification is now complete and published under IETF.An overview and detailed specifications can be found at the official website: http://www.simplecloud.info/The Syncope Core already provides a [full-fledged RESTful interface|https://syncope.apache.org/docs/reference-guide.html#rest] normally available under {{/rest}}; the idea here is to add another RESTful interface available under {{/scim}} compliant to the SCIM v2 specifications as referred above.The new REST interface can be developed as an [extension|https://syncope.apache.org/docs/reference-guide.html#extensions] whose features will be:# expose a fully compliant SCIM v2 RESTful interface# translate the incoming / outgoing payloads from / to SCIM formats to / from Syncope standard formats# invoke the underlying [Logic layer|https://syncope.apache.org/docs/reference-guide.html#logic] for actual operation implementationAn additional feature will be needed for mapping the standard Syncope [Schema|https://syncope.apache.org/docs/reference-guide.html#schema] to SCIM attributes.,6160
Extract Method,Virtual attribute cache Provide a simple cache for virtual attribute values in order to avoid to query external resources every time.,6162
Extract Method,Better way to override console pages Better way to override console pages (for WAR overlays).,6163
Extract Method,New admin UI Refactor and rewrite current console into a new cleaner admin UI.It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html[2] http://markmail.org/message/wtamknssq42pyjjc,6164
Rename Method,CLI admin tool It should not be hard to make a simple command-line tool to get and set SyncopeConf items (and more).,6173
Move Method,Improve virtual attribute value retrieving At the moment when virtual attribute value retrieving is needed then a search per virtual attribute is performed.This behaviour must be changed: it could be optimized performing a single search for all the virtual attributes mapped on the resource.,6176
Rename Method,"Check for mandatory condition on Virtual / Derived attributes ""Mandatory condition"" is a JEXL expression evaluating to boolean that can be specified during schema mapping [1].When 'enforce mandatory condition"" is checked as well Syncope will consider any attribute part of the schema mapping (with mandatory condition evaluated to true) as mandatory even though the corresponding attribute schema is not defined as mandatory.Currently this feature is working for plain attribute schemas not derived or virtual.[1] https://cwiki.apache.org/confluence/display/SYNCOPE/Schema+attributes+and+mapping#Schema%2Cattributesandmapping-SchemaMapping",6177
Extract Method,Role owner Add the notion of (optional and inheritable) role owner a SyncopeUser or a SyncopeRole with rights to manage a role.Role owner will be automatically entitled to make modifications on the owned role and on all descendants with inheritOwner = true,6178
Extract Method,Using Standard JAX-RS API in Syncope (Introducing Apache CXF WS Stack) Current REST Interfaces are based on Spring Webservice framework. Goal of this task is to replace Spring with CXF and to relay on JAX-B and JAX-RS annotations rather then Spring annotations.,6179
Extract Method,Improve UserTestITCase RoleTestITCase and TaskTestITCase The test org.apache.syncope.core.rest.UserTestITCase has some highly repetitive code where AttributeTOs and AttributeMods are created. I created two small helper methods and was able to make the test about 150 lines smaller.,6185
Extract Method,Consolidate task execution in TaskTestITCase The TaskTestITCase has some common code to execute tasks and wait till the task finishes.We should consolidate this code into a helper method.,6186
Move Method,Implement RoleOwnerSchema for role propagation and synchronization SYNCOPE-225 introduced the concept of role owner than could be either a user or another role (not both at the same time).Test content provides an example of how role owner can be propagated by empowering a derived attribute (ownerDN): this approach is working only for propagation and makes the AccountLink expression duplicated.A more complete approach is to define a new type of internal mapping RoleOwnerSchema.During role propagation (in MappingUtil.getIntValues()):* if userOwner != null and the propagating resource has UMapping defined* if roleOwner != null (the propagating resource has RMapping because of the ongoing propagation)the AccountLink (or AccountId if no AccountLink is defined) is generated and given as value for the external attribute mapped to RoleOwnerSchemaDuring role synchronization (in ConnObjectUtil.getAttributableTOFromConnObject()) if a value is present in the ConnectorObject for the role being synchronized this value must be used for searching the same connector for either ObjectClass.ACCOUNT and ObjectClass.GROUP; if a unique match is found the matching ConnectorObject can be used to find the corresponding Syncope entity (user or role); now userOwner or roleOwner of the role being synchronized can be set.Especially in case of roleOwner precedence issues must be taken into account: it might happen in fact that the owned role is being synchronized before the owner role synchronization takes place.,6187
Extract Method,"Java class as sync policy correlation rule Give the possibility to specify a java class for the sync policy to implement a custom correlation rule.If specified this custom rule should be evaluated in place of ""correlation attributes"" rule.",6188
Inline Method,Create transitional Service interfaces and switch tests and console to use them As preparation of the change to use CXF instead of Spring MVC REST controllers this issue is to introduce transitional service interfaces (like as UserService).The UserService interface should later be used in the core to provide the UserController and on the console to access the service remotely.To make the transition easier the idea is to already introduce the interface upfront and change all tests and the console to use it. Before the switch the implementation of the interface will simply use the restTemplate under the covers.This to be applied similarly to all Spring MVC REST controllers.,6189
Extract Method,"Enable Rest IntegrationTests to run more than once (per build) Currently many Rest IntegrationTests can run only once. If you try to rerun some Tests they will fail due to the fact that a resource with the same same already exists or that a resource was deleted previously and is not available any longer. This works fine for ""mvn clean verify"" since all test run exactly once. But for development phase this is inconvenient because while testing new features or other refactorings one would like to run tests several times (especially if they fail) without the need to rebuild/package/deploy the whole core module.Tasks of this JIRA ticket is to use random identifier for resources (user role ...) to avoid collisions when running tests multiple times. In some cases it will also be preferable to use a try { } final { } statement to cleanup previously created resources.",6193
Inline Method,Connector instance timeout Provide execution timeout for ConnectorFacadeProxy methods.Timeout must be specified:1. for 1_0_X by using a global configuration parameter (the same timeout for each connector instance)2. for 1_1_X by using a connector instance configuration parameter (different timeout for different instance),6195
Rename Method,Show information (version license ...) Currently there is only a static label with core and console versions.Add a new Link to open a ModalPage with version and other project information (eg. link to license),6199
Rename Method,Provide access to user / role data on external resources From StatusPanel show read-only data read via ResourceController#getObject for each associated resource.,6202
Rename Method,"Disable mapping tab when the underlying connector does not support correspondent ObjectClass If a given connector supports ObjectClass.ACCOUNT enable the ""User mapping"" tab otherwise keep it disabled.If a given connector supports ObjectClass.GROUP enable the ""Role mapping"" tab otherwise keep it disabled.",6203
Extract Method,Make password management optional Currently SyncopeUser#password is annotated as @NotNull - this has several consequences to propagation / synchronization and even to admin console.However it would be a nice addition to make the password storage and management optional - in complex IdM scenarios in fact it might even be a business requirement to NOT store passwords in Syncope internal storage.,6204
Rename Method,Full reconciliation from syncope to resource Implement a full reconciliation from syncope towards a specific resource.Unmatching (user found on syncope but not on the resource):* ignore;* unlink the resource (keep user on syncope and remove resource link)* create (create user on resource - if create capability is given)* delete (remove user on syncope) Matching (users found on syncope and on the resource):* ignore* update* unlink (perform deprovisioning - if delete capability is given)* delete (delete on syncope and perform deprovisioning - if delete capability is given),6205
Move Method,Inconsistent status of user edit form after exception returned by bad propagation on primary resource During creation (and save) of user to be propagated on a primary resource if propagation fails (due to an error) it returns a propagation exception to console and to user edit form which goes in an inconsistent status and stucks.A possible solution is to go directly in case of propagation exception to summary page reporting propagation status on resource(s); in particular propagation signaling icon of failure may be abled to show exception message caught from PropagationStatusTO.,6206
Rename Method,Add claim for user requests and trace user request history into SyncopeUser bean Add claim for user requests and trace user request history into SyncopeUser bean,6207
Extract Method,Add the ability to delete a user by username via the REST API It is currently not possible to delete a user by username via the REST API only via the user Id. This task is to add support for this.,6208
Extract Method,Improve audit info 1. provide info in case of operation failures as well (currently missing)2. increase audit info message length bounds3. increase audit info in case of create/delete/update user/role/membership/resource,6209
Rename Method,Provide resource link associate and provision independent features Complete AbstractResourceAssociator [1] created by SYNCOPE-393 with 'positive' methods e.g. * link()* associate()* provision()[1] https://svn.apache.org/repos/asf/syncope/trunk/core/src/main/java/org/apache/syncope/core/rest/controller/AbstractResourceAssociator.java,6211
Rename Method,Replace role action labels with icons As available throughout the admin console replace the links for accessing role management features ('Add child' 'Edit' 'Drop') with icons.,6213
Move Method,Add ability to search for Roles via the REST API It is currently not possible to search for roles via the REST API. You can however search for a list of users that are members of a particular role. But you may want the ability to search for a role by attributes etc.Don't forget to refactor console's search panel accordingly.,6215
Extract Method,"Default datasource uses BasicDataSource Replace org.springframework.jdbc.datasource.DriverManagerDataSource by org.apache.commons.dbcp.BasicDataSource as recommended by spring:http://docs.spring.io/spring/docs/current/spring-framework-reference/html/jdbc.html#jdbc-connectionsMake property ""isolationLevelName"" configurable in persistence.properties with default ISOLATION_READ_COMMITTEDSee full discussion in dev@ ML: http://markmail.org/message/33aheawb5irzl3xc",6217
Rename Method,Add information to what components refer to a certain policy Add a third tab to policy modal windows showing which other components are referring to the policy being edited.,6218
Extract Method,make Velocity tools available in templates for notifications As discussed on the dev@ mailing list [1] it could be useful to make the Velocity tools available in templates for notifications e.g. to be able to URL-encode user names inside generated links.[1] http://syncope-dev.1063484.n5.nabble.com/Escaping-in-Velocity-templates-for-notifications-td5714981.html,6219
Extract Method,Support propagating non-cleartext passwords to external resources Similarly to SYNCOPE-313 during synchronization it seems feasible to provide some Propagation Actions classes (say {{DBPasswordPropagationActions}} and {{LDAPPasswordPropagationActions}} that will propagate non-cleartext password values to external resources.This might require some changes in the related connector bundles.,6221
Extract Method,Report default values for connector properties Currently default values for connector properties are not returned by REST methods nor shown in the admin console.Such information however is available from the ConnId framework and needs only to be picked.,6223
Extract Method,Externalize all WAR configuration The main webapps (core and console) require several configuration files (.properties) to work properly.Currently such configuration files need to reside into webapps' classpath.Ideally it should be possible to define a configuration directory defined via a Maven property where all such files can be loaded from: this would allow to implement some handful deployment scenarios (e.g. deploy the same .war files either in test and production environments).The same idea is to apply to e-mail templates.,6224
Rename Method,Code re-organization Heading to 2.0.0 a proposal is [being discussed|http://markmail.org/message/xfxn2xc6iolcmysp] for code reorganization in order to obtain more modularity with our growing codebase.,6228
Rename Method,Provisioning manager integration Last year with the help of Syncope team I proposed an early reformulation of provisioning phase. Provisioning refers to methods used to provide users and roles functionality: the main goal of my proposal was to re-organize this phase allowing custom provisioning behaviours through a definition of a provisioning manager (see here [1] more details).The current strategy adopted in Syncope makes difficult the definition of custom provisioning behaviour. The solution enclosed with this issue aim to decompose the provisioning phase. Previously the user/role controller deals directly with provisioning while now with this proposal both the controller delegates this task to provisioning manager. A first task was to move all provisioning functionality into the provisioning manager. If you now inspect the user/role controller code you will not find anymore workflow and propagation dependencies.During the development we thought to make the provisioning pluggable in order to allow the choice of provisioning engine. The default provisioning engine can be choosed editing the provisioning.properties file. For this reason we hardcoded previous strategies as default provisioning manager in order to keep the standard Syncope behaviour. As we proposed initially we wanted also to experiment a provisionig manager based on a Apache Camel. Camel is a powerful integration framework implementing enterprise integration pattern. Our current solution embeds the provisioning logics into camel routes.Moreover the current solution extends also Syncope Console: we added new a new functionality - related to Camel case - that allows to read and edit routes definitions. You can find this new service under the Console->Configuration->Routes section. In this case routes are expressed through Spring DSL. To finish I thought to create a github pull request [2]in order to give to Syncope Team members to examine this work and possibly integrate it.[1] http://syncope-dev.1063484.n5.nabble.com/Proposal-An-Apache-Camel-Integratation-Proposal-td5714531.html[2] https://github.com/apache/syncope/pull/2,6230
Extract Method,Camel provisioning manager: separate user / role route management and introduce Unit Test The current Camel provisioning manager does not distinguish between user and role operations for REST calls. Only the console module (partially) supports this difference. Maybe it's better to have fine-grained methods.,6231
Extract Method,Notification Configuration: missing some labels in events Sync process include many operations for example “create””assign””link””unlink” etc… but into notification configuration we find only “create””update””delete””none”.How to intercept a failure/success during “assing” or during an operation different from [“create””update””delete””none”]?,6233
Extract Method,"Domains The purpose of this new feature is to provide the possibility of defining separated ""containers"" for all entities currently managed by Syncope in order to allow the execution in multitenant environments.Discussion on wiki: https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Domains",6234
Move Method,Extend control over asynchronous job execution Asynchronous job execution generally delegated to Quartz can currently be checked only at completion and only by indirectly checking the status on related JPA entities (TaskExec ReportExec ...).A REST endpoint should be provided for reporting on jobs execution and providing the ability to control (e.g. stop / pause / resume / ...) .,6236
Rename Method,Option to ignore users / roles during synchronization or push Currently all users or roles provided by an external resource during synchronization - or sent to an external resource during push - are considered for synchronization or push (with matching / unmatching rules).It is a powerful addition to provide a mean to safely ignore user or roles - a proper place for implementing such custom logic is {{SyncActions}} / {{PushActions}}.,6237
Rename Method,"Any objects Introduce the concept of Any object e.g. to extend the provisioning engine to support general-purpose definable entities besides current users and groups (see this page about realms to understand why former roles were renamed to groups).With this feature onboard Syncope will be suitable for managing printers services or any other ""thing"" (as in Internet-Of-Things).Details at https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Any+objects",6238
Rename Method,Deferred tasks The current {{SchedTask}} / {{SyncTask}} / {{PushTask}} are immediately set for execution - with or without cron expression.This aspect can be enhanced by adding an instant in the future when the given task will be scheduled from.,6240
Extract Method,Use ConnId 1.4 pagination API ConnId 1.4 introduces standard pagination and sorting API for querying connectors.While at the moment only the LDAP connector is [featuring|https://connid.atlassian.net/browse/LDAP-16] this the Active Directory connector is [expected|https://connid.atlassian.net/browse/AD-47] to align soon.,6244
Extract Method,Role/Membership attributes propagation Propagation of role/membership attributes must be possible during user provisioning.Mapped Role/Membership attributes will be ignored during synchronization.,6245
Extract Method,Swagger extension Recently CXF has added [native support|https://cxf.apache.org/docs/swagger2feature.html] for Swagger.This means Syncope can generate Swagger description for its JAX-RS services and also provide an extension for interacting with it based on [Swagger UI|http://swagger.io/].,6246
Extract Method,"Conform the Logger ""service stack"" to others I notice that LoggerServiceImpl#read method performs some code for LoggerLogic.Create a new method called read also in LoggerLogic to put the actual LoggerServiceImpl#read code.",6247
Rename Method,Add the possibility to override the capabilities of the connector Currently in the resource configuration you can override only the connector instance properties. It would be useful to extend this behaviour to the connector capabilities.,6248
Rename Method,Filtered reconciliation for synchronization As [discussed in mailing list|http://markmail.org/message/ck64v6jy4nseau2s] a nice improvement for synchronization would be to allow definition of filtered reconciliation allowing to synchronize only certain entities from an external resource.At the moment synchronization is allowed as:# full reconciliation (relying on ConnId's {{search()}} without any filter)# active synchronization (relying on ConnId's {{sync()}})By leveraging ConnId's {{search()}} with appropriate filter filtered reconciliation can be obtained.,6249
Move Method,Exchange JSON by default REST endpoints are currently configured for dealing with both XML and JSON formats and XML is set as default when {{Accept}} and / or {{Content-Type}} HTTP headers are not set.Changing the default format to JSON appear wiser nowadays.,6250
Extract Method,Selectively delete task and report executions Introduce the ability to delete executions of a given task / report with date parameters (before after ...).,6251
Rename Method,Statistics Simple and inexpensive numbers (memory usage users count resources count ...) can be provided by default.Additional metrics when enabled can be collected on purpose.The idea is to let the admin console display such numbers in the dashboard as graphs.Usage of JMX may be worth.,6252
Rename Method,Allow dynamic reloading of report stylesheets As with SYNCOPE-760 for mail templates manage report's and reportlet's XSLT / XSL-FO content as entities do not use the filesystem.,6253
Extract Method,Allow admins to force users' password change at next login The admin console lacks the capability to force user(s) to change their password at next login.Such operation should be accessible either for single user and bulk operation.,6254
Move Method,Use Kendo UI Boostrap DateTimePicker Replace actual Bootstrap DateTimePicker component (Wicket 7.2.0) that doesn't support all java date formats (see SYNCOPE-730) with Kendo UI DateTimePicker (http://demos.telerik.com/kendo-ui/datetimepicker/angular),6255
Move Method,Show the propagation task(s) linked to a given user / group / any object Besides the ability to see all propagation tasks for a given resource it is helpful to provide a direct link to the propagation tasks executed for each user group and any object.,6256
Move Method,"Add deletion query across all components If you try to delete a user in the Syncope console a prompt pops up asking you if you ""Really want to delete the selected item(s)"". However you can delete resources/connectors/schema types without any such prompt popping up. For consistency the message should always appear when deleting something in the UI.",6257
Rename Method,Replace Long autogenerated keys with UUIDs As [discussed in mailing list|http://markmail.org/message/fhdrwerdwdm3opdx] switch the JPA entities currently set for using Long autogenerated keys to UUIDs.,6259
Inline Method,Allow to specify user / group / any object filters for push tasks Allow to specify user / group / any object filters for push tasks.,6260
Rename Method,Associate notification tasks to related notifications Currently the {{NotificationManager}} generates {{NotificationTask}} instances for all defined {{Notification}} instance under the given conditions.Once generated however there is no link any more between {{Notification}} and {{NotificationTask}}: such connection can be useful to trace from task to notification to see when a given {{Notification}} became effective and so on.,6262
Extract Method,Single WebSocketBehavior per page As discussed in [Wicket's users mailing list|http://markmail.org/message/bjtquixggo5klviu] it is needed with current Wicket release 7.2.0 to consolidate the various WebSocketBehavior instances added by several widgets (namely {{ApprovalsWidget}} {{JobWidget}} and {{ReconciliationWidget}}) into a single instance to be added to {{BasePage}} which will selectively embed the logic currently implemented by the various instances.,6263
Rename Method,Use gzip compression by default Since SYNCOPE-705 it is possible to optionally configure the client library to transparently support the GZIP content-encoding.Enable this by default for all client applications (console enduser cli).,6264
Extract Method,Include provision information in VirSchemaTO Currently VirSchemaTO reports the related provision as UUID: It would make more sense to report instead the (External Resource AnyType) pair.On console side editing or Virtual Schema might be also improved with autocompletion for external attribute names.,6265
Rename Method,"Realm provisioning When realms were [introduced|https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Realms] the main purpose was to set a simpler and neat approach to the internal security model.There are though at least some popular ConnId connectors (LDAP Active Directory GoogleApps ...) bearing the concept of ""organizational unit"" as user / group / other container.Extending realms capabilities to match such concept will further improve the Syncope capabilities to handle complex scenarios.",6268
Rename Method,"Identity Recertification Identify Re-certification is required for many national and international standards like SOX GxP etc.The idea is to implement one scheduled task that filter users basing on some attributes (example: last recertification date role...) and move then on one state ""to be certified"" and assign one task to some group that has the responsibility to recertified the user or delete it from the system.Some report should report evidence about when the users have been recertified and who was the certifier.This feature would be also the starting point to create account role and groups re-certifications.",6269
Inline Method,Allow reference to username and group / any object name as search parameters Several search clauses require user group or any object key values (e.g. UUID values) which are difficult to handle for humans.Improve the search feature by transparently allowing insertion of username and group / any object name besides keys.,6271
Rename Method,Introduce new Camel propagation component This task is to introduce a new Camel propagation component instead of having Camel Processor instances handle each individual use-case. It'll make the routes a bit more compact and easier to read.,6274
Rename Method,Use Java 8 language features As Java 8 is now the minimum requirement for 2.1 all the codebase should be scanned for making profit of Java 8 language features - which might eventually lead to remove {{commons-lang3}} / {{commons-collections4}} / ...,6276
Inline Method,Leave WebApplicationException to default processing As discussed in [mailing list|https://lists.apache.org/thread.html/a15ab3f335a9eb27a15482789c351bf3ddb694558961975bc39b0fa5@%3Cdev.syncope.apache.org%3E] there is currently some code in {{RestServiceExceptionMapper.java}} which is never going to be invoked by the CXF runtime.The simplest and safer option at the moment is to just remove that code.,6277
Rename Method,Allow for scripted customizations The core can be customized in [several ways|http://syncope.apache.org/docs/reference-guide.html#customization-core]; all customizations require to be written as Java classes - which is generally good but requires redeploy to be made effective unless some class reloading mechanism is in place (as JRebel).By leveraging Groovy we could overcome such limitation and allow to write customizations which can be immediately available for execution at runtime.Once implemented in the core such feature will require editing capabilities to be added to console and IDE plugins.,6279
Extract Method,Method to check if token has expired I would like to check a token has expired not just that it's invalid (either being wrong or expired)I think it makes sense to add a method to SyncopeUser (hasTokenExpired) for that.,6280
